{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (연구&보람) 데이터정리\n",
        "\n",
        "김보람  \n",
        "2024-04-24\n",
        "\n",
        "# 1. Imports"
      ],
      "id": "1548a957-bdcf-4c4a-ad5e-7c869c351566"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle \n",
        "import itertools\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "b4d41752-1bab-4f48-b98c-e2de3fe495d2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def throw(df, fraud_rate, random_state=42):\n",
        "    # Separate fraud and non-fraud transactions and make copies\n",
        "    df1 = df[df['is_fraud'] == 1].copy()  # Fraud transactions\n",
        "    df0 = df[df['is_fraud'] == 0].copy()  # Non-fraud transactions\n",
        "    \n",
        "    # Downsample non-fraud transactions\n",
        "    # Downsample the non-fraud transactions considering the fraud rate\n",
        "    df0_downsample = (len(df1) * (1 - fraud_rate)) / (len(df0) * fraud_rate)\n",
        "    df0_down = df0.sample(frac=df0_downsample, random_state=random_state)\n",
        "    \n",
        "    # Concatenate fraud transactions and downsampled non-fraud transactions\n",
        "    df_p = pd.concat([df1, df0_down])\n",
        "    \n",
        "    # Return the concatenated DataFrame\n",
        "    return df_p\n",
        "    \n",
        "def split_dataframe(data_frame, test_fraud_rate, test_rate=0.3, random_state=42):\n",
        "    # Split the data into 70:30 ratio with test_fraud_rate as input\n",
        "    n = len(data_frame)\n",
        "\n",
        "    # Separate fraud and non-fraud transactions\n",
        "    fraud_data = data_frame[data_frame['is_fraud'] == 1]\n",
        "    normal_data = data_frame[data_frame['is_fraud'] == 0]\n",
        "\n",
        "    # Calculate the size of the test data\n",
        "    test_samples = int(test_fraud_rate * (n * test_rate))\n",
        "    remaining_test_samples = int(n * test_rate) - test_samples\n",
        "\n",
        "    # Randomly sample test data from fraud and non-fraud transactions\n",
        "    test_fraud_data = fraud_data.sample(n=test_samples, replace=False, random_state=random_state)\n",
        "    test_normal_data = normal_data.sample(n=remaining_test_samples, replace=False, random_state=random_state)\n",
        "\n",
        "    # Concatenate test data\n",
        "    test_data = pd.concat([test_normal_data, test_fraud_data])\n",
        "\n",
        "    # Create training data\n",
        "    train_data = data_frame[~data_frame.index.isin(test_data.index)]\n",
        "\n",
        "    return train_data, test_data"
      ],
      "id": "00669038-e135-4610-92ce-a419c2cf51ae"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GCN에서 mask를 위한 함수 \n",
        "def _concat(df_tr, df_tst):   \n",
        "    df = pd.concat([df_tr, df_tst])\n",
        "    train_mask = np.concatenate((np.full(len(df_tr), True), np.full(len(df_tst), False)))    # index꼬이는거 방지하기 위해서? ★ (이거,, 훔,,?(\n",
        "    test_mask =  np.concatenate((np.full(len(df_tr), False), np.full(len(df_tst), True))) \n",
        "    mask = (train_mask, test_mask)\n",
        "    return df, mask\n",
        "\n",
        "# 거래시간차이 계산 \n",
        "def _compute_time_difference(sub_df):\n",
        "    t = sub_df.unix_time.to_numpy() \n",
        "    t_diff = abs(t.reshape(-1,1) - t.reshape(1,-1)).reshape(-1,1)\n",
        "    idx = np.array(list(itertools.product(sub_df.index,sub_df.index)))\n",
        "    result = np.concatenate([idx,t_diff],axis=1)\n",
        "    return result\n",
        "\n",
        "def _parallel_apply(func, data_frames, num_processes=None):\n",
        "    \"\"\"\n",
        "    Apply a function to a list of DataFrames in parallel.\n",
        "    \n",
        "    :param func: The function to apply.\n",
        "    :param data_frames: The list of DataFrames to process.\n",
        "    :param num_processes: The number of processes to use.\n",
        "    :return: A list containing the processed results.\n",
        "    \"\"\"\n",
        "    if num_processes is None:\n",
        "        num_processes = multiprocessing.cpu_count()  # Number of available CPU cores\n",
        "\n",
        "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "        results = pool.map(func, data_frames)\n",
        "\n",
        "    return results\n",
        "    \n",
        "def df2geodata(df_train,df_test,theta,gamma):\n",
        "    df2, mask = _concat(df_train, df_test)   \n",
        "    df = df2.reset_index()\n",
        "    groups = df.groupby('cc_num')\n",
        "    data_frames = [sub_df for _,sub_df in groups]  # 여기에 데이터프레임들을 리스트로 \n",
        "    processed_results = _parallel_apply(_compute_time_difference, data_frames)\n",
        "    processed_results = np.concatenate(processed_results).astype(np.float64)\n",
        "    edge_index = processed_results[:,:2]\n",
        "    dist = processed_results[:,2]\n",
        "    weight = (dist != 0) * np.exp(-dist/theta)\n",
        "    edge_index = torch.tensor(edge_index[weight > gamma]).long().t()    \n",
        "    x = torch.tensor(df['amt'].values, dtype=torch.float).reshape(-1,1)\n",
        "    y = torch.tensor(df['is_fraud'].values,dtype=torch.int64)\n",
        "    data = torch_geometric.data.Data(x=x, edge_index = edge_index, y=y, train_mask = mask[0], test_mask= mask[1])\n",
        "    return data"
      ],
      "id": "860810ec-5375-4faa-8945-00ffcbc4a3b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Load Data"
      ],
      "id": "e12de582-8d69-4624-8743-ec4d444209df"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('fraudTrain.pkl', 'rb') as file:\n",
        "    fraudTrain = pickle.load(file)    "
      ],
      "id": "7dd4b71e-0153-4379-89b2-0ce9eb57ebba"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "6006"
            ]
          }
        }
      ],
      "source": [
        "fraudTrain.is_fraud.sum()"
      ],
      "id": "ccf123fc-8a88-4a38-a6d1-89bf8c46fffa"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.005727773406766326"
            ]
          }
        }
      ],
      "source": [
        "FRAUD_RATE = fraudTrain.is_fraud.mean()\n",
        "FRAUD_RATE"
      ],
      "id": "ff9b81a4-3c26-4510-a8e2-b3fc081870f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 데이터의 정리\n",
        "\n",
        "## A. 분리"
      ],
      "id": "2a9d5105-c994-4c38-b0e2-e245877680ac"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train1, df_test = split_dataframe(fraudTrain,test_fraud_rate=FRAUD_RATE)"
      ],
      "id": "8e488c1a-a2bc-425c-82df-68a6a3de1ffd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train2 = throw(df_train1, fraud_rate=0.01) \n",
        "df_train3 = throw(df_train1, fraud_rate=0.05) \n",
        "df_train4 = throw(df_train1, fraud_rate=0.1) \n",
        "df_train5 = throw(df_train1, fraud_rate=0.2) \n",
        "df_train6 = throw(df_train1, fraud_rate=0.3) \n",
        "df_train7 = throw(df_train1, fraud_rate=0.4) \n",
        "df_train8 = throw(df_train1, fraud_rate=0.5) "
      ],
      "id": "2f834cee-fa3c-47e7-8d6a-725d146093fc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<df1>\n",
            "training set := (734003, 22), 0.0057, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df2>\n",
            "training set := (420500, 22), 0.0100, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df3>\n",
            "training set := (84100, 22), 0.0500, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df4>\n",
            "training set := (42050, 22), 0.1000, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df5>\n",
            "training set := (21025, 22), 0.2000, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df6>\n",
            "training set := (14017, 22), 0.3000, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df2>\n",
            "training set := (10512, 22), 0.4000, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n",
            "<df8>\n",
            "training set := (8410, 22), 0.5000, 4205\n",
            "test set := (314572, 22), 0.0057, 1801\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"<df1>\n",
        "training set := {df_train1.shape}, {df_train1.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df2>\n",
        "training set := {df_train2.shape}, {df_train2.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df3>\n",
        "training set := {df_train3.shape}, {df_train3.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df4>\n",
        "training set := {df_train4.shape}, {df_train4.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df5>\n",
        "training set := {df_train5.shape}, {df_train5.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df6>\n",
        "training set := {df_train6.shape}, {df_train6.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df2>\n",
        "training set := {df_train7.shape}, {df_train7.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "<df8>\n",
        "training set := {df_train8.shape}, {df_train8.is_fraud.mean():.4f}, {df_train1.is_fraud.sum()}\n",
        "test set := {df_test.shape}, {df_test.is_fraud.mean():.4f}, {df_test.is_fraud.sum()}\n",
        "\"\"\")"
      ],
      "id": "8d0847b5-9713-4c01-8be1-53c8718d0c61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. DF저장"
      ],
      "id": "40cc10c8-88f2-4242-8c14-9accfb483f1e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train1.to_csv(\"./data/df_train1.csv\",index=False)\n",
        "df_train2.to_csv(\"./data/df_train2.csv\",index=False)\n",
        "df_train3.to_csv(\"./data/df_train3.csv\",index=False)\n",
        "df_train4.to_csv(\"./data/df_train4.csv\",index=False)\n",
        "df_train5.to_csv(\"./data/df_train5.csv\",index=False)\n",
        "df_train6.to_csv(\"./data/df_train6.csv\",index=False)\n",
        "df_train7.to_csv(\"./data/df_train7.csv\",index=False)\n",
        "df_train8.to_csv(\"./data/df_train8.csv\",index=False)\n",
        "df_test.to_csv(\"./data/df_test.csv\",index=False)"
      ],
      "id": "6e680e6e-e51b-4d56-9a46-749fc78f5839"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Make_Graph"
      ],
      "id": "d9758f44-13f3-49af-87b6-d9c509b96116"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "THETA = 1e7\n",
        "GAMMA = 0.95"
      ],
      "id": "efdcebc4-316b-4e91-98a9-35e0a16b2600"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch_geometric_data1 = df2geodata(df_train1,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data2 = df2geodata(df_train2,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data3 = df2geodata(df_train3,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data4 = df2geodata(df_train4,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data5 = df2geodata(df_train5,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data6 = df2geodata(df_train6,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data7 = df2geodata(df_train7,df_test,theta=THETA,gamma=GAMMA)\n",
        "torch_geometric_data8 = df2geodata(df_train8,df_test,theta=THETA,gamma=GAMMA)"
      ],
      "id": "608500bb-3231-41b7-995a-da31b8367c88"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch_geometric_data_list = [\n",
        "    torch_geometric_data1,\n",
        "    torch_geometric_data2,\n",
        "    torch_geometric_data3,\n",
        "    torch_geometric_data4,\n",
        "    torch_geometric_data5,\n",
        "    torch_geometric_data6,\n",
        "    torch_geometric_data7,\n",
        "    torch_geometric_data8\n",
        "]\n",
        "df_train_list = [\n",
        "    df_train1,\n",
        "    df_train2,\n",
        "    df_train3,\n",
        "    df_train4,\n",
        "    df_train5,\n",
        "    df_train6,\n",
        "    df_train7,\n",
        "    df_train8\n",
        "]"
      ],
      "id": "ec66b44a-9ad0-4eef-81ef-cd93ba1a5c4d"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data, df_train in zip(torch_geometric_data_list,df_train_list):\n",
        "    data._train_size = len(df_train)\n",
        "    data._train_frate = df_train.is_fraud.mean()\n",
        "    data._test_size = len(df_test)\n",
        "    data._test_frate = df_test.is_fraud.mean()\n",
        "    data._theta = THETA\n",
        "    data._gamma = GAMMA "
      ],
      "id": "a6f4fde7-ba1f-4563-92e7-3e6d272548d8"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "for (i,data) in enumerate(torch_geometric_data_list):\n",
        "    with open(f'data/torch_geometric_data{i+1}_{THETA:.1e}_{GAMMA}.pkl', 'wb') as f:\n",
        "        pickle.dump(data, f)"
      ],
      "id": "b22a05ae-b999-4f25-889d-92dd330b0bca"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  }
}