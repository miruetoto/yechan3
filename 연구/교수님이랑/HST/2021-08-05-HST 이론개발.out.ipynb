{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (연구&교수님) HST 이론개발\n",
        "\n",
        "신록예찬  \n",
        "2021-08-05\n",
        "\n",
        "### snow distance 의 분해\n",
        "\n",
        "`-` 제안하는 snow-distance의 분해결과를 $\\tau$로 나누면 아래와 같다.\n",
        "\n",
        "$\\frac{1}{\\tau}\\Sigma_{ij}^{(\\tau)}=[f(v_i)-f(v_j)]^2+\\frac{1}{\\tau}\\sum_{\\ell=1}^{\\tau}\\left\\{\\sum_{k=1}^{\\ell}\\xi(v_i,k)-\\sum_{k=1}^{\\ell}\\xi(v_j,k) \\right\\}^2+\\frac{2}{\\tau}(f(v_i)-f(v_j))\\sum_{\\ell=1}^{\\tau}\\left\\{\\sum_{k=1}^{\\ell}\\xi(v_i,k)-\\sum_{k=1}^{\\ell}\\xi(v_j,k) \\right\\}$\n",
        "\n",
        "`-` 따라서 $\\frac{1}{\\tau}\\sum_{ij}^{(\\tau)}$는 아래와 같은 3가지의\n",
        "term으로 구분할 수 있다.\n",
        "\n",
        "-   (유클리드 차이)$^2$\n",
        "-   ($1\\sim \\tau$의 적재량 차이)$^2$의 합 $\\times$ $\\frac{1}{\\tau}$\n",
        "-   ($1\\sim \\tau$의 적재량 차이)의 합 $\\times$ 유클리드 차이 $\\times$\n",
        "    $\\frac{2}{\\tau}$\n",
        "\n",
        "`-` 편의상 유클리드텀, 그래프텀, 크로스텀이라고 부르자.\n",
        "\n",
        "#### 그래프텀\n",
        "\n",
        "$graphterm(i,j):=\\sum_{\\ell=1}^{\\tau}\\Delta(\\ell,i,j)$ where\n",
        "$\\Delta(\\ell,i,j)$는\n",
        "\n",
        "$\\Delta(\\ell,i,j)=\\left(\\sum_{k=1}^{\\ell}\\xi(v_i,k)-\\sum_{k=1}^{\\ell}\\xi(v_j,k)\\right)^2\\approx |f(v_i)-f(v_j)|^2$\n",
        "\n",
        "$=\\left(h(v_i,\\ell)-f(v_i)-h(v_j,\\ell)+f(v_j)\\right)^2$\n",
        "\n",
        "$=\\left(h(v_i,\\ell)-h(v_j,\\ell)\\right)^2+\\left(f(v_i)-f(v_j)\\right)^2+2\\left(f(v_j)-f(v_i)\\right)\\left(h(v_i,\\ell)-h(v_j,\\ell)\\right)$\n",
        "\n",
        "$=\\big(\\sum_{k=1}^{\\ell}\\big[\\xi(v_i,k)-\\xi(v_j,k)\\big]\\big)^2$\n",
        "\n",
        "$=\\big(\\xi(v_i,1)-\\xi(v_j,1)\\big)^2+\\dots+\\big(\\xi(v_i,\\ell)-\\xi(v_j,\\ell)\\big)^2+gd$\n",
        "\n",
        "$=\\big(\\xi(v_i,1)+\\xi(v_j,1)\\big)+\\dots+\\big(\\xi(v_i,\\ell)+\\xi(v_j,\\ell)\\big)+gd$\n",
        "\n",
        "$=\\sum_{k=1}^{\\ell}\\xi(v_i,k)+\\sum_{k=1}^{\\ell}\\xi(v_j,k)+gd$\n",
        "\n",
        "`-` gd 아래와 같이 쓸 수 있다.\n",
        "\n",
        "$gd=2\\big(\\xi(v_i,1)-\\xi(v_j,1)\\big)\\big(\\xi(v_i,2)-\\xi(v_j,2)\\big)+\\dots+2\\big(\\xi(v_i,\\ell-1)-\\xi(v_j,\\ell-1)\\big)\\big(\\xi(v_i,\\ell)-\\xi(v_j,\\ell)\\big)$\n",
        "\n",
        "$:=d(1,2)+d(1,3)+\\dots+d(1,\\ell)+d(2,3)+\\dots+d(2,\\ell)+\\dots+2d(\\ell-1,\\ell)$\n",
        "\n",
        "여기에서 $d(1,2)$의 값을 해석하여 보자.\n",
        "\n",
        "-   $d(1,2)=0$: 시점1에서의 차이=0 OR 시점2에서의 차이=0\n",
        "-   $d(1,2)=2$: 시점1에서 $i$에 눈이 내림 & 시점2에서도 $i$에 눈이 내림\n",
        "\n",
        "`-` regular graph + ergodicity를 이용하면 예제2의 CASE-A와 CASE-B의\n",
        "그래프텀이 같은 극한값을 가짐을 보일 수 있다.\n",
        "\n",
        "#### 크로스텀\n",
        "\n",
        "`-` 크로스텀을 계산하자.\n",
        "\n",
        "$\\frac{2}{\\tau}(f(v_i)-f(v_j))\\sum_{\\ell=1}^{\\tau}\\left\\{\\sum_{k=1}^{\\ell}\\xi(v_i,k)-\\sum_{k=1}^{\\ell}\\xi(v_j,k) \\right\\}$\n",
        "\n",
        "`-` (직관) $\\xi(v_i,k)-\\xi(v_j,k)$가 1 혹은 -1을 가질 확률은 같다.\n",
        "\n",
        "이제 $\\Delta_{ij}(\\ell):=\\sum_{k=1}^{\\ell}\\xi(v_i,k)-\\xi(v_j,k)$ 을\n",
        "정의하자.\n",
        "\n",
        "분명히 $\\Delta_{ij}(\\ell)$는 평균이 0인 확률변수이다. (분산은??)\n",
        "\n",
        "$\\sum_{k=1}^{\\ell}\\Delta_{ij}(k)=\\#\\big(A_i(\\ell)\\big) - \\#\\big(A_j(\\ell)\\big)$,\n",
        "where $A_i(\\ell)=\\{\\}$\n",
        "\n",
        "### Snow distance의 order\n",
        "\n",
        "`–` $\\frac{1}{\\tau b^2}\\Sigma_{ij}^{(\\tau)}=O_p(1)$ 임을 보이자.\n",
        "\n",
        "`-`\n",
        "$\\Sigma_{ij}^{(\\tau)}=\\big[f(v_i,0)-f(v_j,0)\\big]^2+\\big[f(v_i,0)+\\xi(v_i,1)-f(v_j)-\\xi(v_j,1)\\big]^2+\\dots+\\big[f(v_i,0)+\\xi(v_i,1)+\\dots+\\xi(v_i,\\tau)-f(v_j)-\\xi(v_j,1)-\\dots-\\xi(v_j,\\tau)\\big]^2$\n",
        "\n",
        "`-` 편의성 아래와 같이 정의하자.\n",
        "\n",
        "-   $\\Delta_{ij}^{(0)}=\\big[f(v_i,0)-f(v_j,0)\\big]^2$\n",
        "-   $\\dots$\n",
        "-   $\\Delta_{ij}^{(\\tau)}=\\big[f(v_i,0)+\\xi(v_i,1)+\\dots+\\xi(v_i,\\tau)-f(v_j)-\\xi(v_j,1)-\\dots-\\xi(v_j,\\tau)\\big]^2$\n",
        "\n",
        "`-` 그러면\n",
        "$\\Sigma_{ij}^{(\\tau)}=\\sum_{\\ell=0}^{\\tau}\\Delta_{ij}^{(\\ell)}$\n",
        "\n",
        "`–` WLOG, $b=1$ then, $\\xi(v_i,1)=0~ or~ 1$\n",
        "\n",
        "`–` 아래를 살펴보자. -\n",
        "$\\frac{1}{\\tau^2}\\Delta_{ij}^{(0)}=O(\\frac{1}{\\tau^2})$ -\n",
        "$\\frac{1}{\\tau^2}\\Delta_{ij}^{(1)}=\\frac{1}{\\tau^2}o_p(1)+(p_i-p_j)$ -\n",
        "$\\frac{1}{\\tau^2}\\Delta_{ij}^{(2)}=\\frac{2^2}{\\tau^2}o_p(1)+(p_i-p_j)$ -\n",
        "$\\dots$ -\n",
        "$\\frac{1}{\\tau^2}\\Delta_{ij}^{(\\tau-1)}=\\frac{(\\tau-1)^2}{\\tau^2}o_p(1)+(p_i-p_j)$ -\n",
        "$\\frac{1}{\\tau^2}\\Delta_{ij}^{(\\tau)}=\\frac{\\tau^2}{\\tau^2}o_p(1)+(p_i-p_j)$\n",
        "\n",
        "`-` 따라서 \\$\\_{ij}^{()}=(o_p(1)+(p_i-p_j)) \\$\n",
        "\n",
        "`-` $p_i=p_j$라고 하면 $\\frac{1}{\\tau^2}\\sum_{ij}^{(\\tau)}=o_p(\\tau)$\n",
        "\n",
        "#### 단순한예제\n",
        "\n",
        "`-` 2개의 노드가 있고 $i$,$j$에서 반복된다고 하자.\n",
        "\n",
        "`-` $\\Sigma^{(\\tau)}_{ij}=\\Delta_{ij}^{(0)}=0$"
      ],
      "id": "0df55abc-809b-485e-b476-6b822e440ebf"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "version_major": 2,
        "version_minor": 0
      }
    }
  }
}