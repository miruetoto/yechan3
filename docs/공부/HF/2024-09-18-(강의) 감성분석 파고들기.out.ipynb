{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (강의) 감성분석 파고들기\n",
        "\n",
        "신록예찬  \n",
        "2024-09-18\n",
        "\n",
        "# 1. Imports"
      ],
      "id": "dfae6be1-df76-4e2b-b007-4b835151d15e"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. 이전코드\n",
        "\n",
        "`-` Step1~4를 위한 준비"
      ],
      "id": "c6851e79-1b6e-4fc2-8782-bc746fa13738"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Step1 \n",
        "# 데이터불러오기 = datasets.load_dataset\n",
        "# 데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
        "# def 데이터전처리하기2(examples):\n",
        "#     return 데이터전처리하기1(examples[\"text\"], truncation=True)\n",
        "# ## Step2 \n",
        "# 인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n",
        "# ## Step3 \n",
        "# 데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\n",
        "# def 평가하기(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=1)\n",
        "#     accuracy = evaluate.load(\"accuracy\")\n",
        "#     return accuracy.compute(predictions=predictions, references=labels)\n",
        "# 트레이너세부지침생성기 = transformers.TrainingArguments\n",
        "# 트레이너생성기 = transformers.Trainer\n",
        "# ## Step4 \n",
        "# 강인공지능생성하기 = transformers.pipeline"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` Step 1~4"
      ],
      "id": "bc15ac51-0c29-4692-8d44-1c3b85168e1e"
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Step1 \n",
        "# 데이터 = 데이터불러오기('imdb')\n",
        "# 전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n",
        "# 전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']\n",
        "# ## Step2 \n",
        "# 인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
        "# ## Step3 \n",
        "# 트레이너세부지침 = 트레이너세부지침생성기(\n",
        "#     output_dir=\"my_awesome_model\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=16,\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n",
        "#     weight_decay=0.01,\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     load_best_model_at_end=True,\n",
        "#     push_to_hub=False,\n",
        "# )\n",
        "# 트레이너 = 트레이너생성기(\n",
        "#     model=인공지능,\n",
        "#     args=트레이너세부지침,\n",
        "#     train_dataset=전처리된훈련자료,\n",
        "#     eval_dataset=전처리된검증자료,\n",
        "#     tokenizer=토크나이저,\n",
        "#     data_collator=데이터콜렉터,\n",
        "#     compute_metrics=평가하기,\n",
        "# )\n",
        "# 트레이너.train()\n",
        "# ## Step4 \n",
        "# 강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\n",
        "# print(강인공지능(\"This movie was a huge disappointment.\"))\n",
        "# print(강인공지능(\"This was a masterpiece.\"))"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. DataSet\n",
        "\n",
        "`-` 가장 중요한 원초적 질문: 데이터 셋을 바꿔치기 하려면?\n",
        "\n",
        "`-` 내가 원하는 데이터를 아래와 같은 형태로 정리가능해야함."
      ],
      "id": "989eec25-fc83-4aa4-87e9-8e2a939d2124"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          }
        }
      ],
      "source": [
        "데이터 = datasets.load_dataset('imdb')\n",
        "데이터"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "     train: Dataset({\n",
              "         features: ['text', 'label'],\n",
              "         num_rows: 25000\n",
              "     })\n",
              "     test: Dataset({\n",
              "         features: ['text', 'label'],\n",
              "         num_rows: 25000\n",
              "     })\n",
              "     unsupervised: Dataset({\n",
              "         features: ['text', 'label'],\n",
              "         num_rows: 50000\n",
              "     })\n",
              " }),\n",
              " datasets.dataset_dict.DatasetDict)"
            ]
          }
        }
      ],
      "source": [
        "데이터, type(데이터)"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text', 'label'],\n",
              "     num_rows: 25000\n",
              " }),\n",
              " datasets.arrow_dataset.Dataset)"
            ]
          }
        }
      ],
      "source": [
        "데이터['train'], type(데이터['train'])"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` `datasets.arrow_dataset.Dataset` 의 인스턴스 만들기\n",
        "\n",
        "아래와 같은 함수가 있음"
      ],
      "id": "94162768-d472-49ca-91e1-e6ebf4c8d172"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# datasets.Dataset.from_dict?\n",
        "# 클래스메소드"
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 함수를 쓰는 방법?\n",
        "\n",
        "### $\\star$ `datasets.Dataset.from_dict` 사용법\n",
        "\n",
        "`datasets.Dataset.from_dict`는 Python의 딕셔너리(`dict`)를 `Dataset`\n",
        "객체로 변환하는 함수입니다. 주로 딕셔너리 형태의 데이터를 빠르게\n",
        "데이터셋으로 변환할 때 사용됩니다.\n",
        "\n",
        "**간단한 사용법**\n",
        "\n",
        "``` python\n",
        "from datasets import Dataset\n",
        "\n",
        "# 딕셔너리를 Dataset으로 변환\n",
        "data_dict = {\n",
        "    'text': [\"Hello world\", \"How are you?\", \"Fine, thanks!\"],\n",
        "    'label': [0, 1, 1]\n",
        "}\n",
        "\n",
        "# Dataset 생성\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# 출력\n",
        "print(dataset)\n",
        "```\n",
        "\n",
        "**주요 매개변수:** - `mapping`: 필수, 문자열을 키로 하고 리스트 또는\n",
        "배열을 값으로 하는 딕셔너리. - `features`: 선택, 데이터셋의 각 필드\n",
        "타입을 정의. - `info`: 선택, 데이터셋에 대한 추가 정보(설명, 인용 등). -\n",
        "`split`: 선택, 데이터셋의 나누기(‘train’, ‘test’ 등).\n",
        "\n",
        "**반환값:** - `Dataset`: PyArrow 기반의 데이터셋 객체."
      ],
      "id": "3bab7ef9-b717-429b-9c18-f3987649da1e"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dict = {\n",
        "    'text': [\n",
        "        \"I prefer making decisions based on logic and objective facts.\",\n",
        "        \"I always consider how others might feel when making a decision.\",\n",
        "        \"Data and analysis drive most of my decisions.\",\n",
        "        \"I rely on my empathy and personal values to guide my choices.\"\n",
        "    ],\n",
        "    'label': [0, 1, 0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
        "}\n",
        "\n",
        "test_dict = {\n",
        "    'text': [\n",
        "        \"I find it important to weigh all the pros and cons logically.\",\n",
        "        \"When making decisions, I prioritize harmony and people's emotions.\"\n",
        "    ],\n",
        "    'label': [0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
        "}\n",
        "train = datasets.Dataset.from_dict(train_dict)\n",
        "test = datasets.Dataset.from_dict(test_dict)"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 4\n",
              "})"
            ]
          }
        }
      ],
      "source": [
        "train"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          }
        }
      ],
      "source": [
        "test"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` `datasets.dataset_dict.DatasetDict`의 인스턴스 만들기"
      ],
      "id": "0ff07858-1775-40dd-842e-8158271c3437"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "_데이터 = datasets.DatasetDict({'train':train, 'test':test})"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 4\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2\n",
              "    })\n",
              "})"
            ]
          }
        }
      ],
      "source": [
        "_데이터"
      ],
      "id": "cell-24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 일단 아래와 같은 형태로 분석할 데이터를 정리할 수만 있다면, 나머지는\n",
        "코드를 복붙해서 일단 돌릴 수 있음.\n",
        "\n",
        "``` python\n",
        "train_dict = {\n",
        "    'text': [\n",
        "        \"I prefer making decisions based on logic and objective facts.\",\n",
        "        \"I always consider how others might feel when making a decision.\",\n",
        "        \"Data and analysis drive most of my decisions.\",\n",
        "        \"I rely on my empathy and personal values to guide my choices.\"\n",
        "    ],\n",
        "    'label': [0, 1, 0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
        "}\n",
        "\n",
        "test_dict = {\n",
        "    'text': [\n",
        "        \"I find it important to weigh all the pros and cons logically.\",\n",
        "        \"When making decisions, I prioritize harmony and people's emotions.\"\n",
        "    ],\n",
        "    'label': [0, 1]  # 0은 T(사고형), 1은 F(감정형)\n",
        "}\n",
        "```\n",
        "\n",
        "# 4. 토크나이저\n",
        "\n",
        "`-` 기본적인 사용방법"
      ],
      "id": "7e6d2874-1499-4515-a2b3-3bd7b767497a"
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn("
          ]
        }
      ],
      "source": [
        "토크나이저 = 데이터전처리하기1 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") "
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "#토크나이저?"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **$\\star$ `토크나이저` 사용법 (ref: ChatGPT)**\n",
        ">\n",
        "> **주요 파라미터**:\n",
        ">\n",
        "> 1.  **text**:\n",
        ">     -   `Union[str, List[str], List[List[str]]]`\n",
        ">     -   주어진 텍스트를 토큰화합니다. 이 텍스트는 문자열일 수도 있고,\n",
        ">         문자열의 리스트 또는 리스트 안의 리스트일 수도 있습니다.\n",
        "> 2.  **text_pair**:\n",
        ">     -   `Union[str, List[str], List[List[str]], NoneType]`\n",
        ">     -   두 개의 텍스트를 함께 모델에 입력할 때 사용됩니다. 예를 들어,\n",
        ">         질문-답변 쌍 같은 경우 이 두 번째 텍스트를 넣습니다.\n",
        "> 3.  **text_target**:\n",
        ">     -   `Union[str, List[str], List[List[str]]]`\n",
        ">     -   토큰화를 할 때 목표(target) 텍스트에 해당하는 부분입니다. 주로\n",
        ">         시퀀스 생성 모델에서 활용됩니다.\n",
        "> 4.  **text_pair_target**:\n",
        ">     -   `Union[str, List[str], List[List[str]], NoneType]`\n",
        ">     -   위의 `text_pair`와 유사하게 목표(target) 텍스트의 두 번째\n",
        ">         텍스트를 나타냅니다.\n",
        "> 5.  **add_special_tokens**:\n",
        ">     -   `bool`\n",
        ">     -   문장의 시작, 끝, 구분자 같은 특별한 토큰을 추가할지 여부를\n",
        ">         결정합니다. 기본값은 `True`입니다.\n",
        "> 6.  **padding**:\n",
        ">     -   `Union[bool, str, transformers.utils.generic.PaddingStrategy]`\n",
        ">     -   문장 길이가 다를 때 패딩을 넣어 문장의 길이를 동일하게\n",
        ">         맞춥니다. 패딩 전략에는 `True`, `False`, `'longest'`,\n",
        ">         `'max_length'` 등이 있습니다.\n",
        "> 7.  **truncation**:\n",
        ">     -   `Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]`\n",
        ">     -   문장이 너무 길 경우 지정된 최대 길이에 맞춰 잘라내는\n",
        ">         옵션입니다. 전략에는 `True`, `False`, `'longest_first'`,\n",
        ">         `'only_first'`, `'only_second'` 등이 있습니다.\n",
        "> 8.  **max_length**:\n",
        ">     -   `Optional[int]`\n",
        ">     -   문장의 최대 길이를 설정합니다. `None`일 경우 기본 설정을\n",
        ">         따릅니다.\n",
        "> 9.  **stride**:\n",
        ">     -   `int`\n",
        ">     -   텍스트를 자를 때 중첩을 만들기 위한 옵션입니다. 즉, 자른\n",
        ">         부분과 다음 부분 사이의 겹치는 범위를 설정합니다.\n",
        "> 10. **is_split_into_words**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트가 이미 단어 단위로 분리되어 있는지 여부를 나타냅니다.\n",
        ">         기본적으로는 `False`로, 텍스트가 단어 단위로 분리되지 않았다고\n",
        ">         가정합니다.\n",
        "> 11. **return_tensors**:\n",
        ">     -   `Union[str, transformers.utils.generic.TensorType, NoneType]`\n",
        ">     -   출력 형식으로 텐서를 반환할지 여부를 설정합니다.\n",
        ">         `'pt'`(PyTorch), `'tf'`(TensorFlow), `'np'`(NumPy) 등을 지정할\n",
        ">         수 있습니다.\n",
        "> 12. **return_token_type_ids**:\n",
        ">     -   `Optional[bool]`\n",
        ">     -   토큰 타입 ID를 반환할지 여부를 설정합니다. 주로 두 개의 문장을\n",
        ">         함께 처리할 때 문장을 구분하기 위해 사용됩니다.\n",
        "> 13. **return_attention_mask**:\n",
        ">     -   `Optional[bool]`\n",
        ">     -   `attention_mask`를 반환할지 여부를 설정합니다. 패딩된 토큰이\n",
        ">         모델의 어텐션에 영향을 주지 않도록 마스크를 설정합니다.\n",
        "> 14. **return_overflowing_tokens**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트가 최대 길이를 초과하는 경우, 잘린 토큰을 반환할지\n",
        ">         여부를 결정합니다.\n",
        "> 15. **return_special_tokens_mask**:\n",
        ">     -   `bool`\n",
        ">     -   특별한 토큰에 대한 마스크를 반환할지 여부를 설정합니다.\n",
        "> 16. **return_offsets_mapping**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트의 각 토큰이 원본 텍스트에서 어느 위치에 있는지 나타내는\n",
        ">         오프셋 맵핑을 반환할지 여부를 설정합니다.\n",
        "> 17. **return_length**:\n",
        ">     -   `bool`\n",
        ">     -   토큰화된 문장의 길이를 반환할지 여부를 설정합니다.\n",
        "> 18. **verbose**:\n",
        ">     -   `bool`\n",
        ">     -   디버깅 메시지를 출력할지 여부를 설정합니다. 기본값은 `True`로\n",
        ">         설정되어 있습니다.\n",
        ">\n",
        "> **사용 예시**:\n",
        ">\n",
        "> ``` python\n",
        "> from transformers import AutoTokenizer\n",
        ">\n",
        "> # 토크나이저 불러오기\n",
        "> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        ">\n",
        "> # 텍스트 토큰화\n",
        "> encoding = tokenizer(\n",
        ">     text=\"Hello, how are you?\",\n",
        ">     padding=True,\n",
        ">     truncation=True,\n",
        ">     max_length=10,\n",
        ">     return_tensors='pt'\n",
        "> )\n",
        ">\n",
        "> print(encoding)\n",
        "> ```\n",
        ">\n",
        "> 이 코드에서는 “Hello, how are you?”라는 텍스트를 `bert-base-uncased`\n",
        "> 토크나이저로 토큰화하고, 패딩과 트렁케이션을 적용하며, PyTorch 텐서\n",
        "> 형식으로 반환하도록 설정했습니다.\n",
        ">\n",
        "> 이러한 파라미터는 주로 자연어 처리(NLP) 모델을 훈련하거나 추론할 때\n",
        "> 데이터 전처리 과정에서 많이 사용됩니다.\n",
        "\n",
        "`-` 기본사용1: 단어별로 다른숫자를 맵핑 + 처음과 끝은 항상 `101`, `102`"
      ],
      "id": "92970f0b-eebc-4115-8004-27609377411e"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hello\")"
      ],
      "id": "cell-32"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hi hello\")"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7632, 7592, 7592, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hi hello hello hello\")"
      ],
      "id": "cell-34"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7632, 7592, 102]], 'attention_mask': [[1, 1, 1, 1]]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저([\"hi hello\"])"
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 기본사용2: 텍스트 혹은 텍스트의 리스트, 리스트의 리스트를 전달가능"
      ],
      "id": "cf8c663c-ba67-4e26-ade6-ff82eb65bd46"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7632, 7592, 102], [101, 7592, 7592, 102], [101, 7632, 7632, 102]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저([\"hi hello\", \"hello hello\", \"hi hi\"])"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` `truncation=True` 의 역할"
      ],
      "id": "811aab83-af47-4e6f-a490-530e6cef6613"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "dct = 토크나이저('hi hello'*1000,truncation=True)\n",
        "dct"
      ],
      "id": "cell-39"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          }
        }
      ],
      "source": [
        "len(dct['input_ids']), len(dct['attention_mask'])"
      ],
      "id": "cell-40"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` maxlen, padding, attention_mask"
      ],
      "id": "b07b4543-0aa0-4e07-9239-3a56bad4c3a3"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
            ]
          }
        }
      ],
      "source": [
        "dct = 토크나이저('hi hello',max_length=10,padding=\"max_length\")\n",
        "dct"
      ],
      "id": "cell-42"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 인공지능 ($\\star$)\n",
        "\n",
        "## A. 1단계\n",
        "\n",
        "> 인공지능에 대한 이해\n",
        "\n",
        "`-` 인공지능 불러오기"
      ],
      "id": "c594580c-4d76-4954-ab13-4b5c966d35a2"
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(43052)\n",
        "인공지능 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", \n",
        "    num_labels=2\n",
        ")"
      ],
      "id": "cell-47"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 인공지능의 정체? 엄청나게 많은 숫자들이 포함된 어떠한 물체 (엄청나게\n",
        "많은 파라메터들이 포함된 네트워크)"
      ],
      "id": "1b061c02-9770-4970-9d7d-cec9a9898f1a"
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          }
        }
      ],
      "source": [
        "인공지능"
      ],
      "id": "cell-49"
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n",
              "        [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n",
              "       requires_grad=True)"
            ]
          }
        }
      ],
      "source": [
        "인공지능.classifier.weight"
      ],
      "id": "cell-50"
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0122,  0.0030,  0.0301,  ...,  0.0003,  0.0327, -0.0123],\n",
              "        [-0.0041,  0.0008,  0.0169,  ..., -0.0163, -0.0117, -0.0135],\n",
              "        [ 0.0020, -0.0215, -0.0021,  ...,  0.0016,  0.0102, -0.0483],\n",
              "        ...,\n",
              "        [-0.0026, -0.0327,  0.0099,  ...,  0.0341, -0.0184, -0.0109],\n",
              "        [ 0.0088, -0.0345, -0.0011,  ...,  0.0018,  0.0172, -0.0122],\n",
              "        [-0.0148,  0.0147, -0.0184,  ..., -0.0166,  0.0241,  0.0201]],\n",
              "       requires_grad=True)"
            ]
          }
        }
      ],
      "source": [
        "인공지능.pre_classifier.weight"
      ],
      "id": "cell-51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 인공지능? “입력정보 -\\> 정리된숫자 -\\> 계산 -\\> 계산된숫자 -\\>\n",
        "출력정보” 의 과정에서 “계산”을 담당\n",
        "\n",
        "-   인공지능이 가지고 있는 숫자들은 계산에 사용되는 숫자들임..\n",
        "\n",
        "`-` 입력정보에 영화에 대한 부정적 평가를 넣는다면?"
      ],
      "id": "733eca3e-aeac-4c60-9342-fc763dffeadf"
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          }
        }
      ],
      "source": [
        "입력정보_원시텍스트 = \"This movie was a huge disappointment.\"\n",
        "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
        "정리된숫자_토큰화된자료"
      ],
      "id": "cell-54"
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1173,  0.0261]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          }
        }
      ],
      "source": [
        "인공지능(**정리된숫자_토큰화된자료)"
      ],
      "id": "cell-55"
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[-0.11731046,  0.02610319]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
        "계산된숫자_로짓"
      ],
      "id": "cell-56"
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[0.46420792, 0.53579205]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
        "출력정보_확률 # 0일확률, 1일확률"
      ],
      "id": "cell-57"
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "1"
            ]
          }
        }
      ],
      "source": [
        "출력정보_확률.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
      ],
      "id": "cell-58"
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "1"
            ]
          }
        }
      ],
      "source": [
        "계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측 <-- 이렇게 구해도됩니다.. 왜??"
      ],
      "id": "cell-59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 입력정보에 영화에 대한 긍정적 평가를 넣는다면?"
      ],
      "id": "3531657c-ffab-4209-a813-d2e07d6b51d2"
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
            ]
          }
        }
      ],
      "source": [
        "입력정보_원시텍스트 = \"This was a masterpiece.\"\n",
        "정리된숫자_토큰화된자료 = 토크나이저(입력정보_원시텍스트,truncation=True,return_tensors='pt')\n",
        "정리된숫자_토큰화된자료"
      ],
      "id": "cell-61"
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-0.1080,  0.0264]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          }
        }
      ],
      "source": [
        "인공지능(**정리된숫자_토큰화된자료)"
      ],
      "id": "cell-62"
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[-0.10802954,  0.02638793]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "계산된숫자_로짓 = 인공지능(**정리된숫자_토큰화된자료).logits.detach().numpy()\n",
        "계산된숫자_로짓"
      ],
      "id": "cell-63"
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[0.46644613, 0.53355384]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "출력정보_확률 = np.exp(계산된숫자_로짓) / np.exp(계산된숫자_로짓).sum()\n",
        "출력정보_확률 # 0일확률, 1일확률"
      ],
      "id": "cell-64"
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "1"
            ]
          }
        }
      ],
      "source": [
        "출력정보_확률.argmax()\n",
        "계산된숫자_로짓.argmax() # 부정적 영화평가에 대한 인공지능의 예측"
      ],
      "id": "cell-65"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 아무숫자나 뱉어내는듯 $\\to$ 멍청한 인공지능.. (옹호: 당연하지\n",
        "학습전이니까)\n",
        "\n",
        "`-` 인공지능에 대한 이해1: 인공지능은 “정리된숫자”를 입력으로 하고\n",
        "일련의 계산을 거쳐 “계산된숫자”를 출력해주는 함수라 생각할 수 있음.\n",
        "\n",
        "`-` 인공지능에 대한 이해2: 인공지능은 (1) 많은숫자들과 (2) 고유의\n",
        "계산방식을 가지고 있음.\n",
        "\n",
        "-   인공지능이 내부에 자체적으로 저장하고 있는 숫자를 **파라메터**라고\n",
        "    부름.\n",
        "-   인공지능은 나름의 법칙에 따라 “데이터”와 “파라메터”를 계산함. 즉\n",
        "    인공지능은 자체적으로 데이터와 파라메터를 어떻게 계산할지\n",
        "    알고있는데, 이러한 고유의 계산방식을 **아키텍처**라고 말함.\n",
        "\n",
        "`-` 인공지능에 대한 이해3: 두 인공지능이 서로 다른 고유의 계산방식을\n",
        "가지고 있다면 두 인공지능은 “다른 모델”임.\n",
        "\n",
        "`-` 인공지능에 대한 이해3’: 동일한 생성방식으로 만들어진 인공지능들은\n",
        "모두 같은 모델임. 예를들면 아래의 인공지능1,2는 같은 모델임\n",
        "\n",
        "``` python\n",
        "인공지능1 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", \n",
        "    num_labels=2\n",
        ")\n",
        "인공지능2 = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", \n",
        "    num_labels=2\n",
        ")\n",
        "```\n",
        "\n",
        "`-` 인공지능에 대한 이해4: 두 인공지능이 같은 모델이라고 해도, 항상 같은\n",
        "결과를 주는건 아님. 파라메터에 따라 다른결과를 줄 수 도 있음. (예를들면\n",
        "위의 인공지능1,2는 같은 모델이지만 다른 파라메터를 가지므로 다른 결과를\n",
        "줌)\n",
        "\n",
        "## B. 2단계\n",
        "\n",
        "> 미니배치를 이해하자.\n",
        "\n",
        "`-` 예비학습"
      ],
      "id": "060a73ee-0b14-4037-9992-5a4dbb046a74"
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [2, 3],\n",
              "       [3, 4]])"
            ]
          }
        }
      ],
      "source": [
        "arr = np.array([[1,2],[2,3],[3,4]])\n",
        "arr"
      ],
      "id": "cell-76"
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[0.33333333, 0.66666667],\n",
              "       [0.4       , 0.6       ],\n",
              "       [0.42857143, 0.57142857]])"
            ]
          }
        }
      ],
      "source": [
        "arr / arr.sum(axis=1).reshape(-1,1)"
      ],
      "id": "cell-77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 예비개념1: 인공지능은 사실 영화평을 하나씩 하나씩 처리하지 않는다.\n",
        "덩어리로 처리한다.\n",
        "\n",
        "`-` 예비개념2: 그렇다고 해서 인공지능이 25000개를 모두 덩어리로\n",
        "처리하는건 아니다 $\\to$ 16개씩, 혹은 32개씩 묶어서 작은덩어리로 만든 후\n",
        "처리한다.\n",
        "\n",
        "-   16,32 와 같은 숫자를 `batch_size` 라고 한다.\n",
        "-   16개, 32개로 모인 작은덩어리를 미니배치라고 한다.\n",
        "\n",
        "`-` 16개의 입력정보를 한번에 처리"
      ],
      "id": "6b3c7843-4f0b-40de-81f4-6a52f69d6d9f"
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[0.47823825, 0.5217617 ],\n",
              "       [0.47215328, 0.5278467 ],\n",
              "       [0.4804948 , 0.5195052 ],\n",
              "       [0.46609667, 0.5339033 ],\n",
              "       [0.48814487, 0.5118551 ],\n",
              "       [0.48004225, 0.5199578 ],\n",
              "       [0.49288097, 0.50711906],\n",
              "       [0.49470255, 0.5052974 ],\n",
              "       [0.4941453 , 0.50585467],\n",
              "       [0.49016201, 0.50983804],\n",
              "       [0.495789  , 0.504211  ],\n",
              "       [0.48260576, 0.51739424],\n",
              "       [0.49386355, 0.5061364 ],\n",
              "       [0.49013382, 0.5098662 ],\n",
              "       [0.48240176, 0.5175982 ],\n",
              "       [0.49301967, 0.5069803 ]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "입력정보들_원시텍스트 = 데이터['train'][:16]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.data.numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "출력정보들_확률"
      ],
      "id": "cell-81"
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          }
        }
      ],
      "source": [
        "#출력정보들_확률.argmax(axis=1) \n",
        "계산된숫자들_로짓.argmax(axis=1)"
      ],
      "id": "cell-82"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 기억할 것: `정리된숫자들_토큰화된자료` 는 모두 길이가 512임. (그렇게\n",
        "되도록 패딩함)"
      ],
      "id": "f23f33c0-14e8-4e3e-9407-b0afeaf7994b"
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(torch.Size([16, 512]), torch.Size([16, 512]))"
            ]
          }
        }
      ],
      "source": [
        "정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape"
      ],
      "id": "cell-84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 실제 단어수"
      ],
      "id": "866f6781-02d5-473f-b96d-9021adb74f14"
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([363, 304, 133, 185, 495, 154, 143, 388, 512, 297, 365, 171, 192, 173,\n",
              "        470, 263])"
            ]
          }
        }
      ],
      "source": [
        "정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)"
      ],
      "id": "cell-86"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 패딩된단어수"
      ],
      "id": "3857ee65-2905-4b02-9293-a946f59ea0f4"
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([149, 208, 379, 327,  17, 358, 369, 124,   0, 215, 147, 341, 320, 339,\n",
              "         42, 249])"
            ]
          }
        }
      ],
      "source": [
        "512 - 정리된숫자들_토큰화된자료['attention_mask'].sum(axis=1)"
      ],
      "id": "cell-88"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 이러한 변환이 이루어지는 이유? 인공지능은 항상 `(n,m)` 차원으로\n",
        "정리된 정리된숫자들을 입력으로 받아야함.\n",
        "\n",
        "-   왜? 사실 인공지능은 행렬계산을 하도록 설계되어있음.\n",
        "-   그래서 할수없이 패딩처리를 해야하는 것임. (실제로는 행렬로 만들수\n",
        "    없지만 억지로 만들기 위해서..)\n",
        "\n",
        "## C. 3단계\n",
        "\n",
        "> 동적패딩을 이해하자.\n",
        "\n",
        "`-` 만약에 `batch_size=4` 로 설정하여 처리한다면?"
      ],
      "id": "575ab549-3722-40be-b0e1-7529c6c1ee35"
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([[0.47823825, 0.5217618 ],\n",
              "       [0.47215328, 0.5278467 ],\n",
              "       [0.4804948 , 0.5195052 ],\n",
              "       [0.46609667, 0.5339033 ]], dtype=float32)"
            ]
          }
        }
      ],
      "source": [
        "입력정보들_원시텍스트 = 데이터['train'][:4]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시텍스트,truncation=True,return_tensors='pt',padding=True)\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.data.numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "출력정보들_확률"
      ],
      "id": "cell-93"
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1])"
            ]
          }
        }
      ],
      "source": [
        "계산된숫자들_로짓.argmax(axis=1)"
      ],
      "id": "cell-94"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` `정리된숫자들_토큰화된자료`의 차원은 어떠할까? (4,512)로 예상되지\n",
        "않을까?"
      ],
      "id": "ee51cb31-720d-457a-a4d0-ab7bb1a79f6a"
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(torch.Size([4, 363]), torch.Size([4, 363]))"
            ]
          }
        }
      ],
      "source": [
        "정리된숫자들_토큰화된자료['input_ids'].shape, 정리된숫자들_토큰화된자료['attention_mask'].shape"
      ],
      "id": "cell-96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   끝차원이 512가 아니라 363이다.. 왜??\n",
        "\n",
        "`-` 덩어리의 상태에 따라서 유동적으로 패딩 $\\to$ 그래도 잘 돌아감"
      ],
      "id": "ed06adb6-4e25-4fe9-ae06-108ef417894e"
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 363])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n",
            "        [-0.0830,  0.0285],\n",
            "        [-0.0600,  0.0180],\n",
            "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
        "print(인공지능(**정리된숫자들_토큰화된자료))"
      ],
      "id": "cell-99"
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 495])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0422,  0.0053],\n",
            "        [-0.0646,  0.0152],\n",
            "        [-0.0172,  0.0112],\n",
            "        [-0.0283, -0.0072]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][4:8]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
        "print(인공지능(**정리된숫자들_토큰화된자료))"
      ],
      "id": "cell-100"
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0246, -0.0012],\n",
            "        [-0.0594, -0.0200],\n",
            "        [-0.0240, -0.0071],\n",
            "        [-0.0836, -0.0140]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][8:12]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
        "print(인공지능(**정리된숫자들_토큰화된자료))"
      ],
      "id": "cell-101"
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 470])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0440, -0.0195],\n",
            "        [-0.0469, -0.0074],\n",
            "        [-0.0542,  0.0162],\n",
            "        [-0.0316, -0.0037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][12:16]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
        "print(인공지능(**정리된숫자들_토큰화된자료))"
      ],
      "id": "cell-102"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 싹다 512로 통일한것대비 큰 차이없음.."
      ],
      "id": "0e9c10ca-e349-4f90-a48c-5918276c13ba"
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 512])\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0847,  0.0024],\n",
            "        [-0.0830,  0.0285],\n",
            "        [-0.0600,  0.0180],\n",
            "        [-0.0919,  0.0440],\n",
            "        [-0.0422,  0.0053],\n",
            "        [-0.0646,  0.0152],\n",
            "        [-0.0172,  0.0112],\n",
            "        [-0.0283, -0.0072],\n",
            "        [-0.0246, -0.0012],\n",
            "        [-0.0594, -0.0200],\n",
            "        [-0.0240, -0.0071],\n",
            "        [-0.0836, -0.0140],\n",
            "        [-0.0440, -0.0195],\n",
            "        [-0.0469, -0.0074],\n",
            "        [-0.0542,  0.0162],\n",
            "        [-0.0316, -0.0037]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:16]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(정리된숫자들_토큰화된자료['input_ids'].shape)\n",
        "print(인공지능(**정리된숫자들_토큰화된자료))"
      ],
      "id": "cell-104"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 4단계\n",
        "\n",
        "> 손실(=loss)의 개념을 이해하자.\n",
        "\n",
        "`-` `정리된숫자들_토큰화된자료`에서 `labels` 를 추가 전달하면\n",
        "`인공지능(**정리된숫자들_토큰화된자료)`의 결과로 `loss`가 추가계산됨"
      ],
      "id": "f4d321df-f90f-44f1-80a3-1b42dcc13668"
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.7461, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0847,  0.0024],\n",
              "        [-0.0830,  0.0285],\n",
              "        [-0.0600,  0.0180],\n",
              "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          }
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "#데이터['train'][:4]['label']\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor([0,0,0,0]) # 정답입력\n",
        "인공지능(**정리된숫자들_토큰화된자료)"
      ],
      "id": "cell-108"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 정리를 해보자."
      ],
      "id": "14ae4cf9-3dc7-446c-9e81-8b742c6497ee"
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제정답: [0, 0, 0, 0]\n",
            "인공지능의예측: [1 1 1 1]\n",
            "인공지능확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n",
            "손실: 0.7461"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(f'실제정답: {데이터['train'][:4]['label']}')\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
        "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
        "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
      ],
      "id": "cell-110"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` loss는 작을수록 좋은 것임. 문제를 많이 틀릴수록 loss가 큼, 문제를\n",
        "조금 틀릴수록 loss가 작음\n",
        "\n",
        "**텍스트0~텍스트3**"
      ],
      "id": "bbae732b-9d16-480f-87e1-b0f7389d5a8b"
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제정답: [0, 0, 0, 0]\n",
            "인공지능의예측: [1 1 1 1]\n",
            "인공지능확신정도: [0.5217618 0.5278467 0.5195052 0.5339033]\n",
            "손실: 0.7461"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:4]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(f'실제정답: {데이터['train'][:4]['label']}')\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:4]['label']) # 정답입력\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
        "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
        "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
      ],
      "id": "cell-113"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**텍스트12498~텍스트12501**"
      ],
      "id": "3e5401d5-557b-4819-a7ab-f3897444067c"
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제정답: [0, 0, 1, 1]\n",
            "인공지능의예측: [1 1 1 1]\n",
            "인공지능확신정도: [0.52731967 0.5243837  0.51578873 0.5351162 ]\n",
            "손실: 0.6950"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][12498:12502]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(f'실제정답: {데이터['train'][12498:12502]['label']}')\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][12498:12502]['label']) # 정답입력\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
        "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
        "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
      ],
      "id": "cell-115"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 똑같이 틀려도 오답에 대한 확신이 강할수록 loss가 크다.\n",
        "\n",
        "**텍스트0~텍스트1**"
      ],
      "id": "bf73a85e-cc30-4bb5-8f98-06f70964b240"
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제정답: [0, 0]\n",
            "인공지능의예측: [1 1]\n",
            "인공지능확신정도: [0.5217617 0.5278467]\n",
            "손실: 0.7440"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][:2]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(f'실제정답: {데이터['train'][:2]['label']}')\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][:2]['label']) # 정답입력\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
        "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
        "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
      ],
      "id": "cell-118"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**텍스트1~텍스트2**"
      ],
      "id": "b2db2771-f71e-4a2d-95c9-afbb26c9108e"
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제정답: [0, 0]\n",
            "인공지능의예측: [1 1]\n",
            "인공지능확신정도: [0.5278467 0.5195052]\n",
            "손실: 0.7417"
          ]
        }
      ],
      "source": [
        "입력정보들_원시데이터 = 데이터['train'][1:3]['text']\n",
        "정리된숫자들_토큰화된자료 = 토크나이저(입력정보들_원시데이터,truncation=True,return_tensors='pt',padding=True)\n",
        "print(f'실제정답: {데이터['train'][1:3]['label']}')\n",
        "정리된숫자들_토큰화된자료['labels'] = torch.tensor(데이터['train'][1:3]['label']) # 정답입력\n",
        "계산된숫자들_로짓 = 인공지능(**정리된숫자들_토큰화된자료).logits.detach().numpy()\n",
        "출력정보들_확률 = np.exp(계산된숫자들_로짓) / np.exp(계산된숫자들_로짓).sum(axis=1).reshape(-1,1)\n",
        "print(f'인공지능의예측: {출력정보들_확률.argmax(axis=1)}')\n",
        "print(f'인공지능확신정도: {출력정보들_확률.max(axis=1)}')\n",
        "print(f'손실: {인공지능(**정리된숫자들_토큰화된자료).loss.item():.4f}')"
      ],
      "id": "cell-120"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 손실 = 인공지능의 “멍청한정도” 혹은 “똑똑한정도”을 숫자화한것\n",
        "\n",
        "> **$\\star\\star\\star$ 학습이 가능한 이유 (대충 아이디어만)**\n",
        ">\n",
        "> `1`. 랜덤으로 1개의 인공지능을 생성한다. (아래의 코드로 가능)\n",
        ">\n",
        "> ``` python\n",
        "> 인공지능 = 인공지능생성기()\n",
        "> ```\n",
        ">\n",
        "> `2`. 인공지능의 파라메터중 하나를 찍는다. 예를들면 아래와 같은 상황이\n",
        "> 있다고 하자.\n",
        ">\n",
        "> ``` python\n",
        "> 인공지능.classifier.weight\n",
        "> ```\n",
        ">\n",
        ">     Parameter containing:\n",
        ">     tensor([[-0.0234,  0.0279,  0.0242,  ...,  0.0091, -0.0063, -0.0133],\n",
        ">             [ 0.0087,  0.0007, -0.0099,  ...,  0.0183, -0.0007,  0.0295]],\n",
        ">            requires_grad=True)\n",
        ">\n",
        "> 하나의 숫자 `-0.0234` 를 선택한다.\n",
        ">\n",
        "> `3`. `-0.0234`의 값을 조금 변화시켜본다. 예를들면, `-0.0235`,\n",
        "> `-0.0233` 와 같은 식으로 변화시켜본뒤 loss를 관찰한다.\n",
        ">\n",
        "> `4`. 원래값과 변화시킨값들중 loss를 가장 작게 만드는 값으로\n",
        "> `-0.0234`를 update한다.\n",
        ">\n",
        "> `5`. 다른 모든 파라메터에 대하여 1~4를 반복한다. (과정을 반복할수록\n",
        "> loss가 작아지겠죠, 즉 인공지능은 똑똑해지겠죠)\n",
        "\n",
        "`-` 인공지능의 학습은 마법같은 신비한 현상이 아니고 극한의 노가다를 통해\n",
        "얻어지는 산물일 뿐이다.\n",
        "\n",
        "# 6. 데이터전처리2"
      ],
      "id": "948089c1-c8be-4854-bb86-274c7b354b29"
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {},
      "outputs": [],
      "source": [
        "def 데이터전처리하기2(examples):\n",
        "    return 데이터전처리하기1(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-125"
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|█████████████████████████████████████████████████████████████████████████| 25000/25000 [00:03<00:00, 8260.93 examples/s]"
          ]
        }
      ],
      "source": [
        "데이터 = datasets.load_dataset('imdb')\n",
        "전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)"
      ],
      "id": "cell-126"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. 데이터콜렉터\n",
        "\n",
        "`-` 데이터콜렉터 사용방법"
      ],
      "id": "2c504701-9414-4ba7-ad7e-c8c95a550c9f"
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
            ]
          }
        }
      ],
      "source": [
        "데이터콜렉터 = 동적패딩처리하기 = transformers.DataCollatorWithPadding(tokenizer=토크나이저,return_tensors='pt')\n",
        "데이터콜렉터"
      ],
      "id": "cell-129"
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "dict_keys(['text', 'label', 'input_ids', 'attention_mask'])"
            ]
          }
        }
      ],
      "source": [
        "작은덩어리_딕셔너리 = 전처리된데이터['train'][:4]\n",
        "작은덩어리_딕셔너리.keys()"
      ],
      "id": "cell-130"
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {},
      "outputs": [],
      "source": [
        "작은덩어리_리스트 = [{\n",
        "    'label': 작은덩어리_딕셔너리['label'][i],\n",
        "    'input_ids': 작은덩어리_딕셔너리['input_ids'][i],\n",
        "    'attention_mask': 작은덩어리_딕셔너리['attention_mask'][i]\n",
        "} for i in range(4)]"
      ],
      "id": "cell-131"
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045, 12524,  ...,  5436,  1012,   102],\n",
              "        [  101,  1000,  1045,  ...,     0,     0,     0],\n",
              "        [  101,  2065,  2069,  ...,     0,     0,     0],\n",
              "        [  101,  2023,  2143,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0])}"
            ]
          }
        }
      ],
      "source": [
        "데이터콜렉터(작은덩어리_리스트)"
      ],
      "id": "cell-132"
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=tensor(0.7461, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0847,  0.0024],\n",
              "        [-0.0830,  0.0285],\n",
              "        [-0.0600,  0.0180],\n",
              "        [-0.0919,  0.0440]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          }
        }
      ],
      "source": [
        "인공지능(**데이터콜렉터(작은덩어리_리스트))"
      ],
      "id": "cell-133"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터콜렉터의 기능: batch_size 에 따라서 패딩시켜줌\n",
        "\n",
        "# 8. 평가하기\n",
        "\n",
        "`-` `accuracy.compute`의 기능"
      ],
      "id": "39dbdea1-044b-48d9-9e29-13c89ec9bdb4"
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "id": "cell-137"
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'accuracy': 0.3333333333333333}"
            ]
          }
        }
      ],
      "source": [
        "accuracy.compute(references=[0, 0, 0], predictions=[0, 1, 1])"
      ],
      "id": "cell-138"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 함수내용"
      ],
      "id": "90e5f2ef-099b-431f-adf4-5f541a174c31"
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def 평가하기(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     predictions = np.argmax(predictions, axis=1)\n",
        "#     accuracy = evaluate.load(\"accuracy\")\n",
        "#     return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "id": "cell-140"
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {},
      "outputs": [],
      "source": [
        "def 평가하기(eval_pred):\n",
        "    계산된숫자들_로짓, 실제정답 = eval_pred\n",
        "    인공지능의예측 = np.argmax(계산된숫자들_로짓, axis=1)\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    return accuracy.compute(predictions=인공지능의예측, references=실제정답)"
      ],
      "id": "cell-141"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `평가하기` 함수는 `eval_dataset`에 적용됨\n",
        "\n",
        "# 9. 트레이너 세부지침"
      ],
      "id": "312f3497-3f63-4ec5-a428-096a76f81c36"
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {},
      "outputs": [],
      "source": [
        "트레이너세부지침 = transformers.TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\", \n",
        "    save_strategy=\"epoch\", \n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "id": "cell-144"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **옵션 설명:**\n",
        ">\n",
        "> 1.  **`output_dir=\"my_awesome_model\"`**:\n",
        ">     -   학습된 모델과 관련 파일(예: 체크포인트, 로그 등)이 저장될\n",
        ">         디렉토리 경로를 지정합니다.\n",
        "> 2.  **`learning_rate=2e-5`**:\n",
        ">     -   학습률(learning rate)을 설정합니다. 모델이 가중치를 업데이트할\n",
        ">         때 사용하는 스텝 크기로, 작은 값일수록 학습 속도가 느려지지만\n",
        ">         안정성이 높습니다.\n",
        "> 3.  **`per_device_train_batch_size=16`**:\n",
        ">     -   **훈련** 시, 각 GPU 또는 CPU 디바이스에서 사용할 배치 크기를\n",
        ">         설정합니다. 이 경우 각 디바이스에서 16개의 샘플을 한 번에\n",
        ">         처리합니다.\n",
        "> 4.  **`per_device_eval_batch_size=16`**:\n",
        ">     -   **평가** 시, 각 GPU 또는 CPU 디바이스에서 사용할 배치 크기를\n",
        ">         설정합니다. 평가 시에는 훈련과 같은 배치 크기를 사용하는 것이\n",
        ">         일반적입니다.\n",
        "> 5.  **`num_train_epochs=2`**:\n",
        ">     -   전체 훈련 데이터셋을 몇 번 반복(에포크)할지 설정합니다.\n",
        ">         여기서는 데이터셋 전체를 2번 학습하게 됩니다.\n",
        "> 6.  **`weight_decay=0.01`**:\n",
        ">     -   가중치 감쇠(weight decay)를 적용하여 모델의 가중치가 지나치게\n",
        ">         커지지 않도록 제어합니다. 0.01의 값은 가중치가 조금씩 감소하게\n",
        ">         하여 모델의 일반화 성능을 높이는 데 도움을 줄 수 있습니다.\n",
        "> 7.  **`eval_strategy=\"epoch\"`**:\n",
        ">     -   평가 전략을 설정합니다. 여기서는 `epoch`으로 설정되어, 매\n",
        ">         에포크가 끝날 때마다 평가가 진행됩니다.\n",
        ">     -   다른 값으로는 `steps` (일정한 스텝마다 평가) 등이 있습니다.\n",
        "> 8.  **`save_strategy=\"epoch\"`**:\n",
        ">     -   모델을 언제 저장할지 설정합니다. `epoch`으로 설정되면, 매\n",
        ">         에포크가 끝날 때마다 체크포인트를 저장합니다.\n",
        ">     -   다른 값으로는 `steps` (일정한 스텝마다 저장) 등이 있습니다.\n",
        "> 9.  **`load_best_model_at_end=True`**:\n",
        ">     -   학습이 끝난 후, 가장 성능이 좋았던 체크포인트를 불러옵니다.\n",
        ">         평가 지표에 따라 가장 성능이 좋았던 모델을 자동으로 불러와\n",
        ">         최종 모델로 사용하게 됩니다.\n",
        "> 10. **`push_to_hub=False`**:\n",
        ">     -   모델 학습이 끝난 후 Hugging Face Hub에 모델을 업로드할지\n",
        ">         여부를 설정합니다. `False`로 설정하면 업로드하지 않습니다.\n",
        ">     -   `True`로 설정하면 모델과 관련된 파일들이 Hugging Face Hub에\n",
        ">         업로드되어 다른 사람들과 공유할 수 있습니다.\n",
        "\n",
        "`-` lr = 학습률 = 답이 틀렸을 경우 혼내는 정도\n",
        "\n",
        "-   정확한 느낌은 경사하강법을 이해해야함.\n",
        "\n",
        "# 10. 트레이너"
      ],
      "id": "c8031c19-8915-4b4c-b036-5c099afc420e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<p>"
            ]
          }
        }
      ],
      "source": [
        "트레이너 = transformers.Trainer(\n",
        "    model=인공지능,\n",
        "    args=트레이너세부지침,\n",
        "    train_dataset=전처리된데이터['train'],\n",
        "    eval_dataset=전처리된데이터['test'],\n",
        "    #tokenizer=토크나이저, # 왜 필요하지??\n",
        "    data_collator=데이터콜렉터,\n",
        "    compute_metrics=평가하기,\n",
        ")\n",
        "트레이너.train()"
      ],
      "id": "cell-148"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. 파이프라인\n",
        "\n",
        "`-` 강인공지능?\n",
        "\n",
        "> ref: <https://zdnet.co.kr/view/?no=20160622145838>"
      ],
      "id": "b5a83d9e-019a-4dec-9408-168d15a86ac3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "강인공지능 = transformers.pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\n",
        "print(강인공지능(\"This movie was a huge disappointment.\"))\n",
        "print(강인공지능(\"This was a masterpiece.\"))"
      ],
      "id": "cell-152"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  }
}