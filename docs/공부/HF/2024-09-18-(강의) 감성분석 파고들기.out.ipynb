{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (강의) 감성분류 파고들기\n",
        "\n",
        "신록예찬  \n",
        "2024-09-18\n",
        "\n",
        "# 1. Imports"
      ],
      "id": "efc75ea1-27ce-4d98-8029-a0472c8f1dcd"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. 데이터셋"
      ],
      "id": "e8639c0c-d6b9-4306-9834-ea4e4236747e"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn("
          ]
        }
      ],
      "source": [
        "## Step1 \n",
        "데이터불러오기 = datasets.load_dataset\n",
        "데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") \n",
        "def 데이터전처리하기2(examples):\n",
        "    return 데이터전처리하기1(examples[\"text\"], truncation=True)\n",
        "## Step2 \n",
        "인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n",
        "## Step3 \n",
        "데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저)\n",
        "def 평가하기(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "트레이너세부지침생성기 = transformers.TrainingArguments\n",
        "트레이너생성기 = transformers.Trainer\n",
        "## Step4 \n",
        "강인공지능생성하기 = transformers.pipeline"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Step1 \n",
        "# 데이터 = 데이터불러오기('imdb')\n",
        "# 전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)\n",
        "# 전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']\n",
        "# ## Step2 \n",
        "# 인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
        "# ## Step3 \n",
        "# 트레이너세부지침 = 트레이너세부지침생성기(\n",
        "#     output_dir=\"my_awesome_model\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=16,\n",
        "#     per_device_eval_batch_size=16,\n",
        "#     num_train_epochs=2, # 전체문제세트를 2번 공부하라..\n",
        "#     weight_decay=0.01,\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     save_strategy=\"epoch\",\n",
        "#     load_best_model_at_end=True,\n",
        "#     push_to_hub=False,\n",
        "# )\n",
        "# 트레이너 = 트레이너생성기(\n",
        "#     model=인공지능,\n",
        "#     args=트레이너세부지침,\n",
        "#     train_dataset=전처리된훈련자료,\n",
        "#     eval_dataset=전처리된검증자료,\n",
        "#     tokenizer=토크나이저,\n",
        "#     data_collator=데이터콜렉터,\n",
        "#     compute_metrics=평가하기,\n",
        "# )\n",
        "# 트레이너.train()"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Step4 \n",
        "# 강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model\")\n",
        "# print(강인공지능(text0))\n",
        "# print(강인공지능(text1))"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 토크나이저\n",
        "\n",
        "`-` 기본적인 사용방법"
      ],
      "id": "f407d98e-d5ed-40c4-a325-2e0b6b2006e2"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<pre><span class=\"ansi-red-fg\">Signature:</span>     \n",
              "토크나이저<span class=\"ansi-blue-fg\">(</span>\n",
              "    text<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    text_pair<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> NoneType<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    text_target<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    text_pair_target<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> List<span class=\"ansi-blue-fg\">[</span>List<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> NoneType<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    add_special_tokens<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    padding<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>bool<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">,</span> transformers<span class=\"ansi-blue-fg\">.</span>utils<span class=\"ansi-blue-fg\">.</span>generic<span class=\"ansi-blue-fg\">.</span>PaddingStrategy<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    truncation<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>bool<span class=\"ansi-blue-fg\">,</span> str<span class=\"ansi-blue-fg\">,</span> transformers<span class=\"ansi-blue-fg\">.</span>tokenization_utils_base<span class=\"ansi-blue-fg\">.</span>TruncationStrategy<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    max_length<span class=\"ansi-blue-fg\">:</span> Optional<span class=\"ansi-blue-fg\">[</span>int<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    stride<span class=\"ansi-blue-fg\">:</span> int <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    is_split_into_words<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    pad_to_multiple_of<span class=\"ansi-blue-fg\">:</span> Optional<span class=\"ansi-blue-fg\">[</span>int<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_tensors<span class=\"ansi-blue-fg\">:</span> Union<span class=\"ansi-blue-fg\">[</span>str<span class=\"ansi-blue-fg\">,</span> transformers<span class=\"ansi-blue-fg\">.</span>utils<span class=\"ansi-blue-fg\">.</span>generic<span class=\"ansi-blue-fg\">.</span>TensorType<span class=\"ansi-blue-fg\">,</span> NoneType<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_token_type_ids<span class=\"ansi-blue-fg\">:</span> Optional<span class=\"ansi-blue-fg\">[</span>bool<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_attention_mask<span class=\"ansi-blue-fg\">:</span> Optional<span class=\"ansi-blue-fg\">[</span>bool<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_overflowing_tokens<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_special_tokens_mask<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_offsets_mapping<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    return_length<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    verbose<span class=\"ansi-blue-fg\">:</span> bool <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span>\n",
              "    <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">,</span>\n",
              "<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-&gt;</span> transformers<span class=\"ansi-blue-fg\">.</span>tokenization_utils_base<span class=\"ansi-blue-fg\">.</span>BatchEncoding\n",
              "<span class=\"ansi-red-fg\">Type:</span>           DistilBertTokenizerFast\n",
              "<span class=\"ansi-red-fg\">String form:</span>   \n",
              "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, mode &lt;...&gt; Token(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "           }\n",
              "<span class=\"ansi-red-fg\">Length:</span>         30522\n",
              "<span class=\"ansi-red-fg\">File:</span>           ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/tokenization_distilbert_fast.py\n",
              "<span class=\"ansi-red-fg\">Docstring:</span>     \n",
              "Construct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n",
              "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
              "refer to this superclass for more information regarding those methods.\n",
              "Args:\n",
              "    vocab_file (`str`):\n",
              "        File containing the vocabulary.\n",
              "    do_lower_case (`bool`, *optional*, defaults to `True`):\n",
              "        Whether or not to lowercase the input when tokenizing.\n",
              "    unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n",
              "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
              "        token instead.\n",
              "    sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n",
              "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
              "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
              "        token of a sequence built with special tokens.\n",
              "    pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n",
              "        The token used for padding, for example when batching sequences of different lengths.\n",
              "    cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n",
              "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
              "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
              "    mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n",
              "        The token used for masking values. This is the token used when training this model with masked language\n",
              "        modeling. This is the token which the model will try to predict.\n",
              "    clean_text (`bool`, *optional*, defaults to `True`):\n",
              "        Whether or not to clean the text before tokenization by removing any control characters and replacing all\n",
              "        whitespaces by the classic one.\n",
              "    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n",
              "        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n",
              "        issue](https://github.com/huggingface/transformers/issues/328)).\n",
              "    strip_accents (`bool`, *optional*):\n",
              "        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
              "        value for `lowercase` (as in the original BERT).\n",
              "    wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n",
              "        The prefix for subwords.\n",
              "<span class=\"ansi-red-fg\">Call docstring:</span>\n",
              "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
              "sequences.\n",
              "Args:\n",
              "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
              "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
              "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
              "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
              "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
              "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
              "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
              "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
              "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
              "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
              "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
              "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
              "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
              "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
              "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
              "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
              "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
              "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
              "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
              "        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
              "        automatically.\n",
              "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
              "        Activates and controls padding. Accepts the following values:\n",
              "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
              "          sequence if provided).\n",
              "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
              "          acceptable input length for the model if that argument is not provided.\n",
              "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
              "          lengths).\n",
              "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
              "        Activates and controls truncation. Accepts the following values:\n",
              "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
              "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
              "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
              "          sequences (or a batch of pairs) is provided.\n",
              "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
              "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
              "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
              "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
              "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
              "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
              "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
              "          greater than the model maximum admissible input size).\n",
              "    max_length (`int`, *optional*):\n",
              "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
              "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
              "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
              "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
              "    stride (`int`, *optional*, defaults to 0):\n",
              "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
              "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
              "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
              "        argument defines the number of overlapping tokens.\n",
              "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
              "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
              "        which it will tokenize. This is useful for NER or token classification.\n",
              "    pad_to_multiple_of (`int`, *optional*):\n",
              "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
              "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
              "        `&gt;= 7.5` (Volta).\n",
              "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
              "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
              "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
              "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
              "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
              "    return_token_type_ids (`bool`, *optional*):\n",
              "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
              "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
              "        [What are token type IDs?](../glossary#token-type-ids)\n",
              "    return_attention_mask (`bool`, *optional*):\n",
              "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
              "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
              "        [What are attention masks?](../glossary#attention-mask)\n",
              "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
              "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
              "        of returning overflowing tokens.\n",
              "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to return special tokens mask information.\n",
              "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to return `(char_start, char_end)` for each token.\n",
              "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
              "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
              "    return_length  (`bool`, *optional*, defaults to `False`):\n",
              "        Whether or not to return the lengths of the encoded inputs.\n",
              "    verbose (`bool`, *optional*, defaults to `True`):\n",
              "        Whether or not to print more information and warnings.\n",
              "    **kwargs: passed to the `self.tokenize()` method\n",
              "Return:\n",
              "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
              "    - **input_ids** -- List of token ids to be fed to a model.\n",
              "      [What are input IDs?](../glossary#input-ids)\n",
              "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
              "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
              "      [What are token type IDs?](../glossary#token-type-ids)\n",
              "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
              "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
              "      [What are attention masks?](../glossary#attention-mask)\n",
              "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
              "      `return_overflowing_tokens=True`).\n",
              "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
              "      `return_overflowing_tokens=True`).\n",
              "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
              "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
              "    - **length** -- The length of the inputs (when `return_length=True`)</pre>"
            ]
          }
        }
      ],
      "source": [
        "토크나이저?"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **주요 파라미터:**\n",
        ">\n",
        "> 1.  **text**:\n",
        ">     -   `Union[str, List[str], List[List[str]]]`\n",
        ">     -   주어진 텍스트를 토큰화합니다. 이 텍스트는 문자열일 수도 있고,\n",
        ">         문자열의 리스트 또는 리스트 안의 리스트일 수도 있습니다.\n",
        "> 2.  **text_pair**:\n",
        ">     -   `Union[str, List[str], List[List[str]], NoneType]`\n",
        ">     -   두 개의 텍스트를 함께 모델에 입력할 때 사용됩니다. 예를 들어,\n",
        ">         질문-답변 쌍 같은 경우 이 두 번째 텍스트를 넣습니다.\n",
        "> 3.  **text_target**:\n",
        ">     -   `Union[str, List[str], List[List[str]]]`\n",
        ">     -   토큰화를 할 때 목표(target) 텍스트에 해당하는 부분입니다. 주로\n",
        ">         시퀀스 생성 모델에서 활용됩니다.\n",
        "> 4.  **text_pair_target**:\n",
        ">     -   `Union[str, List[str], List[List[str]], NoneType]`\n",
        ">     -   위의 `text_pair`와 유사하게 목표(target) 텍스트의 두 번째\n",
        ">         텍스트를 나타냅니다.\n",
        "> 5.  **add_special_tokens**:\n",
        ">     -   `bool`\n",
        ">     -   문장의 시작, 끝, 구분자 같은 특별한 토큰을 추가할지 여부를\n",
        ">         결정합니다. 기본값은 `True`입니다.\n",
        "> 6.  **padding**:\n",
        ">     -   `Union[bool, str, transformers.utils.generic.PaddingStrategy]`\n",
        ">     -   문장 길이가 다를 때 패딩을 넣어 문장의 길이를 동일하게\n",
        ">         맞춥니다. 패딩 전략에는 `True`, `False`, `'longest'`,\n",
        ">         `'max_length'` 등이 있습니다.\n",
        "> 7.  **truncation**:\n",
        ">     -   `Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]`\n",
        ">     -   문장이 너무 길 경우 지정된 최대 길이에 맞춰 잘라내는\n",
        ">         옵션입니다. 전략에는 `True`, `False`, `'longest_first'`,\n",
        ">         `'only_first'`, `'only_second'` 등이 있습니다.\n",
        "> 8.  **max_length**:\n",
        ">     -   `Optional[int]`\n",
        ">     -   문장의 최대 길이를 설정합니다. `None`일 경우 기본 설정을\n",
        ">         따릅니다.\n",
        "> 9.  **stride**:\n",
        ">     -   `int`\n",
        ">     -   텍스트를 자를 때 중첩을 만들기 위한 옵션입니다. 즉, 자른\n",
        ">         부분과 다음 부분 사이의 겹치는 범위를 설정합니다.\n",
        "> 10. **is_split_into_words**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트가 이미 단어 단위로 분리되어 있는지 여부를 나타냅니다.\n",
        ">         기본적으로는 `False`로, 텍스트가 단어 단위로 분리되지 않았다고\n",
        ">         가정합니다.\n",
        "> 11. **return_tensors**:\n",
        ">     -   `Union[str, transformers.utils.generic.TensorType, NoneType]`\n",
        ">     -   출력 형식으로 텐서를 반환할지 여부를 설정합니다.\n",
        ">         `'pt'`(PyTorch), `'tf'`(TensorFlow), `'np'`(NumPy) 등을 지정할\n",
        ">         수 있습니다.\n",
        "> 12. **return_token_type_ids**:\n",
        ">     -   `Optional[bool]`\n",
        ">     -   토큰 타입 ID를 반환할지 여부를 설정합니다. 주로 두 개의 문장을\n",
        ">         함께 처리할 때 문장을 구분하기 위해 사용됩니다.\n",
        "> 13. **return_attention_mask**:\n",
        ">     -   `Optional[bool]`\n",
        ">     -   `attention_mask`를 반환할지 여부를 설정합니다. 패딩된 토큰이\n",
        ">         모델의 어텐션에 영향을 주지 않도록 마스크를 설정합니다.\n",
        "> 14. **return_overflowing_tokens**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트가 최대 길이를 초과하는 경우, 잘린 토큰을 반환할지\n",
        ">         여부를 결정합니다.\n",
        "> 15. **return_special_tokens_mask**:\n",
        ">     -   `bool`\n",
        ">     -   특별한 토큰에 대한 마스크를 반환할지 여부를 설정합니다.\n",
        "> 16. **return_offsets_mapping**:\n",
        ">     -   `bool`\n",
        ">     -   텍스트의 각 토큰이 원본 텍스트에서 어느 위치에 있는지 나타내는\n",
        ">         오프셋 맵핑을 반환할지 여부를 설정합니다.\n",
        "> 17. **return_length**:\n",
        ">     -   `bool`\n",
        ">     -   토큰화된 문장의 길이를 반환할지 여부를 설정합니다.\n",
        "> 18. **verbose**:\n",
        ">     -   `bool`\n",
        ">     -   디버깅 메시지를 출력할지 여부를 설정합니다. 기본값은 `True`로\n",
        ">         설정되어 있습니다.\n",
        ">\n",
        "> ### 사용 예시:\n",
        ">\n",
        "> ``` python\n",
        "> from transformers import AutoTokenizer\n",
        ">\n",
        "> # 토크나이저 불러오기\n",
        "> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        ">\n",
        "> # 텍스트 토큰화\n",
        "> encoding = tokenizer(\n",
        ">     text=\"Hello, how are you?\",\n",
        ">     padding=True,\n",
        ">     truncation=True,\n",
        ">     max_length=10,\n",
        ">     return_tensors='pt'\n",
        "> )\n",
        ">\n",
        "> print(encoding)\n",
        "> ```\n",
        ">\n",
        "> 이 코드에서는 “Hello, how are you?”라는 텍스트를 `bert-base-uncased`\n",
        "> 토크나이저로 토큰화하고, 패딩과 트렁케이션을 적용하며, PyTorch 텐서\n",
        "> 형식으로 반환하도록 설정했습니다.\n",
        ">\n",
        "> 이러한 파라미터는 주로 자연어 처리(NLP) 모델을 훈련하거나 추론할 때\n",
        "> 데이터 전처리 과정에서 많이 사용됩니다.\n",
        "\n",
        "`-` 기본사용1: 단어별로 다른숫자를 맵핑 + 처음과 끝은 항상 `101`, `102`"
      ],
      "id": "a47bc1b7-c833-45ba-8709-5853d638e282"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hello\")"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hi hello\")"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7632, 7592, 7592, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"hi hi hello hello hello\")"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7632, 7592, 102]], 'attention_mask': [[1, 1, 1, 1]]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저([\"hi hello\"])"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 기본사용2: 텍스트 혹은 텍스트의 리스트, 리스트의 리스트를 전달가능"
      ],
      "id": "455a00c8-a261-46f4-934a-4f7c543779a2"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 7632, 7592, 102], [101, 7592, 7592, 102], [101, 7632, 7632, 102]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저([\"hi hello\", \"hello hello\", \"hi hi\"])"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` `truncation=True` 의 역할"
      ],
      "id": "40e544ff-78db-4c97-aca7-02fe510a8d63"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "dct = 토크나이저('hi hello'*1000,truncation=True)\n",
        "dct"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          }
        }
      ],
      "source": [
        "len(dct['input_ids']), len(dct['attention_mask'])"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` maxlen, padding, attention_mask"
      ],
      "id": "db6ff569-65cb-4f5b-bc5d-72add5505215"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7632, 7592, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
            ]
          }
        }
      ],
      "source": [
        "dct = 토크나이저('hi hello',max_length=10,padding=\"max_length\")\n",
        "dct"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 고급내용: 토크나이저는 의미적으로 비슷한 단어를 비슷한 숫자들로\n",
        "바꾸지 않는다."
      ],
      "id": "d0dae95c-fd29-42c7-a0be-5c705851f603"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_words = [\n",
        "    \"fun\",\n",
        "    \"heartwarming\",\n",
        "    \"exciting\",\n",
        "    \"amazing\",\n",
        "    \"enjoyable\"\n",
        "]\n",
        "negative_words = [\n",
        "    \"boring\",\n",
        "    \"dull\",\n",
        "    \"bad\",\n",
        "    \"slow\",\n",
        "    \"predictable\"\n",
        "]"
      ],
      "id": "cell-24"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  }
}