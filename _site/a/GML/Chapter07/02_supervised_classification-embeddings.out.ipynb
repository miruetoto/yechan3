{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **\\[GML\\]** Chap7: Shallow-Learning Topic Modelling\n",
        "\n",
        "신록예찬  \n",
        "2023-02-10\n",
        "\n",
        "# Shallow-Learning Topic Modelling\n",
        "\n",
        "In the following we will show you how to create a topic model, using a\n",
        "shallow-learning approach. Here we will use the results and the\n",
        "embeddings obtained from the document-document projection of the\n",
        "bipartite graph.\n",
        "\n",
        "**NOTE: This Notebook can only be run after the 01_nlp_graph_creation\n",
        "notebook, as some of the results computed in the first notebook will be\n",
        "here reused.**\n",
        "\n",
        "### Load Dataset"
      ],
      "id": "bb0a82de-4700-4240-b83d-081e03a6ccef"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = pd.read_pickle(\"corpus.p\")"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "topics = Counter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common(10)"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "topics"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "topicsList = [topic[0] for topic in topics]\n",
        "topicsSet = set(topicsList)\n",
        "dataset = corpus[corpus[\"label\"].apply(lambda x: len(topicsSet.intersection(x))>0)]"
      ],
      "id": "cell-8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a class to “simulate” the training of the embeddings"
      ],
      "id": "63f463c1-e4c1-43b9-8ed0-3771db7b2cfe"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class EmbeddingsTransformer(BaseEstimator):\n",
        "    \n",
        "    def __init__(self, embeddings_file):\n",
        "        self.embeddings_file = embeddings_file\n",
        "        \n",
        "    def fit(self, *args, **kwargs):\n",
        "        self.embeddings = pd.read_pickle(self.embeddings_file)\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X):\n",
        "        return self.embeddings.loc[X.index]\n",
        "    \n",
        "    def fit_transform(self, X, y):\n",
        "        return self.fit().transform(X)\n",
        "\n"
      ],
      "id": "cell-10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob \n",
        "files = glob(\"./embeddings/*\")"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "graphEmbeddings = EmbeddingsTransformer(files[0]).fit()"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train/Test split"
      ],
      "id": "59feb641-9b1b-4fc3-b707-971294bd5cdd"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_labels(corpus, topicsList=topicsList):\n",
        "    return corpus[\"label\"].apply(\n",
        "        lambda labels: pd.Series({label: 1 for label in labels}).reindex(topicsList).fillna(0)\n",
        "    )[topicsList]"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_features(corpus):\n",
        "    return corpus[\"parsed\"] #graphEmbeddings.transform(corpus[\"parsed\"])"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_features_and_labels(corpus):\n",
        "    return get_features(corpus), get_labels(corpus)"
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_split(corpus):\n",
        "    graphIndex = [index for index in corpus.index if index in graphEmbeddings.embeddings.index]\n",
        "    \n",
        "    train_idx = [idx for idx in graphIndex if \"training/\" in idx]\n",
        "    test_idx = [idx for idx in graphIndex if \"test/\" in idx]\n",
        "    return corpus.loc[train_idx], corpus.loc[test_idx]"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = train_test_split(dataset)"
      ],
      "id": "cell-18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the model and cross-validation"
      ],
      "id": "d4485052-037f-4002-8945-4a36781faea1"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.multioutput import MultiOutputClassifier"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MultiOutputClassifier(RandomForestClassifier())"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"embeddings\", graphEmbeddings),\n",
        "    (\"model\", model)\n",
        "])"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "id": "cell-24"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "files"
      ],
      "id": "cell-25"
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    \"embeddings__embeddings_file\": files,\n",
        "    \"model__estimator__n_estimators\": [50, 100], \n",
        "    \"model__estimator__max_features\": [0.2,0.3, \"auto\"], \n",
        "    #\"model__estimator__max_depth\": [3, 5]\n",
        "}"
      ],
      "id": "cell-26"
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [],
      "source": [
        "features, labels = get_features_and_labels(train)"
      ],
      "id": "cell-27"
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score "
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1, \n",
        "                           scoring=lambda y_true, y_pred: f1_score(y_true, y_pred,average='weighted'))"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
            "  category=UserWarning"
          ]
        }
      ],
      "source": [
        "model = grid_search.fit(features, labels)"
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.best_params_"
      ],
      "id": "cell-32"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate performance"
      ],
      "id": "8c6423f9-5a7e-4163-89ea-394e5c833585"
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predictions(model, features):\n",
        "    return pd.DataFrame(\n",
        "        model.predict(features), \n",
        "        columns=topicsList, \n",
        "        index=features.index\n",
        "    )"
      ],
      "id": "cell-34"
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = get_predictions(model, get_features(test))\n",
        "labels = get_labels(test)"
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors = 1 - (labels - preds).abs().sum().sum() / labels.abs().sum().sum()"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "errors"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "id": "cell-38"
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      1087\n",
            "           1       0.93      0.74      0.83       719\n",
            "           2       0.79      0.45      0.57       179\n",
            "           3       0.96      0.64      0.77       149\n",
            "           4       0.95      0.59      0.73       189\n",
            "           5       0.95      0.45      0.61       117\n",
            "           6       0.87      0.41      0.56       131\n",
            "           7       0.83      0.21      0.34        89\n",
            "           8       0.69      0.34      0.45        71\n",
            "           9       0.61      0.25      0.35        56\n",
            "\n",
            "   micro avg       0.94      0.72      0.81      2787\n",
            "   macro avg       0.85      0.50      0.62      2787\n",
            "weighted avg       0.92      0.72      0.79      2787\n",
            " samples avg       0.76      0.75      0.75      2787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))"
          ]
        }
      ],
      "source": [
        "print(classification_report(labels, preds))"
      ],
      "id": "cell-39"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  }
}