[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "신록예찬's Blog",
    "section": "",
    "text": "About this blog\nThis blog was created for my personal research, study and lecture preparation. Therefore, the contents of the blog can be thought of as my practice notes. As a result, sometimes the content of a post may be left unstructured or unfinished. The blog is named after my favorite essay ‘신록예찬’, which is also a nickname I use informally. You can check the written article in the sidebar on the left. It is a great honor for me if these posts can help others to learn and research.\n\nSome links that help me\nfirst url bunch\n\nmatplotlib: https://matplotlib.org/stable/gallery/index.html\njupyterlab: https://jupyterlab.readthedocs.io/en/stable/index.html\npandas: https://pandas.pydata.org/docs/user_guide/index.html#user-guide\njulia: https://docs.julialang.org/en/v1/\nkeras: https://keras.io/examples/\njulia plots: https://docs.juliaplots.org/stable/\npytorch lightning: https://www.pytorchlightning.ai/\nplotly: https://plotly.com/graphing-libraries/\nquarto: https://quarto.org/\n\nsecond url bunch\n\nlatex: https://editor.codecogs.com/\ntable: https://www.tablesgenerator.com/\npytorch lightning (codes in book): https://github.com/PacktPublishing/Deep-Learning-with-PyTorch-Lightning\nPyG: https://github.com/rusty1s/pytorch_geometric\nrayshader: https://www.rayshader.com/reference/plot_gg.html\nSouth Korea (map): https://github.com/southkorea\nregexp: https://zvon.org/comp/m/regexp.html\nrpy2: https://rpy2.github.io/doc/v3.1.x/html/index.html\npywave: https://pywavelets.readthedocs.io/en/latest/ref/wavelets.html\nPyGSP: https://pygsp.readthedocs.io/en/stable/#\nlatex (neural networks): https://tikz.net/neural_networks/\nplotly overview: https://plotly.com/python/plotly-express/\nfastai (official): https://docs.fast.ai/\nfastai (lecture): https://course.fast.ai/#\nfastai (github codes): https://github.com/fastai/fastai/tree/master/dev_nbs/course\nGML (codes in book): https://github.com/PacktPublishing/Graph-Machine-Learning\nGMLKOR (codes in book): https://github.com/AcornPublishing/graph-ml\nmathNET https://gtribello.github.io/mathNET/index.html\n\nlectures notes\n\nhttp://personal.psu.edu/drh20/asymp/fall2006/lectures/\nhttps://web.ma.utexas.edu/users/gordanz/lecture_notes_page.html\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 30, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)\n\n\n신록예찬 \n\n\n\n\nJul 20, 2023\n\n\n[SOLAR] EPT + RGCN (시뮬레이션)\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[PyG] lesson5: GCN\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[Review] 사기탐지 + 그래프 관련연구\n\n\n신록예찬 \n\n\n\n\nJul 17, 2023\n\n\n[SOLAR] EPT + RGCN\n\n\n신록예찬 \n\n\n\n\nJul 14, 2023\n\n\n[PyG] lesson5: Learning Methods on Graphs\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson3: 미니배치\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson4: Data Transform???\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[Essays] 다중척도방법\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)\n\n\n신록예찬 \n\n\n\n\nJul 4, 2023\n\n\n[Essays] 토폴로지\n\n\n신록예찬 \n\n\n\n\nJul 2, 2023\n\n\n[PyG] lesson1: 자료형\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN ToyEX\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬 \n\n\n\n\nMay 6, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Start\n\n\n신록예찬 \n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연 \n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬 \n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 1, 2023\n\n\n[SEOYEONC] type1 err, type2 err\n\n\n신록예찬 \n\n\n\n\nJan 24, 2023\n\n\n[HST] CommunityDetection\n\n\n신록예찬 \n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬 \n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬 \n\n\n\n\nJan 12, 2023\n\n\n[SEOYEONC] 지수분포 평균검정\n\n\n신록예찬 \n\n\n\n\nJan 1, 2023\n\n\n[PINKOCTO] 면담\n\n\n신록예찬 \n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬 \n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬 \n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬 \n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬 \n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬 \n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬 \n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬 \n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬 \n\n\n\n\nJan 8, 2000\n\n\n[Note] 깃(Git)\n\n\n신록예찬 \n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬 \n\n\n\n\nJan 6, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬 \n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬 \n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "4_notes.html",
    "href": "4_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 8, 2000\n\n\n[Note] 깃(Git)\n\n\n신록예찬 \n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬 \n\n\n\n\nJan 6, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬 \n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬 \n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_reviews.html",
    "href": "2_reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 19, 2023\n\n\n[Review] 사기탐지 + 그래프 관련연구\n\n\n신록예찬 \n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_pinkocto.html",
    "href": "3_pinkocto.html",
    "title": "PINKOCTO",
    "section": "",
    "text": "https://github.com/pinkocto\nhttps://pinkocto.github.io/noteda/\nhttps://pinkocto.github.io/Quarto-Blog/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 20, 2023\n\n\n[SOLAR] EPT + RGCN (시뮬레이션)\n\n\n신록예찬 \n\n\n\n\nJul 17, 2023\n\n\n[SOLAR] EPT + RGCN\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬 \n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\nJan 1, 2023\n\n\n[PINKOCTO] 면담\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html",
    "href": "posts/1_Essays/2023-01-20-추정.html",
    "title": "[Essays] 추정",
    "section": "",
    "text": "using Distributions, Plots"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "href": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "title": "[Essays] 추정",
    "section": "MLE의 일치성에 대한 구체적인 논의",
    "text": "MLE의 일치성에 대한 구체적인 논의\n\\(X_1,\\dots,X_{10} \\overset{i.i.d.}{\\sim} Ber(\\theta)\\) 이라고 하자.\n\nx = rand(Bernoulli(0.3),10)\nx\n\n10-element Vector{Bool}:\n 0\n 0\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 0\n\n\n여기에서 \\(\\theta\\)는 추정해야할 미지의 모수이지만 우리는 시뮬레이션의 편의상 \\(\\theta\\)의 참값을 \\(\\theta_0=\\frac{1}{3}\\)로 알고 있다고 하자. MLE를 논의함에 있어 핵심적인 역할을 하는 것은 \\(Y_1=\\log f(X_1;\\theta)\\)이다. 아래는 \\(Y_1\\)에 대한 몇가지 코멘트이다.\n(1) \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n우선 \\(X_1\\)의 함수이므로 \\(Y_1\\)역시 확률변수이다. 따라서 \\(Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\\(Y_1\\)은 \\(\\theta\\)에 대한 함수이므로 \\(\\theta\\)에 대하여 미분할 수 있다.\n\n(베르누이 예제)\n우리의 베르누이 예제에서 \\(Y_1\\)은 아래와 같이 계산된다.\n\\[Y_1 = \\log f(X_1;\\theta)= X_1 \\log \\theta + (1-X_1)\\log(1-\\theta)\\]\n보는 것 처럼 \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수임\n(2) \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta\\) 만의 함수이다. 적당한 조건4이 만족된다면 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta_0\\) 에서 최대화 된다.\n4 identifiable & common support(베르누이 예제)\n\\(\\mathbb{E}_{\\theta_0}(Y_1) = \\mathbb{E}_{\\theta_0}(X_1)\\log\\theta + (1-\\mathbb{E}_{\\theta_0}(X_1))\\log(1-\\theta) = \\frac{1}{3} \\log\\theta + (1-\\frac{1}{3})\\log(1-\\theta)\\)\n\n일반적인 상황에서는 참모수를 모르지만 우리는 시뮬레이션을 \\(\\theta=1/3\\)에서 하였으므로 참모수 \\(\\theta_0=\\mathbb{E}_{\\theta_0}(X_1)=\\frac{1}{3}\\)을 알고 있다고 가정한다.\n\n\nplot(θ -&gt; (1/3)*log(θ) + (1-1/3)*log(1-θ)) \n\n\n\n\n보는것처럼 이 함수 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta=\\theta_0=\\frac{1}{3}\\) 에서 최대값을 가진다.\n(3) \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n따라서 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 확률변수이고 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\n(베르누이 예제)\n\\(\\frac{\\partial}{\\partial\\theta}Y_1 = X_1\\frac{1}{\\theta} + (1-X_1)\\frac{-1}{1-\\theta}\\)\n(4) \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial \\theta}Y_1]=0\\) 이다.\n(베르누이 예제)\n\\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1] = \\theta\\frac{1}{\\theta} + (1-\\theta)\\frac{-1}{1-\\theta}=0\\)\n(5) \\(\\mathbb{V}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=\\mathbb{E}_{\\theta}[-\\frac{\\partial^2}{\\partial \\theta^2}Y_1]=I(\\theta)\\)\n(베르누이 예제)\n\\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]=\\mathbb{E}_{\\theta}\\big[(\\frac{\\partial}{\\partial\\theta}Y_1)^2\\big]=\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]=\\frac{1}{\\theta(1-\\theta)}\\)\n\n두번째 등호는 \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=0\\)을 이용하여 증명가능하다.\n언뜻 보면 \\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]\\)를 계산하는 것이 \\(\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]\\)를 계산하는것보다 훨씬 쉬워보인다. 그런데 \\(X_1\\)와 \\(1-X_1\\)이 독립이 아니라서 \\(\\mathbb{V}(X+Y)=\\mathbb{V}(X)+\\mathbb{V}(V)+2\\text{Cov}(X,Y)\\)와 같이 공분산 term을 계산해야 하므로 계산이 까다롭다.\n\n\n베르누이에 대한 피셔정보량은 https://en.wikipedia.org/wiki/Fisher_information 에서 확인할 수 있음"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html",
    "title": "[Essays] 다중척도방법",
    "section": "",
    "text": "\\({\\boldsymbol y}={\\boldsymbol X}^\\top\\)\n\n이 논문에서는 몇 가지 멀티스케일 방법을 리뷰하고자 한다.\n멀티스케일 방법이란 ~ + 왜 멀티스케일 방법을 써야하는가.\n우리는 그중에서 시각화와 분해에 관심이 있다. + 다른 접근법들도 소개\n시각화가 사용되는 사례, 어떻게 이용할 수 있는가?\n분해가 이용되는 사례, 어떻게 이용할 수 있는가?\n많은 멀티스케일 방법 중 우리는 TPT, EPT, EBT 를 중점적으로 소개하는데 초점을 맞출 것이다.\n각각의 transform은 어떠한 공통점이 있는가?\n왜 이렇게 많은 트랜스폼이 있어야하는가?\n미래에는 어떠한 트랜스폼을 만드는 것이 좋을까?"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html#motivation-of-emd",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html#motivation-of-emd",
    "title": "[Essays] 다중척도방법",
    "section": "Motivation of EMD",
    "text": "Motivation of EMD\n- A necessary condition to define a meaningful instantaneous frequency is that signals are symmetric with respect to the local zero mean.\n- EMD is developed to decompose signals so that HT works well.\n- Suppose we have a signal \\(x(t) = 0.5 t + \\sin(\\pi t) + \\sin(2 \\pi t) + \\sin(6 \\pi t)\\).\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html#sifting-process-for-extracting-imfs",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html#sifting-process-for-extracting-imfs",
    "title": "[Essays] 다중척도방법",
    "section": "Sifting process for extracting IMF’s",
    "text": "Sifting process for extracting IMF’s\n\nIdentify local extrema.\nObtain two functions interpolated by local maxima and minima.\nTake an average of envelopes.\nAn oscillated signal is extracted by subtracting the envelope mean from the original signal \\(x(t)\\).\n\n\n\n\nimage.png\n\n\n- One iteration of the above procedure does not guarantee that the signal is an IMF. The same procedure is applied to the signal until the properties of IMF are satisfied\n- This iterative algorithm is called sifting process.\n- Sifting makes the remaining signal more symmetric with respect to the local zero mean.\n- Finally, we have \\(L\\) empirical modes and a residue, \\[\nx(t) = \\sum_{\\ell=1}^{L} \\text{imf}_\\ell(t) + r(t).\n\\]\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/3_Researches/YECHAN/HST/2023-01-23-CommunityDetection.html",
    "href": "posts/3_Researches/YECHAN/HST/2023-01-23-CommunityDetection.html",
    "title": "[HST] CommunityDetection",
    "section": "",
    "text": "ref\n- Roddenberry et al. (2020)\n\nRoddenberry, T Mitchell, Michael T Schaub, Hoi-To Wai, and Santiago Segarra. 2020. “Exact Blind Community Detection from Signals on Multiple Graphs.” IEEE Transactions on Signal Processing 68: 5016–30.\n\nhttps://arxiv.org/pdf/2001.10944.pdf\nintro에 그래프는 single correct한 structure를 알 수 없다는 이야기가 있는데 이를 이용해야함.\n\n\n\n방법론"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-02-14-Tables.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-02-14-Tables.html",
    "title": "[IT-STGCN] Tables",
    "section": "",
    "text": "import pandas as pd\n\n- Dataset 5nodes\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test',]\n\n\ndf = pd.DataFrame(columns=col)\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n\n\n\n\n\n- 실험마다 아래와 같은식으로 추가\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.0, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = None,  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = None, # None, 'mean', 'linear'\ndf_row['MSE_train'] = 0.96 \ndf_row['MSE_test'] = 0.55\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n0\nfivenodes\n1\nstgcn\n0.0\nNone\n1\n16\nNone\n0.96\n0.55\n\n\n\n\n\n\n\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.2, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = 'randomly',  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = 'mean', # None, 'mean', 'linear'\ndf_row['MSE_train'] = 1.23\ndf_row['MSE_test'] = 0.88\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n0\nfivenodes\n1\nstgcn\n0.0\nNone\n1\n16\nNone\n0.96\n0.55\n\n\n0\nfivenodes\n1\nstgcn\n0.2\nrandomly\n1\n16\nmean\n1.23\n0.88"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "",
    "text": "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "data",
    "text": "data\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "Learn",
    "text": "Learn\n\n# from tqdm import tqdm\n\n# model = RecurrentGCN(node_features=14, filters=32)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# model.train()\n\n# for epoch in tqdm(range(50)):\n#     for time, snapshot in enumerate(train_dataset):\n#         y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n#         cost = torch.mean((y_hat-snapshot.y)**2)\n#         cost.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n\n\nmodel = RecurrentGCN(node_features=14, filters=32)\nmodel.train()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nfor s in train_dataset:\n    print((s.y-model(s.x,s.edge_index,s.edge_attr)).shape)\n\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "예제의 차원 조사",
    "text": "예제의 차원 조사\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\n1068: number of nodes // 1068개의 노드가 있음\n14: number of features // 하나의 노드에 맵핑된 차원의수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n1068: number of nodes\n\n\n_x.shape\n\ntorch.Size([1068, 14])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "title": "[IT-STGCN] ITSTGCN-Tutorial",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\n\n\n\n예제1: vanilla STGCN\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n\n\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset,dataset_name='five_nodes')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) \nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제2: padding missing values\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 임의로 결측치 발생\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex=mindex,mtype='rand')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n- 적절한 method로 결측치를 채움 (default 는 linear)\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n다른 method로 결측치를 채울수도 있음. 사용할 수 있는 방법들은 아래에 정리되어 있음\n\nref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='nearest')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='quadratic')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='cubic')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.utils.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n- 블락으로 결측치 발생\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n예제3: vanilla STGCN with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.learners.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제4: vanilla STGCN with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제5: threshold example (random)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.utils.plot(f_miss,'o')\nitstgcn.utils.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제6: threshold example (block)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.plot(f_miss,'o')\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제7: iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제8: iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제9: GNAR (random missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제10: GNAR (block missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/3_Researches/YEON/2023-02-14-수통문제-지수분포평균검정.html",
    "href": "posts/3_Researches/YEON/2023-02-14-수통문제-지수분포평균검정.html",
    "title": "[SEOYEONC] 지수분포 평균검정",
    "section": "",
    "text": "using Distributions, Plots\n\n다음의 분포를 따르는\n\n\\(f(x; \\theta) = \\theta \\exp (-x\\theta )I(x&gt;0)\\)\n\n모집단으로부터 랜덤표본 \\(X_1,\\dots,X_n\\)을 이용하여 \\(\\theta\\)에 대한 신뢰구간을 구하고, 다음 가설\n\n\\(H_0: \\theta = \\theta_0\\) vs \\(H_1:\\theta \\neq \\theta_0\\)\n\n을 검정하고자 한다. 다음에 답하라.\n(a) \\(\\theta\\)에 대한 적절한 추축변량을 구하고, 해당 추축변량의 분포를 명시하라.\n(풀이)\n추축변량은 \\(2n\\bar{X}\\theta\\) 이며 분포는 자유도가 \\(2n\\)인 카이제곱분포를 따름. 왜냐하면\n\n\\(X_1 \\sim \\text{Exp}\\) with mean \\(1/\\theta\\)\n\\(2X_1\\theta \\sim \\text{Exp}\\) with mean \\(2\\)\n\\(2n\\bar{X}\\theta \\sim \\chi^2(2n)\\) (왜? 평균이2인 지수분포를 \\(n\\)번 더하면 자유도가 \\(2n\\)인 카이제곱분포가 되므로. –&gt; 참고)\n\n이기 떄문에.\n(시뮬레이션)\n\nn = 10\nθ = 2\npivotal_variable = [mean(rand(Exponential(1/2),n))*2*n*θ for i in 1:140000]\n# pivotal_variable = 추축변량\n\n140000-element Vector{Float64}:\n 19.866862184254465\n 17.492982610528998\n 13.463211578260314\n 19.472994478042697\n 13.424495255444352\n 21.744281289967418\n 22.929701779207697\n 13.561641745647645\n 25.277641996704048\n 16.566359730027052\n 19.550767074240074\n  9.914621718787568\n 11.857966801675106\n  ⋮\n 38.05226144210052\n 17.120691971399836\n 21.66831336505733\n 26.476356317147957\n 21.930886178263727\n 18.33570158249295\n 22.39822061516168\n 27.42899896779973\n 14.770009603430982\n 13.776401174299512\n 21.511825086436673\n 21.867878699644073\n\n\n\nhistogram(pivotal_variable)\nhistogram!(rand(Chisq(2*n),140000))\n\n\n\n\n\n추축변량은 자유도 \\(2n\\)인 카이제곱분포를 따름\n\n(b) \\(\\theta\\)에 대한 95% 신뢰구간을 구하라.\n(풀이)\n\\(P\\left(\\chi^2_{0.025}(2n) \\leq 2n\\bar{X}\\theta \\leq \\chi^2_{0.975}(2n) \\right) =0.95\\)\nThus the CI of \\(\\theta\\): \\(\\left(\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}},\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}} \\right)\\)\n(시뮬레이션)\n\nl = quantile(Chisq(2*n),0.025) ./ (pivotal_variable./θ)\nu = quantile(Chisq(2*n),0.975) ./ (pivotal_variable./θ)\n[l[i]&lt;θ&lt;u[i] for i in 1:140000] |&gt; mean\n\n0.95035\n\n\n(c) \\(P(X&gt;1)\\)에 대한 95% 신뢰구간\n(풀이)\n\\(P(X&gt;1) = \\int_{1}^{\\infty}\\theta \\exp(-x\\theta)dx =\\left[e^{-x\\theta} \\right]_1^{\\infty}=e^{-\\theta}\\)\nThus the CI of \\(P(X&gt;1)\\): \\(\\left(\\exp\\big(-\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}}\\big),\\exp\\big(-\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}}\\big) \\right)\\)\n(시뮬레이션)\n검토1: \\(P(X&gt;1)=E[I(X&gt;1)]=e^{-\\theta}\\) 임을 검토\n\nexp(-θ)\n\n0.1353352832366127\n\n\n\nmean(rand(Exponential(1/2),1000000) .&gt; 1)\n\n0.135678\n\n\n검토2: 신뢰구간\n\nu2 = exp.(-l)\nl2 = exp.(-u)\n[l2[i]&lt;exp(-θ)&lt;u2[i] for i in 1:140000] |&gt; mean\n\n0.95035\n\n\n(d)\n\\(\\Omega = \\{\\theta: \\theta&gt;0\\}\\), \\(\\Omega_0 =\\{2\\}\\)\n(e) \\(\\theta\\)의 가능도 함수를 기술하시오\n(풀이)\n\\(L(\\theta)=\\theta^n\\exp(-\\theta n\\bar{x})\\)\n(f) \\(\\theta\\)의 \\(\\Omega\\)에서의 최대가능도 추정량과 \\(\\Omega_0\\)에서의 최대가능도 추정량을 구하시오\n(풀이)\n\\(\\hat{\\theta}^{\\Omega}=1/\\bar{x}\\), \\(\\hat{\\theta}^{\\Omega_0}=\\theta_0\\)\n(g) 일반화 가능도 비1 \\(\\Lambda\\)를 구하시오.\n1 이게 뭐지..?(풀이)\n\\(\\frac{L\\big(\\hat{\\theta}^{\\Omega}\\big)}{L\\big(\\hat{\\theta}^{\\Omega_0}\\big)}=\\frac{1/\\bar{x}^n\\exp(-n)}{\\theta_0^n\\exp(-\\theta_0n\\bar{x})}\\)\n(h) 유의수준 \\(\\alpha\\)의 가능도비 검정법의 기각역을 \\(\\chi^2\\)의 분위수를 사용하여 표현하시오.\n(풀이)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\ndef amtano1(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano=0)\n    normalize = lambda arr: (arr-np.median(arr))/np.std(arr) if np.std(arr)!=0 else arr*0 \n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano']] = normalize(sub_df.amt).cumsum()\n    return df"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\n# from ctgan import CTGAN\n# from ctgan import load_demo\n\n\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.columns\n\nIndex(['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt',\n       'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat',\n       'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat',\n       'merch_long', 'is_fraud'],\n      dtype='object')\n\n\n\nfraudTrain['is_fraud']\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1048570    0\n1048571    0\n1048572    0\n1048573    0\n1048574    0\nName: is_fraud, Length: 1048575, dtype: int64\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\nNameError: name 'sklearn' is not defined\n\n\n\ndf50\n\nNameError: name 'df50' is not defined\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-02-CTGAN-TOY.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-02-CTGAN-TOY.html",
    "title": "[CTGAN] CTGAN ToyEX",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\n\n\ndata\n\nx1 = np.random.randn(1000).tolist()\ny1 = ['A']*1000\nx2 = (np.random.randn(100)+5).tolist()\ny2 = ['B']*100\n\n\ndf = pd.DataFrame({'x':x1+x2, 'y':y1+y2})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-0.905509\nA\n\n\n1\n0.844968\nA\n\n\n2\n-0.374190\nA\n\n\n3\n-1.311042\nA\n\n\n4\n-1.657453\nA\n\n\n...\n...\n...\n\n\n1095\n4.890136\nB\n\n\n1096\n6.158887\nB\n\n\n1097\n4.367205\nB\n\n\n1098\n4.704626\nB\n\n\n1099\n4.428676\nB\n\n\n\n\n1100 rows × 2 columns\n\n\n\n\nplt.hist(df.x,bins=50)\n\n(array([ 3.,  0.,  1.,  3.,  7.,  9., 20., 26., 43., 39., 44., 46., 72.,\n        76., 78., 69., 80., 89., 59., 49., 59., 48., 27., 22., 12.,  5.,\n         8.,  3.,  2.,  1.,  1.,  4.,  1.,  4.,  3.,  8.,  9.,  7., 11.,\n         6.,  6.,  6.,  8.,  8.,  6.,  4.,  5.,  2.,  0.,  1.]),\n array([-3.13891042, -2.93493634, -2.73096226, -2.52698818, -2.3230141 ,\n        -2.11904002, -1.91506594, -1.71109186, -1.50711778, -1.3031437 ,\n        -1.09916962, -0.89519554, -0.69122146, -0.48724738, -0.2832733 ,\n        -0.07929922,  0.12467486,  0.32864894,  0.53262302,  0.7365971 ,\n         0.94057118,  1.14454526,  1.34851934,  1.55249342,  1.7564675 ,\n         1.96044158,  2.16441566,  2.36838974,  2.57236382,  2.7763379 ,\n         2.98031198,  3.18428606,  3.38826014,  3.59223422,  3.7962083 ,\n         4.00018238,  4.20415646,  4.40813053,  4.61210461,  4.81607869,\n         5.02005277,  5.22402685,  5.42800093,  5.63197501,  5.83594909,\n         6.03992317,  6.24389725,  6.44787133,  6.65184541,  6.85581949,\n         7.05979357]),\n &lt;BarContainer object of 50 artists&gt;)\n\n\n\n\n\n\n\nCTGAN\n\n# Names of the columns that are discrete\ndiscrete_columns = ['y']\nctgan = CTGAN(epochs=500) # 겁나많이해야하네?\nctgan.fit(df, discrete_columns)\n\n# Create synthetic data\ndf2 = ctgan.sample(1000)\n\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n\n\n\ndf2.groupby('y').count()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\nA\n364\n\n\nB\n636\n\n\n\n\n\n\n\n\nplt.hist(df.x,bins=50,alpha=0.5,label='real')\nplt.hist(df2.x,bins=50,alpha=0.5,label='syn')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc14a3222c0&gt;\n\n\n\n\n\n\ndf2[df2.y=='A'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)\n\n\n\n\n\n\ndf2[df2.y=='B'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-20-시뮬레이션.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-20-시뮬레이션.html",
    "title": "[SOLAR] EPT + RGCN (시뮬레이션)",
    "section": "",
    "text": "%run 0720.py\n\n\nimport itertools\nimport time\n\n\n# read dataframe \ndf = pd.read_csv('data_eng_230710.csv')\n\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)\nt = df.loc[:,'date']\nregions = list(df.loc[:,'Bukchoncheon':'Gyeongju-si'].columns)\n\nTRAIN_RATIO = 0.8\nT,N = len(t),len(regions) \nLEN_TEST = int(np.floor(T*(1-TRAIN_RATIO)))\nLEN_TR = T - LEN_TEST\n\n\nimport datetime\n\n\nmethod = ['classic','proposed']\nlags = [1,2]\nfilters = [1,2] \nepoch = [1,2] \n\nmodel_name = 'RecurrentGCN' ### 모델변경시 수정필요\n\ncol = ['model', 'method', 'lags', \n       'nof_filters', 'epoch', 'calculation_time',\n       'index_time',\n       'node', 'mse']\ndf = pd.DataFrame(columns=col)\n\n\nfor iteration in range(30):\n    for m,l,f,e in itertools.product(method, lags, filters, epoch):\n        if m == 'classic':\n            t1 = time.time()\n            model = RecurrentGCN(node_features=l, filters=f) ### 모델변경시 수정필요\n            yhat = rgcn(y, model = model, train_ratio = TRAIN_RATIO, lags=l, epoch=e)\n            yhat[yhat &lt; 0]= 0   \n            t2 = time.time()\n            c = t2-t1\n            t = str(datetime.datetime.now())\n            _df2 = pd.concat([pd.Series(regions),pd.Series(((y - yhat)[LEN_TR:, :] ** 2).mean(axis=0))],axis=1)\n            _df1 = pd.DataFrame([[model_name,m,l,f,e,c,t]]*N)\n            _df = pd.concat([_df1,_df2],axis=1).set_axis(col,axis=1)\n            df = pd.concat([df,_df])\n        else: # proposed \n            t1 = time.time()\n            model = RecurrentGCN(node_features=l, filters=f) ### 모델변경시 수정필요\n            yUhat = rgcn(yU, model = model, train_ratio = 0.8, lags=l, epoch=e)\n            model = RecurrentGCN(node_features=l, filters=f) ### 모델변경시 수정필요\n            yPhat = rgcn(yP, model = model, train_ratio = 0.8, lags=l, epoch=e)\n            yPhat[yPhat &lt; 0] =0 \n            t2 = time.time()\n            c = t2-t1\n            t = str(datetime.datetime.now())\n            _df2 = pd.concat([pd.Series(regions),pd.Series(((y - yhat)[LEN_TR:, :] ** 2).mean(axis=0))],axis=1)\n            _df1 = pd.DataFrame([[model_name,m,l,f,e,c,t]]*N)\n            _df = pd.concat([_df1,_df2],axis=1).set_axis(col,axis=1)\n            df = pd.concat([df,_df])\n\n2/2\n\n\n\ndf.groupby('index_time').agg({'mse':np.mean}).reset_index().rename({'mse':'mse_mean'},axis=1).merge(df)\n\n\n\n\n\n\n\n\nindex_time\nmse_mean\nmodel\nmethod\nlags\nnof_filters\nepoch\ncalculation_time\nnode\nmse\n\n\n\n\n0\n2023-07-20 17:52:17.128344\n0.395941\nRecurrentGCN\nclassic\n1\n1\n1\n14.640620\nBukchoncheon\n0.451428\n\n\n1\n2023-07-20 17:52:17.128344\n0.395941\nRecurrentGCN\nclassic\n1\n1\n1\n14.640620\nCheorwon\n0.459097\n\n\n2\n2023-07-20 17:52:17.128344\n0.395941\nRecurrentGCN\nclassic\n1\n1\n1\n14.640620\nDaegwallyeong\n0.431739\n\n\n3\n2023-07-20 17:52:17.128344\n0.395941\nRecurrentGCN\nclassic\n1\n1\n1\n14.640620\nChuncheon\n0.392595\n\n\n4\n2023-07-20 17:52:17.128344\n0.395941\nRecurrentGCN\nclassic\n1\n1\n1\n14.640620\nBaengnyeongdo\n0.734857\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n699\n2023-07-20 17:59:51.353652\n0.121436\nRecurrentGCN\nproposed\n2\n2\n2\n49.591688\nUiryeong-gun\n0.125202\n\n\n700\n2023-07-20 17:59:51.353652\n0.121436\nRecurrentGCN\nproposed\n2\n2\n2\n49.591688\nHamyang-gun\n0.108885\n\n\n701\n2023-07-20 17:59:51.353652\n0.121436\nRecurrentGCN\nproposed\n2\n2\n2\n49.591688\nGwangyang-si\n0.118268\n\n\n702\n2023-07-20 17:59:51.353652\n0.121436\nRecurrentGCN\nproposed\n2\n2\n2\n49.591688\nCheongsong-gun\n0.130977\n\n\n703\n2023-07-20 17:59:51.353652\n0.121436\nRecurrentGCN\nproposed\n2\n2\n2\n49.591688\nGyeongju-si\n0.093857\n\n\n\n\n704 rows × 10 columns"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-03-일사량자료정리.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-03-일사량자료정리.html",
    "title": "[SOLAR] 일사량자료정리",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndf0 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data0.csv', encoding='cp949') # 2021-01-01 ~ 2021-12-31\ndf1 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data1.csv') # 2022-01-01 ~ 2023-12-31\ndf2 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/test_raw.csv', encoding='cp949') # 2023-01-01 ~ 2023-01-15\n\n- df_raw\n\ndf_raw = pd.concat([df0, df1])\ndf_raw\n\n\n\n\n\n\n\n\n지점\n지점명\n일시\n일사(MJ/m2)\n\n\n\n\n0\n93\n북춘천\n2021-01-01 08:00\n0.00\n\n\n1\n93\n북춘천\n2021-01-01 09:00\n0.37\n\n\n2\n93\n북춘천\n2021-01-01 10:00\n0.96\n\n\n3\n93\n북춘천\n2021-01-01 11:00\n1.40\n\n\n4\n93\n북춘천\n2021-01-01 12:00\n1.72\n\n\n...\n...\n...\n...\n...\n\n\n229672\n283\n경주시\n2022-12-31 14:00:00\n1.82\n\n\n229673\n283\n경주시\n2022-12-31 15:00:00\n1.52\n\n\n229674\n283\n경주시\n2022-12-31 16:00:00\n0.96\n\n\n229675\n283\n경주시\n2022-12-31 17:00:00\n0.35\n\n\n229676\n283\n경주시\n2022-12-31 18:00:00\n0.01\n\n\n\n\n444720 rows × 4 columns\n\n\n\n- 지점칼럼 삭제 // 일시 \\(\\to\\) 날짜,시간 으로 분리\n\ndf_temp = df_raw.assign(날짜= list(map(lambda x: x[:10],df_raw['일시'])))\\\n.assign(시간= list(map(lambda x: x[11:16],df_raw['일시'])))\\\n.drop(['일시','지점'],axis=1).rename({'일사(MJ/m2)':'일사'},axis=1).reset_index(drop=True)\ndf_temp\n\n\n\n\n\n\n\n\n지점명\n일사\n날짜\n시간\n\n\n\n\n0\n북춘천\n0.00\n2021-01-01\n08:00\n\n\n1\n북춘천\n0.37\n2021-01-01\n09:00\n\n\n2\n북춘천\n0.96\n2021-01-01\n10:00\n\n\n3\n북춘천\n1.40\n2021-01-01\n11:00\n\n\n4\n북춘천\n1.72\n2021-01-01\n12:00\n\n\n...\n...\n...\n...\n...\n\n\n444715\n경주시\n1.82\n2022-12-31\n14:00\n\n\n444716\n경주시\n1.52\n2022-12-31\n15:00\n\n\n444717\n경주시\n0.96\n2022-12-31\n16:00\n\n\n444718\n경주시\n0.35\n2022-12-31\n17:00\n\n\n444719\n경주시\n0.01\n2022-12-31\n18:00\n\n\n\n\n444720 rows × 4 columns\n\n\n\n- 파주,상주,동두천,충주,제천은 삭제\n\ndf_temp = df_temp.query(\"지점명 not in ['파주','상주','동두천','충주','제천']\").reset_index(drop=True)\ndf_temp\n\n\n\n\n\n\n\n\n지점명\n일사\n날짜\n시간\n\n\n\n\n0\n북춘천\n0.00\n2021-01-01\n08:00\n\n\n1\n북춘천\n0.37\n2021-01-01\n09:00\n\n\n2\n북춘천\n0.96\n2021-01-01\n10:00\n\n\n3\n북춘천\n1.40\n2021-01-01\n11:00\n\n\n4\n북춘천\n1.72\n2021-01-01\n12:00\n\n\n...\n...\n...\n...\n...\n\n\n420955\n경주시\n1.82\n2022-12-31\n14:00\n\n\n420956\n경주시\n1.52\n2022-12-31\n15:00\n\n\n420957\n경주시\n0.96\n2022-12-31\n16:00\n\n\n420958\n경주시\n0.35\n2022-12-31\n17:00\n\n\n420959\n경주시\n0.01\n2022-12-31\n18:00\n\n\n\n\n420960 rows × 4 columns\n\n\n\n- 시간이 비어있지 않도록..\n\nreg = df_temp['지점명'].unique().tolist() \nday = df_temp['날짜'].unique().tolist() \ntime = list(df_temp['시간'].unique())\ntime = ['0{}:00'.format(i) for i in range(0,8)] + time\n\n\ndf_temp2 = pd.DataFrame(itertools.product(reg,day,time)).rename({0:'지점명',1:'날짜',2:'시간'},axis=1).merge(df_temp,how='left').fillna(0)\ndf_temp2\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.0\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.0\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.0\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.0\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.0\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.0\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.0\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.0\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.0\n\n\n\n\n803000 rows × 4 columns\n\n\n\n\ndf_temp2[:20]\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.00\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.00\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.00\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.00\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.00\n\n\n5\n북춘천\n2021-01-01\n05:00\n0.00\n\n\n6\n북춘천\n2021-01-01\n06:00\n0.00\n\n\n7\n북춘천\n2021-01-01\n07:00\n0.00\n\n\n8\n북춘천\n2021-01-01\n08:00\n0.00\n\n\n9\n북춘천\n2021-01-01\n09:00\n0.37\n\n\n10\n북춘천\n2021-01-01\n10:00\n0.96\n\n\n11\n북춘천\n2021-01-01\n11:00\n1.40\n\n\n12\n북춘천\n2021-01-01\n12:00\n1.72\n\n\n13\n북춘천\n2021-01-01\n13:00\n1.84\n\n\n14\n북춘천\n2021-01-01\n14:00\n1.74\n\n\n15\n북춘천\n2021-01-01\n15:00\n1.30\n\n\n16\n북춘천\n2021-01-01\n16:00\n0.93\n\n\n17\n북춘천\n2021-01-01\n17:00\n0.29\n\n\n18\n북춘천\n2021-01-01\n18:00\n0.01\n\n\n19\n북춘천\n2021-01-01\n19:00\n0.00\n\n\n\n\n\n\n\n\ndf_temp2[-20:]\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n802980\n경주시\n2022-12-31\n05:00\n0.00\n\n\n802981\n경주시\n2022-12-31\n06:00\n0.00\n\n\n802982\n경주시\n2022-12-31\n07:00\n0.00\n\n\n802983\n경주시\n2022-12-31\n08:00\n0.02\n\n\n802984\n경주시\n2022-12-31\n09:00\n0.41\n\n\n802985\n경주시\n2022-12-31\n10:00\n1.05\n\n\n802986\n경주시\n2022-12-31\n11:00\n1.52\n\n\n802987\n경주시\n2022-12-31\n12:00\n1.86\n\n\n802988\n경주시\n2022-12-31\n13:00\n1.93\n\n\n802989\n경주시\n2022-12-31\n14:00\n1.82\n\n\n802990\n경주시\n2022-12-31\n15:00\n1.52\n\n\n802991\n경주시\n2022-12-31\n16:00\n0.96\n\n\n802992\n경주시\n2022-12-31\n17:00\n0.35\n\n\n802993\n경주시\n2022-12-31\n18:00\n0.01\n\n\n802994\n경주시\n2022-12-31\n19:00\n0.00\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.00\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.00\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.00\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.00\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.00\n\n\n\n\n\n\n\n\ndf_temp2\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.0\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.0\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.0\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.0\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.0\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.0\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.0\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.0\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.0\n\n\n\n\n803000 rows × 4 columns\n\n\n\n- 시간,날짜 \\(\\to\\) 일시\n\ndf_temp3=df_temp2.assign(일시 = list(map(lambda x,y: x+'-'+y,df_temp2['날짜'],df_temp2['시간'])))\\\n.drop(['날짜','시간'],axis=1)\ndf_temp3\n\n\n\n\n\n\n\n\n지점명\n일사\n일시\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n- 저장\n\ndf_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\n\n\n\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n\ndf = df_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\ndf.to_csv(\"solar_radiation.csv\",index=False)\n!git add .\n!git commit -m .\n!git push \n\n[main 299d058] .\n 3 files changed, 806273 insertions(+)\n create mode 100644 \"posts/3_Researches/SOLAR/.ipynb_checkpoints/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211-checkpoint.ipynb\"\n create mode 100644 \"posts/3_Researches/SOLAR/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211.ipynb\"\n create mode 100644 posts/3_Researches/SOLAR/solar_radiation.csv\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (7/7), done.\nWriting objects: 100% (7/7), 8.74 KiB | 8.74 MiB/s, done.\nTotal 7 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/miruetoto/yechan3.git\n   495d9ce..299d058  main -&gt; main\n\n\n- 불러오기\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv\")\ndf\n\n\n\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n- 다운로드\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "title": "[PINKOCTO] 커널리그레션",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nx = np.linspace(0,1,5)\nx\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\n\ny = x*2 + np.random.randn(5)*0.3\ny\n\narray([0.01044984, 0.93755458, 0.43942577, 1.0639859 , 2.1133726 ])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\n\n\n\n\n\\[y_i= \\sum_{i=1}^{5}\\theta_i\\exp\\left(-\\frac{|x-x_i|^2}{2h^2}\\right)\\]\n\nh= 0.15\nx0 = lambda xstar: np.exp(-(xstar-x[0])**2 / 2 / (h**2))\nx1 = lambda xstar: np.exp(-(xstar-x[1])**2 / 2 / (h**2))\nx2 = lambda xstar: np.exp(-(xstar-x[2])**2 / 2 / (h**2))\nx3 = lambda xstar: np.exp(-(xstar-x[3])**2 / 2 / (h**2))\nx4 = lambda xstar: np.exp(-(xstar-x[4])**2 / 2 / (h**2))\n\n\nθ0 = 0\nθ1 = 0.2\nθ2 = 0.5\nθ3 = 1.0\nθ4 = 1.5\n\n\nxstar = 0.6 \n\n\nθ0*x0(0.6)+θ1*x1(0.6)+θ2*x2(0.6)+θ3*x3(0.6)+θ4*x4(0.6)\n\n1.062893318071169\n\n\n\n_yhat = lambda xstar : θ0*x0(xstar)+\\\nθ1*x1(xstar)+\\\nθ2*x2(xstar)+\\\nθ3*x3(xstar)+\\\nθ4*x4(xstar)\n\n\n(_yhat(0)-y[0])**2\n\n0.0017104251892816106\n\n\n\nloss = (_yhat(0)-y[0])**2 + (_yhat(0.25)-y[1])**2+ \\\n(_yhat(0.5)-y[2])**2+(_yhat(0.75)-y[3])**2+\\\n(_yhat(1)-y[4])**2\n\n\\(loss(\\theta_0,\\theta_1,\\theta_2,\\theta_3,\\theta_4)\\) 를 최소화하는 \\({\\boldsymbol \\theta}\\)를 구한다.\n\n_x = np.linspace(0,1,1000)\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\nplt.plot(_x,_yhat(_x))"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "이론 및 예시",
    "text": "이론 및 예시\n- 이론: real-valued signal은 무조건 \\(|X[k]|^2\\)의 그래프가 대칭으로 나옴 (단, \\(X[0]\\)은 제외)\n- 예시1:\n\nx = np.array([1,2,3,4,5])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시2:\n\nx = np.array([1,2,3,-3,-2,-1])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시3: \\({\\bf x}\\)가 복소수일 경우는 첫항을 제외하고 대칭이 되지 않음\n\nx = np.array([1+1j,2+2j,3+3j,-3-3j,-2-2j,1-1j]) \nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?",
    "text": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?\n- 예비학습1\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\cos(2\\pi \\alpha) =\\cos(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.cos(2*np.pi*alpha),np.cos(2*np.pi*(1-alpha))\n\n(0.30901699437494745, 0.30901699437494723)\n\n\n\n그래프를 잘 그려보세여\n\n- 예비학습2\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\sin(2\\pi \\alpha) = -\\sin(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.sin(2*np.pi*alpha),np.sin(2*np.pi*(1-alpha))\n\n(0.9510565162951535, -0.9510565162951536)\n\n\n\n그래프를 잘 그려보세여\n\n- 왜 실수일경우만 대칭인지? (어디 정리된걸 아무리 찾아도 못찾겠어서 그냥 직접 수식을 썼는데요, 이걸 기억할 필요는 없어요.. 아마 제가 쓴것보다 쉽게 설명하는 방법이 있을겁니다)\n(해설) \\(k=0,1,2,\\dots,N-1\\)에 대하여 \\(X[k]\\)는 아래와 같이 표현가능하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]e^{-\\frac{j2\\pi kn}{N}}\\]\n오일러공식을 사용하면 아래와 같이 정리할 수 있다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(-\\frac{2\\pi kn}{N}\\right) + i \\sum_{n=0}^{N-1}x[n]\\sin\\left(-\\frac{2\\pi kn}{N}\\right)\\]\ncos은 짝함수, sin은 홀함수임을 이용하여 다시정리하면\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\ncase1 \\(k=1\\) 인 경우와 \\(k=N-1\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음을 보이자.\n\\[X[1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi n}{N}\\right)\\]\n\\[X[N-1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\]\n여기에서 예비학습1,2를 떠올리면 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 임을 알 수 있다. 따라서 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수이다. 즉\n\\[X[1] = X[N-1]^\\ast, \\quad X[1]^\\ast = X[N-1]\\]\n이다. 그런데 임의의 복소수 \\(x=a+bi\\)에 대하여 \\(|x|^2 = a^2+b^2 = (a+bi)(a-bi)= x x^\\ast\\) 가 성립하므로\n\\[|X[1]|^2 = |X[N-1]|^2\\]\n이 성립한다.\n\n만약에 \\(x[n]\\)이 실수가 아닌경우는 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 이라고 하여도 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수라고 주장할수 없다.\n\ncase2 \\(k=2\\) 인 경우와 \\(k=N-2\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음도 비슷한 논리로 보일 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "허수파트 해석",
    "text": "허수파트 해석\n관찰1: 모든 \\(k\\)에 대하여 \\(X[k]\\)의 허수파트는 항상 0이다.\nk=0\n\nk=0\nsin_part_0 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_0\n\narray([ 0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n        0.,  0.])\n\n\n\nsum(x*sin_part_0)\n\n0.0\n\n\nk=1\n\nk=1\nsin_part_1 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_1\n\narray([ 0.        ,  0.37157241,  0.49726095,  0.29389263, -0.10395585,\n       -0.4330127 , -0.47552826, -0.20336832,  0.20336832,  0.47552826,\n        0.4330127 ,  0.10395585, -0.29389263, -0.49726095, -0.37157241])\n\n\n\nsum(x*sin_part_1)\n\n1.0547118733938987e-15\n\n\n약간을 직관을 위해서 그림을 그려보자.\n\nplt.plot(x,'--o')\nplt.plot(sin_part_1,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 \\(\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\)에 대응하는 식은 \\(\\int_0^{2\\pi}\\cos(t)\\sin(t)dt\\)라고 볼 수 있어서 sum(x*sin_part_1)=0임을 더 쉽게 이해할 수 있다.\nk=2\n\nk=2\nsin_part_2 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(sin_part_2,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\sin(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nsum(x*sin_part_2)\n\n-1.2212453270876722e-15\n\n\n\\(\\cos(t)\\)는 임의의 \\(\\sin(kt)\\)와 항상 직교하므로, 임의의 \\(k\\)에 대하여 허수파트는 항상 0이다.\n따라서 이 경우 \\(X[k]\\)는 아래와 같이 써도 무방하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right)\\]"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "실수파트 해석",
    "text": "실수파트 해석\n관찰2: \\(X[k]\\)의 실수파트는 \\(k=1\\)혹은 \\(k=N-1\\)일때 아래와 같이 정리된다.\n\\[\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)^2\\]\n그외의 경우에는 아래와 같이 된다.\nk=0\n\nk=0\ncos_part_0 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_0,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(1\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nk=2\n\nk=2\ncos_part_2 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_2,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\cos(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\n임의의 \\(k\\)에 대하여 \\(\\cos(t)\\)와 \\(\\cos(kt)\\)는 항상 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n- 요약: 만약에 \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\]\n이때 퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=1,N-1\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)\n\n\n\n\n- 응용: \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{6\\pi n}{N} \\right)\\]\n퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=3,N-3\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nN = 15 \nx = np.array([np.cos(6*np.pi*n/N) for n in range(N)]) \nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "title": "[PINKOCTO] 퓨리에변환4jy",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n회귀모형 (1)\n\nx = np.linspace(-10,10,1000)\nx0 = x*0+1\nx1 = x \nbeta0 = 3 \nbeta1 = 2\ny = x0*beta0+x1*beta1+np.random.randn(1000)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n회귀모형 (2)\n- 관측한자료\n\nN=1000\nx=np.linspace(0,1,N)\neps = np.random.randn(N)\nX0 = np.sin(x*0*np.pi)\nX1 = np.sin(x*2*np.pi)\nX2 = np.sin(x*4*np.pi)\nX3 = np.sin(x*6*np.pi)\n\ny=2*X1+1*X2+3*X3+eps\n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n\nobserved signal\n\n- 위의 자료를 해석하는 방법\n\ndef spec(y):\n    N= len(y)\n    return abs(np.fft.fft(y)/N)*2 \n\n\ny=2*X1+1*X2+3*X3+eps\nyfft =spec(y) \ny1=2*X1\ny2=1*X2\ny3=3*X3\nyfft1=spec(y1)\nyfft2=spec(y2)\nyfft3=spec(y3)\nepsfft=spec(eps)\n\n\nplt.plot(yfft[:20],'o',alpha=0.5)\nplt.plot(yfft1[:20],'x',alpha=1,)\nplt.plot(yfft2[:20],'x',alpha=1)\nplt.plot(yfft3[:20],'x',alpha=1)\nplt.plot(epsfft[:20],'x',alpha=1)\n\n\n\n\n- 퓨리에변환 -&gt; threshold -&gt; 역퓨리에변환을 이용한 스킬\n\nyfft=np.fft.fft(y)\n\n\nplt.plot(abs(yfft[1:50]),'o')\n\n\n\n\n\nyfft[abs(yfft)&lt;100] = 0\n\n\nplt.plot(y,'o',alpha=0.1)\nyhat=np.fft.ifft(yfft)\nplt.plot(yhat,'--')\nplt.plot(y-eps,'-')\n\n\n\n\n\nplt.plot(spec2(y)[:50],'o')\nplt.plot(spec2(yhat)[:50],'x')\n\n\n\n\n\n\n삼성전자 주가자료를 스무딩해보기\n- 삼성전자 자료\n\nimport yfinance as yf\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ny = yf.download(\"005930.KS\", start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nplt.plot(y)\n\n\n\n\n- 스펙트럼\n\nyfft = np.fft.fft(y)\n\n\nplt.plot(abs(yfft))\n\n\n\n\n- 처음 50개정도만 관찰\n\nplt.plot(abs(yfft[:50]),'o')\n\n\n\n\n\n첫값이 너무커서 나머지는 잘안보임\n\n- 2번째부터 50번째까지만 관찰\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=22500, color='r', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f6250d5bfd0&gt;\n\n\n\n\n\n\n대충 이정도 짜르면 될것같음\n\n- thresholded value\n\ntresh_value = 22500\n\n\nyfft[abs(yfft)&lt;tresh_value] =0 \n\n- 퓨리에역변환\n\nyhat = np.fft.ifft(yfft)\nyhat[:5]\n\narray([59664.72193044+8.87311904e-14j, 58572.98839934+8.87311904e-14j,\n       58066.07369126+3.39894326e-14j, 58169.18671667-6.87747670e-14j,\n       58706.41986821-1.14383435e-13j])\n\n\n실수화\n\nyhat = np.real(yhat)\nyhat[:5]\n\narray([59664.72193044, 58572.98839934, 58066.07369126, 58169.18671667,\n       58706.41986821])\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat,'--')\n\n\n\n\n- 숙제: treshold value를 관찰하며 시각화해볼것\n\n\nminor topics\n- y의 FFT 결과는 항상 y와 같은길이임\n\nlen(y)\n\n82\n\n\n\nlen(np.fft.fft(y))\n\n82\n\n\n- 에일리어싱: number of observation은 얼마나 세밀한 주파수까지 측정가능하냐를 결정함\n예시1: 에일리어싱\n\nx = np.linspace(-3.14,3.14,10)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.        , -0.99975131],\n       [-0.99975131,  1.        ]])\n\n\n\nplt.plot(x1,label='x1')\nplt.plot(x2,label='x2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6252112ee0&gt;\n\n\n\n\n\n\n실제로는 x2가 더 고주파인데, 같은 주파수처럼 보임\n\n예시2: 에일리어싱이 없는 경우\n\nx = np.linspace(-3.14,3.14,100000)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.00000000e+00, -6.45767105e-08],\n       [-6.45767105e-08,  1.00000000e+00]])\n\n\n\nplt.plot(x1)\nplt.plot(x2)\n\n\n\n\n\n주파수 왜곡떄문에 실제로는 corr ceof = 0 일지라도 관측되는건 corr coef &gt;0 일 수 있음"
  },
  {
    "objectID": "posts/4_Notes/2000-01-08-깃(Git).html",
    "href": "posts/4_Notes/2000-01-08-깃(Git).html",
    "title": "[Note] 깃(Git)",
    "section": "",
    "text": "clone\ngithub repository \\(\\to\\) code \\(\\to\\) clone tab, ssh를 복사 (git@github.com:miruetoto/yechan.git처럼 생김)\n터미널에서 아래를 입력\ngit clone git@github.com:miruetoto/yechan.git 01_yechan\n\n\npull\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력한다.\ngit pull\n\n\nbranch\n- 서버에 이미 guebin이라는 브랜치가 있다면 아래와 같이 동기화 시킨다.\ngit chechout guebin\ngit push -u origin guebin\n여기에서 git push -u origin guebin을 안해도 동기화가 잘될때도 있는데 아닐때도 있다.\n\n\nremote\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력하면 깃허브의 url 주소를 확인할 수 있다.\n(base) lgcgb2@lgcgb2:~/Documents/GitHub/miruetoto.github.io$ git remote -v\norigin https://github.com/miruetoto/miruetoto.github.io.git (fetch)\norigin https://github.com/miruetoto/miruetoto.github.io.git (push)\nupstream https://github.com/daattali/beautiful-jekyll.git (fetch)\nupstream https://github.com/daattali/beautiful-jekyll.git (push)\n\n\nconfig\n- 설정보기\ngit config —list \n- 설정삭제\ngit config --unset user.name\ngit config --unser user.email\n- 전역설정삭제\ngit config --unset --global user.name\ngit config --unset --global user.email\n- 중복값 설정삭제\ngit config --unset-all user.name\ngit config --unset-all user.email\n- 중복값 전역으로 설정삭제\ngit config --unset-all --global user.name\ngit config --unset-all --global user.email\n- 비번안치고 푸쉬하는법?\ngit config credential.helper store\n입력이후에 git push\n\n\nGit token\nhttps://github.com/settings/tokens 에서 확인가능\n\nAppendix\n\n리눅스에서 github desktop 설치\n\n여기로 간다.\n한 챕터의 (2.3.1 Linux RC1 와 같이 되어있음) 아래쪽에 보면 ▶ Assets 라고 되어있는데 이걸 클릭하면 다운받을 수 있는 파일들이 나온다. 확장자가 .deb로 끝나는걸 골라서 다운받은뒤에 실행한다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-06-주피터랩- 설정 및 몇가지 팁.html",
    "href": "posts/4_Notes/2000-01-06-주피터랩- 설정 및 몇가지 팁.html",
    "title": "[Note] 주피터랩: 설정 및 몇가지 팁",
    "section": "",
    "text": "주피터에 R커널을 연결할 경우 그림크기 조정\noptions(repr.plot.width=10, repr.plot.height=3,repr.plot.res=300)\n\n\n깃허브에서 *.py파일 불러오기\nimport requests\nexec(requests.get('http://miruetoto.github.io/my_code/datahandling.py').text)\n\n\nrpy2 magic\nimport rpy2\n%load_ext rpy2.ipython\n\n\n깃허브에서 *.R파일 불러오기\nimport rpy2\n%load_ext rpy2.ipython\n%R library(devtools)\n%R source_url(\"http://miruetoto.github.io/my_code/datahandling.r\")\n\n\nmatplotlib 그림크기조정\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \nIpython_default=plt.rcParams.copy() # save initial value \nfrom matplotlib import cycler\nplt.rc('figure',dpi=150) # default value 4 figure.dpi is 72.0 \n# plt.rcParams.update(Ipython_default) # load initial value \n\n\nGPU 사용여부 체크\nfrom keras import backend as K\nprint('GPU check 4 Keras: '+ str(K.tensorflow_backend._get_available_gpus()))\nimport torch\nprint('GPU check 4 Pytorch: '+ str(torch.cuda.get_device_name(0)))\n\n\n깃랩관련 (회사아니면 필요없음)\n- load *.py from gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\n\n# (1) load RF.py, RF_withGIT.py, RF_withR.py\nRF_py = project.files.get(file_path='modeling/RF.py', ref='fridge').decode()\nRF_GIT_py = project.files.get(file_path='utils/RF_withGIT.py', ref='fridge').decode()\nRF_R_py = project.files.get(file_path='utils/RF_withR.py', ref='fridge').decode()\nexec(str(RF_py, 'utf-8'))\nexec(str(RF_GIT_py, 'utf-8'))\nexec(str(RF_R_py, 'utf-8'))\n- load *.R in gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\nRF_R_rcode = project.files.get(file_path='utils/RF_Rfunctions.r', ref='fridge').decode()\n# tricks for source('Rfunctions.r')\nfile1 = open(\"RF_Rfunctions.r\",\"w\") \nfile1.write(str(RF_R_rcode, 'utf-8'))\nfile1.close() \nro.r(\"source('RF_Rfunctions.r')\")\nimport os\nos.remove('RF_Rfunctions.r')\n\n\n& 옵션으로 주피터 실행\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n아래와 같이 끝에 &을 붙이면 된다.\nconda activate py20190129\njupyter lab &\n실행하고 난뒤에는 엔터를 쳐서 빠져나온다. 이렇게 하면 서버자체에 모니터를 연결하고 커널창을 띄운것과 같은 효과를 준다. 즉 서버에 접속한 컴퓨터를 끄는것과 상관없이 서버에서는 항상 주피터가 열려 있게 된다.\n\n\n& 옵션으로 실행한 주피터프로세스 죽이기\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n실행된 프로세스를 찾기위해 아래를 실행한다.\nps aux | grep jupyter-lab\n결과는 아래와 같이 나온다.\nlgcgb    26888  0.2  0.1 326760 86724 ?        Sl   10:14   0:12 /home/lgcgb/anaconda3/envs/py20190129/bin/python3.7 /home/lgcgb/anaconda3/envs/py20190129/bin/jupyter-lab\nlgcgb    27146  0.0  0.0  15720  1008 pts/3    S+   11:56   0:00 grep --color=auto jupyter-lab\n26888에 해당하는 것이 주피터를 띄운 커널이다. 이 번호를 기억했다가 프로세스를 아래와 같은 명령으로 죽인다.\nkill 26888\n\n\n패스워드 없이 주피터 실행\n- 아래와 같이 하면 외부에서 접속할때 패스워드를 입력하지 않음.\njupyter lab --LabApp.token='' --LabApp.password=''\njupyter notebook --NotebookApp.token='' --NotebookApp.password=''"
  },
  {
    "objectID": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "href": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "title": "[PL] Lesson1: 단순선형회귀",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport pytorch_lightning as pl \n\n\nref\nref: https://guebin.github.io/DL2022/posts/II.%20DNN/2022-09-20-3wk-2.html\n\n\nRegression 1: CPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1)\n\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1767: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n  category=PossibleUserWarning,\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nRegression 2: GPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1, accelerator='gpu', devices=1)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "href": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "title": "[CGSP] Chap 12.4: Node Subsampling for PSD Estimation",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics\n\n\ncolumnwise_kron = \n(C,D) -&gt; hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#49 (generic function with 1 method)\n\n\n\n12.4.1 The Sampling Problem\n아래와 같이 길이가 \\(N=10\\) 인 신호 \\({\\bf x}\\)를 고려하자.\n\nx = rand(10)\n\n10-element Vector{Float64}:\n 0.03235208758206609\n 0.5069925854414447\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n 0.24116013388795854\n 0.8439116925218157\n 0.6362602319916778\n 0.386069828675059\n 0.5313655894235898\n\n\n여기에서 1,3,4,5 번째 원소만 추출하여길이가 \\(K=4\\) 인 신호 \\({\\bf y}\\)를 만들고 싶다.\n\ny = x[[1,3,4,5]]\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n이 과정은 아래와 같이 수행할 수도 있다.\n\nΦ= [1 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0]\n\n4×10 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0\n 0  0  0  1  0  0  0  0  0  0\n 0  0  0  0  1  0  0  0  0  0\n\n\n\nΦ*x\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n즉 적당한 \\(K\\times N\\) selection matrix를 선언하여 subsampling을 수행할 수 있다. 이때 매트릭스 \\({\\bf \\Phi}\\)를 subsampling matrix 혹은 sparse sampling matrix 라고 부른다.\n\n\n12.4.2 Compressed LS Estimator\n\nN = 10\nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x) \n\n10×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n\n\n\nG = columnwise_kron(conj(V),V)\n\n100×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im   0.809017-0.587785im     …   0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n    ⋮                                ⋱  \n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.809017+0.587785im     …   0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0-1.11022e-16im          -1.0+2.27596e-15im\n 1.0+0.0im  -0.809017-0.587785im     …  -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n\n\n- 방법1\n\nĉx = vec(x*x')\np̂ = inv(G' * G) * G' * ĉx\n\n10-element Vector{ComplexF64}:\n    0.25854107856772546 + 2.245922875954761e-20im\n   0.004743491121735806 - 1.3138893409553828e-18im\n   0.006946482731189413 - 9.791191432641327e-19im\n   0.001721693617954179 - 1.9827974128203887e-18im\n   0.011344167525098774 + 2.6827005818057562e-19im\n 0.00012662617844242917 - 3.748573865136995e-20im\n   0.011344167525098762 + 2.7448152053954017e-18im\n  0.0017216936179541913 - 9.35534609073096e-19im\n   0.006946482731189404 + 1.954408900185458e-18im\n   0.004743491121735756 - 2.561030398375897e-18im\n\n\n- 방법2\n\nĉy = vec(y*y')\np̂ = (kron(Φ,Φ)*G)' * ĉy\n\n10-element Vector{ComplexF64}:\n   3.759462826821233 + 0.0im\n   2.765185174577697 - 2.0816681711721685e-17im\n   1.077337414764992 + 2.7755575615628914e-17im\n 0.11594812606807317 + 2.0816681711721685e-17im\n 0.08838298603932843 + 3.903127820947816e-17im\n 0.32863702713833354 + 4.622231866529366e-33im\n 0.08838298603932859 + 9.540979117872439e-18im\n  0.1159481260680729 - 2.0816681711721685e-17im\n  1.0773374147649915 + 0.0im\n  2.7651851745776965 - 2.0816681711721685e-17im"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "using LinearAlgebra, FFTW"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Cyclic shfit operator \\({\\bf B}\\)",
    "text": "Cyclic shfit operator \\({\\bf B}\\)\nThe matrix \\({\\bf B}\\) representing the periodic shift is\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0]\n\n5×5 Matrix{Int64}:\n 0  0  0  0  1\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n\n\nThis matrix is the cyclic shift.\nnote: \\({\\bf B}\\) is orthogonal matrix.\n\nB'B\n\n5×5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n(ex1) Define \\({\\bf s}\\) as\n\ns = [1,2,3,4,5]\ns\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nObserve that\n\nB*s\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\n\nB^3*s\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 2\n\n\nThus we can interprete the matrix \\({\\bf B}\\) as cyclic shift operator such that\n\\[\n{\\bf B}s_n =s_{n-1}\n\\]\nfor \\(n=1,\\dots, N-1\\) and \\({\\bf B}s_0 =s_N\\).\nnote: \\({\\bf B}\\)는 시계열에서 다루는 backshift operator 와 비슷함."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\nThe matrix \\({\\bf B}\\) can be expressed as\n\\({\\bf B}={\\bf DFT}^\\ast \\cdot {\\bf \\Lambda} \\cdot {\\bf DFT}\\)\nwhere \\({\\bf DFT}\\) is unitary and symmetric matrix and \\(\\bf \\Lambda\\) is diagonal matrix.\n\nλ, Ψ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5×5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  …  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB ≈ Ψ * Diagonal(λ) * Ψ'\n\ntrue\n\n\nDefine \\({\\boldsymbol \\Psi}^\\ast={\\bf DFT}\\).\n\nDFT = Ψ'\n\n5×5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  …  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\nNote that the eigenvalues are not ordered in julia.\n\nλ[5], exp(-im* 2π/5 * 0)\n\n(0.9999999999999998 + 0.0im, 1.0 - 0.0im)\n\n\n\nλ[3], exp(-im* 2π/5 * 1)\n\n(0.30901699437494734 - 0.9510565162951536im, 0.30901699437494745 - 0.9510565162951535im)\n\n\n\nλ[1], exp(-im* 2π/5 * 2)\n\n(-0.8090169943749472 - 0.5877852522924725im, -0.8090169943749473 - 0.5877852522924732im)\n\n\n\nλ[2], exp(-im* 2π/5 * 3)\n\n(-0.8090169943749472 + 0.5877852522924725im, -0.8090169943749475 + 0.587785252292473im)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral components and Frequencies",
    "text": "Spectral components and Frequencies\nWe remark:\n(1) Spectral components: For \\(k = 0,1,2,\\dots, N-1\\), the \\(k\\)-th column of \\({\\bf DFT}^\\ast\\) is defined by\n\\[\\Psi_k:=\\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 \\\\ e^{j\\frac{2\\pi}{N}k} \\\\ e^{j\\frac{2\\pi}{N}2k} \\\\ e^{j\\frac{2\\pi}{N}3k} \\\\  \\dots \\\\ e^{j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}.\\]\nNote that \\(\\Psi_k\\) can be also interpreted as \\(\\ell\\)-th eigenvector of \\({\\bf A}\\) correspoding \\(\\lambda_\\ell = e^{-j\\frac{2\\pi}{N}k}\\). Those eigenvectors\n\\[\\big\\{{\\bf 1},\\Psi_1,\\Psi_2, \\dots, \\Psi_{N-1}\\big\\}\\]\nform a complete orthonomal basis of \\(\\mathbb{C}^N\\). These vectors are called spectral components.\n(2) Frequencies: The diagonal entries of \\({\\bf \\Lambda}\\) are the eigenvalues of the time shift \\({\\bf B}\\). In Physics and in operator theory, these eigenvalues are the frequencies of the signal. In DSP it is more common to call frequencies\n\\[\\Omega_k=\\frac{-1}{2\\pi j}\\ln\\lambda_k=\\frac{-1}{2\\pi j}\\ln e^{-j \\frac{2\\pi}{N}k}=\\frac{k}{N}, \\quad k=0,1,2,\\dots,N-1.\\]\n\nThe \\(N\\) (time) frequencies \\(\\Omega_k\\) are all distinct, positive, equally spaced, and increasing from \\(0\\) to \\(\\frac{N-1}{N}\\). The spectral components are the complex exponential sinusiodal functions. For example, corresponding to the zero frequency is the DC spectral component (a vector whose entries are constant and all equal to \\(\\frac{1}{\\sqrt{N}}\\))."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft-1",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft-1",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\n일반적으로 우리가 알고있는 DFT1는 아래와 같다. (이 그림은 위키피디아에서 캡쳐한 것이다)\n1 discrete Fourier transform\n\n\n그림1: 위키에서 긁어온 DFT의 정의\n\n\n즉 DFT는 임의의 신호 \\(\\{{\\bf x}_n\\}:=x_0,x_1,\\dots,x_{N-1}\\)를 적당한 규칙2에 따라서 \\(\\{{\\bf X}_k\\}:=X_0,X_1,\\dots,X_{N-1}\\)로 바꾸는 변환을 이라고 이해할 수 있다. 이때 사용되는 적당한 규칙은 구체적으로 아래의 수식을 의미한다.\n2 \\(X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\)\\[X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\]\n그런데 매트릭스를 활용하면 위의 수식을 아래와 같이 표현할 수 있다.\n\\[\\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\dots \\\\ X_{N-1} \\end{bmatrix}\n=\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\dots \\\\ x_{N-1} \\end{bmatrix}\\]\n편의상 \\({\\bf X}\\)와 \\({\\bf x}\\)를 \\(N \\times 1\\) col-vec이라고 생각하고 DFT를 아래와 같은 matrix로 정의하자.\n\\[{\\bf DFT} = \\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n그러면\n\\[{\\bf X} = {\\bf DFT} \\cdot {\\bf x}\\]\n와 같이 표현할 수 있고 \\({\\bf x}\\)에서 \\({\\bf X}\\)로 바꾸는 과정을 단순히 \\({\\bf DFT}\\)행렬을 \\({\\bf x}\\)의 왼쪽에 곱하는 과정으로 이해할 수 있다.\n(참고) 사실 아래와 같이 \\({\\bf DFT}\\)를 정의하는 버전도 있다. (둘이 혼용해서 쓰인다)\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\n예제1 아래는 위키에서 긁어온 예제이다. 이 예제를 따라가보자.\n\n\n\n그림2: 위키에서 긁어온 예제이미지\n\n\n예제를 풀기위해서 우선 아래와 같은 벡터를 선언하다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n(풀이1)\n\\(4\\times 4\\)의 크기를 가지는 DFT행렬을 선언한다.\n(step1) 아래의 매트릭스 생성\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4×4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n(step2) _DFT의 각 원소에 함수 \\(f: x \\to \\exp(-i\\frac{2\\pi}{4}x)\\)를 취함\n\nf = x -&gt; exp(-im * (2π/4) * x)\nDFT = _DFT .|&gt; f\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n이제 \\({\\bf X}\\)를 구하면 아래와 같다.\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n위키의 답이 잘 나옴\n(풀이2)\n참고로 아래와 같이 패키지를 이용하여 구할 수도 있다.\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#inverse-dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#inverse-dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Inverse DFT",
    "text": "Inverse DFT\n앞으로는 \\({\\bf DFT}\\)를 아래와 같이 정의하자.\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\\({\\bf DFT}\\)행렬에는 몇 가지 특징이 있다.\n특징1: 유니터리행렬이다. 즉 \\({\\bf DFT}^\\ast \\cdot {\\bf DFT} = {\\bf DFT}^\\ast \\cdot{\\bf DFT} = {\\bf I}\\) 이다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nf = x -&gt; exp(-im * (2π/4) * x)\nDFT = _DFT .|&gt; f\nDFT # 아까의 예제의 DFT!\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT = (1/√4)*DFT # 새로운 DFT의 정의 \nDFT'DFT .|&gt; round # 유니터리행렬임을 확인!\n\n4×4 Matrix{ComplexF64}:\n  1.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   1.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   1.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   1.0+0.0im\n\n\n특징2: \\({\\bf DFT}\\)는 대칭행렬이다. 따라서 이 행렬의 켤레전치는 DFT의 각 원소에서 단순히 \\(i=\\sqrt{-1}\\) 대신에 \\(-i\\) 를 넣은 것과 같다.\n특징1-2를 조합하면 아래와 같이 \\({\\bf DFT}\\)에서 \\(i\\) 대신에 \\(-i\\)를 넣은 행렬이 변환 DFT를 취소시킬 수 있음을 이해할 수 있다. 3\n3 아래의 행렬은 \\({\\bf DFT}^\\ast\\) 혹은 \\({\\bf DFT}\\)의 conjugate matrix 혹은 \\({\\bf DFT}^{-1}\\)로 생각할 수 있음\\[\\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 1} & e^{i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 2} & e^{i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n행렬 \\({\\bf DFT}\\)를 discrete Fourier transform으로 생각했듯이 위의 행렬을 inverse discrete Fourier transform으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT의 또 다른 정의",
    "text": "DFT의 또 다른 정의\n이번에는 \\({\\bf DFT}\\)에 대한 다른 정의를 생각해보자. 우선 아래와 같은 행렬 \\({\\bf B}\\)를 고려하자.\n\nB= [0 0 0 1 \n    1 0 0 0 \n    0 1 0 0\n    0 0 1 0]\n\n4×4 Matrix{Int64}:\n 0  0  0  1\n 1  0  0  0\n 0  1  0  0\n 0  0  1  0\n\n\n이것은 길이가 4인 임의의 column vector를 아래로 한칸씩 이동시키는 매트릭스이다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\nB*x # 아래로 한칸이동 \n\n4-element Vector{Complex{Int64}}:\n -1 + 2im\n  1 + 0im\n  2 - 1im\n  0 - 1im\n\n\n\nB^2*x # 아래로 두칸이동, B^2*x = B*(Bx) 이므로 \n\n4-element Vector{Complex{Int64}}:\n  0 - 1im\n -1 + 2im\n  1 + 0im\n  2 - 1im\n\n\n한편 이 매트릭스 \\({\\bf B}\\)는 아래와 같이 고유분해가 가능하다.\n\\[ {\\bf B} = {\\bf \\Psi} {\\bf \\Lambda} {\\bf \\Psi}^\\ast\\]\n\n\\({\\bf \\Psi}\\): make \\(\\frac{1}{\\sqrt{N}}[e^{\\sqrt{-1} \\frac{2\\pi}{N} ij}~\\text{ for }~ i=0,1,2,\\dots,N-1~\\text{ for }~j=0,1,2,\\dots,N-1]\\) and apply reshape function with \\((N,N)\\).\n\\({\\bf \\Lambda}\\): make \\([e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}~\\text{ for }~ i=0,1,2\\dots,N-1]\\) and apply Diagonal function.\n\n\nN = 4 \nλ = [exp(-im * (2π/N) *i) for i in 0:(N-1)]\nΛ = Diagonal(λ)\n_Ψ = 1/√N *[exp(im * (2π/N) * i*j) for i in 0:(N-1) for j in 0:(N-1)]\nΨ = reshape(_Ψ, (N,N))\nB ≈ Ψ * Λ * Ψ'\n\ntrue\n\n\n그런데 위에서 정의된 \\({\\bf \\Psi}^\\ast\\)는 우리가 그전에 정의하였던 \\({\\bf DFT}\\)의 행렬과 같다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nDFT = _DFT .|&gt; (x -&gt; exp(-im * (2π/4) * x)) \nDFT = DFT * 1/√N\n\n4×4 Matrix{ComplexF64}:\n 0.5-0.0im           0.5-0.0im          …           0.5-0.0im\n 0.5-0.0im   3.06162e-17-0.5im             -9.18485e-17+0.5im\n 0.5-0.0im          -0.5-6.12323e-17im             -0.5-1.83697e-16im\n 0.5-0.0im  -9.18485e-17+0.5im              2.75546e-16-0.5im\n\n\n\nΨ' == DFT \n\ntrue\n\n\n결국 요약하면 길이가 \\(N\\)인 신호의 \\({\\bf DFT}\\)행렬은 아래의 과정으로 구할 수 있음을 알 수 있다.\n\nForward operator \\({\\bf A}\\)를 정의한다.\n\\({\\bf A}\\)의 고유벡터행렬 \\({\\bf \\Psi}\\)을 구한다. 4\n\\({\\bf \\Psi}\\)의 conjugate transpose matrix \\({\\bf \\Psi}^\\ast\\) 를 구한다. 이것이 \\({\\bf DFT}\\) matrix 이다. 5\n\n4 고유벡터행렬은 고유값 \\(e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}\\)에 의하여 정렬되어 있어야 함.5 사실 이미 대칭행렬이므로 conjugate matrix만 구하면 된다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral component and Frequencies",
    "text": "Spectral component and Frequencies\n\\({\\bf A}\\)의 고유벡터 \\({\\bf \\Psi}\\)의 각 column을 spectral component라고 부른다.\n\nψ₁ = Ψ[:,1] # ψ₁ is first spectral component \nψ₂ = Ψ[:,2] # ψ₂ is seconde spectral component \nψ₃ = Ψ[:,3] # ψ₃ is third spectral component \nψ₄ = Ψ[:,4] # ψ₄ is last spectral component\n\n그리고 아래와 같은 수열을 \\(\\Omega_{k}=\\frac{k}{N}\\)을 frequency 라고 부른다.\n\nN=4 \nΩ = [k/N for k in 0:(N-1)]\nΩ\n\n4-element Vector{Float64}:\n 0.0\n 0.25\n 0.5\n 0.75"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "",
    "text": "ref: https://www.jstatsoft.org/article/view/v012i08"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Ebayesthresh로 무엇을 할 수 있는가?",
    "text": "Ebayesthresh로 무엇을 할 수 있는가?\n아래와 같은 상황을 가정하자.\n\\[X_i = \\mu_i +\\epsilon_i.\\]\n여기에서 아래를 가정한다.\n\n\\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\)\neach \\(\\mu_i\\) is zero with probability \\((1−w)\\), while, with probability \\(w\\), \\(\\mu_i\\) is drawn from a symmetric heavy-tailed density \\(\\gamma\\).\n\n일반적으로 \\(w\\), 즉 \\(\\mu_i\\)가 0이 아닐 확률은 매우 작은값으로 설정된다. 따라서 위와 같은 구조로 \\(\\epsilon_i\\)와 \\(\\mu_i\\)를 생성하면 아래와 같이 된다.\n\n\\(\\epsilon_i\\): 절대값이 작은 신호들이 dense하게 있음.\n\\(\\mu_i\\): 절대값이 큰 신호들이 sparse하게 있음. (sparse한 이유는 \\(w\\)가 작으므로)\n\n따라서 \\(X_i\\)의 모양은 아래의 그림의 왼쪽과 같다.\n\n이 논문의 목표는 왼쪽의 그림 \\(X_i= \\mu_i +\\epsilon_i\\)로부터 오른쪽의 그림 \\(\\hat{\\mu}_i\\)을 구하는 것이다. 즉 작은 절대값의 노이즈 \\(\\epsilon_i\\)에서 큰 절대값의 신호 \\(\\mu_i\\)를 골라내는 일을 목표로 한다. 저자들은 이러한 작업을 “건초더미에서 바늘찾기”라는 말로 비유하였다. 이러한 “건초더미에서 바늘찾기”는 여러 분야에 응용될 수 있다. 구체적으로는 천문학, 이미지프로세싱, 데이터마이닝, 모형선택등에 사용될 수 있다고 한다. 언급한 분야에 대한 자세한 discussion은 Johnstone and Silverman (2004)에서 찾을 수 있다. 또한 “건초더미에서 바늘찾기”는 위에서 언급한 분야 이외에 퓨리에, 웨이블릿 혹은 다른 dictionaries에 의한 함수추정문제를 해결할 수 있다. 이는 퓨리에나 웨이블릿변환과 같은 multiscale trasnform이 원래 신호를 sparese한 구조로 바꾸기 때문이다. 즉 퓨리에변환 웨이블릿변환으로 underlying function을 추정할 수 있다는 의미이다. 우리는 이러한 접근법에 좀 더 초점을 맞추도록 하겠다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "간단한 사용법",
    "text": "간단한 사용법\nR을 이용하여 Ebayesthresh를 사용하는 간단한 방법을 살펴보도록 하자. 논문에 표현된 그림1을 재현하여 보자.\n\nlibrary(EbayesThresh)\n\n\nset.seed(1)\nx &lt;- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\nplot(x,type='l',lwd=0.2)\n\n\n\n\n위와 같은 자료 \\(X_i\\)를 관측하였다고 가정하자. 이 신호에는 “건초(\\(\\epsilon_i\\))”더미에 25개의 “바늘(\\(\\mu_i\\))”이 섞여있다. 여기에서 “바늘”만 골라내는 코드는 아래와 같이 작성할 수 있다.\n\nmuhat &lt;- ebayesthresh(x, sdev=1)\n\n결과를 시각화하면 아래와 같다.\n\nplot(x,type='l',lwd=0.2)\nlines(muhat,col=2,lwd=2)"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "arguments",
    "text": "arguments\n일반적으로 ebayesthresh 함수를 사용하는 방법은 아래와 같다.\n\nmuhat &lt;- ebayesthresh(\n    x,\n    prior = \"laplace\", \n    a = 0.5, \n    bayesfac = FALSE, \n    sdev = NA, \n    verbose = FALSE, \n    threshrule = \"median\"\n)\n\nprior, a: \\(\\mu_i\\)의 density. 보통 \\(\\frac{1}{2}a \\exp(-a|u|)\\)라고 가정한다. parameter \\(a\\)는 Section 2.1에서 자시해 나옴.\nbayesfac, threshrule: Section 2.2, 2.3에 자세히 나온다.\nsdev: \\(\\epsilon_i\\)의 sd를 의미한다. 이 값을 알고 있다면 설정하면 되지만 보통은 이 값을 모른다고 가정한다. \\(\\epsilon_i\\)의 sd를 모르는 경우는 observed data로 부터 추정하는데 보통 \\({\\tt median}(|X_i|)\\)로 추정한다.\n\\(\\epsilon_i\\)의 sd를 \\({\\tt median}(|X_i|)\\)로 추정하는 motivation을 이해하는 것이 중요하다. 이는 sparse assumption of \\(\\mu_i\\)에서 시작한다. 신호 \\(\\mu_i\\)가 합리적인 수준에서 sparse하다면 median absolute value of \\(X_i\\)는 \\(\\mu_i\\)의 값들과 상관이 없을 것이다. 하지만 당연히 신호가 sparse하지 않다면 이러한 방식으로 sdev를 추정하는 것은 매우 조심스럽게 수행되어야 할 것이다.\n\nn &lt;- 1000\nx &lt;- rnorm(n) + sample(c(runif(25,-7,7), rep(0,n-25)))\nprint(sd(x))\nprint(median(abs(x)))\n\n[1] 1.117016\n[1] 0.6787613\n\n\n\n실제로는 잘 추론하지 못하는 것 같다?"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "원리",
    "text": "원리\n어떻게 \\(\\hat{\\mu}_i\\)를 추정할 수 있을까? 가장 간단한 방법은 thresholding이다.\n많은 실제예제에서 \\(\\mu_i\\)는 어떤 의미에서 (in some sense) sparse하다고 여길 수 있다. EbayesThresh 패키지는 이처럼 \\(\\mu_i\\)가 sparse하다는 구조 (혹은 가정)을 이용하여 \\(\\mu_i\\)를 적절하게 추정한다.\nSparsity를 이용하는 자연스러운 방법은 threshoding이다: 여기에서 threshold의 값 \\(t\\)를 너무 크게 잡으면 신호를 잡음으로 잘못 판단할 것이고 \\(t\\)의 값이 너무 작다면 잡음을 신호로 잘못 판단할 수 있다. 따라서 \\(t\\)의 선택은 이 양쪽 기준사이의 tradeoff가 있는데 EbayesThresh는 이러한 tradeoff를 자동으로 조정하는 효과가 있다.\n\n\\(\\mu_i\\)는 \\(w\\)의 확률로 0 이며 \\((1-w)\\)의 확률로 0이 아니다. \\(\\mu_i\\)가 0이 아닐경우에는 symmetric heavy-tailed density \\(\\gamma\\)에서 추출된다고 가정한다. 여기에서 prior에 대한 key parameter인 \\(w\\)는 데이터로부터 자동으로 추정된다. (marginal maximum likelihood 를 이용한다) 그리고 추정된 \\(w\\)는 Bayesian model로 다시 대입된다.\n\\(w\\)가 추정되면 Bayesian model은 thresholding procedure를 수행할 수 있다. 왜냐하면 \\(w\\)를 추정하면 \\(t(w)\\)를 선택한다는 말과 같은말이기 때문이다.\n\nargument"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "The Bayesian model",
    "text": "The Bayesian model\n\\[X_i \\sim N(\\mu_i,1)\\]\n\\(f_{\\text{prior}}(\\mu)=(1-w)\\delta_0(\\mu)+w \\gamma_a(\\mu), \\quad \\gamma_a(\\mu)=\\frac{1}{2}a\\exp(-a|\\mu|)\\)\n여기에서 \\(\\gamma_a(\\mu)\\)는 하나의 예시일 뿐이다. Ebayesthresh에 디폴트로 설정된 prior=\"laplace\"를 셋팅하면 \\(\\gamma_a(\\mu)\\)가 사용된다. \\(\\gamma\\)의 선택은 tail이 polynomial rates로 줄어드는 어떠한 분포를 사용해도 무방하다. 저자들은 quasi-Cauchy분포를 제안하였는데 이는 Johnstone and Sliverman이 만든 theoretical assumption을 만족하는 분포중 가장 꼬리가 두꺼운 분포이다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Thresholding rules",
    "text": "Thresholding rules\n모수 \\(\\mu\\)는 사전분포(prior distribution)를 가진다고 가정하고 \\(X \\sim N(\\mu,1)\\)이라고 가정하자. 이 경우 \\(X=x\\)가 given되었을 경우 \\(\\mu\\)의 사후분포(posterior distribution)를 구할 수 있다. (자세한 내용은 Section 6을 참고해야함) 사후분포의 중앙값을 \\(\\hat{\\mu}(x;w)\\)라고 하자. (사후분포의 중앙값이 \\(w\\)에 영향받는 이유는 사전분포가 \\(w\\)에 depend하기 때문이다. 여기에서 \\(w\\)는 marginal MLE로 적절히 추론한다고 가정한다)\n\\(X_i\\)는 독립이라고 가정한다. 여기에서 \\(X_i\\)가 독립이 아니라면 약간의 정보손실이 있을 수 있다. 하지만 \\(X_i\\) 사이에 너무 많은 dependency가 존재하는 경우가 아니라면 Ebayesthresh는 어느정도 합리적인 결과를 제공한다.\n만약에 bayesfac=TRUE를 사용하면 \\(\\mu\\)의 사후분포의 중앙값 대신에 Bayes factor threshold 를 쓸 수도 있다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Choosing the threshold",
    "text": "Choosing the threshold\n\\(X_i\\)의 marginal density는\n\\((1-w)\\phi(x) +w(\\gamma \\star \\phi)(x)\\)\n\\(l(w) = \\sum_{i=1}^{n}\\log \\big\\{(1-w)\\phi(X_i)+wg(X_i) \\big\\}\\)\n와 같이 정의가능하다. 단, 여기에서 \\(g:= \\gamma\\star \\phi\\) 이다.\n이제 우리는 아래의 식을 풀면된다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]\n여기에서 \\(\\sqrt{2\\log n}\\)은 흔히 말하는 universal threshold 이다.\n만약에 \\(w\\)이외에 \\(a\\)도 추정해야 한다면 아래와 같이 추정할 수 있다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls3.html",
    "href": "posts/2_Studies/PyG/ls3.html",
    "title": "[PyG] lesson3: 미니배치",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls3.ipynb\n\n\n미니배치\n\nimport torch_geometric\n\n\ndataset = torch_geometric.datasets.TUDataset(\n    root='/tmp/ENZYMES', \n    name='ENZYMES'\n)\n\n\n(ChatGPT) ENZYMES는 그래프 분류를 위한 벤치마크 데이터셋 중 하나입니다. 이 데이터셋은 600개의 그래프로 구성되어 있으며, 6개의 클래스로 분류됩니다. 각 그래프는 효소(enzyme) 분자의 구조를 나타내며, 그래프의 노드는 원자(atom)를 나타내고, 엣지(edge)는 원자 간의 연결을 나타냅니다. ENZYMES 데이터셋은 화학 및 생물 정보학 분야에서 그래프 분류 알고리즘의 성능을 평가하기 위해 사용될 수 있습니다. 그래프 분류 알고리즘은 주어진 그래프를 특정 클래스 레이블로 분류하는 작업을 수행하는데 사용됩니다. 예를 들어, ENZYMES 데이터셋의 그래프는 특정 효소 종류를 나타내며, 그래프 분류 알고리즘은 주어진 효소 그래프가 어떤 종류의 효소인지 예측할 수 있습니다. PyG를 사용하여 ENZYMES 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 필요한 전처리를 자동으로 수행할 수 있습니다. 그래프 데이터를 다루는 머신 러닝 모델을 구축하고 훈련시키기 위해 ENZYMES 데이터셋을 사용할 수 있습니다.\n\n\nlen(dataset) # 이 데이터셋에는 600개의 그래프가 있음\n\n600\n\n\n\ndataset.num_classes # 6개의 클래스\n\n6\n\n\n\ndataset.num_node_features # 각 노드에는 3개의 피처가 있음\n\n3\n\n\n- 600개의 그래프중 첫번째 그래프에 접근\n\ndataset[0]\n\nData(edge_index=[2, 168], x=[37, 3], y=[1])\n\n\n\nx=[37, 3]: \\(|{\\cal V}|=37\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 168]: \\(|{\\cal E}|=168\\)\n\n- 600개중에서 두번째 그래프에 접근\n\ndataset[1]\n\nData(edge_index=[2, 102], x=[23, 3], y=[1])\n\n\n\nx=[23, 3]: \\(|{\\cal V}|=23\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 102]: \\(|{\\cal E}|=102\\)\n\n- dataset \\(\\to\\) loader\n\nloader = torch_geometric.loader.DataLoader(dataset, batch_size=2, shuffle=False)\n\n\nfor i,batch in enumerate(loader):\n    if i&lt;5:\n        print(i,batch)\n\n0 DataBatch(edge_index=[2, 270], x=[60, 3], y=[2], batch=[60], ptr=[3])\n1 DataBatch(edge_index=[2, 182], x=[49, 3], y=[2], batch=[49], ptr=[3])\n2 DataBatch(edge_index=[2, 182], x=[47, 3], y=[2], batch=[47], ptr=[3])\n3 DataBatch(edge_index=[2, 384], x=[114, 3], y=[2], batch=[114], ptr=[3])\n4 DataBatch(edge_index=[2, 184], x=[55, 3], y=[2], batch=[55], ptr=[3])\n\n\n\n600개 그래프를 2개씩 묶어서 배치를 만듬\n\n\ndataset[0], dataset[1]\n\n(Data(edge_index=[2, 168], x=[37, 3], y=[1]),\n Data(edge_index=[2, 102], x=[23, 3], y=[1]))\n\n\n\n이게 합쳐져서 0 DataBatch(edge_index=[2, 270], x=[60, 3], y=[2], batch=[60], ptr=[3])\n\n\ndataset[2], dataset[3]\n\n(Data(edge_index=[2, 92], x=[25, 3], y=[1]),\n Data(edge_index=[2, 90], x=[24, 3], y=[1]))\n\n\n\n이게 합쳐져서 1 DataBatch(edge_index=[2, 182], x=[49, 3], y=[2], batch=[49], ptr=[3])"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls6.html",
    "href": "posts/2_Studies/PyG/ls6.html",
    "title": "[PyG] lesson6: GCN",
    "section": "",
    "text": "Kipf and Welling (2016): https://arxiv.org/abs/1609.02907\n\nimport torch\nimport torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls6.html#예제",
    "href": "posts/2_Studies/PyG/ls6.html#예제",
    "title": "[PyG] lesson6: GCN",
    "section": "예제",
    "text": "예제\n- data\n\nx = torch.tensor([[20],\n                  [21],\n                  [19],\n                  [1],\n                  [2],\n                  [1]], dtype=torch.float)\n\nedge_index = torch.tensor([[0, 1, 2, 0, 1, 2, 3, 4, 3, 5, 4, 5],\n                           [1, 0, 0, 2, 2, 1, 4, 3, 5, 3, 5, 4]], dtype=torch.long)\n\ny = torch.tensor([1,1,1,0,0,0], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index, y=y) \n#data.train_mask = torch.tensor([True,False,True,True,False,True])\n#data.test_mask = torch.tensor([False,True,False,False,True,False])\ndata\n\nData(x=[6, 1], edge_index=[2, 12], y=[6])\n\n\n- GCNConv\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n        [ 11.6402, -15.0337, -13.0234, -15.7613],\n        [ 11.6402, -15.0337, -13.0234, -15.7613],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.5820],\n         [-0.7517],\n         [-0.6512],\n         [-0.7881]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[ 0.5820],\n        [-0.7517],\n        [-0.6512],\n        [-0.7881]], requires_grad=True)\n\n\n\nA = torch.tensor([[0., 1., 1., 0., 0., 0.],\n                  [1., 0., 1., 0., 0., 0.],\n                  [1., 1., 0., 0., 0., 0.],\n                  [0., 0., 0., 0., 1., 1.],\n                  [0., 0., 0., 1., 0., 1.],\n                  [0., 0., 0., 1., 1., 0.]])\nAtilde = A+torch.eye(6)\nAtilde\n\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.]])\n\n\n\nAtilde@data.x@W.T/3, gconv(data.x,data.edge_index)\n\n(tensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;DivBackward0&gt;),\n tensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;AddBackward0&gt;))\n\n\n- 즉 아래의 수식에서\n\\[H^{(l+1)} = \\sigma\\big(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H^{(l)}W^{(l)}\\big).\\]\n\\(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H^{(l)}W^{(l)}\\)를 계산하는 Layer가 torch_geometric.nn.GCNConv() 으로 구현되어있음."
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html",
    "href": "posts/2_Studies/PyG/ls1.html",
    "title": "[PyG] lesson1: 자료형",
    "section": "",
    "text": "import torch\nimport torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제1-아래와-같은-그래프자료를-고려하자.",
    "href": "posts/2_Studies/PyG/ls1.html#예제1-아래와-같은-그래프자료를-고려하자.",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제1: 아래와 같은 그래프자료를 고려하자.",
    "text": "예제1: 아래와 같은 그래프자료를 고려하자.\n\n- 이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index) # torch_geometric.data.Data는 그래프자료형을 만드는 클래스\n\n- data 의 자료형\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n- data의 __str__\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nx=[3, 1]: 이 자료는 3개의 노드가 있으며, 각 노드에는 1개의 feature가 있음\nedge_index=[2, 4]: \\({\\cal E}\\)는 총 4개의 원소가 있음.\n\n- 각 노드의 feature를 확인하는 방법 (즉 \\(f:{\\cal V} \\to \\mathbb{R}^k\\)를 확인하는 방법)\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n- \\({\\cal E}\\)를 확인하는 방법\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])\n\n\n-\n\nlen(data)\n\n2"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제2-잘못된-사용",
    "href": "posts/2_Studies/PyG/ls1.html#예제2-잘못된-사용",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제2: 잘못된 사용",
    "text": "예제2: 잘못된 사용\n- edge_index는 예제1과 같이 \\([2,|{\\cal E}|]\\) 의 shape으로 넣어야 한다. 그렇지 않으면 에러가 난다.\n\nedge_index = torch.tensor([[0, 1],\n                           [1, 0],\n                           [1, 2],\n                           [2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index)\n\n\n#data.validate(raise_on_error=True)\ndata.validate()\n\nValueError: 'edge_index' needs to be of shape [2, num_edges] in 'Data' (found torch.Size([4, 2]))"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제3-예제2의-수정",
    "href": "posts/2_Studies/PyG/ls1.html#예제3-예제2의-수정",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제3: 예제2의 수정",
    "text": "예제3: 예제2의 수정\n- edge_index의 shape이 \\([|{\\cal E}|,2]\\) 꼴로 저장되어 있었을 경우 트랜스포즈이후 countiguous()함수를 사용하면 된다.1\n1 그런데 그냥 transpose만 해도되는것 같음\nedge_index = torch.tensor([[0, 1],\n                           [1, 0],\n                           [1, 2],\n                           [2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n\ndata = torch_geometric.data.Data(\n    x=x, \n    edge_index=edge_index.t().contiguous()\n)\n\n\n#data.validate(raise_on_error=True)\ndata.validate()\n\nTrue"
  },
  {
    "objectID": "2_pyg.html",
    "href": "2_pyg.html",
    "title": "PyG",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 19, 2023\n\n\n[PyG] lesson5: GCN\n\n\n신록예찬 \n\n\n\n\nJul 14, 2023\n\n\n[PyG] lesson5: Learning Methods on Graphs\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson4: Data Transform???\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson3: 미니배치\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)\n\n\n신록예찬 \n\n\n\n\nJul 2, 2023\n\n\n[PyG] lesson1: 자료형\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_pl.html",
    "href": "2_pl.html",
    "title": "PyTorch Lightning",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_boram.html",
    "href": "3_boram.html",
    "title": "BORAM-COCO",
    "section": "",
    "text": "https://github.com/boram-coco\nhttps://boram-coco.github.io/coco/\nhttps://boram-coco.github.io/Scribbling/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 30, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN ToyEX\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬 \n\n\n\n\nMay 6, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Start\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls4.html",
    "href": "posts/2_Studies/PyG/ls4.html",
    "title": "[PyG] lesson4: Data Transform???",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls4.ipynb\n\n\nRef\n\nhttps://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n\n\n\n데이터 변환\n\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'])\n\ndataset[0]\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])\n\n\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'],\n                    pre_transform=T.KNNGraph(k=6))\n\ndataset[0]\n\n/home/cgb2/anaconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/data/dataset.py:209: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete '/tmp/ShapeNet/processed' first\n  warnings.warn(\n\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])\n\n\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'],\n                    pre_transform=T.KNNGraph(k=6),\n                    transform=T.RandomJitter(0.01))\n\ndataset[0]\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls5.html",
    "href": "posts/2_Studies/PyG/ls5.html",
    "title": "[PyG] lesson5: Learning Methods on Graphs",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls5.ipynb\n\n\nRef\n\nhttps://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n\n\n\ndata\n\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\n\n\ndataset[0]\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\nimport torch_torch_geometric\n\n\ntorch_geometric.data.Data\n\nNameError: name 'torch_geometric' is not defined\n\n\n\ndataset[0].edge_index.shape\n\ntorch.Size([2, 10556])\n\n\n\ndataset[0].y.dtype\n\ntorch.int64\n\n\n\ndataset[0].train_mask\n\ntensor([ True,  True,  True,  ..., False, False, False])\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, 16)\n        self.conv2 = GCNConv(16, dataset.num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = GCN().to(device)\ndata = dataset[0].to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\nmodel.train()\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask]) # train에 대한 loss만 따로 처리해야함\n    loss.backward()\n    optimizer.step()\n\n\nout.shape # 카테고리가 7개\n\ntorch.Size([2708, 7])\n\n\n\ndata.y.unique() # 카테고리가 7개\n\ntensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.8050\n\n\n\n\nFRAUD data에 활용?\n\n고객정보별로 그래프생성\n커다란 하나의 그래프 생성\n\n\n\nmodel 분석\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1433, 16)\n  (conv2): GCNConv(16, 7)\n)\n\n\n\ndataset.data\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\n2708개의 노드가 있음 \\(\\to\\) 이걸 observation으로 해석해야함.\n각 노드에는 1433개의 특징(X)과 0-6까지의 label(y)이 연결되어 있음.\n2708개의 노드는 각각 tr,val,test로 나누어짐\n\n\nb,W = list(model.conv1.parameters())\n\n\nW,W.shape # 1433개의 특징을 16개로 줄임\n\n(Parameter containing:\n tensor([[ 0.0086, -0.0062, -0.1213,  ...,  0.1313, -0.0251,  0.0856],\n         [-0.0033, -0.0384, -0.1193,  ...,  0.0362, -0.1130,  0.0426],\n         [ 0.0118,  0.0722,  0.0481,  ..., -0.0677,  0.0497,  0.0095],\n         ...,\n         [ 0.0158,  0.1560, -0.0517,  ..., -0.0317,  0.1272,  0.0131],\n         [-0.0259,  0.0146, -0.0539,  ...,  0.0069, -0.0665,  0.0016],\n         [ 0.0191, -0.1023,  0.0411,  ..., -0.0415,  0.0125,  0.0015]],\n        device='cuda:0', requires_grad=True),\n torch.Size([16, 1433]))\n\n\n\nb,b.shape\n\n(Parameter containing:\n tensor([0.2343, 0.2136, 0.1581, 0.1914, 0.3095, 0.0851, 0.3675, 0.2414, 0.3099,\n         0.2571, 0.1340, 0.2693, 0.1588, 0.2984, 0.1877, 0.2014],\n        device='cuda:0', requires_grad=True),\n torch.Size([16]))\n\n\n\n\nGCNConv?? Kipf and Welling (2016)\n\nKipf, Thomas N, and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907.\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html\n\n아래의 논문에서 제안되었음.. (레퍼수가..)\nhttps://arxiv.org/abs/1609.02907\n대략적인 설명을 캡쳐하면 아래와 같음\n\n\n\nimage.png\n\n\n여기에서\n\n\\({\\bf A}\\)는 연결정보를 의미\n\\(\\hat{\\bf A}\\)는 연결정보에 자기자신의 노드를 추가\n\\({\\bf D}\\)는 \\({\\bf A}\\)를 표준화하기 위한 매트릭스\n결국 \\(\\hat{\\bf D}^{-1/2}\\hat{\\bf A}\\hat{\\bf D}^{-1/2}\\) 는 통째로 연결정보에 대한 matrix\n\\(\\hat{\\bf D}^{-1/2}\\hat{\\bf A}\\hat{\\bf D}^{-1/2}{\\bf X}\\) 는 통째로 \\({\\bf X}\\)를 평행이동한것을 의미 (혹은 그 비슷한 것을 의미)\n\\({\\bf \\Theta}\\)는 weight를 곱하는 과정임"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html",
    "href": "posts/2_Studies/PyG/ls2.html",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "",
    "text": "import torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#정보",
    "href": "posts/2_Studies/PyG/ls2.html#정보",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "정보",
    "text": "정보\n- 기본정보: ENZYMES dataset\n\n(ChatGPT) ENZYMES는 그래프 분류를 위한 벤치마크 데이터셋 중 하나입니다. 이 데이터셋은 600개의 그래프로 구성되어 있으며, 6개의 클래스로 분류됩니다. 각 그래프는 효소(enzyme) 분자의 구조를 나타내며, 그래프의 노드는 원자(atom)를 나타내고, 엣지(edge)는 원자 간의 연결을 나타냅니다. ENZYMES 데이터셋은 화학 및 생물 정보학 분야에서 그래프 분류 알고리즘의 성능을 평가하기 위해 사용될 수 있습니다. 그래프 분류 알고리즘은 주어진 그래프를 특정 클래스 레이블로 분류하는 작업을 수행하는데 사용됩니다. 예를 들어, ENZYMES 데이터셋의 그래프는 특정 효소 종류를 나타내며, 그래프 분류 알고리즘은 주어진 효소 그래프가 어떤 종류의 효소인지 예측할 수 있습니다. PyG를 사용하여 ENZYMES 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 필요한 전처리를 자동으로 수행할 수 있습니다. 그래프 데이터를 다루는 머신 러닝 모델을 구축하고 훈련시키기 위해 ENZYMES 데이터셋을 사용할 수 있습니다.\n\n\ndataset # 데이터셋 이름\n\nENZYMES(600)\n\n\n\nlen(dataset) # 이 데이터셋에는 600개의 그래프가 있음\n\n600\n\n\n\ndataset.num_classes # 6개의 클래스\n\n6\n\n\n\ndataset.num_node_features # 각 노드에는 3개의 피처가 있음\n\n3\n\n\n- 600개의 그래프중 첫번째 그래프에 접근\n\ndataset[0]\n\nData(edge_index=[2, 168], x=[37, 3], y=[1])\n\n\n\nx=[37, 3]: \\(|{\\cal V}|=37\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 168]: \\(|{\\cal E}|=168\\)"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#traintest-분리",
    "href": "posts/2_Studies/PyG/ls2.html#traintest-분리",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "Train/Test 분리",
    "text": "Train/Test 분리\n- 600개의 그래프중 540를 train으로, 60개를 test로\n\ntrain_dataset = dataset[:540]\ntest_dataset = dataset[540:]"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#정보-1",
    "href": "posts/2_Studies/PyG/ls2.html#정보-1",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "정보",
    "text": "정보\n\nChatGPT: Cora는 그래프 분류를 위한 벤치마크 데이터셋 중 하나로, PyG에서도 사용할 수 있습니다. 이 데이터셋은 기계 학습 및 정보 검색 분야에서 널리 사용되는 학술 논문들의 인용 네트워크를 나타냅니다. Cora 데이터셋은 컴퓨터 과학 분야의 논문을 대상으로 합니다. 각 논문은 그래프의 노드로 표현되며, 노드는 논문을 나타냅니다. 노드 간의 엣지는 논문들 사이의 인용 관계를 나타냅니다. 따라서 Cora 데이터셋은 논문의 텍스트 기반 정보와 인용 관계에 대한 그래프 구조를 제공합니다. Cora 데이터셋은 7개의 클래스로 분류되며, 각 논문은 특성 벡터(feature vector)로 표현됩니다. 이 특성 벡터에는 논문의 단어 등 다양한 정보가 포함될 수 있습니다. PyG를 사용하여 Cora 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 전처리를 자동으로 수행할 수 있습니다. 이를 통해 머신 러닝 모델을 훈련시켜 Cora 데이터셋의 논문을 분류하거나 다양한 작업을 수행할 수 있습니다.\n\n- 기본정보\n\nlen(dataset) # 하나의 그래프가 있음\n\n1\n\n\n\ndataset.num_classes # 7개의 클래스가 있음\n\n7\n\n\n\ndataset.num_node_features # 각 노드는 1433개의 특징이 있음. (논문에 포함된 단어등 다양한 특성이 담겨있을 수 있음) \n\n1433\n\n\n- 그래프에 접근\n\ndataset[0] # 기본정보\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\nx=[2708, 1433]: 2708개의 논문이 있고, 각 논문은 1433개의 특징벡터들로 이루어져 있음.\nedge_index=[2, 10556]: 논문간의 인용은 약 10556.\ny=[2708]:\n\n\ndataset[0].x.shape # 2708개의 논문이 있고 1433개의 특징벡터를 가짐\n\ntorch.Size([2708, 1433])\n\n\n\ndataset[0].y.unique() # 논문이 7개의 카테고리로 분류되는듯\n\ntensor([0, 1, 2, 3, 4, 5, 6])"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#traintest-이미-분리되어-있음",
    "href": "posts/2_Studies/PyG/ls2.html#traintest-이미-분리되어-있음",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "Train/Test (이미 분리되어 있음)",
    "text": "Train/Test (이미 분리되어 있음)\n\ndataset[0].train_mask \n# dataset[0].train_mask 는 True, False로 이루어져 있는 길이가 2708(=노드수=논문수)인 벡터\n# 여기에서 True인 노드만 훈련함\n\ntensor([ True,  True,  True,  ..., False, False, False])\n\n\n\ndataset[0].train_mask.sum() # 140개의 노드만 훈련함? \n\ntensor(140)\n\n\n\ndataset[0].val_mask.sum() # val은 500개의 노드?\n\ntensor(500)\n\n\n\ndataset[0].test_mask.sum() # test set은 1000?\n\ntensor(1000)"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#abstract",
    "href": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#abstract",
    "title": "[Review] 사기탐지 + 그래프 관련연구",
    "section": "Abstract",
    "text": "Abstract\n이 논문에서는 최근 몇 년간 그래프 신경망(Graph Neural Networks, GNNs)이 사기 탐지 문제에 널리 적용되어, 다양한 관계를 통해 이웃 정보를 집계함으로써 노드의 수상성을 밝혀내는 것을 소개하고 있습니다. 그러나 몇몇 이전 연구들은 사기꾼들의 가장 큰 문제인 위장 행위(camouflage behavior)에 주목하지 않았습니다. 이러한 위장 행위는 GNN 기반 사기 탐지기의 집계 과정에서 성능을 저하시킬 수 있습니다. 따라서 이 논문에서는 최근의 경험적 연구를 기반으로 두 가지 유형의 위장 행위, 즉 특성 위장과 관계 위장을 소개하고 있습니다. 기존의 GNN은 이러한 두 가지 위장 행위에 대응하지 않았기 때문에 사기 탐지 문제에서 성능이 떨어지는 것입니다. 이에 대응하여 새로운 모델인 CAmouflage-REsistant GNN (CARE-GNN)을 제안하고 있으며, 이 모델은 위장 행위에 대응하기 위해 세 가지 독특한 모듈을 포함하고 있습니다. 구체적으로, 먼저 정보성이 있는 이웃 노드를 찾기 위해 레이블 기반 유사도 측정 방법을 고안합니다. 그런 다음, 강화 학습 (Reinforcement Learning, RL)을 활용하여 선택할 최적의 이웃 수를 결정합니다. 마지막으로, 다양한 관계 사이에서 선택된 이웃들을 함께 집계합니다. 두 개의 실제 사기 데이터셋에 대한 포괄적인 실험을 통해 RL 알고리즘의 효과를 입증하였습니다. 제안된 CARE-GNN은 최첨단 GNN 및 GNN 기반 사기 탐지기보다 뛰어난 성능을 보여줍니다. 또한, 모든 GNN 기반 사기 탐지기를 통합하여 오픈 소스 도구 상자(https://github.com/YingtongDou/CARE-GNN)로 제공하고 있습니다. CARE-GNN 코드와 데이터셋을 이용할 수 있습니다.]"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#introduction",
    "href": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#introduction",
    "title": "[Review] 사기탐지 + 그래프 관련연구",
    "section": "Introduction",
    "text": "Introduction\n인터넷 서비스의 번창과 함께 다양한 유형의 사기 행위도 발생하고 있습니다 [14]. 사기꾼들은 일반 사용자로 위장하여 안티 사기 시스템을 우회하고 불명확한 정보를 퍼뜨리거나 최종 사용자의 개인정보를 빼앗습니다 [32]. 이러한 사기 행위를 탐지하기 위해 그래프 기반 방법이 학계 [7, 21, 38]와 산업계 [2, 28, 50] 모두에서 효과적인 접근 방법이 되었습니다. 그래프 기반 방법은 서로 다른 관계로 엔티티를 연결하고, 동일한 목표를 가진 사기꾼들은 서로 연결되기 때문에 이러한 엔티티들의 수상성을 그래프 수준에서 드러낼 수 있습니다 [1].\n최근에는 그래프 신경망(Graph Neural Networks, GNNs)의 발전으로 많은 GNN 기반 사기 탐지기들이 제안되었습니다. (예: GCN [17], GAT [34], 그리고 GraphSAGE [12]) 이들은 의견 사기 [19, 25, 39], 금융 사기 [23, 24, 37], 모바일 사기 [41], 그리고 사이버 범죄 [48]를 탐지하기 위해 사용됩니다. 기존의 전통적인 그래프 기반 접근 방법과는 달리, GNN 기반 방법은 이웃 정보를 집계하여 중심 노드의 표현을 학습합니다. 이들은 end-to-end 및 반지도 학습 방식으로 훈련될 수 있으며, 이는 많은 특성 엔지니어링과 데이터 주석 비용을 절약할 수 있습니다.\n그러나 기존의 GNN 기반 사기 탐지 연구들은 GNN을 제한적인 범위에서만 적용하면서 사기꾼들의 위장 행위를 무시하고 있습니다. 이러한 위장 행위는 연구자들 [8, 15, 16, 49]과 실무자들 [2, 19, 41] 양쪽에서 큰 관심을 받고 있습니다. 한편, 이론적인 연구들은 그래프에 노이즈가 있는 노드와 엣지가 있는 경우 GNN의 한계와 취약점을 입증하고 있습니다 [3, 4, 13, 33]. 따라서, 위장된 사기꾼들에 대응하지 못한다면 GNN 기반 사기 탐지기의 성능을 저하시킬 수 있습니다. 최근 몇몇 연구들 [4, 9, 13, 25, 41]은 비슷한 도전에 주목했지만, 이들의 해결책은 사기 탐지 문제에 적합하지 않거나 GNN의 end-to-end 학습 방식을 파괴하는 경우가 있습니다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Kronecker product",
    "text": "Kronecker product\n크로네커곱의 정의는 아래와 같다.\n\\[{\\bf A} \\otimes {\\bf B}\n=\\begin{bmatrix}\na_{11}{\\bf B} & a_{12}{\\bf B} & \\dots & a_{1m}{\\bf B} \\\\\na_{21}{\\bf B} & a_{22}{\\bf B} & \\dots & a_{2m}{\\bf B} \\\\\n\\dots & \\dots & \\dots & \\dots \\\\\na_{n1}{\\bf B} & a_{n2}{\\bf B} & \\dots & a_{nm}{\\bf B} \\\\\n\\end{bmatrix}\\]\n두 행렬 \\({\\bf A}_{m\\times n}\\), \\({\\bf B}_{p\\times q}\\)의 크로네커곱 \\({\\bf A}\\otimes {\\bf B}\\)의 차원은 \\(mp \\times nq\\) 가 된다. 계산예시는 아래와 같다.\n\n\n\n위키에서 긁은 예제, 글씨가 좀 작음\n\n\n크로네커곱에 대한 성질들이 위키에 많이 있으니 참고하면 좋다.\n(예제1)\n\nA= [1 2\n    3 4]\nB= [0 5\n    6 7]\nC = kron(A, B)\n\n4×4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n(예제2)\n\nA= [1 -4 7; -2 3 3]\nB= [8 -9 -6 -5; 1 -3 -4 7; 2 8 -8 -3; 1 2 -5 -1]\nC = kron(A, B)\n\n8×12 Matrix{Int64}:\n   8   -9  -6   -5  -32   36   24   20  56  -63  -42  -35\n   1   -3  -4    7   -4   12   16  -28   7  -21  -28   49\n   2    8  -8   -3   -8  -32   32   12  14   56  -56  -21\n   1    2  -5   -1   -4   -8   20    4   7   14  -35   -7\n -16   18  12   10   24  -27  -18  -15  24  -27  -18  -15\n  -2    6   8  -14    3   -9  -12   21   3   -9  -12   21\n  -4  -16  16    6    6   24  -24   -9   6   24  -24   -9\n  -2   -4  10    2    3    6  -15   -3   3    6  -15   -3"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Khatri–Rao product",
    "text": "Khatri–Rao product\n카트리-라오곱은 매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 같은 차원의 블락매트릭스로 정의될때 각 서브매트릭스의 크로네커 곱으로 정의된다. 정의와 계산예시는 아래와 같다.\n\n\n\n예시1: 위키에서 긁은 그림\n\n\n또 다른 계산예시는 아래와 같다. 이 예제는 중요하니까 구현해보자.\n\n\n\n예시2: 위키에서 긁은 그림\n\n\n(예제1)\n\nC= [1 2 3 \n    4 5 6 \n    7 8 9] \nD= [1 4 7\n    2 5 8\n    3 6 9]\n\n3×3 Matrix{Int64}:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:3]...)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81\n\n\n이건 자주 쓸일이 있을것 같으니까 함수로 저장하자.\n\ncolumnwise_kron = \n(C,D) -&gt; hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#181 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프 표현",
    "text": "그래프 표현\n아래의 그림을 살펴보자.\n\n\n\n그래프의 개념을 이해하는 필요한 그림, 일단 오른쪽의 \\({\\bf S}\\)는 무시할 것\n\n\n오른쪽의 \\({\\bf S}\\)는 무시하고 왼쪽의 그래프만 살펴보자. 이 그림에는 6개의 노드가 있고 각각의 노드는 저 마다의 연결구조를 가진다. 이러한 연결구조는 \\({\\bf G}=({\\bf N},{\\bf E})\\) 으로 표현할 수 있는데 여기에서 \\({\\bf N}\\)은 노드들의 집합이고 \\({\\bf E}\\)는 엣지들의 집합이다.1 보통 \\({\\cal E}\\)는 복잡하므로 연결정보를 매트릭스 \\({\\bf E}\\)로 표현하는데 이러한 \\({\\bf E}\\)를 인접행렬이라고 부른다. 인접행렬의 각 원소는 \\(E_{ij}= \\begin{cases} 1 & (i,j) \\in {\\cal E} \\\\ 0 & o.w \\end{cases}\\) 와 같이 정의한다. 이 그림의 경우 \\({\\cal N}\\) 와 \\({\\cal E}\\), \\({\\bf E}\\) 는 아래와 같다.\n1 노드 \\(i\\)에서 노드 \\(j\\)로 향하는 연결이 있다면 \\((i,j) \\in {\\cal E}\\)이다.\n\\({\\cal N}=\\{1,2,3,4,5,6\\}\\)\n\\({\\bf E}=\\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 \\\\ 1 & 0 & 1 & 0 & 1 & 0\\\\ 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\({\\cal E} = \\{(i,j) : E_{ij}=1 \\}\\)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "GSO",
    "text": "GSO\n후에 자세히 서술하겠지만 전통적인 시계열분석기법을 그래프신호로 확장하기 위해서는 단지 퓨리에변환 대신에 그래프퓨리에 변환을 사용하면 된다. 즉 퓨리에변환을 일반화한 그래프퓨리에변환을 잘 정의하면 된다.\n전통적인 신호처리 영역에서의 퓨리에변환은 시계열자료의 인접성을 의미하는 행렬 \\({\\bf B}\\)2의 고유행렬의 켤레전치로 정의할 수 있다. 이를 이용하면 그래프 퓨리에변환은 그래프자료의 인접성을 의미하는 행렬3의 고유행렬의 켤레전치로 정의할 수 있음을 유추할 수 있다. 즉 비유클리드 자료에서도 \\({\\bf B}\\)에 대응하는 어떠한 매트릭스가 정의되어야 하는데 (그리고 이 매트릭스는 그래프자료의 인접성에 대한 정보가 있어야 한다) 이 매트릭스를 \\({\\bf S}\\)라고 정의하고 grahp shift operator (GSO) 라고 이름 붙인다.\n2 원래는 평행이동을 의미하는 행렬이지만, 이걸 인접성을 의미하는 행렬로 해석할 수도 있다. 어차피 인접한 곳으로 이동할 수 있으니까..3 예를들면 인접행렬 \\({\\bf E}\\)와 같은 행렬주어진 그래프 \\({\\cal G}=({\\cal N},{\\cal E})\\) 에 대하여 GSO \\({\\bf S}\\)는 \\({\\bf E}+{\\bf I}\\)의 값이 1인 영역에만 값이 있는 어떠한 행렬이다. 다시 아래의 그림을 생각하여 보자.\n\n\n\nGSO의 개념을 이해하는데 필요한 그림\n\n\n왼쪽그래프의 GSO는 오른쪽과 같은 행렬 \\({\\bf S}\\)가 된다. 이제 \\({\\bf S}\\) 의 고유벡터행렬을 구한 뒤에 그것의 켤레전치를 \\({\\bf GFT}\\) 행렬로 정의하면 될 것 같다. 문제는 “\\({\\bf S}\\)의 고유벡터행렬이 항상 존재하는가?” 인데, 사실 이게 항상 존재한다는 보장이 없다. 즉 \\({\\bf S}\\)의 고유벡터 행렬이 존재 안할 수도 있다. 따라서 GSO \\({\\bf S}\\)가 고유분해가능하다는 조건이 추가적으로 필요한데 이러한 조건을 만족하는 GSO를 normal GSO라고 부른다. 우리는 당연히 normal GSO에 대해서만 관심이 있으므로 앞으로 특별한 언급이 없는한 GSO는 모두 normal GSO라고 가정한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Periodogram, correlogram, and LS estimator",
    "text": "Periodogram, correlogram, and LS estimator\nFrom \\({\\bf C}_{\\tilde{\\bf x}}:= \\mathbb{E}\\left[\\tilde{\\bf x}\\tilde{\\bf x}^H \\right]=\\mathbb{E}\\left[({\\bf V}^H{\\bf x})({\\bf V}^H{\\bf x})^H \\right]=\\text{diag}({\\bf p})\\) it follows that one may express the PSD as \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\). That is, the PSD is given by the expected value of the squared frequency components of the random process. This leads to a natural approach for the estimation of \\({\\bf p}\\) from a finite set of \\(R\\) realizations of the process \\({\\bf x}\\). Indeed, we compute the \\({\\bf GFT} \\tilde{\\bf x}_r = {\\bf V}^H{\\bf x}_r\\) of each observed signal \\({\\bf x}_r\\) and estimate \\({\\bf p}\\) as\n\\[\n\\hat{\\bf p}_{pg}:= \\frac{1}{R}\\sum_{r=1}^R|\\tilde{\\bf x}_r|^2=\\frac{1}{R}\\sum_{r=1}^{R}|{\\bf V}^H{\\bf x}_{r}|^2.\n\\]\nThe estimator \\(\\hat{\\bf p}_{pg}\\) is termed periodogram due to its evident similarity with its homonym5 in classical estimation. It is simple to show that \\({\\bf p}_{pg}\\) is an unbiased estimator, that is, \\(\\mathbb{E}[\\hat{\\bf p}_{pg}]= {\\bf p}\\). A more detailed analysis of the performance of \\(\\hat{\\bf p}_{pg}\\), for the case where the observations are Gaussian, is given in Proposition 12.1.6\n5 동음이의어6 Proposition 12.1은 뒤에 다루는데 \\(\\hat{\\bf p}_{pg}\\)의 분산에 대한 서술이 있음. 분산은 \\(\\mathbb{V}[\\hat{\\bf p}_{pg}]=\\frac{2}{R}\\text{diag}^2({\\bf p})\\)와 같음An alternative nonparametric estimation scheme, denominated correlogram, can be devised by starting from the definition of \\({\\bf p}\\) in\n\\[{\\bf p}:=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big).\\]\nNamely, one may substitute \\({\\bf C}_{\\bf x}\\) in above equation by the sample covariance \\(\\hat{\\bf C}_{\\bf x} = \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\) computed based on the available observations to obtain\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\nNotice that the matrix \\({\\bf V}^H\\hat{\\bf C}_{\\bf x}{\\bf V}\\) is in general, not diagonal because the eigenbasis of \\(\\hat{\\bf C}_{\\bf x}\\) differs from \\({\\bf V}\\), the eigenbasis of \\({\\bf C}_{\\bf x}\\). Nonetheless, we keep only the diagonal elements \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x}{\\bf v}_i\\) for \\(i = 1, \\dots , N\\) as our PSD estimator. It can be shown that the correlogram \\({\\bf p}_{cg}\\) and the periodogram \\({\\bf p}_{pg}\\) lead to identical estimators, as is the case in classical signal processing.\nThe correlogram can also be interpreted as an LS estimator. The decomposition in \\({\\bf C}_{\\bf x}={\\bf V}\\text{diag}({\\bf p}){\\bf V}^H\\) allows a linear parameterization of the covariance matrix \\({\\bf C}_{\\bf x}\\) as\n\\[\n{\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H.\n\\]\nThis linear parametrization will also be useful for the sampling schemes developed in Section 12.4. Vectorizing \\({\\bf C}_{\\bf x}\\) in \\({\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H\\) results in a set of \\(N^2\\) equations in \\({\\bf p}\\)\n\\[\n{\\bf c}_{\\bf x} = \\text{vec}({\\bf C}_{\\bf x})=\\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf G}_{np}{\\bf p},\n\\]\nwhere \\(\\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf v}_i^\\ast \\otimes {\\bf v}_i\\). Relying on the Khatri-Rao product, we then form the \\(N^2 \\times N\\) matrix \\({\\bf G}_{np}\\) as\n\\[\n{\\bf G}_{np}:= \\left[{\\bf v}_1^\\ast \\otimes {\\bf v}_1, \\dots, {\\bf v}_N^\\ast \\otimes {\\bf v}_N \\right] = {\\bf V}^\\ast \\odot {\\bf V}.\n\\]\n\nHere \\(\\otimes\\) denote the Kronecker matrix product and \\(\\odot\\) denote the Khatri-Rao matrix product.\n\nUsing the sample covariance matrix \\(\\hat{\\bf C}_{\\bf x}\\) as an estimate of \\({\\bf C}_{\\bf x}\\), we can match the estimated covariance vector \\(\\hat{\\bf c}_{\\bf x}=\\text{vec}(\\hat{\\bf C}_{\\bf x})\\) to the true covariance vector \\({\\bf c}_{\\bf x}\\) in the LS sense as\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\nIn other words, the LS estimator minimizes the squared error \\(\\text{tr}\\left[\\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)^T \\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)\\right]\\). From expression \\(\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\) it can be shown that the \\(i\\)th element of \\(\\hat{\\bf p}_{ls}\\) is \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x} {\\bf v}_i\\). Combining this with Eq.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right]\\]\nwe get that the LS estimator \\(\\hat{\\bf p}_{ls}\\) and the correlogram \\(\\hat{\\bf p}_{cg}\\) —and hence the periodogram as well— are all identical estimators. The estimators derived in this subsection do not assume any data distribution and are well suited for cases where the data probability density function is not available. In what follows, we provide performance bounds for these estimators under the condition that the observed signals are Gaussian."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD",
    "text": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD\n\n전통적인 분석방법\n클래식한 정상시계열은 유한차수의 ARMA로 근사할 수 있음이 알려져 있다7. 유한차수의 ARMA의 계수 \\(p\\),\\(q\\)를 적절하게 추정하기 위해서는 시계열 \\({\\bf x}\\)를 SACF plot 혹은 SPACF plot 을 이용하면 된다. 이때 SACF 혹은 SPACF 의 그림을 살펴보고 적절한 모형을 선택하기 위해서는 유한차수 ARMA의 이론적 ACF의 모양을 알면 되는데,8 이를 바꾸어서 말하면 결국 정상시계열 \\({\\bf x}\\)의 모든 정보는 ACF에 들어있다는 의미가 된다. 즉 정상시계열은 ACF만 잘 추정하면 모든 것이 해결된다.\n7 Wold’s theorem8 예를들어 “coef가 0.9인 AR(1)의 경우 lag=1 에 대한 이론적 ACF값이 0.9, lag=2에 대한 ACF값이 0.81, … 와 같이 되더라~” 하는식의그런데 ACF의 모든 정보는 다시 아래의 행렬에 들어있다.\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^T]\\]\n여기에서 \\({\\bf x}\\)는 realization이 아니라 확률벡터를 의미함을 유의하자.9 따라서 정상시계열의 경우 \\({\\bf C}_{\\bf x}\\)를 잘 추정하면 모든것이 해결된다고 생각하면 된다.\n9 보통 수리통계에서는 확률변수를 \\(X\\) realization을 \\(x\\)로 표현하지만 여기에서는 매트릭스를 대문자로 쓰고 있어서 그런식으로 표현하기 어렵다, 그래서 그때 그때 이것이 확률변수인지 realization인지 따져봐야 한다\n참고: 정상시계열의 경우 ACF 만 정확하게 알아도 (반대로 PACF만 정확하게 알아도) 이론상 모든 모형을 특정할 수 있다. 즉 정상시계열의 모형을 특정하기 위해서는 ACF plot, PACF plot 하나만 있어도 충분하다. (Wold’s Thm은 떠올리면 모든 정상시계열은 무한MA로 유니크하게 표현할 수 있는데, 이는 PACF plot을 가지고 모든 정상시계열을 유니크하게 특정할 수 있다는 것을 의미한다) 다만 좀 더 모형을 특정하는 과정을 용이하게 하기 위해서 실전에서는 SACF plot 과 SPACF plot 을 함께 보는 것이 유리하다.\n\n(예제) AR(1) 모형\n왜 ACF의 모든정보를 \\({\\bf C}_{\\bf x}\\)로 부터 알수 있는지 코드를 통하여 실습하여 보자. (바로 이해된다면 사실 이 예제는 스킵해도 무방함) 아래와 같은 모형을 가정하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n여기에서 \\(\\epsilon_t\\)는 서로 독립인 표준정규분포를 따른다. 이 모형에서 길이가 100인 시계열을 임의로 발생시키자.\n\nx = zeros(100*1000)\nx[1] = randn()\nfor t in 2:100\n    x[t] = 0.5*x[t-1] + randn()\nend\n\n모형에서 생성된 하나의 시계열을 시각화 하면 아래와 같다.\n\nplot(x) # 그냥 그려본것임. 별 의미는 없음\n\n\n\n\nlag=1일 경우 이 시계열의 SACF를 계산하면 아래와 같다.\n\nx[1:99] .* x[2:100]\n\n99-element Vector{Float64}:\n  1.587897526021493\n  1.130306190921068\n  0.5698214432110668\n  0.4648189302568683\n  0.3099446153360606\n  0.36362604534744775\n  0.8191871414624922\n -0.1720390842292145\n -0.06301214708310766\n  0.026414715508855904\n -0.007988283356933327\n -0.04178812545299474\n  0.22453267567940685\n  ⋮\n  3.931333581073927\n  1.315564948810858\n  0.9096080102581454\n  0.5410986320348997\n  0.29627801400693676\n  1.0673283524686212\n -1.0394649044573636\n  2.80195248208142\n  4.152973765526384\n  2.316315764368524\n  0.978758337765867\n -0.5840281943972468\n\n\n\n이 계산결과는 각 \\(t\\)에 대하여 \\(x_{t-1}x_t\\) 를 계산한 것과 같다.\n\n이 수열들의 평균은 아래와 같다.\n\nx[1:99] .* x[2:100] |&gt; mean\n\n0.5835563885014224\n\n\n\n이 계산결과는 \\(\\frac{1}{99}\\sum_{t=2}^{100} x_{t-1}x_t\\)를 계산한 것과 같다.\n\n이론적인 값인 0.5 근처의 값이 잘 나옴을 알 수 있다.\nlag=2일 경우도 마찬가지로 구할 수 있다.\n\nx[1:98] .* x[3:100] |&gt; mean\n\n0.38420263596668275\n\n\n이러한 숫자들은 그런데 \\({\\bf x}{\\bf x}^T\\)를 이용하여서도 구할 수 있다.10\n10 참고로 여기에서 \\({\\bf x}\\)는 확률벡터가 아니라 realization을 의미함\nx*x'\n\n100×100 Matrix{Float64}:\n  0.760108    1.5879      0.541064   …  -1.57394    -0.472676    0.939172\n  1.5879      3.31719     1.13031       -3.28802    -0.987441    1.96197\n  0.541064    1.13031     0.385143      -1.12037    -0.336463    0.668527\n  0.800507    1.67229     0.569821      -1.65759    -0.497799    0.989089\n  0.441361    0.922022    0.314172      -0.913915   -0.274462    0.545336\n  0.533784    1.1151      0.379961   …  -1.10529    -0.331936    0.659531\n  0.517803    1.08171     0.368586      -1.0722     -0.321998    0.639786\n  1.20252     2.51212     0.855987      -2.49003    -0.747794    1.48581\n -0.108745   -0.227173   -0.0774074      0.225175    0.0676234  -0.134363\n  0.440444    0.920106    0.313519      -0.912016   -0.273892    0.544203\n  0.0455859   0.0952309   0.0324492  …  -0.0943935  -0.0283478   0.0563249\n -0.133198   -0.278257   -0.0948139      0.27581     0.0828298  -0.164577\n  0.238468    0.498169    0.169748      -0.493789   -0.148292    0.294646\n  ⋮                                  ⋱                          \n  2.04697     4.2762      1.45708       -4.2386     -1.27291     2.52919\n  0.488514    1.02053     0.347736      -1.01155    -0.303784    0.603596\n  1.41531     2.95665     1.00746    …  -2.93065    -0.880119    1.74873\n  0.290602    0.60708     0.206858      -0.601742   -0.180712    0.359062\n  0.774954    1.61891     0.551632      -1.60468    -0.481908    0.957516\n  1.04688     2.18698     0.745197      -2.16775    -0.651007    1.2935\n -0.754723   -1.57665    -0.537231       1.56279     0.469328   -0.932519\n -2.82194    -5.89516    -2.00873    …   5.84333     1.75484    -3.48673\n -1.11863    -2.33686    -0.796269       2.31632     0.695624   -1.38215\n -1.57394    -3.28802    -1.12037        3.25911     0.978758   -1.94472\n -0.472676   -0.987441   -0.336463       0.978758    0.293936   -0.584028\n  0.939172    1.96197     0.668527      -1.94472    -0.584028    1.16042\n\n\n여기에서 각 원소들이 의미하는 바는 아래와 같다.\n\n대각선의 원소: \\(x_t^2,~ t=1,2,\\dots,100\\) 을 의미\n대각선 한칸 위, 혹은 한칸 아래: \\(x_{t-1} x_t~ t=2,3,\\dots,100\\) 을 의미\n대각선 두칸 위, 혹은 두칸 아래: \\(x_{t-2} x_t~ t=3,4,\\dots,100\\) 을 의미\n\n\n\n\nx*x'의 계산결과를 캡쳐한 그림, 이것은 \\(\\hat{\\bf C}_{\\bf x}\\)를 의미함\n\n\n확인해보자.\nlag=1, 스크린샷의 노란색\n\n(x[1:99] .* x[2:100])[1:5]\n\n5-element Vector{Float64}:\n 1.587897526021493\n 1.130306190921068\n 0.5698214432110668\n 0.4648189302568683\n 0.3099446153360606\n\n\n\nlag1에 해당하는 숫자들임. 이는 스크린샷에서 노란색으로 표현된 1.589, 1.13031, 0.569821 … 등과 일치한다.\n\nlag=2, 스크린샷의 빨간색\n\n(x[1:98] .* x[3:100])[1:5]\n\n5-element Vector{Float64}:\n 0.5410642277088621\n 1.6722932576420804\n 0.3141719983177106\n 0.5621541352252872\n 0.30066534927151267\n\n\n\nlag2에 해당하는 숫자들임. 이는 스크린샷에서 빨간색으로 표현된 숫자들인 0.54164, 1.67229, 0.31417 … 등과 일치한다.\n\n\n\n스펙트럼 방법\n지금까지는 정상시계열일 경우 ACF를 이용한 간단한 분석방법을 다시 복습했다. 그리고 \\({\\bf C}_{\\bf x}\\)가 ACF를 구함에 필요한 모든정보를 가지고 있음을 이해했다. 한편 \\({\\bf C}_{\\bf x}\\)은 positive definite matrix 이므로 아래와 같이 분해가능하다.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식표현을 잘 해석하면 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf V}\\)와 \\({\\bf p}\\)에 담겨있다는 사실을 이해할 수 있다. 그런데 정상시계열일 경우 한정하여 \\({\\bf C}_{x}\\)의 고유벡터행렬은 \\({\\bf B}\\)의 고유벡터행렬과 일치한다는 사실을 알고 있다. 따라서 \\({\\bf V}\\)는 \\({\\bf B}\\)로 부터 그냥 알 수 있는 정보이다. 따라서 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf p}\\)에 담겨있다는 사실을 알 수 있다. 이는 적절한 \\({\\bf p}\\)를 추정하는 일은 적절한 \\({\\bf C}_{\\bf x}\\)를 추정하는 것과 같다는 사실을 알려준다.\n요약하면 아래와 같다.\n\n임의의 정상시계열은 이론적인 ACF (혹은 PACF)를 잘 추정하면 유니크하게 특정할 수 있다. (Wold’s Thm)\nACF를 잘 추정한다는 말은 \\({\\bf C}_{\\bf x}\\)를 잘 추정한다는 의미이다.\n그런데 \\({\\bf p}\\)를 잘 추정하면 \\({\\bf C}_{\\bf x}\\)를 잘 추정하는 일이 된다.\n따라서 임의의 정상시계열은 \\({\\bf p}\\)를 잘 추정하면 유니크하게 특정할 수 있다는 결론을 얻는다.\n\n여기에서 \\({\\bf p}\\)를 power spectral density 라고 부른다. 일반적으로 정상시계열을 분석하기 위해서는 \\({\\bf C}_{\\bf x}\\)를 특정하거나, \\({\\bf p}\\)를 특정하면 되는데 여기에서 \\({\\bf p}\\)를 특정한뒤 \\({\\bf p}\\)로 부터 \\({\\bf C}\\)를 역으로 해석하는 방법론을 spectral analysis라고 부른다. 경우에 따라서 \\({\\bf C}_{\\bf x}\\)를 특정하는 것이 용이할 수도 있지만 \\({\\bf p}\\)를 특정하고 해석하는 것이 용이할 때도 있다.\n그렇다면 주어진 시계열 \\({\\bf x}\\)에 대하여 \\({\\bf p}\\)를 어떻게 구할까? 직관적으로 생각하면 단순히 아래의 알고리즘으로 구하면 된다는 것을 알 수 있다.\n\n\\({\\bf C}_{\\bf x}\\)를 알아낸다.\n\\({\\bf C}_{\\bf x}\\)를 고유분해하여 \\({\\bf p}\\)를 구한다.\n\n또 다른 방법으로는 교재에 소개된 바 있는 아래의 수식을 이용하는 것이다.11\n11 이 수식이 성립하는 이유는 조금 손으로 써보면 금방 알 수 있음\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\]\n이것을 이용하면 아래와 같은 알고리즘을 떠올릴 수 있다.\n\n\\({\\bf B}\\)의 고유벡터행렬 \\({\\bf V}\\)를 구하고 \\({\\bf V}^H{\\bf x}\\)를 계산한다.\n계산된 결과를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n그런데 \\({\\bf V}^H{\\bf x}= {\\bf DFT} \\cdot {\\bf x}\\) 이므로 1의 과정을 아래와 같이 바꾸어 서술할 수 있다.\n\n\\({\\bf x}\\)를 퓨리에변환하여 \\(\\tilde{\\bf x} = {\\bf DFT} \\cdot {\\bf x}\\) 를 계산한다.\n\\(\\tilde{\\bf x}\\)를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n즉 임의의 시계열을 퓨리에변환한 뒤 제곱하면 \\({\\bf p}\\)를 얻을 수 있다.\n(예제2) – 하나의 realization에서 \\(\\hat{\\bf p}\\)를 구해보자.\n(예제1에 이어서) 아래의 모형에서 생성된 \\({\\bf x}\\)를 다시 고려하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n\nplot(x)\n\n\n\n\n이 자료의 PSD \\({\\bf p}\\)는 아래와 같이 구할 수 있다.\n단계1: \\({\\bf x}\\)의 DFT를 계산\n\nx̃ = fft(x) \n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\\({\\bf B}\\)를 설정하고 고유값분해 하기 귀찮아서 그냥 DFT해주는 패키지 사용함\n\n단계2: \\(\\hat{\\bf p}\\)를 계산\n\np̂ = abs.(x̃).^2\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n참고\nfft(x) 대신에 아래의 코드를 이용해도 된다.\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\nV'x\n\n100-element Vector{ComplexF64}:\n  -5.756917285643587 + 0.0im\n -19.082672090492103 - 1.0178306444775291im\n   14.23050682476898 - 11.867854578090007im\n  3.8980118254428824 + 1.2603018602424476im\n  -16.15797305318818 + 27.48824632227092im\n   12.32574209329044 - 1.5134316695905325im\n  3.9542122497256385 + 15.369129638224617im\n    9.51693811050782 + 19.371467179753516im\n  -19.38292930624826 + 9.495062886234233im\n -7.8539348514784155 + 4.134711886071595im\n -14.072349901900417 - 5.945064076174276im\n -14.596266922162371 + 3.447776409279244im\n   5.857720447482956 + 5.7388951128385735im\n                     ⋮\n   5.857720447482839 - 5.738895112838781im\n -14.596266922162307 - 3.4477764092792627im\n  -14.07234990190023 + 5.945064076174198im\n  -7.853934851478599 - 4.134711886071242im\n -19.382929306248577 - 9.49506288623372im\n   9.516938110507212 - 19.371467179753736im\n  3.9542122497250025 - 15.369129638224603im\n  12.325742093290597 + 1.5134316695903638im\n  -16.15797305318867 - 27.488246322270854im\n  3.8980118254424903 - 1.2603018602428118im\n  14.230506824769146 + 11.867854578089572im\n   -19.0826720904922 + 1.0178306444775123im\n\n\n진짜 똑같은지 확인\n\nfft(x)\n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\n전통적인 방법과 스펙트럼 방법의 비교\n시계열자료의 전통적인 분석과 spectral analysis는 대충 아래의 과정으로 비교 설명할 수 있다.\n\n\n\n\n\n\n\n\n단계\n전통적인 방법\n스펙트럴 분석\n\n\n\n\n1\n\\({\\bf x}\\)의 plot을 그려봄\n\\({\\bf x}\\)의 plot을 그려봄\n\n\n2\nSACF plot, SPACF plot 을 그려봄\nPSD plot을 그려봄\n\n\n3\nACF를 추정 (=ARMA(\\(p\\),\\(q\\))에 대응하는 파라메터를 추정)\n\\({\\bf p}\\)를 추정\n\n\n4\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n\n\n\n눈여겨 볼 점은 PSD plot의 존재이다. 전통적인 시계열에서 SACF plot 과 비슷하게 스펙트럼 방법에서 시계열을 분석하기 위해 필요한 매우 중요한 시각화 이다. 간단하게 비교를 하면 아래와 같다.\nSACF plot\n\nx축: lag=0, lag=1, ….\ny축: lag에 대응하는 상관계수값\n\nPSD plot\n\nx축: \\(\\Omega=\\big\\{\\frac{k}{N}:~ \\text{for}~ k=0,\\dots, N-1\\big\\}\\), 정규화된 freq를 의미함\ny축: 주파수에 대응하는 power값\n\n전통적인 방법에 비하여 스펙트럴 분석이 가지는 장점은 위의 표에서 소개한 일반적인 분석루틴이 시계열이 아닌 그래프신호로 쉽게 확장가능 하다는 점이다12. 따라서 앞으로는 전통적인 시계열 분석방법 대신 스펙트럴 분석만을 다룰 것이다. 스펙트럴 분석의 핵심적인 부분은 \\({\\bf p}\\)를 추정하는 방법과 추정량의 점근적 성질들을 파악하는 것이다. 이 포스트에서는 \\({\\bf p}\\)를 추정하는 방법만을 다룬다.\n12 퓨리에 변환대신에 그래프 퓨리에 변환을 이용하기만 하면된다"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프신호에서의 PSD의 추정",
    "text": "그래프신호에서의 PSD의 추정\n이제 그래프 신호에서 \\({\\bf p}\\)를 추정하는 방법에 대하여 살펴보자. 그래프이동변환 (Graph Shift Operator, GSO)13 \\({\\bf S}={\\bf V}{\\bf \\Lambda}{\\bf V}^H\\)에 대하여 정상인 시계열 \\({\\bf x}\\)를 고려한다. 이 신호의 그래프퓨리에 변환14은 아래와 같이 구할 수 있다.\n13 Back shift operator의 일반화 버전14 좀 더 정확하게는 \\({\\bf V}^H\\) 에 대한 그래프 변환이라고 한다\\[\\tilde{\\bf x}={\\bf GFT} {\\bf x} = {\\bf V}^H{\\bf x}\\]\n여기에서 \\(\\tilde{\\bf x}\\)를 \\({\\bf x}\\)의 주파수응답(frequency representation)이라고 부른다.15 우리는 아래의 수식에서 \\({\\bf p}\\)의 값에 관심이 있다.\n15 이 \\(\\tilde{\\bf x}\\)를 그냥 graph Fourier transform이라고 부르는 사람도 많다. 즉 그래프퓨리에변환이 (1) 변환매트릭스 \\({\\bf GFT}\\)자체를 지칭할때도 있고 (2) 트랜스폼된 결과 \\(\\tilde{\\bf x}\\)를 지칭할때도 있음. 교재에서는 변환은 graph Fourier transform, 그리고 변환된 결과는 \\({\\bf x}\\)의 주파수응답이라고 한다.\\[{\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\]\n여기에서 \\({\\bf p}\\)를 PSD (power spectrum density) 라고 한다. \\({\\bf p}\\)가 포함된 표현식은 위의 수식 이외에도 2개가 더 있다. 이를 모두 요약하면 아래와 같다16\n16 약간의 계산을 통하면 1,2,3이 쉽게 같은 수식임을 알 수 있음\n\\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)17\n\\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n\\({\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\)\n\n17 이 수식을 살짝 정리하면 \\({\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\) 와 같이 보다 예쁜 수식을 얻을 수 있음위의 표현중 3.에서 \\({\\bf c}_{\\bf x}\\)은 \\({\\bf C}_x\\)를 벡터화한 것이며 \\({\\bf G}_{np}\\)는 \\({\\bf V}^\\ast\\) 와 \\({\\bf V}\\)를 열별-크로네커곱 (column-wise Kronecker product) 이다. 이때 \\({\\bf G}_{np}\\)의 정의가 조금 생소하니 한번 계산하여 보자.\n(예제) 아래와 같은 GSO \\({\\bf B}\\)를 고려하자.\n\nB= [0 1 0 0 \n    0 0 1 0 \n    0 0 0 1 \n    1 0 0 0]\n\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  1  0\n 0  0  0  1\n 1  0  0  0\n\n\n이러한 GSO에 대하여 \\({\\bf G}_{np}\\)는 아래와 같이 구할 수 있다.\n(1) \\({\\bf V}\\)를 정의\n\nV = [i*j for i in 0:3 for j in 0:3] |&gt; \n    x -&gt; reshape(x,(4,4)) .|&gt; \n    x -&gt; exp(im * (2π/4) * x) \n\n4×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n16×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n\n\n위에서 언급한 표현식 1,2,3 을 이용하면 \\({\\bf p}\\)를 추정하는 세 가지 방법을 각각 정의할 수 있다. 하나씩 살펴보자.\n\n1. \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 수식 \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)를 적당히 변형하면 아래를 얻을 수 있다.\n\\[{\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\]\n여기에서\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^H]\\approx \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_t{\\bf x}_r^H\\]\n이므로 이 수식에 근거하여 \\({\\bf p}\\)을 추정한다면 아래와 같이 할 수 있다.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면18, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n18 대부분은 관측한 시계열이 하나겠지..\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H{\\bf x}_r{\\bf x}_r^H{\\bf V} \\right].\\]\n\n주의: 여기에서 \\({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V}\\) 는 항상 대각행렬이지만 \\({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V}\\) 은 대각행렬이 아닐수도 있음을 유의하자. 즉 이론적인 모수는 대각행렬이지만 sample version은 대각행렬이 아닐 수 있다. 대각선이 아닌 원소는 버리면 된다.)\n\n\n아이디어: 혹시 대각선이 아닌 원소들을 이용하여 오차항 \\(\\epsilon_t\\)의 분산을 추정할 수도 있지 않을까? 이미 연구가 있겠지?\n\n(예제)\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n\np̂ = diag(V' * (x*x') * V)\n\n100-element Vector{ComplexF64}:\n 33.142096633741986 + 0.0im\n 365.18435333408354 + 1.5376069362644531e-13im\n  343.3532967764883 + 6.904176529646917e-14im\n 16.782856970223083 - 3.5538396658301444e-14im\n 1016.6837790613963 + 5.475049904926759e-15im\n 154.21439356883144 + 6.4512443306088e-14im\n  251.8459403524346 + 2.1316282072803006e-14im\n 465.82585169550384 + 1.816929057526117e-13im\n 465.85416770456044 + 4.1584439183295984e-14im\n    78.780135032089 + 1.3472456770553478e-14im\n 233.37481863133462 + 6.315728724701355e-14im\n 224.93817023139385 - 3.472109560086835e-14im\n  67.24780595702241 + 7.105427357601002e-14im\n                    ⋮\n   67.2478059570233 + 6.384723798533952e-14im\n 224.93817023139195 + 1.9727655769954595e-14im\n 233.37481863132837 - 2.1872689567834747e-14im\n    78.780135032089 + 1.917599080404094e-14im\n 465.85416770456294 + 4.808950231511622e-14im\n 465.82585169550094 - 4.890486289860305e-14im\n  251.8459403524291 + 2.0146681724568905e-14im\n  154.2143935688347 - 1.0948596967617507e-13im\n 1016.6837790614081 + 1.2114814701286432e-13im\n  16.78285697022108 + 2.376159104534641e-14im\n 343.35329677648286 + 1.1310381241837407e-14im\n 365.18435333408746 + 4.574214786667376e-14im\n\n\n\n\n2. \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\approx \\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n따라서 \\(\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2\\) 를 PSD \\({\\bf p}\\)에 대한 추정량이라고 생각할 수 있다. 이러한 추정량을 기호로 \\(\\hat{\\bf p}_{pg}\\)라고 정의하고 periodogram이라고 부른다. 즉\n\\[\\hat{\\bf p}_{pg}=\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n\\[\\hat{\\bf p}_{pg}=|{\\bf V}^H {\\bf x}_r|^2 \\]\n즉 이 경우 \\(\\hat{\\bf p}_{pg}\\)는 단순히 관측시계열 \\({\\bf x}_r\\)의 그래프 퓨리에 변환 \\(\\tilde{\\bf x}={\\bf V}^H{\\bf x}_r\\) 결과에 절대값을 취하고 제곱한 것과 같다.\n(예제)\n스펙트럼방법챕터 예제2에서 이미 보여준 적 있다. 주어진 시계열 \\({\\bf x}\\)에 대하여 \\(\\hat{\\bf p}_{pg}\\)를 구하는 방법을 요약하면 아래와 같다.\n\nx̃ = fft(x) # 단계1: GFT, 이 신호는 시계열이라서 GFT대신에 DFT를 써도 된다.\np̂ = abs.(x̃).^2 # 단계2: hat p\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n\n\n3. \\({\\bf c}_{\\bf x} = {\\bf G}_{np} {\\bf p}\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식으로부터 아래를 얻을 수 있다.\n\\[{\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\]\n여기에서 \\({\\bf c}_{\\bf x}\\) 대신에 \\(\\hat{\\bf c}_{\\bf x}\\) 를 대입하면 아래와 같이 생각할 수 있다.\n\\[\\hat{\\bf c}_{\\bf x} \\approx  {\\bf G}_{np} {\\bf p}\\]\n이 문제는 아래와 같은 회귀모형으로 생각할 수 있다.\n\n\n\n\n\n\n\n\n\n회귀모형\n우리의 문제\n\n\n\n\n모형\n\\({\\bf y} \\approx {\\bf X}{\\boldsymbol \\beta}\\)\n\\(\\hat{\\bf c}_{\\bf x} \\approx {\\bf G}_{np}{\\bf p}\\)\n\n\n설명변수\n\\({\\bf X}\\)19\n\\({\\bf G}_{np}\\)20\n\n\n반응변수\n\\({\\bf y}\\)21\n\\(\\hat{\\bf c}_{\\bf x}\\)22\n\n\n추정하고 싶은 파라메터\n\\({\\boldsymbol \\beta}\\)23\n\\(\\hat{\\bf p}\\)24\n\n\n오차항\n대부분 정규분포를 가정\n??? 모르겠는데??\n\n\n\n19 (n,p) matrix20 (N²,N) matrix21 (n,1) col-vector22 (N²,1) col-vector23 (p,1) col-vector24 (N,1) col-vector회귀분석에서 아래의 수식이 익숙하다면\n\\[\n\\hat{\\boldsymbol \\beta}_{ls} = \\underset{\\boldsymbol \\beta}{\\operatorname{argmin}} \\|{\\bf y}-{\\bf X}{\\boldsymbol \\beta}\\|_2^2=({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}.\n\\]\n\\({\\bf p}\\)를 추정하기 위한 아래의 수식도 쉽게 이해할 수 있다. (의문: 그런데 왜 MSE를 손실함수로 쓰고 있는 거야? 오차항이 설마 정규분포?)\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\n(예제)\n(1) \\({\\bf V}\\)를 정의\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n10000×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im      0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im   …  0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im      0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im    …  0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n 1.0+0.0im       1.0+0.0im                1.0+0.0im\n\n\n(3) \\(\\hat{\\bf p}_{ls}=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\)\n\nĉₓ = vec(x*x')\np̂ = inv(Gₙₚ' * Gₙₚ) * Gₙₚ' * ĉₓ \n\n100-element Vector{ComplexF64}:\n  0.003314209663374193 - 2.7356277964988863e-19im\n   0.03651843533340838 - 4.01518191768058e-18im\n   0.03433532967764885 + 2.515448157755484e-17im\n 0.0016782856970223292 - 1.0070028487673847e-17im\n   0.10166837790613971 + 3.1129277935880596e-18im\n  0.015421439356883134 + 9.403422807142065e-18im\n  0.025184594035243472 - 3.993782799800785e-18im\n   0.04658258516955039 - 1.850761436988587e-18im\n   0.04658541677045607 + 1.1559103895961936e-17im\n  0.007878013503208905 + 3.559698092088507e-18im\n  0.023337481863133468 + 2.6204945155857973e-18im\n   0.02249381702313939 + 5.304406111488559e-18im\n  0.006724780595702225 - 1.655564138463681e-17im\n                       ⋮\n  0.006724780595702329 + 1.8121162053534517e-18im\n  0.022493817023139205 - 1.0461976779111972e-17im\n   0.02333748186313285 - 6.792203007975684e-18im\n  0.007878013503208907 - 2.3575339315335667e-18im\n  0.046585416770456294 + 1.5392042695643853e-17im\n  0.046582585169550106 - 1.123245521985718e-17im\n  0.025184594035242928 + 1.1628578774983873e-18im\n  0.015421439356883466 + 5.864828990948797e-18im\n   0.10166837790614085 + 2.2712943512935246e-17im\n 0.0016782856970221013 + 4.829637376114682e-18im\n   0.03433532967764831 + 3.3208196889839756e-19im\n  0.036518435333408754 + 1.3795822112205515e-17im\n\n\n\n?? 뭔가 스케일이 안맞음\n\n\nN^2 * p̂\n\n100-element Vector{ComplexF64}:\n  33.14209663374193 - 2.7356277964988864e-15im\n 365.18435333408377 - 4.0151819176805797e-14im\n  343.3532967764885 + 2.515448157755484e-13im\n 16.782856970223293 - 1.0070028487673847e-13im\n 1016.6837790613971 + 3.1129277935880596e-14im\n 154.21439356883135 + 9.403422807142065e-14im\n 251.84594035243472 - 3.9937827998007846e-14im\n  465.8258516955039 - 1.850761436988587e-14im\n  465.8541677045607 + 1.1559103895961937e-13im\n  78.78013503208905 + 3.559698092088507e-14im\n 233.37481863133468 + 2.6204945155857973e-14im\n 224.93817023139388 + 5.304406111488559e-14im\n  67.24780595702225 - 1.655564138463681e-13im\n                    ⋮\n  67.24780595702329 + 1.8121162053534517e-14im\n 224.93817023139206 - 1.0461976779111972e-13im\n  233.3748186313285 - 6.792203007975684e-14im\n  78.78013503208906 - 2.3575339315335666e-14im\n 465.85416770456294 + 1.5392042695643854e-13im\n 465.82585169550106 - 1.123245521985718e-13im\n 251.84594035242927 + 1.1628578774983874e-14im\n 154.21439356883465 + 5.864828990948797e-14im\n 1016.6837790614085 + 2.2712943512935246e-13im\n  16.78285697022101 + 4.8296373761146824e-14im\n  343.3532967764831 + 3.3208196889839758e-15im\n  365.1843533340875 + 1.3795822112205514e-13im\n\n\n\n\\(N^2\\)를 곱해주니까 아까부터 구하던 값이 그대로 잘 나옴. (\\({\\bf DFT}\\) 혹은 \\({\\bf GFT}\\)를 정의할때 \\(\\frac{1}{\\sqrt N}\\)으로 스케일링 하느냐 마느냐 차이때문에 생기는 현상임)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "의문점",
    "text": "의문점\n아래의 그림을 살펴보자.\n\n\n\n그림12.3(교재에서 긁어온 그림): Power spectral density estimation. All estimators are based on the same random process defined on the Karate club network (Zachary 1977). (A) Periodogram estimation with different numbers of observations. (B) Windowed average periodogram from a single realization and a different number of windows. (C) Windowed average periodogram for four windows and a varying number of realizations. (D) Parametric MA estimation for 1 and 10 realizations.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n이 그림은 다양한 방법으로 true PSD \\({\\bf p}\\)를 추정한 결과를 나타내는 PSD plot 이다25. 우리가 적용한 방법은 (A)에서 \\(R=1\\)일 경우이다. 보는것 처럼 true PSD 를 놀라울 정도로 제대로 추정하지 못한다26. 만약에 우리가 모형에서 하나의 시계열이 아니라 1000개의 정도의 시계열을 관측하였다면 좀 더 합리적으로 추정할 수 있다. 그런데 사실 하나의 모형에서 1000개씩이나 되는 시계열을 관측하는 일은 현실적으로 불가능하다27 따라서 우리는 비교적 적은 \\(R\\)에서 합리적인 PSD의 추정치를 이끌어내야 한다. 그림 (B),(C)는 상대적으로 적은 \\(R\\)에 대해 \\({\\bf p}\\)를 추정하는 windowed periodogram 을 이용하여 PSD를 추정한 결과이다. (C)를 살펴보면 \\(R=1\\) 일경우 \\({\\bf p}\\)를 추정한 값들이 나와있는데 (A)와 비교하면 꽤 합리적으로 보인다.\n25 x축이 freq, y축이 PSD26 맞추는게 없는 것 같은데?27 그리고 대부분 \\(R=1\\)이지..28 약간 바이어스가 있어보이긴 하는데, 우연히 생긴건지 이론적으로 항상 생기는 건지는 잘 모르겠네?29 이런걸 세미파라메트릭 모형이라고 해요30 그래서 플랏을 보면서 적당한 ARMA를 찾을 필요도 없고, AIC 니 BIC 를 따져가면서 모형선택을 할 필요도 없고31 적합이후에 잔차분석 같은거 안해도 된다는 의미문제는 (A)-(C)에서 제안된 방법 모두가 (D)에 제시된 전통적인 방법에 비하여 퍼포먼스가 떨어진다는 것이다. (D)는 parametric 모형을 사용한 결과이다. 파라메트릭 방법이므로 특정 모델을 한정하고 거기에 대응하는 한두개의 모수만 추정하면 되므로 추정이 잘 된다.28 반면 (A)-(C)의 경우 한 두개의 파라메터가 아니라 \\({\\bf p}\\)의 모든 원소를 추정해야하므로 추정할 파라메터가 데이터의 수 \\(N\\)과 같다29. 따라서 추정치의 분산이 크다. 사실 이것은 파라메트릭 방법과 세미파라메트릭 방법이라는 구조적인 차이때문에 어쩔 수 없는 것 같다. 그래도 세미파라메트릭 방법은 머리아프게 모델링을 할 필요가 없고30 내가 적합한 모델이 맞는지 확인할 필요도 없다31는 장점이 있다.\n아래는 나름 PSD를 추정하는 신기술인 것 같다.\n\n\n\n그림12.4(교재에서 긁어온 그림): PSD estimation from a subset of nodes. Estimators are based on a random process defined on the Karate club network (Zachary 1977). (A) Graph sampling for nonparametric PSD estimation. Here, 20 out of 34 nodes are observed. The sampled nodes are highlighted by the circles around the nodes. (B) Nonparametric PSD estimation based on observations from 20 nodes and 100 data snapshots. (C) Graph sampling for parametric MA PSD estimation. Here, 4 out of 34 nodes are observed. (D) Parametric MA PSD estimation based on observations from 4 nodes and 100 data snapshots.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n그래프신호의 sub-sampling을 이용하는 것 같은데 교재의 뒤쪽에 서술되어있다. \\(R=100\\)임을 고려하여도 퍼포먼스가 좋은 편인듯 하다32.\n\n\n32 내 생각엔 이게 핵심 기술인 것 같음"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "using LinearAlgebra, DSP"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Simultaneously Diagonalizable",
    "text": "Simultaneously Diagonalizable\n매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 대각화 가능하다는 것은 아래의 표현을 만족하는 적당한 invertible matrix \\({\\bf \\Psi}_A\\), \\({\\bf \\Psi}_B\\)와 대각행렬 \\({\\bf \\Lambda}_A\\), \\({\\bf \\Lambda}_B\\)가 존재한다는 의미가 된다.\n\\[{\\bf A} = {\\bf V}_{A} {\\bf \\Lambda}_A {\\bf V}_{A}^{-1}\\]\n\\[{\\bf B} = {\\bf V}_{B} {\\bf \\Lambda}_B {\\bf V}_{B}^{-1}\\]\n그리고 만약에 \\({\\bf V}_{A}={\\bf V}_{B}\\)이라면 즉\n\\[{\\bf A} = {\\bf V} {\\bf \\Lambda}_A {\\bf V}^{-1}\\]\n\\[{\\bf B} = {\\bf V} {\\bf \\Lambda}_B {\\bf V}^{-1}\\]\n이라면 \\(\\{{\\bf A},{\\bf B}\\}\\)가 simultaneously diagonalzable 하다고 표현한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Commute",
    "text": "Commute\n두 matrix \\({\\bf A}\\)와 \\({\\bf B}\\)에 대하여\n\\[{\\bf A}{\\bf B}= {\\bf B}{\\bf A}\\]\n인 관계가 성립하면 두 매트릭스가 commute 한다고 표현한다. 그런데 \\({\\bf A}{\\bf B}={\\bf A}{\\bf B}\\)의 조건은 \\({\\bf A}, {\\bf B}\\)가 동시대각화가능할 (simultaneously diagonalzable) 조건과 같다. 1 따라서 simultaneously diagonalzable 는 commute와 같은 말이라 생각해도 무방하다.\n1 필요충분조건이다.\n참고: 위키피디아.."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Shift Invariant Filter",
    "text": "Shift Invariant Filter\n\nref: Djuric and Richard (2018) Chap 8.3 의 내용 중 일부\n\nDjuric, Petar, and Cédric Richard. 2018. Cooperative and Graph Signal Processing: Principles and Applications. Academic Press.\n\nDefine the matrix \\({\\bf B}\\) as periodic shift matrix such that\n\\[\n{\\bf B} = \\begin{bmatrix}\n0 & 0 & 0 & \\dots  & 0 & 1 \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\n0 & 0 & \\dots & 1 & 0 & 0 \\\\\n0 & 0 & \\dots & 0 & 1 & 0 \\\\\n\\end{bmatrix}.\\]\nA generic filter \\({\\boldsymbol h}\\) is given by its \\(z\\)-transform\n\\[h(z)=h_0z^0+h_1z^{-1}+\\cdots +h_{N-1}z^{-(N-1)}\\]\nwhere \\(s_{n-1}=z^{-1}s_n\\). In vector notation, and with respect to the standard basis \\({\\bf I}\\), the filter is represented by the matrix \\({\\bf H}\\), a polynomial in the cyclic shift\n\\[{\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+\\cdots+h_{N-1}{\\bf B}^{N-1}.\\]\nFilters are shift invariant iff\n\\[z\\cdot h(z) = h(z)\\cdot z\\]\nor from the matrix representation\n\\[{\\bf B}h({\\bf B})=h({\\bf B}){\\bf B}.\\]\nExample\nLet \\({\\bf B}\\) as\n\nB= [0 1 0 0 0 0 0\n    0 0 1 0 0 0 0 \n    0 0 0 1 0 0 0 \n    0 0 0 0 1 0 0 \n    0 0 0 0 0 1 0 \n    0 0 0 0 0 0 1 \n    1 0 0 0 0 0 0]\n\n7×7 Matrix{Int64}:\n 0  1  0  0  0  0  0\n 0  0  1  0  0  0  0\n 0  0  0  1  0  0  0\n 0  0  0  0  1  0  0\n 0  0  0  0  0  1  0\n 0  0  0  0  0  0  1\n 1  0  0  0  0  0  0\n\n\nDefine \\({\\boldsymbol h}\\) as\n\nh = [1/3,1/3,1/3]\n\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\n\nFurthermore define \\({\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+h_2{\\bf B}^2\\)\n\nH = (1/3)*B^0 + (1/3)*B^1 + (1/3)*B^2 \n\n7×7 Matrix{Float64}:\n 0.333333  0.333333  0.333333  0.0       0.0       0.0       0.0\n 0.0       0.333333  0.333333  0.333333  0.0       0.0       0.0\n 0.0       0.0       0.333333  0.333333  0.333333  0.0       0.0\n 0.0       0.0       0.0       0.333333  0.333333  0.333333  0.0\n 0.0       0.0       0.0       0.0       0.333333  0.333333  0.333333\n 0.333333  0.0       0.0       0.0       0.0       0.333333  0.333333\n 0.333333  0.333333  0.0       0.0       0.0       0.0       0.333333\n\n\nObserve following:\n\nB*H == H*B \n\ntrue\n\n\nThus, filter \\({\\boldsymbol h}\\) is shift invariant filter and matrix \\({\\bf H}\\) is shift invariant operator.\nnote: \\({\\boldsymbol h}\\) is moving average filter.\nnote: for any \\({\\bf x}\\), \\({\\bf H}{\\bf x}\\) is definded by\n\\[\\left[\\frac{x_{n-1}+x_n+x_1}{3},\\frac{x_n+x_1+x_2}{3},\\dots,\\frac{x_{n-3}+x_{n-2}+x_n}{3}\\right].\\]\n\nx = [1,1,1,1,2,2,2]\nH*x\n\n7-element Vector{Float64}:\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666665\n 2.0\n 1.6666666666666665\n 1.3333333333333333\n\n\nnote: In some sense, the matrix \\({\\bf H}{\\bf x}\\) can be thought as generalized version of \\({\\boldsymbol h}\\star {\\bf x}\\) where \\(\\star\\) is convolution up to shift\n\nconv(h,x)\n\n9-element Vector{Float64}:\n 0.3333333333333334\n 0.6666666666666667\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666667\n 2.0\n 1.3333333333333333\n 0.6666666666666667\n\n\nFinally, we observe that, from the Cayley-Hamilton Theorem, \\({\\bf B}\\) satisfies its characteristic polynomial \\(\\Delta({\\bf B})\\), where \\(\\Delta(\\lambda)\\) is the determinant of \\(\\lambda{\\bf I}-{\\bf B}\\). The characteristic polynomial \\(\\Delta({\\bf B})\\) has degree \\(N\\), so, in DSP, as described so far, linear filters are (matrix) polynomial with degree at most \\(N-1\\).\n\n이 부분은 책에 써있길래 가져오긴 했는데, 무슨 의미인지 모르겠음"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Coexisting Approaches",
    "text": "Coexisting Approaches\nStationary graph processes were first defined and analyzed in (Girault 2015). The fundamental problem identified there is that GSOs do not preserve energy in general and therefore cannot be isometric (Gavili and Zhang 2017). This problem is addressed in (Girault, Gonçalves, and Fleury 2015) with the definition of an isometric graph shift that preserves the eigenvector space of the Laplacian GSO but modifies its eigenvalues.\n\nGirault, Benjamin. 2015. “Stationary Graph Signals Using an Isometric Graph Translation.” In 2015 23rd European Signal Processing Conference (EUSIPCO), 1516–20. IEEE.\n\nGavili, Adnan, and Xiao-Ping Zhang. 2017. “On the Shift Operator, Graph Frequency, and Optimal Filtering in Graph Signal Processing.” IEEE Transactions on Signal Processing 65 (23): 6303–18.\n\nGirault, Benjamin, Paulo Gonçalves, and Éric Fleury. 2015. “Translation on Graphs: An Isometric Shift Operator.” IEEE Signal Processing Letters 22 (12): 2416–20.\nA stationary graph process is then defined as one whose probability distributions are invariant with respect to multiplications with the isometric shift. One drawback of this approach is that the isometric shift is a complex-valued operator and has a sparsity structure (if any) different from \\({\\bf S}\\). By contrast, the vertex-based definition in\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf x}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf x}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf x}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf x} \\Big)^H \\bigg]\\]\nis based on the original GSO \\({\\bf S}\\), which is local and real-valued. As a result, above Eq. provides intuition on the relations between stationarity and locality, which can be leveraged to develop stationarity tests or estimation schemes that work with local information. Graph stationarity was also studied in (Perraudin and Vandergheynst 2017) where the requirement of having a covariance matrix diagonalizable by the eigenvectors of the Laplacian GSO is adopted as a definition. This condition is shown to be equivalent to statistical invariance with respect to the translation operator introduced in (Shuman, Ricaud, and Vandergheynst 2016). When the shift \\({\\bf S}\\) coincides with the Laplacian of the graph and the eigenvalues of \\({\\bf S}\\) are all distinct, Definitions 12.1 and 12.2 are equivalent to those in Perraudin and Vandergheynst (2017). Hence, the definitions presented here differ from (Perraudin and Vandergheynst 2017) in that we consider general normal shifts instead of Laplacians and that we see Definition 12.1 as a definition, not a property. These are mathematically minor differences that are important in practice though; see Segarra et al. (2017) for more details.\n\nPerraudin, Nathanaël, and Pierre Vandergheynst. 2017. “Stationary Signal Processing on Graphs.” IEEE Transactions on Signal Processing 65 (13): 3462–77.\n\nShuman, David I, Benjamin Ricaud, and Pierre Vandergheynst. 2016. “Vertex-Frequency Analysis on Graphs.” Applied and Computational Harmonic Analysis 40 (2): 260–91.\n\nSegarra, Santiago, Antonio G Marques, Gonzalo Mateos, and Alejandro Ribeiro. 2017. “Network Topology Inference from Spectral Templates.” IEEE Transactions on Signal and Information Processing over Networks 3 (3): 467–83."
  },
  {
    "objectID": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "href": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅",
    "section": "",
    "text": "About this doc\n- 우분투에서 여러가지 개발환경을 설정하는 방법을 포스팅 하겠다.\n- 이 포스트는 우분투를 메인OS(사무용+연구용)로 사용하고 싶은 사람, 우분투를 활용하여 개발용 서버를 구축하고 싶은 사람에게 모두 유용한다.\n- 이 포스트는 2080 이상의 GPU를 활용한 학습을 원하는 사람에게 유용하다.\n- 이 포스트는 R과 파이썬을 동시에 쓰는 사람에게 유용하다.\n- 이 포스트는 Rstudio, Jupyter Lab을 동시에 쓰는 사람에게 유용하다.\n- 매년 조금씩 셋팅방법이 다른것 같다.\n- 가장 최근에는 2023년 3월8일에 이 블로그 내용으로 셋팅해보았음.\n\n\n우분투설치\n- 22.04부터는 파티션 나누지 않고 그냥 설치해도 잘 되는것 같다.\n\n\n네트워크 설정\n- ?표시있는 아이콘 \\(\\to\\) Wired Connected \\(\\to\\) Wired Settings \\(\\to\\) Connection의 설정 \\(\\to\\) IPv4 \\(\\to\\) Manual \\(\\to\\) Address, Netmask, Gateway, DNS 설정 \\(\\to\\) 네트워크 토글\n\n\n한글설정 (개발용 서버일 경우 생략 가능)\n- 아래와 같이 커맨드에 친다.\nibus-setup\n이걸 치면 IBus Preferences 라는 창이 나오는데 여기에서 (1) Input Method 탭 클릭 (2) Add 버튼 클릭 (3) Korean 선택 (4) Hangul 선택을 한다.\n- 위의 단계에서 Korean이 안보이면 Language Support로 가서 한국어팩을 설치하고 리부팅 하면 된다. (보통 실행하자마자 알아서 설치되더라.. 설치가 안되면 Install / Remove Languages... 이라는 탭을 클릭해서 설치하자) 리부팅을 꼭 해야한다는 것에 주의하자.\n- 이제 Region & Language로 가서 설정하면 된다.\n\n\n그래픽카드 드라이버설치\n- 전체적인 내용은 여기를 참고하자.\n- 준비작업\nsudo apt update \nsudo apt install gcc\nsudo apt install build-essential\n- 우선 gedit를 열고 아래를 복사해서 붙여넣는다.\nblacklist nouveau\noptions nouveau modeset=0\n파일이름을 blacklist-nouveau.conf로 home에 저장\n- 루트권한획득\nsudo -i\n아이디와 비밀번호를 입력하고 루트권한을 얻는다.\n- 아래를 입력한다.\nsudo cp /home/cgb2/blacklist-nouveau.conf /etc/modprobe.d\nsudo update-initramfs -u\nsudo reboot \n- 그래픽카드 다운로드: 드라이버 설치파일을 다운받는다. 앤비디아공식홈페이지에서 다운받자. OS를 리눅스 64-bit으로 선택하고 검색을 누르면 다운받아진다.\n- 그래픽키다 설치: 다운받은뒤에는 파일이 있는 폴더로 이동하여\nchmod +x NVIDIA-Linux-x86_64-410.78.run\n를 실행하자. 보통 NVI까지치고 적당히 탭을 누르면 알아서 뒷부분이 완성된다. 이 과정은 추후에 드라이버를 실행할수 있도록 권한을 풀어두는 것이다. 그리고 아래를 실행한다.\nsudo ./NVIDIA-Linux-x86_64-410.78.run\n그 다음 드라이버가 잘 설치되었는지 확인한다.\nnvidia-smi\n\n\n아나콘다\n- (아나콘다 설치) 아나콘다를 다운받은 폴더로 가서 아래와 같이 실행한다.\nbash Anaconda3-2019.03-Linux-x86_64.sh\n대충 bash Ana 정도까지만 치고 tab을 누르면 알아서 완성된다.\n- (환경만들기) 커맨드를 키고 아래를 실행한다.\n(base) conda create -n py38r40 python=3.8\n(base) conda create --name py38r40 python=3.8\n둘 중 아무거나 실행해도 된다. 파이썬 환경이 너무 높으면 나중에 conda tensorflow-gpu가 먹히지 않으니 환경을 만들때 파이썬버전을 3.8.x로 하자. (현시점 2021년 2월25일기준 3.9.x이면 conda tensorflow-gpu 가 동작하지 않음.)\n\n\nssh연결\n- 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다.\nsudo apt install openssh-server\n22번포트 우회하기\n- step1: /etc/ssh/sshd_config 파일을 연다.\nsudo vi /etc/ssh/sshd_config \n- step2: Port 22 라고 된 부분의 주석을 풀고 원하는 포트번호 설정\n...\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n...\n- step3: 수정내용을 적용\nsudo systemctl restart ssh.service\n- step4: 수정한 포트로 ssh접속\n\n\n주피터랩 원격제어\n- 1단계: 주피터랩설치\n(py38) conda install -c conda-forge jupyterlab\n\nNote: 사실 위에서 주피터랩을 따로 설치안해도 주피터랩이 잘만 실행된다. 하지만 이렇게하니까 나중에 R커널을 만들기위해 IRkernel::installspec()을 실행할때 에러가 난다.\n\n- 2단계: 패스워드 설정\n(py38) jupyter lab --generate-config\n(py38) jupyter lab password\n- 3단계: jupyter lab 환경설정\nnano /home/cgb/.jupyter/jupyter_lab_config.py \n아래를 변경\nc.ServerApp.ip = '192.168.0.4'\nc.ServerApp.port = 1306\nc.ServerApp.open_browser = False\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\n\n주피터노트북 원격제어\n- 1단계: 주피터노트북 설치 (보통 lab을 설치하면 이미 설치되어있음)\n(py38) conda install -c conda-forge notebook \n- 2단계: 패스워드 설정\nfrom notebook.auth import passwd\npasswd()\nEnter password: \nVerify password: \n생성된값 (argon 어쩌고..)을 복사\n- 3단계: 환경설정\njupyter notebook --generate-config\nnano /home/cgb/.jupyter/jupyter_notebook_config.py\n아이피주소와 패스워드를 바꾼다. (port는 선택, browser도 선택 )\nc.NotebookApp.open_browser = False\nc.NotebookApp.ip = '192.168.0.4'\nc.NotebookApp.port = 1307\nc.NotebookApp.password = ''\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\nTip: 주피터노트북과 랩을 양쪽으로 셋팅후 주피터 노트북으로 실행하면 2개를 모두 쓸 수 있음\n\n\n\nR설치ver1: (base)에 설치\n- 설치전: 기존의 R 삭제\nconda remove r-base -y \nsudo apt-get remove r-base-core \nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\n- R설치전 준비작업: 나노에디터를 키고 /etc/apt/sources.list를 연다.\nsudo nano /etc/apt/sources.list\n화살표로 이동하여 맨아래로 간뒤에 아래중 하나를 추가한다. (나는 focal-cran40으로 추가함)\ndeb https://cloud.r-project.org/bin/linux/ubuntu impish-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu hirsute-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran40/\n저장후 나노에디터 종료. 그리고 아래를 실행.\nsudo apt-get update\n경우에 따라서 아래와 같은 에러메시지가 뜰 수 있다.\n...\nW: GPG error: https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9 \n...\n공개키가 없어서 생기는 에러이므로 아래와 같이 가져온다.\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n#sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 51716619E084DAB9\n그리고 다시 아래를 실행\nsudo apt-get update\n에러가 없이 뭔가 마무리 되어야한다.\n(base) cgb3@cgb3:~$ sudo apt-get update\nIgn:1 http://linux.dropbox.com/ubuntu disco InRelease\nHit:2 http://security.ubuntu.com/ubuntu focal-security InRelease  \nHit:3 http://kr.archive.ubuntu.com/ubuntu focal InRelease                                 \nHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease                \nHit:5 http://linux.dropbox.com/ubuntu disco Release                 \nGet:6 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\nHit:8 http://kr.archive.ubuntu.com/ubuntu focal-updates InRelease\nGet:9 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [46.4 kB]\nHit:10 http://kr.archive.ubuntu.com/ubuntu focal-backports InRelease\nFetched 50.0 kB in 1s (36.5 kB/s)                   \nReading package lists... Done\n- R설치\nwget http://security.ubuntu.com/ubuntu/pool/main/i/icu/libicu66_66.1-2ubuntu2_amd64.deb\nsudo dpkg -i libicu66_66.1-2ubuntu2_amd64.deb\nsudo apt-get install r-base\n- tidyverse 설치 (R studio 설치전에 tidyverse 설치해야함)\n- Rstudio 설치: https://www.rstudio.com/products/rstudio/download-server/debian-ubuntu/\n\n우분투22로 설정할것!!\n\nsudo apt remove rstudio-server\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.12.0-353-amd64.deb\nsudo gdebi rstudio-server-2022.12.0-353-amd64.deb\n- Rstudio를 설치하면 ~/R/x86_64-conda-linux-gnu-library/4.1이 새로 생성된다.\n\nRstudio에서 설치한 패키지는 이 폴더에 저장된다.\n\n- 주피터와 R커널 연결\nR # sudo R \ninstall.packages(\"IRkernel\")\nIRkernel::installspec()\n\n\nR설치ver2: (py38r40)에 설치\n- R설치\n(py38r40) conda install -c conda-forge r-essentials=4.0\n이러면 콘다환경에는 R이 깔리고 base에는 R이 깔리지 않는다.\n- 커널연결\n콘다환경에서 R을 실행한다. Rstudio가 아니라 커맨드에서 R을 실행해야한다. 그리고 아래를 실행하면 주피터랩과 R환경이 연결된다.\nIRkernel::installspec()\n이제 주피터랩에서 R kernel을 사용할 수 있다.\n\n\n가상환경에서 Rstudio server 설치 (어려움)\n- 이제 Rstudio server를 설치하는 방법을 다룬다.\n- 먼저 Rstudio를 설치한다. 참고로 Rstudio server 설치하는법은 여기를 참고하라. 요약하면 터미널에서 아래3줄을 입력하기만 하면된다.\n(py38r40) sudo apt-get install gdebi-core\n(py38r40) wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb\n(py38r40) sudo gdebi rstudio-server-1.2.5033-amd64.deb\n\nWarning: Rstudio 1.3x 이상을 설치하지말고 1.2x를 설치해야 한다. 이상하게 1.3x이상은 후에 서술할 Gregor Strurm가 그의 깃허브에서 제안하는 방식이 잘 동작하지 않았다. 이는 알려진 문제였고 이를 해결하는 해결책을 서술한 스레드가 있어보이긴 했지만 나는 그냥 Rstudio 1.2x를 설치하고 쓰는 것을 선택했다.\n\n\nNote: 이미 rstudio server 가 다른버전으로 깔려있다면 sudo apt remove rstudio-server 를 통하여 삭제하고 설치하자.\n\n- 이제 Rstudio 설치가 끝났다. 설치된 Rstudio를 아나콘다 가상환경에 설치된 R과 연결해보자. 우선 아래를 실행한다.\n(py38r40) sudo apt install uuid\n(py38r40) sudo apt install git\n(py38r40) git clone https://github.com/grst/rstudio-server-conda.git\n위에 두줄은 Gregor Sturm이 만든 어떤 프로그램을 쓰기 위한 사전준비작업이다. 마지막줄을 실행하면 Gregor Sturm이 만든 프로그램이 다운받아진다. 이게 프로그램 설치가 완료된것이다. 이제 컴퓨터 껐다 킬때마다 아래를 실행한다.\n(py38r40) ./rstudio-server-conda/local/start_rstudio_server.sh 8787 # use any free port number here. \n이제 192.168.0.4:8787 따위의 주소로 접속하면 Rstudio를 쓸 수 있다. 참고로 system-wide Rstudio server를 죽여야 할 때가 있다. 그럴땐 아래 명령을 치면 된다.\n(py38r40) sudo systemctl disable rstudio-server.service\n(py38r40) sudo systemctl stop rstudio-server.service\n\n\n자주 설치하는 패키지 리스트\n- 아래를 미리 깔아두자..\n# conda install -c conda-forge jupyterlab \nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\npip install fastai\npip install plotly \npip install ipywidgets\npip install jupyter-dash\npip install dash \npip install plotnine\npip install seaborn\npip install opencv-python\npip install folium\npip install pandas_datareader\nconda install -c conda-forge r-essentials=4 \npip install rpy2\nconda install -c conda-forge python-graphviz\n- tensorflow-gpu 는 현재(2022-03-06) python=3.10 에서 동작함\nconda create -n py310 python=3.10 \nconda activate py310 \nconda install -c conda-forge tensorflow-gpu \n- 아래를 설치하면 좋음\nsudo apt install mc \n\n\n터미널 예쁘게 만들기\n- zsh 설치 + oh my zsh 설치\nsudo install zsh \nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n- 테마변경\n\n.zshrc 파일 열기\n\nnano ~/.zshrc \n\n아래의 내용 수정\n\n...\nZSH_THEME=\"agnoster\"\n...\n- 색상변경\n\n아래의 파일 열기\n\ncd ~/.oh-my-zsh/themes/\nnano agnoster.zsh-theme  \n\n내용수정\n\n...\nprompt_dir() {\n  prompt_segment 39d $CURRENT_FG '%~'\n}\n...\n\n\nsublime text and TeX (개발용 서버일 경우 생략 가능)\n- ‘Ubuntu Software’에 가서 ’sublime Text’를 치면 다운받을 수 있다. 다운받은뒤에 ’file’ -&gt; ’open folder’를 활용하여 깃허브의 로칼저장소를 열어두면 편리하다.\n- 아래를 실행하여 TeX을 깐다.\nsudo apt install texlive-full\n- 이제 sublime과 latex을 연결하여보자. 여기를 참고하자. (1) sublime을 키고 ‘컨트롤+쉬프트+p’를 눌러 ’Install Package Control’ 선택 (2) 다시 ‘컨트롤+쉬프트+p’ 를 눌러 ‘Package Control: Install Package’를 실행 (3) 그러면 바로 검색창이 나오는데 거기서 ’LaTeXTools’를 입력해서 실행 (4) 다시 ’컨트롤+쉬프트+p’를 누르고 ’LaTeXTools: Check system’ 선택. 모두 ’available’이 나오면 잘 설치된 것이다.\n- *.tex파일을 열고 ’컨트롤+b’를 누르자. 처음이면 어떤 메뉴들이 보일텐데 그냥 ’Latex’을 선택하자. 그러면 코딩결과가 pdf로 나온다.\n- (수식미리보기) ‘Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-User’를 선택한다. ’93번째라인’에 ’preview_math_mode’를 “all”로 바꾼다. 그러면 수식들이 미리 출력된다. 그외에도 자유롭게 셋팅을 조정할 수 있다. 원래셋팅은 ’Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-Defaults’ 에 있다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "href": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "title": "[Note] 줄리아 설치 및 실행",
    "section": "",
    "text": "설치\n- 여기에 접속한다. 스크롤링하여 ’Generic Linux Binaries for x86 / 64-bit(GPG)’를 찾는다. 그리고 ’64-bit’를 클릭해서 다운받는다. (참고로 왼쪽에 ’help’를 누르면 설치페이지 설명서가 나온다.) 그러면 아래와 같은 파일이 나온다.\njulia-1.3.1-linux-x86_64.tar.gz\n이 파일을 더블클릭해서 압축을 풀어준다. 압축을 풀면 julia-1.3.1라는 폴더가 생긴다. 이 폴더를 원하는 위치로 (줄리아가 설치되기를 원하는 위치) 이동시킨다. 나는 home에 이동시켰다.\n- 아래를 실행하면 줄리아가 실행된다. (둘중 아무거나)\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n\n\n주피터와 연결\n- 아래중 하나를 실행하여 줄리아를 킨다.\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n- 줄리아를 실행한뒤에 아래를 입력하면 주피터노트북에 연결된다.\nusing Pkg\nPkg.add(\"IJulia\")\n- 한 가지 의문점이 있다. 나같은 경우는 ’(base)’에서 줄리아를 실행하고 연결하였다. 그런데 혹시 몰라서 (py38r40)에서도 줄리아를 실행해봤는데 잘 실행되었다. 줄리아를 실행시키고 위의 명령 Pkg.add(\"IJulia\")를 다시쳤는데, 이미 연결되어서 더이상 변화시킨게 없다는 메시지가 떴다. 이러면 (base)에 설치된 줄리아가 (py38r40)에서도 실행된 줄리아와 동일하다는 의미일까? \\(\\Longrightarrow\\) 그렇다. 왜냐하면 줄리아는 anaconda내의 폴더에 설치한 것이 아니기 때문에. home에 보통 설치하니깐.\n\n\n환경변수 조정\n- 참고로 어디서든 줄리아를 실행시키고 싶다면 환경변수를 조작하면 된다. 아래를 실행해서 나노에디터를 킨다.\nsudo nano /etc/environment\n맨끝에 다음과 같이 되어있을 것이다.\n~~ usr/local/games\"\n마지막에 /home/cgb/julia-1.3.1/bin/julia를 추가한다. 즉 아래와 같이 만든다.\n~~ usr/local/games:/home/cgb/julia-1.3.1/bin/julia\"\n세이브하고 나온다. (그런데 이 과정을 안거쳐도 되는것 같음.) 이제 커맨드에서 아래를 실행한다.\nexport PATH=$PATH:/home/cgb/julia-1.3.1/bin\n이렇게하면 이제 단순히 julia라고만 쳐도 julia가 실행된다.\n\n\n플루토에서 강의영상 넣는 방법\n- 아래를 삽입\nhtml\"\"\"\n&lt;div style=\"display: flex; justify-content: center;\"&gt;\n&lt;div  notthestyle=\"position: relative; right: 0; top: 0; z-index: 300;\"&gt;\n&lt;iframe src=\n\"\nhttps://www.youtube.com/embed/\n\"\nwidth=600 height=375  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n\"\"\"\n\n\n플루토를 이용한 홈페이지 만드는 방법\n- 단계1: https://github.com/JuliaPluto/static-export-template 에 가서 Clone\n- 단계2: Setting -&gt; GitHub Pages -&gt; Source -&gt; gh-pages / root\n\n\n플루토 키는 방법\nimport Pluto\nPluto.run(host=\"0.0.0.0\",port=1234,launch_browser=false,require_secret_for_open_links=false,require_secret_for_access=false,threads=\"8\")"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "title": "[Note] 우분투 익히기",
    "section": "",
    "text": "날짜, 달력\ndate \ncal \n\n\n파일시스템\n- 작업경로, 디렉토리 이동, 파일확인\npwd \ncd cgb2 \ncd ~cgb2 ## 어디서든 실행가능\ncd ## cgb2로 로그인 되어있을 경우 cd ~cgb2와 동일 \ncd ~cgb2/Dropbox\nls \nls -a \nls -l # 자세히\nls -lt # 자세히+파일수정시간에 따른 정렬\nls -lt --reverse # 자세히+파일수정시간의 역순으로 정렬 \n- 엄밀하게는 cd cgb2 는 cd ./cgb2로 써야한다. 하지만 ./는 생략가능하므로 그냥 cd cgb2라고 쓰는것\n- 뭐하는 파일인지 알고싶다면?\nfile picture.jpg\n- 카피, 이동, 새폴더, 삭제, 바로가기\ncp file1 file2 # file1을 복사하여 file2를 새로 만듬. file2가 이미 있다면 file1의 내용을 덮어씀 \ncp file1 file2 -i # 위와 동일한데 덮어쓰기 여부에 대한 확인메시지 생성\ncp file1 file2 --interactive # 위와 동일\ncp -r dir1 dir2 # dir1의 모든파일을 복사하여 dir2로 이동한뒤 붙어넣음. dir2가 없다면 새로 만듬. 기존의 dir2에 있던 파일이 삭제되는건 아님 \ncp --recursive dir1 dir2 # 위와 동일 \nmv # 카피사용방법과 동일 \nmkdir temp\nmkdir temp1, temp2, temp3 \nrm file1 # file1삭제 \nrm -r file1 dir1 # file1삭제 dir1폴더삭제 \nrm -rf file1 dir1 # 위와 동일한데 file1이나 dir1이 존재하지 않더라고 rm이 실행\nln ## 바로 가기 만드는건데 쓸줄모름 배우기 싫어\n- -v(verbose)를 쓰면 친절한 느낌이 든다.\n(base) cgb2@cgb2-desktop:~/Dropbox$ cp *.txt temppp -v\n'colab.txt' -&gt; 'temppp/colab.txt'\n\n\necho\n- echo 기본기능\ncgb2@cgb2-desktop:~/Dropbox/temppp$ echo test\ntest\n- echo + * : *가 현재 디렉토리에 있는 모든 파일이름으로 확장된이후에 echo가 동작함.\n(base) cgb2@cgb2-desktop:~/Dropbox/temppp$ ls\ncolab.txt  sample.txt\n(base) cgb2@cgb2-desktop:~/Dropbox/temppp$ echo *\ncolab.txt sample.txt\n- 응용\n(base) cgb2@cgb2-desktop:~$ echo D*\nDesktop Documents Downloads Dropbox\n- 응용2\n(base) cgb2@cgb2-desktop:~$ echo /usr/*\n/usr/bin /usr/games /usr/include /usr/lib /usr/lib32 /usr/lib64 /usr/libexec /usr/libx32 /usr/local /usr/sbin /usr/share /usr/src\n- 응용3\n(base) cgb2@cgb2-desktop:~$ echo ~\n/home/cgb2\n- 응용4\n(base) cgb2@cgb2-desktop:~/Dropbox$ echo $((2+2))\n4\n\n\n기본디렉토리 설명\n/bin 시스템 부팅과 실행에 필요한 바이너리(프로그램)들을 포함\n/etc 시스템 전반의 환경설정. 이 디렉터리의 모든 파일은 텍스트형식임.\n\n/etc/password 사용자 계정정보\n\n/lib 시스템 프로그램에서 사용하는 공유 라이브러리가 저장. 윈도우즈의 DLL과 비슷한 것.\n/usr 사용자가 사용하는 모든 프로그램과 지원파일들 (Program files + 프로그램들의 설정값)\n\n/usr/bin 리눅스 배포판이 설치한 실행 프로그램들이 있다. (여기에 R이 깔린다!!)\n\nhp-align, hp-check, hp-config_usb-printer …\nX11\nvi\ngcc\nsu, sudo\nsar\nssh, ssh-agent, ssh-keygen, ….\nnvidia-smi\n\n/usr/lib 여기에는 /usr/bin에 있는 프로그램들을 위한 공유라이브러리가 저장된다. (여기에 R folder가 있다)\n\n여기에 R폴더가 있다. 그런데 R패키지가 여기에 깔리진 않음\n\n/usr/local/bin 소스코드로 컴파일된 파일, 보통 비어있음\n/usr/local/lib/R/site-library R패키지가 설치되어있음, 예를들면 tidyverse\n\n\n\nvim\n- 파일여는법/만드는법 (sudo는 읽기전용 파일을 만들수있음)\nsudo vim /etc/apt/sources.list\n- 에디터에서 사용법\ni \nESC \n:w \n:q\n:wq\n/keyword  \ndd # 현재행삭제 \n\n\nwget\nwget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb\n\n\ngdebi\nsudo gdebi rstudio-server-1.2.5033-amd64.deb\n\n\napt\nsudo apt-get remove r-base-core\nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\nsudo apt list \nsudo apt update\nsudo apt install openssh-server \nsudo apt-get install gdebi-core\n- sudo apt-get과 sudo apt 차이? 별 차이 없는듯 - https://askubuntu.com/questions/445384/what-is-the-difference-between-apt-and-apt-get\n\nThey are very similar command line tools available in Trusty (14.04) and later. apt-get and apt-cache’s most commonly used commands are available in apt. apt-get may be considered as lower-level and “back-end”, and support other APT-based tools. apt is designed for end-users (human) and its output may be changed between versions.\n\n\n\nconda\nconda \nconda env -h \nconda install -h \nconda remove -h  \nconda update -h \nconda env list\nconda create -n py38r40 python=3.8\nconda env remove -n py38r40 \nconda install -c conda-forge jupyterlab \nconda remove jupyterlab \nconda remove r-base -y \nconda remove -n py38r40 jupyterlab \nconda update scipy\nconda update -n py38r40 scipy\nconda list \n\n\npip\npip\npip list\npip list &gt; list.txt\npip freeze # 좀 더 자세히 나온다 \npip freeze &gt; list.txt \npip show matplotlib # 설치된패키지 정보가 나옴. 좋음.\npip install rpy2\npip install -r list.txt \npip install dash==1.13.3\npip install jupyterlab \"ipywidgets&gt;=7.5\"\npip install -U numpy\npip install --upgrade pip\npip install --upgrade tensorflow\npip uninstall matplotlib\n\n\n리소스 모니터링\ndf # disk \nfree # memory \nnvidia-smi \nwatch -n 5 nvidia-smi -a --display=utilization\ntop\nsar -1 r \n\n# 나노에디터\n\n`-` 파일열기\n\n```default\nsudo nano /etc/apt/sources.list\n- 읽기전용 파일만들기: 파일을 만드는것도 파일 여는방법과 동일함. 즉 아래와 같이 하면 exam.txt가 현재 폴더에 있다면 열고아니면 (읽기전용으로) 만든다.\nsudo nano /home/cgb/Desktop/exam.txt\n- 읽기전용이 아닌 일반파일 만들기\nnano /home/cgb/Desktop/exam.txt\n- 나노에디터종료: 컨트롤+X\n- 파일저장: 컨트롤+O\n- 찾기: 컨트롤+W\n- 되돌리기: 알트+U\n\n\nssh\n- 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다.\nsudo apt install openssh-server\n\n22번포트 우회하기\n- step1: /etc/ssh/sshd_config 파일을 연다.\nsudo vi /etc/ssh/sshd_config \n- step2: Port 22 라고 된 부분의 주석을 풀고 원하는 포트번호 설정\n...\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n...\n- step3: 수정내용을 적용\nsudo systemctl restart ssh.service\n- step4: 수정한 포트로 ssh접속\n\n\n\nsublime\n- 찾아바꾸기: 컨트롤+h\n- 화면분할: 옵션+커맨드+2,3,4 …\n\n\n파일생성, 파일삭제\ncat &gt; sample.txt \nrm -r folderName \nrm -rf .local/share/Trash/files/* # 휴지통삭제 \n\n\ndeb\ndpkg -i quarto-1.2.335-linux-amd64.deb # 설치\ndpkg -r quarto # 삭제"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/notes.html",
    "href": "posts/3_Researches/PINKOCTO/notes.html",
    "title": "[PINKOCTO] 면담",
    "section": "",
    "text": "23-07-17\n- 실험해볼것 (모형, 필터/lag, 에폭 변화하여서) 50번정도..\n\n에폭이 적을수록 우리가 잘맞았음.\n필터나 lag도 단순할수록 괜찮았음\nlag은 24를 넘길 필요는 없음.\n필터는 64이상은 필요없는듯\n\n- quarto 팁\nquarto preview --host 210.117.173.182 --no-browser\n- 파라메트릭 vs 넌파 vs 세미\n- 비트시그널\n- 발표자료 + 페이퍼작업\n- 레퍼런스정리\n\nGNN -&gt; GCN / RGNN -&gt; STGCN, ST-GCN\n\n23-07-20\n- 라마: https://www.youtube.com/watch?v=OZbarkziC14\n- 저널 vs 학회:\n\n원래개념: SCIE (~10) &gt;&gt;&gt;&gt;&gt;&gt; SCIE (10~50) &gt;&gt; SCIE(50~) &gt;&gt;&gt;&gt;&gt;&gt; KCI &gt;&gt; 컨퍼런스\nAI분야 한정: SCIE (~10), 일부top컨퍼런스 (NIPS, AAAI, ICML, ICLR) &gt;&gt;&gt;&gt;&gt;&gt; SCIE (10~50) &gt;&gt; SCIE(50~) &gt;&gt;&gt;&gt;&gt;&gt; KCI &gt;&gt; 컨퍼런스\n\n- 오타발견시알려주세요\n\nhttps://guebin.github.io/PP2023SUM/"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 비편향추정량(UB)란 \\(\\theta\\)의 추정량 중\n\\[\\forall \\theta\\in \\Theta:~ E(\\hat{\\theta})=\\theta\\]\n를 만족하는 추정량 \\(\\hat{\\theta}\\)을 의미한다.\n- (예시) 아래와 같은 상황을 가정하자.\n\\[X_n \\overset{iid}{\\sim} N(\\theta,1)\\]\n여기에서\n\n\\(\\hat{\\theta}_1=0\\) 은 \\(\\theta=0\\) 일 경우에는 \\(E(\\hat{\\theta})=\\theta\\) 를 만족하지만 그 외의 경우에는 \\(E(\\hat{\\theta})\\neq\\theta\\) 이므로 UB가 아니다.\n\\(\\hat{\\theta}_2=X_1\\) 은 UB이다.\n\\(\\hat{\\theta}_3=\\frac{X_1+X_2}{2}\\) 역시 UB이다.\n\\(\\hat{\\theta}_4=X_1+X_2-X_3\\) 역시 UB이다.\n\\(\\hat{\\theta}_5=-99X_1+100X_2\\) 역시 UB이다.\n\\(\\hat{\\theta}_6=\\frac{X_1+0}{2}\\) 은 1과 동일한 이유로 UB가 아니다.\n\\(\\hat{\\theta}_7=\\bar{X}\\)는 UB이다.\n\\(\\hat{\\theta}_8=w_1X_1+\\dots+w_nX_n\\) ,where \\(\\sum_{i=1}^{n}w_i=1\\) 형태의 estimator는 모두 UB이다.\n\n- 최소분산비편향추정량(MVUE)란 \\(\\theta\\)에 대한 비편향추정량을 모아놓은 집합 \\(\\hat{\\Theta}_{UB}\\) 에서 최소분산을 가지는 추정량을 의미한다. MVUE를 구하는 방법은 아래와 같다.\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n예를들어 위의 예제에서 \\(V(\\hat{\\theta}_2)=1\\) 이고 \\(V(\\hat{\\theta}_3)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}\\) 이므로 \\(\\hat{\\theta}_3\\) 이 더 좋은 추정량이라 볼 수 있다.\n- (의문) 왜 비편향추정량만 모아서 그중에서 최소분산을 구할까?\n\n\\(\\hat{\\theta}_1\\)와 같은 추정량은 \\(V(\\hat{\\theta}_1)=0\\) 이므로 그냥 최소분산을 만족한다. 따라서 이러한 추정량은 제외해야지 게임이 성립함.\n\n- 불만: 아래의 방법으로 구하는건 거의 불가능하지 않나?\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n- 이론: 크래머라오 하한값(편의상 \\(L^\\star\\)이라고 하자)이라고 있는데, 이는 \\({\\Theta}_{UB}\\)에 존재하는 모든 추정량에 대한 분산의 하한값을 제공한다.1 즉 아래가 성립한다.\n1 (사실 \\({\\Theta}_{UB}\\)가 아닌 집합에 대해서도 하한값을 제공함, 그런데 교재에서는 \\({\\Theta}_{UB}\\)에 대한 하한값만 다루는듯\n\\(L^\\star\\) is Cramer-Rao lower bound \\(\\Rightarrow\\) \\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L^\\star\\)\n\n역은 성립하지 않음을 주의하자. 즉 아래를 만족하는 \\(L\\)이 존재할 수 있다.\n\n\\(V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) for some \\(\\hat{\\theta} \\in \\Theta_{UB}\\)\n\n- 위의 이론을 이용하면 아래의 논리전개를 펼 수 있다.\n\n\\(L^\\star\\)를 구한다.\n왠지 MVUE가 될 것 같은 \\(\\hat{\\theta}\\)을 하나 찍고 그것의 분산 \\(V(\\hat{\\theta})\\)를 구한다.\n만약에 \\(V(\\hat{\\theta})=L^\\star\\)를 만족하면 그 \\(\\hat{\\theta}\\)이 MVUE라고 주장할 수 있다.\n\n- 위의 논리전개에 대한 불만 [p.212]\n\n\\(V(\\hat{\\theta})=L^\\star\\) 이길 기도해야함.\n\\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) 와 같은 \\(L\\)이 존재하는 경우는 쓸 수 없음.\n\n- 또 다른 방법: 완비충분통계량을 이용함\n\n\n아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)\n\n\n\n\n- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#충분통계량",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#충분통계량",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#최소충분통계량",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#최소충분통계량",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#라오블랙웰",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#라오블랙웰",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#레만쉐페정리",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#레만쉐페정리",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-17-EPT-RGCN.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-17-EPT-RGCN.html",
    "title": "[SOLAR] EPT + RGCN",
    "section": "",
    "text": "Import\n\n%run 0720.py\n\n\n\nLoad data\n\n# read dataframe \ndf = pd.read_csv('data_eng_230710.csv')\n\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)\nt = df.loc[:,'date']\nregions = list(df.loc[:,'Bukchoncheon':'Gyeongju-si'].columns)\n\n# plot rawdata \nstart = 50 \nend = 50+24*3\ncity = 19 # 광주 \nwith plt.style.context('cyberpunk'):\n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(t[start:end],y[start:end,city],'o',label='y')\n    plt.plot(t[start:end],yU[start:end,city],'--',label='yU')\n    plt.plot(t[start:end],yP[start:end,city],'o',label='y/yU')\n    tick_interval = 4\n    plt.xticks(range(1, len(t) + 1, tick_interval),rotation=25)        \n    plt.legend()\n    plt.title(\"The solar radiation data in {} from {} to {} \".format(regions[city],t.to_list()[0][:10],t.to_list()[-1][:10]))\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n\n\nLearn\n- y \\(\\to\\) yhat\n\n# hyper params\nLAGS = 4\nFILTERS = 8\nEPOCH = 5\n\n# global params\nTRAIN_RATIO = 0.8\nT,N = len(t),len(regions) \nLEN_TEST = int(np.floor(T*(1-TRAIN_RATIO)))\nLEN_TR = T - LEN_TEST\n\n# 방법1\nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyhat = rgcn(y, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\nyhat[yhat &lt; 0]= 0 \n\n# 방법2 \nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyUhat = rgcn(yU, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\n\nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyPhat = rgcn(yP, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\nyPhat[yPhat &lt; 0] =0 \n\n\nclassic = ((y - yhat)[LEN_TR:, :] ** 2)\nproposed = ((y - yUhat*yPhat)[LEN_TR:, :] ** 2)\n\n# print mse\nprint(\"\"\"\ntotal_mse(classic): {:.4f}\ntotal_mse(proposed): {:.4f}\n\"\"\".format(classic.mean(), proposed.mean()))\n\n5/5\ntotal_mse(classic): 0.0573\ntotal_mse(proposed): 0.0476\n\n\n\n\n# plot \nstart = LEN_TR \nend = LEN_TR + LEN_TEST \ncity = 20\ntick_interval = 24 \n\nclassic = (y - yhat)[start:end, :] ** 2\nproposed = (y - yUhat*yPhat)[start:end, :] ** 2\n\nprint(\"\"\"\ntotal_mse(classic): {:.4f}\ntotal_mse(proposed): {:.4f}\n\"\"\".format(classic.mean(), proposed.mean()))\n\nwith plt.style.context('cyberpunk'): \n    plt.rcParams['figure.figsize'] = [20, 4]  # [가로 크기, 세로 크기]\n    plt.plot(t[start:end],y[start:end,city],'-',label='real')\n    plt.plot(t[start:end],yhat[start:end,city],'--',label='predited(classic)')\n    plt.plot(t[start:end],(yUhat*yPhat)[start:end,city],'--',label='predited(proposed)')\n    plt.xticks(range(1, len(t[start:end]) + 1, tick_interval),rotation=25,ha='right')\n    plt.title(\"The predicted results in {} from {} to {} \".format(regions[city],t[start:end].to_list()[0][:10],t[start:end].to_list()[-1][:10]))    \n    plt.legend()\n    mplcyberpunk.add_glow_effects()\n\n\ntotal_mse(classic): 0.0573\ntotal_mse(proposed): 0.0476\n\n\n\n\n\n\n\n\nAnalyze\n- y vs yhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(y[50:150,0],'o-',label='real')\n    plt.plot(yhat[50:150,0],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- yU vs yUhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(yU[:,25],label='real')\n    plt.plot(yUhat[:,25],label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- yP vs yPhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(yP[50:150,city],'o-',label='real')\n    plt.plot(yPhat[50:150,city],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- y vs (yUhat*yPhat)\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(y[50:150,0],'o-',label='real')\n    plt.plot((yUhat*yPhat)[50:150,0],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- 지역별분석\n\nwith plt.style.context('cyberpunk'):\n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(regions,classic.mean(axis=0)-proposed.mean(axis=0),'--o');\n    plt.xticks(regions, rotation=55, ha='right');    \n    mplcyberpunk.add_glow_effects()"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-04-EPT.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-04-EPT.html",
    "title": "[SOLAR] EPT",
    "section": "",
    "text": "ref: https://www.sciencedirect.com/science/article/pii/S2352711021000492\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(EPT)\n\n\nurl = 'https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv'\ndf = read_csv(url)\n\nRows: 803000 Columns: 3\n── Column specification ────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): region, date\ndbl (1): solar_radiation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf = df |&gt; filter(region == '북춘천') |&gt; mutate(date=ymd_hm(date))\ndf\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dttm&gt;\n\n\n\n\n북춘천\n0.00\n2021-01-01 00:00:00\n\n\n북춘천\n0.00\n2021-01-01 01:00:00\n\n\n북춘천\n0.00\n2021-01-01 02:00:00\n\n\n북춘천\n0.00\n2021-01-01 03:00:00\n\n\n북춘천\n0.00\n2021-01-01 04:00:00\n\n\n북춘천\n0.00\n2021-01-01 05:00:00\n\n\n북춘천\n0.00\n2021-01-01 06:00:00\n\n\n북춘천\n0.00\n2021-01-01 07:00:00\n\n\n북춘천\n0.00\n2021-01-01 08:00:00\n\n\n북춘천\n0.37\n2021-01-01 09:00:00\n\n\n북춘천\n0.96\n2021-01-01 10:00:00\n\n\n북춘천\n1.40\n2021-01-01 11:00:00\n\n\n북춘천\n1.72\n2021-01-01 12:00:00\n\n\n북춘천\n1.84\n2021-01-01 13:00:00\n\n\n북춘천\n1.74\n2021-01-01 14:00:00\n\n\n북춘천\n1.30\n2021-01-01 15:00:00\n\n\n북춘천\n0.93\n2021-01-01 16:00:00\n\n\n북춘천\n0.29\n2021-01-01 17:00:00\n\n\n북춘천\n0.01\n2021-01-01 18:00:00\n\n\n북춘천\n0.00\n2021-01-01 19:00:00\n\n\n북춘천\n0.00\n2021-01-01 07:00:00\n\n\n북춘천\n0.00\n2021-01-01 20:00:00\n\n\n북춘천\n0.00\n2021-01-01 06:00:00\n\n\n북춘천\n0.00\n2021-01-01 21:00:00\n\n\n북춘천\n0.00\n2021-01-01 05:00:00\n\n\n북춘천\n0.00\n2021-01-02 00:00:00\n\n\n북춘천\n0.00\n2021-01-02 01:00:00\n\n\n북춘천\n0.00\n2021-01-02 02:00:00\n\n\n북춘천\n0.00\n2021-01-02 03:00:00\n\n\n북춘천\n0.00\n2021-01-02 04:00:00\n\n\n⋮\n⋮\n⋮\n\n\n북춘천\n0.00\n2022-12-30 07:00:00\n\n\n북춘천\n0.00\n2022-12-30 20:00:00\n\n\n북춘천\n0.00\n2022-12-30 06:00:00\n\n\n북춘천\n0.00\n2022-12-30 21:00:00\n\n\n북춘천\n0.00\n2022-12-30 05:00:00\n\n\n북춘천\n0.00\n2022-12-31 00:00:00\n\n\n북춘천\n0.00\n2022-12-31 01:00:00\n\n\n북춘천\n0.00\n2022-12-31 02:00:00\n\n\n북춘천\n0.00\n2022-12-31 03:00:00\n\n\n북춘천\n0.00\n2022-12-31 04:00:00\n\n\n북춘천\n0.00\n2022-12-31 05:00:00\n\n\n북춘천\n0.00\n2022-12-31 06:00:00\n\n\n북춘천\n0.00\n2022-12-31 07:00:00\n\n\n북춘천\n0.00\n2022-12-31 08:00:00\n\n\n북춘천\n0.22\n2022-12-31 09:00:00\n\n\n북춘천\n0.50\n2022-12-31 10:00:00\n\n\n북춘천\n0.80\n2022-12-31 11:00:00\n\n\n북춘천\n2.13\n2022-12-31 12:00:00\n\n\n북춘천\n1.78\n2022-12-31 13:00:00\n\n\n북춘천\n1.45\n2022-12-31 14:00:00\n\n\n북춘천\n0.78\n2022-12-31 15:00:00\n\n\n북춘천\n0.38\n2022-12-31 16:00:00\n\n\n북춘천\n0.15\n2022-12-31 17:00:00\n\n\n북춘천\n0.00\n2022-12-31 18:00:00\n\n\n북춘천\n0.00\n2022-12-31 19:00:00\n\n\n북춘천\n0.00\n2022-12-31 07:00:00\n\n\n북춘천\n0.00\n2022-12-31 20:00:00\n\n\n북춘천\n0.00\n2022-12-31 06:00:00\n\n\n북춘천\n0.00\n2022-12-31 21:00:00\n\n\n북춘천\n0.00\n2022-12-31 05:00:00\n\n\n\nA tibble: 18250 × 3\n\n- 지역을 북춘천으로 고정\n\ndf2 = df |&gt; filter(region =='북춘천') \ndf2 = df2[order(df2$date),]\n\n\ny = df2$solar_radiation\ny\n\n\n0000000000000.370.961.41.721.841.741.30.930.290.010000000000000000.320.951.461.791.911.821.50.970.370.010000000000000000.290.891.41.751.761.350.820.520.30.010000000000000000.330.931.321.531.671.5110.790.1900000000000000000.190.791.411.761.931.851.561.050.450.020000000000000000.511.381.811.881.931.851.5210.410.020000000000000000.110.741.441.811.981.911.611.090.450.020000000000000000.360.971.491.8521.911.611.10.480.03000⋯000000000000.010.471.31.711.871.881.721.380.880.3200000000000000000.190.681.351.712.191.891.380.820.2600000000000000000.150.420.931.071.181.051.160.830.300000000000000000.080.280.430.741.41.841.530.850.2700000000000000000.411.021.841.921.851.721.420.930.340.010000000000000000.471.281.681.81.851.741.40.890.30.010000000000000000.150.641.11.732.121.741.290.860.310.010000000000000000.220.50.82.131.781.450.780.380.150000\n\n\n\nplot(y[1:500])\nlines(y[1:500],lty=2)\n\n\n\n\n- EPT 수행\n\nept = function(y){\n    EpM = eptransf(signal=y,tau=24,process=c(\"envelope\",\"average\"))$EpM\n    EpM*2\n}\n\n\nyU = ept(y)\n\n\nplot(y[1:500])\nlines(yU[1:500],col=2,lty=2)\n\n\n\n\n- todo: 모든 지역에대하여 yU를 구하여 저장"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-01-CTGAN.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-01-CTGAN.html",
    "title": "[CTGAN] CTGAN",
    "section": "",
    "text": "!conda env list\n\n# conda environments:\n#\nbase                     /home/cgb2/anaconda3\nctgan                 *  /home/cgb2/anaconda3/envs/ctgan\npy38                     /home/cgb2/anaconda3/envs/py38\nstgcn                    /home/cgb2/anaconda3/envs/stgcn\n\n\n\n- Ref: https://github.com/sdv-dev/CTGAN\n\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\nreal_data = load_demo()\n\n# Names of the columns that are discrete\ndiscrete_columns = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n    'income'\n]\n\nctgan = CTGAN(epochs=10)\nctgan.fit(real_data, discrete_columns)\n\n# Create synthetic data\nsynthetic_data = ctgan.sample(1000)\n\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/torch/cuda/__init__.py:126: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1688109080043/work/c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount() &gt; 0\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n\n\n- 실제자료\n\nreal_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n3\n53\nPrivate\n234721\n11th\n7\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n4\n28\nPrivate\n338409\nBachelors\n13\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFemale\n0\n0\n40\nCuba\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n32557\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n32558\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n32559\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n32560\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n32561 rows × 15 columns\n\n\n\n- 합성된자료\n\nsynthetic_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n31\nPrivate\n317142\nAssoc-acdm\n10\nMarried-spouse-absent\nTransport-moving\nOwn-child\nWhite\nMale\n92\n0\n56\nChina\n&lt;=50K\n\n\n1\n40\nPrivate\n248817\nSome-college\n11\nMarried-civ-spouse\nProf-specialty\nOwn-child\nWhite\nMale\n-101\n4\n40\nUnited-States\n&lt;=50K\n\n\n2\n21\nPrivate\n201022\nBachelors\n2\nNever-married\nOther-service\nOwn-child\nWhite\nMale\n-20\n-4\n40\nUnited-States\n&lt;=50K\n\n\n3\n33\nFederal-gov\n407226\nMasters\n5\nNever-married\nOther-service\nNot-in-family\nWhite\nFemale\n-103\n1683\n61\nYugoslavia\n&lt;=50K\n\n\n4\n49\nFederal-gov\n261481\nSome-college\n14\nNever-married\nFarming-fishing\nOwn-child\nWhite\nFemale\n5\n-4\n40\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n39\nPrivate\n228408\nHS-grad\n13\nMarried-civ-spouse\nProf-specialty\nNot-in-family\nAsian-Pac-Islander\nFemale\n-73\n-2\n16\nPhilippines\n&lt;=50K\n\n\n996\n31\nPrivate\n125109\nHS-grad\n0\nNever-married\nCraft-repair\nNot-in-family\nWhite\nFemale\n-86\n-4\n59\nUnited-States\n&lt;=50K\n\n\n997\n31\nPrivate\n291146\n11th\n13\nNever-married\nAdm-clerical\nWife\nBlack\nFemale\n-52\n5\n40\nUnited-States\n&gt;50K\n\n\n998\n25\nPrivate\n170115\nSome-college\n13\nNever-married\nMachine-op-inspct\nOwn-child\nWhite\nFemale\n-124\n-4\n40\nUnited-States\n&lt;=50K\n\n\n999\n37\nWithout-pay\n363805\nProf-school\n10\nMarried-civ-spouse\nSales\nUnmarried\nWhite\nFemale\n51\n-5\n40\nIran\n&lt;=50K\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#imports",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#imports",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"fraudTrain.csv\")\n_df1=  df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42)\n_df2 = df[df[\"is_fraud\"] == 1]\ndf = pd.concat([_df1,_df2])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#embedding",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#embedding",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#learn",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#learn",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#evaluate",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#evaluate",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data-1",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data-1",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\nlrnr2, _,_,_ = anal(our_sampling1(df),n_estimators=100)\n\nComputing transition probabilities: 100%|██████████| 1289/1289 [00:49&lt;00:00, 26.00it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:10&lt;00:00,  1.10s/it]"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#네트워크-토폴로지",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#네트워크-토폴로지",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#커뮤니티-감지",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#커뮤니티-감지",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\ndf.shape\n\n(318777, 23)\n\n\n\ndf_minority.shape\n\n(6006, 23)\n\n\n\ndf_majority.shape\n\n(312771, 23)\n\n\n\n6006 / 312771 \n\n0.019202547550763976\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.45it/s]\n\n\n\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(yhat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7236842105263158\nRecall: 0.1407849829351536\nF1-Score: 0.2357142857142857\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 0])\n\n\n\nyhat = rf.predict_proba(test_embeddings)\n\n\nyhat\n\narray([[0.457, 0.543],\n       [0.634, 0.366],\n       [0.609, 0.391],\n       ...,\n       [0.577, 0.423],\n       [0.59 , 0.41 ],\n       [0.557, 0.443]])\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-19-그래프자료로 데이터정리.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-19-그래프자료로 데이터정리.html",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport pandas as pd\nimport torch \nimport torch_geometric\n\n- 모든엣지를 고려\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\ndiff = fraudTrain.trans_date_trans_time[10]-fraudTrain.trans_date_trans_time[0]\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9965337989703691\n\n\n\n!git add .\n!git commit -m. \n!git push \n!quarto publish --no-browser --no-prompt"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "",
    "text": "import itstgcn\nimport torch\nimport itstgcn.planner"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 2, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.0, 0.2],\n    'lags': [2, 4], \n    'nof_filters': [4, 8], \n    'inter_method': ['nearest'],\n    'epoch': [3]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/2 is done\n2/2 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-03.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex= [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_stgcn_block = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='five_nodes')\n\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n1/3 is done\n2/3 is done\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-55.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n#    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-56.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'inter_method': ['nearest','linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader,dataset_name='five_nodes')\nplnr.simulate(mindex,mtype='block')\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-57.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n&lt;torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7faad2716a60&gt;\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:16&lt;00:00,  1.52s/it]\n\n\n\ndataset.features[0].shape\n\n(20, 4)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "title": "[IT-STGCN] Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nT = 100\nt = np.arange(T)/T * 10 \n\nx = 0.3*np.sin(2*t)+0.1*np.sin(4*t)+0.1*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=4)\n\n\nmindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\ndataset_miss05031.edge_index\n\narray([[0],\n       [1]])\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n\ndataset_padded_cubic05031.edge_index\n\narray([[0],\n       [1]])\n\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nlags4/0.3/filter16\nlags4/0.38/filter8\nlags8/0.38/filter8\n\n\nlrnr05031.learn(filters=16,epoch=50)\n\n50/50\n\n\n- 모형 평가 및 시각화\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nevtor05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2,figsize=(30,20))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'.',color='C0')\nax2.plot(df1['y'][:],'.',color='C0')\nax3.plot(df1['xer'][:],'.',color='C1')\nax4.plot(df1['yer'][:],'.',color='C1')\nax5.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax6.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax7.plot(df1['x'][:],'.',color='C0')\nax7.plot(evtor05031.fhat_tr[:,0],color='C3')\nax8.plot(df1['y'][:],'.',color='C0')\nax8.plot(evtor05031.fhat_tr[:,1],color='C3')\nax9.plot(df1['x'][:],'.',color='C0')\nax9.plot(df1['xer'][:],'.',color='C1')\nax9.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax9.plot(evtor05031.fhat_tr[:,0],color='C3')\nax10.plot(df1['y'][:],'.',color='C0')\nax10.plot(df1['yer'][:],'.',color='C1')\nax10.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax10.plot(evtor05031.fhat_tr[:,1],color='C3')\n\nfor ax in fig.get_axes():\n    ax.label_outer()"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html#import",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html#import",
    "title": "[IT-STGCN] Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nT = 100\nt = np.arange(T)/T * 10 \n\nx = 0.3*np.sin(2*t)+0.1*np.sin(4*t)+0.1*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=4)\n\n\nmindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\ndataset_miss05031.edge_index\n\narray([[0],\n       [1]])\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n\ndataset_padded_cubic05031.edge_index\n\narray([[0],\n       [1]])\n\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nlags4/0.3/filter16\nlags4/0.38/filter8\nlags8/0.38/filter8\n\n\nlrnr05031.learn(filters=16,epoch=50)\n\n50/50\n\n\n- 모형 평가 및 시각화\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nevtor05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2,figsize=(30,20))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'.',color='C0')\nax2.plot(df1['y'][:],'.',color='C0')\nax3.plot(df1['xer'][:],'.',color='C1')\nax4.plot(df1['yer'][:],'.',color='C1')\nax5.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax6.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax7.plot(df1['x'][:],'.',color='C0')\nax7.plot(evtor05031.fhat_tr[:,0],color='C3')\nax8.plot(df1['y'][:],'.',color='C0')\nax8.plot(evtor05031.fhat_tr[:,1],color='C3')\nax9.plot(df1['x'][:],'.',color='C0')\nax9.plot(df1['xer'][:],'.',color='C1')\nax9.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax9.plot(evtor05031.fhat_tr[:,0],color='C3')\nax10.plot(df1['y'][:],'.',color='C0')\nax10.plot(df1['yer'][:],'.',color='C1')\nax10.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax10.plot(evtor05031.fhat_tr[:,1],color='C3')\n\nfor ax in fig.get_axes():\n    ax.label_outer()"
  },
  {
    "objectID": "posts/3_Researches/YEON/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "href": "posts/3_Researches/YEON/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "title": "[SEOYEONC] type1 err, type2 err",
    "section": "",
    "text": "using Distributions, Plots\n\n(문제) \\(X_1,X_2\\)가 평균이 \\(\\theta\\)인 지수분포에서 추출한 랜덤표본이라고 하자. 가설 \\(H_0: \\theta=2\\) vs \\(H_1:\\theta=1\\) 에 대하여, \\(H_0\\)에 대한 기각영역을\n\\[\\frac{f(x_1;\\theta=2)f(x_2;\\theta=2)}{f(x_1;\\theta=1)f(x_2;\\theta=1)}&lt;\\frac{1}{2}\\]\n와 같이 설정하자. 이와 같은 검정법에 대한 \\(\\alpha\\)와 \\(\\beta\\)를 구하라.\n(풀이)\n문제요약\n\n\\(f(x) = \\frac{1}{\\theta} \\exp(-\\frac{x}{\\theta})\\) -&gt; 평균이 \\(\\theta\\) 인 지수분포\n검정통계량: \\(T=\\frac{f(x_1;2)f(x_2;2)}{f(x_1;1)f(x_2;1)}\\)\n\\(\\alpha = P(\\text{Reject $H_0$|$H_0$ is true}) = P(T&lt;\\frac{1}{2} | \\text{$H_0$ is true})\\)\n\\(\\beta = P(\\text{Accept $H_0$|$H_1$ is true}) = P(T&gt;\\frac{1}{2} | \\text{$H_1$ is true})\\)\n\n풀이시작\n\nT= x -&gt; 0.5*exp(-0.5*x[1]) * 0.5*exp(-0.5*x[2])  / (exp(-x[1])*exp(-x[2]))\n\n#1 (generic function with 1 method)\n\n\n\\(\\alpha\\)를 구해보자. (시뮬)\n\nθ=2 \nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.4932782791687658\n 3.904496314340747\n\n\n\nT(x)\n\n3.715796051759978\n\n\n\nTs = [rand(Exponential(θ),2) |&gt; T for i in 1:1400000]\nmean(Ts .&lt; 1/2)\n\n0.1535007142857143\n\n\n\\(\\beta\\)를 구해보자. (시뮬)\n\nθ=1\nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.0915718974295616\n 3.322182470278192\n\n\n\nTs = [rand(Exponential(θ),2) |&gt; T for i in 1:1400000]\nmean(Ts .&gt; 1/2)\n\n0.5967985714285714\n\n\n\\(\\alpha\\)를 구해보자. (이론)\n\\(T(X_1,X_2) = \\frac{0.25\\exp(-0.5X_1 -0.5X_2)}{\\exp(-X_1-X_2)}=0.25\\exp(0.5X_1+0.5X_2)\\)\n$T(X_1,X_2)&lt; (0.5X_1+0.5X_2) &lt; 2 X_1+X_2&lt; 2 $\n그런데 \\(X_1+X_2 \\sim \\chi^2(4)\\) under \\(H_0\\)\n\\(P(X_1+X_2 &lt; 2\\ln2) = \\int_0^{2\\ln2} \\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_0^{\\ln2} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_0^{\\ln2}\\)\n\nt = log(2) \nu = t*(-exp(-t)) - exp(-t)\nt = 0\nl = t*(-exp(-t)) - exp(-t)\n\n-1.0\n\n\n\nu-l\n\n0.1534264097200273\n\n\n\\(\\beta\\)를 구해보자. (이론)\n$T(X_1,X_2)&gt; (0.5X_1+0.5X_2) &gt; 2 (X_1+X_2)&gt; 4 $\n그런데 \\(2(X_1+X_2) \\sim \\chi^2(4)\\) under \\(H_1\\)\n\\(P(2(X_1+X_2) &gt; 4\\ln2) = \\int_{4\\ln2}^{\\infty}\\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_{2\\ln2}^{\\infty} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_{2\\ln2}^{\\infty}\\)\n\nu = 0\nt = 2*log(2)\nl = t*(-exp(-t)) - exp(-t)\nu-l\n\n0.5965735902799727"
  },
  {
    "objectID": "posts/1_Essays/2023-07-04-토폴로지(1).html",
    "href": "posts/1_Essays/2023-07-04-토폴로지(1).html",
    "title": "[Essays] 토폴로지",
    "section": "",
    "text": "About this doc\n- 수학공부\n- 학부수준\n- 이 문서는 논문을 읽을때 등장하는 topology 용어들을 좀더 명확하게 이해하고 싶어서 작성하였다. 가볍게 정의만 훑어보는 것이라 깊게 들어가지는 않을 예정이다. 교재는 Schaum’s General Topology 를 참고하였다. - Lipschutz, S. (1965). Schaum’s outline of theory and problems of general topology. Schaum’s Outline Series.\n- 여기에서는 토폴로지의 정의와 메트릭스페이스의 정의 그리고 컴플리션의 정의에 대하여 다룬다.\n\n\nChap 5: 토폴로지\n- (Lipschutz (1965), p.66) \\({\\cal T}\\) 가 \\(X\\) 의 subset 으로 이루어진 collection 이라고 하자. \\({\\cal T}\\) 가 \\(X\\) 를 포함하며 uncountable union 에 닫혀있고 finite intersection 에 닫혀있다면 \\({\\cal T}\\) 를 \\(X\\) 의 topology 라고 한다. 그리고 \\((X,{\\cal T})\\) 를 topological space 라고 한다. \\({\\cal T}\\) 가 \\(X\\)의 토플로지일때 \\({\\cal T}\\) 의 원소를 \\({\\cal T}\\)-open set 이라고 한다. 따라서 원래 오픈셋은 마치 확률변수처럼 단독으로 정의할 수 없고 어떠한 토폴로지 \\({\\cal T}\\)와 같이 정의된다.\n- (Lipschutz (1965), p.66) 아래와 같은 collection 을 생각하자.\n\\[{\\cal O}:=\\{O: O=\\cup_i(a_i,b_i), a_i,b_i \\in \\mathbb{R} \\} \\]\n컬렉션 \\({\\cal O}\\) 는 \\(\\mathbb{R}\\) 의 토폴로지가 된다. (증명은 알아서..) 이러한 토폴로지(=오픈인터벌의 카운터블-유니온으로 표현가능한 집합들의 모임)을 특별히 usual topology 라고 한다. 그리고 이 토폴로지의 원소를 \\({\\cal O}\\)-오픈셋이라고 부른다. 따라서 어떤 집합 \\(O\\) 가 \\({\\cal O}\\)-오픈셋 이라는 말은 그 집합이 오픈인터벌의 카운터블-유니온으로 표현가능한 집합임을 의미한다.\n- 참고로 \\({\\cal O}\\) 가 우리가 일반적으로 생각하는 ‘오픈셋들의 모임’ 이고 \\({\\cal O}\\)-오픈셋이 보통 우리가 일반적으로 유클리드 공간에서 상상하는 오픈셋이다. 그래서 앞으로 특별한 언급없이 그냥 ‘오픈셋’ 이라고 부르면 토폴로지 \\((\\mathbb{R},{\\cal O})\\) 에서 정의가능한 ‘\\({\\cal O}\\)-오픈셋’ 을 의미하는 것이라고 생각하면 된다.\n- 즉 우리가 일반적으로 생각하는 오픈셋1은 오픈인터벌 \\((a,b)\\) 의 countable-many union 으로 표현가능한 집합이라고 이해해도 된다.\n1 정확하게는 \\(\\cal O\\)-오픈셋- 오픈셋 \\(O\\)의 원소를 interior point of \\(O\\) 라고 한다. \\({\\cal O}\\)의 정의에 의해서 인테리어포인트는 모두 아래의 성질을 만족한다.\n\\[\\forall o \\in O ~ \\exists a,b \\in \\mathbb{R}~ st.~  o \\in (a,b)\\]\n증명은 귀류법을 쓰면 쉽게 된다.\n- 저 정리가 생각보다 중요하다. 그리고 이 정리를 나이테정리 라고 기억하자. 이 정리는 \\({\\cal O}\\)-오픈셋이 아닌 일반적인 \\({\\cal T}\\)-오픈셋에 대하여서도 성립한다. 즉 \\((X,{\\cal T})\\)가 위상공간이고 \\(T\\)가 \\({\\cal T}\\)의 임의의 집합이라 하자. \\(T\\)의 임의의 원소 \\(p\\)에 대하여 (1) \\(p\\) 를 포함하지만 (2) \\(T\\) 보다 작은 다른 \\({\\cal T}\\)-오픈셋이 항상 존재한다.\n- 그리고 교재에 따라서는 위와 같은 성질을 만족하는 것을 오픈셋이라고 정의하기도 한다. 이와 같은 논리흐름으로는 오픈인터벌 \\((a,b)\\)를 정의하고 그로부터 인테리어포인트 \\(o\\)와 오픈셋 \\(O\\)를 정의하고 그로부터 토폴로지 \\({\\cal O}\\)를 정의할 수 있다. 하지만 이러한 방식의 contruction 으로는 \\((\\mathbb{R},{\\cal O})\\) 만 만들수있다. 일반적으로는 적당한 \\({\\cal T}\\)가 \\(X\\)의 토폴로지임을 밝히고 그로부터 오픈셋을 정의하고 그 다음 인테리어포인트를 정의하는 식으로 각 요소들을 contruction 한다.\n- 오픈셋의 여집합을 클로즈드셋이라고 한다. 여기서 사람들이 “모든 집합은 오픈셋이거나 클로즈드셋 이어야 한다” 라고 착각하기 쉬운데 사실 그런것은 아니다.\n- 어떠한 construction을 사용하든지 아래의 사실들이 성립한다. 따로 설명을 쓰지 않은 것은 아주 약간의 머리를 쓰면 쉽게 증명할 수 있는 것들이다. (하지만 그냥 받아들이거나 외우는 것이 편하다.) 참고로 아래의 모든 사실들은 보통위상공간 즉 \\((\\mathbb{R},{\\cal O})\\) 를 전제하고 서술한 것이다.\n(1) \\((a,b)\\) 는 오픈셋이다.\n(2) \\(\\mathbb{R}\\) 은 오픈셋이다. 동시에 클로즈드셋이다2.\n2 공집합이 오픈셋이므로3 \\(\\mathbb{R}\\)이 오픈셋이므로(3) \\(\\emptyset\\) 은 오픈셋이다. 동시에 클로즈드셋이다3.\n(4) 오픈셋은 uncountable union 에 닫혀있다. 즉 \\(O_t\\)가 각각 오픈셋일때 \\(\\cup_t O_t\\) 역시 오픈셋이다.\n(5) 오픈셋은 finite intersection 에 닫혀있다. 즉 \\(O_i\\)가 각각 오픈셋이면 \\(\\cap_{i=1}^{n} O_i\\) 역시 오픈셋이다\n(6) 한점 \\(p\\)로 이루어진 집합 \\(\\{p\\}\\)는 오픈셋이 아니다. 이것이 오픈셋이 되려면 \\(\\{p\\}\\)의 모든원소(라고 해봤자 \\(p\\) 밖에 없음)가 내점이어야 하고 \\(p\\)가 \\(\\{p\\}\\)의 내점이려면 \\(p\\)를 포함하는 오픈인터벌 \\((a,b)\\)가 \\(\\{p\\}\\)의 부분집합으로 존재해야하는데 이것이 불가능하기 때문이다. 4\n4 다만 이것은 위상공간을 \\((\\mathbb{R},{\\cal O})\\)로 생각하였을때 이야기이고 위상공간을 \\((\\mathbb{R},2^{\\mathbb{R}})\\)로 생각한다면 \\(\\{p\\}\\) 도 오픈셋이 된다.(7) 오픈셋의 countable-many intersection 은 오픈셋이 아니다. 왜냐하면 \\(\\cap_{n=1}^{\\infty}(-1/n,1/n)=\\{0\\}\\) 인데 \\(\\{0\\}\\)은 오픈셋이 아니기 때문이다.\n- (Lipschutz (1965), p. 69) 위상공간 \\((X,{\\cal T})\\) 를 상상하자. \\(X\\)의 부분집합 \\(A\\)를 상상하자. “집합 \\(A\\)가 \\(X\\)에서 dense 하다” 라는 의미는 \\(\\bar{A}=X\\)라는 의미이다. 가장 빈번하게 사용하는 표현은 “유리수집합 \\(\\mathbb{Q}\\)는 실수 \\(\\mathbb{R}\\)에서 dense 하다” 인데 이것은\\(\\bar{\\mathbb{Q}}=\\mathbb{R}\\)이 성립하기 때문이다 (예제 4.3).\n\n\nChap 6: 기저\n- 오픈인터벌 \\((a,b)\\)를 적당히 countable-many union 하면 \\(\\mathbb{R}\\) 에 존재하는 어떠한 오픈셋 \\(O\\)도 표현할 수 있다. 이럴때 \\((a,b)\\) 모아놓은 collection \\({\\cal B}:=\\{(a,b): a&lt;b \\in \\mathbb{R}\\}\\) 를 토폴로지 \\({\\cal O}\\)의 base 라고 한다. 이처럼 어떠한 위상공간 \\((X,{\\cal T})\\) 가 있을때 토폴로지 \\({\\cal T}\\) 의 임의의 집합을 \\({\\cal B}\\)의 원소들의 uncountable union 으로 표현가능때 \\({\\cal B}\\)를 \\({\\cal T}\\)의 base 라고 한다. 그리고 추가적으로 base의 모든 원소는 \\({\\cal T}\\)-오픈셋이어야 한다는 조건도 포함된다.\n- 토폴로지 \\({\\cal T}\\)의 base는 유일하지 않다.\n- 아래와 같은 collection을 상상하여 보자.\n\\[\\tilde{\\cal B}:=\\{all~ ray~ in~\\mathbb{R} \\} := \\{(-\\infty,b): b\\in \\mathbb{R} \\} \\cup \\{(a,\\infty): a \\in \\mathbb{R}\\}\\]\n보는 것처럼 \\(\\tilde{\\cal B}\\)는 위상의 정의를 만족한다. 그리고 \\(\\tilde{\\cal B}\\)는 \\({\\cal O}\\)의 base가 아니다. 하지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\((a,b)\\)를 포함하고 있기에 \\({\\cal O}\\) 의 base가 된다. 여기에서 \\(\\pi(\\tilde{\\cal B})\\) 는 \\(\\tilde{\\cal B}\\) 에 의해서 생성된 가장 작은 \\(\\pi\\)-system 이다.\n- 참고로 \\(\\pi\\)-시스템은 모든 원소가 finite intersection 에 닫혀있는 collection 을 의미한다. 전체집합은 empty intersection 으로 해석할 수 있으므로 모든 파이시스템은 전체집합을 포함한다. 따라서 파이시스템을 정의하면 전체집합을 같이 정의하는것과 마찬가지이다. 따라서 파이시스템 역시 시그마필드와 토폴로지처럼 전체집합과 동시에 정의된다. 그리고 정의에 따라서 임의의 집합에 대한 토폴로지와 시그마필드 모두 파이시스템이 된다.\n- 위에서 예를 든 \\(\\tilde{\\cal B}\\) 와 같이 그것 자체가 어떤 위상 \\({\\cal T}\\) 의 base는 아니지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\({\\cal T}\\) 의 base가 될때 \\(\\tilde{\\cal B}\\)를 \\({\\cal T}\\)의 subbase 라고 한다.\n- \\(\\tilde{\\cal B}\\) 가 토폴로지 \\({\\cal T}\\)의 subbase이면 \\(\\tilde{\\cal B}\\)로 \\({\\cal T}\\)를 generate 할 수 있다.\n\n\nChap 8: Metric and Normed Spaces\n- \\(d:X \\times X \\to \\mathbb{R}\\) 가 (1) 음이 아니고 (2) 대칭이며 (3) 삼각부등식을 만족하면 집합 \\(X\\) 에서의 metric 이라고 한다. 이때 음이 아닐 조건은\n\\[\\begin{cases}\nd(a,b) &gt; 0 & a \\neq b \\\\\nd(a,b) = 0 & a=b\n\\end{cases}\\]\n이다. 만약에 메트릭의 모든 조건을 만족하는데 \\(d(a,b)=0\\) 인 서로 다른 \\(a,b \\in X\\) 가 존재하는 경우 \\(d\\) 를 pseudometric 이라고 한다.\n- \\(d\\) 을 집합 \\(X\\) 에서의 메트릭이라고 하자. 메트릭이 존재한다는 것은 집합 \\(X\\)의 어떠한 두 원소라도 그 사이의 거리를 잴 수 있다는 말이고 그것은 집합 \\(X\\)의 임의의 점 \\(p\\)에서 아래와 같은 ball 을 정의할 수 있는 말이다.\n\\[S(p,\\delta) := \\{x:d(p,x)&lt;\\delta,x \\in X \\}\\]\n참고로 위와 같은 ball 들을 모은 collection 을 \\({\\cal B}\\)라고 하자. 그리고 \\({\\cal B}\\)의 임의의 원소를 언카운터블-유니온하여 얻을 수 있는 집합들의 모임을 \\({\\cal T}\\)라고 하자. 그러면 (1) \\({\\cal T}\\) 가 \\(X\\) 의 토폴로지임을 보이고 (2) \\({\\cal B}\\)의 모든 원소가 \\({\\cal T}\\)-오픈셋임을 보인다면 \\({\\cal B}\\)는 \\({\\cal T}\\)의 base가 된다고 주장할 수 있다(Thm 8.4). 그런데 (2)는 (1)이 성립하면 자동으로 성립하므로 (1)만 보이면 된다. 그러기 위해서는 아래의 (i)-(iii)을 보이면 된다.\n(i) 우선 \\({\\cal T}\\)가 언카운터블-유니온에 닫혀있음은 associative laws 에 의해서 쉽게 증명된다.\n(ii) 이제 \\({\\cal T}\\)가 파이나이트-인터섹션에 닫혀있음을 보이자. \\({\\cal T}\\)의 임의의 두 원소는 각각 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다. 가령 예를들어 임의의 \\(T,S \\in {\\cal T}\\) 가 아래와 같이 표현되었다고 치자.\n\\[T=\\bigcup_{t\\in [0,1]}B_{t}, \\quad S=\\bigcup_{s\\in [2,3]}B_{s}\\]\n따라서 \\(T\\cap S\\) 는 distributive laws 에 의해서 아래와 같이 표현가능하다.\n\\[T \\cap S = \\bigcup_{(t,s) \\in [0,1]\\times[2,3]} B_t \\cap B_s \\]\n(i)에 의해서 \\(B_t \\cap B_s\\)가 \\({\\cal T}\\)의 원소이기만 하면 \\(T \\cap S\\) 역시 \\({\\cal T}\\)의 원소가 되는 구조라 (ii)가 증명된다. 따라서 이제 우리가 할일은 \\(B_t\\cap B_s\\)가 \\({\\cal T}\\)의 원소임을 보이는 것이고 이것은 \\(B_t \\cap B_s\\)가 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다는 조건과 동치이다. 우선 \\(B_t \\cap B_s\\)에 속하는 임의의 원소를 $b^* $ 라고 하자. 이 점에 대하여 나이테정리를 만족시키는 ball이 존재한다. 즉\n\\[\\exists S(b^* ,\\delta)~ st. ~ S(b^* ,\\delta) \\subset B_t \\cap B_s\\]\n이다(Lemma 8.3). 그런데 \\(B_t \\cap B_s\\)의 모든점에서 이런식으로 나이테정리를 만족하는 ball을 잡을 수 있다. 이러한 ball들의 합집합을\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)\\]\n이라고 하자. 자명하게 이 집합은 \\(B_t\\cap B_s\\) 보다 작다(부분집합들의 합이므로). 하지만 \\(B_t\\cap B_s\\)의 모든 원소는 이 집합에 포함되므로 이 집합은 \\(B_t\\cap B_s\\)보다 크다. 따라서\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)=B_t \\cap B_s\\]\n이 성립한다.\n(iii) \\({\\cal T}\\)가 \\(X\\)를 포함한다는 것을 보이는것은 볼의 반지름을 크게 만들면 쉽게 증명할 수 있다.\n- 참고로 위의 (i)-(iii)을 요약하면 (1) \\(X\\)가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하고 (2) \\({\\cal B}\\)의 임의의 두 원소가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하기만 하면 볼들이 집합이 아니라 어떠한 \\({\\cal B}\\)라도 특정 토폴로지의 base라고 주장할 수 있다. 이것이 교재의 Thm 6.1 이다.\n- 아무튼 위의 과정을 거치면 \\(X\\)위에서 거리를 정의할 수 있을때 그 거리에 의해서 ball을 정의할 수 있고 ball들의 콜렉션을 base \\({\\cal B}\\)로 정의하고 \\({\\cal B}\\) 원소들의 언카운터블-유니온으로 표현가능한 집합모임을 토폴로지 \\({\\cal T}\\)로 정의해도 논리적모순점이 없다. 즉 \\(X\\)에서 메트릭이 정의되기만 하면 그것에 의해서 순차적으로 토폴로지 \\({\\cal T}\\)를 자연스럽게 유도할 수 있는데 이러한 토폴로지를 특별히 \\(X\\)와 \\(d\\)에 의해서 유도된 metric topology 라고 한다. 그리고 \\((X,d)\\)를 metric-space 라고 한다.\n- \\(\\mathbb{R}\\)에서 \\({\\cal O}\\)를 유도하는 메트릭은 우리가 보통 생각하는 유클리드거리이다. 이러한 메트릭을 usual metric 이라고 한다.\n- \\(\\mathbb{R}\\)에서 아래와 같은 거리를 정의할 수 있다.\n\\[d(a,b)=\\begin{cases}\n0 & a=b \\\\\n1 & a\\neq b\n\\end{cases}\\]\n이러한 거리를 trivial metric 이라고 한다. 그리고 이 거리가 유도하는 토폴로지는 \\(2^{\\mathbb{R}}\\) 이다. (아 몰라.. 따지기 싫어.. 그냥 외워..)\n- 만약에 집합 \\(X\\)에서 정의된 2개의 메트릭 \\(d_1\\), \\(d_2\\)가 같은 토폴로지를 유도한다면 두 메트릭 \\(d_1\\)과 \\(d_2\\)는 equivalent 하다고 말한다.\n- 토폴로지컬-스페이스 \\((X,{\\cal T})\\) 가 있다고 하자. 그런데 \\(X\\) 에서 어떠한 메트릭 \\(d\\)가 존재해 그것이 \\({\\cal T}\\)를 유도하였다고 하자. 그럼 \\({\\cal T}\\)는 메트릭-토폴로지가 된다. 이와 같이 (1) \\(X\\)에서 정의되고 (2) 메트릭-토폴로지 \\({\\cal T}\\)를 유도하는 적당한 메트릭 \\(d\\)가 명시된것은 아니지만 그런 메트릭의 존재를 하나 이상 우리가 알고 있을때 위상공간 \\((X,{\\cal T})\\)를 metrizable 하다고 한다.\n- 두 메트릭스페이스 \\((X,d_1)\\) 와 \\((Y,d_2)\\) 가 isometric 하다는 것은 아래가 만족하는 one-one, onto 인 \\(f:X \\to Y\\) 가 존재한다는 것이다.\n\\[d_1(p,q) = d_2(f(p),f(q))\\]\n- 이때 isometric 이라는 relation 은 보는것 처럼 모든 메트릭공간들의 집합 \\({\\cal M}\\)에서 equivalence relation 이다. 즉 아래가 성립한다.\n(i) \\((X,d_1) \\overset{ism}{\\sim} (X,d_1)\\),\n(ii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) implies \\((Y,d_2) \\overset{ism}{\\sim} (X,d_1)\\),\n(iii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) and \\((Y,d_2) \\overset{ism}{\\sim} (Z,d_3)\\) imply \\((X,d_1) \\overset{ism}{\\sim} (Z,d_3)\\).\n\n\nChap 9: Countability\n- (Lipschutz (1965), p. 132) 위상공간 \\((X,{\\cal T})\\)가 separable 하다는 의미는 \\((X,{\\cal T})\\) countable dense subset 을 가진다는 의미이다.\n- 위상공간 \\((\\mathbb{R}, {\\cal O})\\)를 상상하자.\n\n\\(A_n = [-n,n] \\cap \\mathbb{Q}\\)\n\n라고 한다면 (1) \\({\\cal A} = \\{A_n\\}\\) 은 countable 하고 (2) \\(A_n\\)은 모두 (\\([-n,n]\\)에서) dense 하다. 따라서 \\(\\mathbb{R}\\)은 countable한 dense subset을 가진다. 따라서 \\((\\mathbb{R},{\\cal O})\\)는 seperable 하다.\n\n\nChap 10: Separation Axioms\n- 예비학습1: 위상공간 \\((X,{\\cal T})\\)를 고려하자. \\(X\\)의 임의의 닫힌집합 \\(F\\)를 상상하자. 그리고 \\(F\\)에 소속되지 않은 한 점 \\(p \\in X\\)를 상상하자. 이제 닫힌집합 \\(F\\)를 포함하는 아주 작은 열린집합 \\(G\\)와 \\(p\\)를 포함하는 아주 작은 열린집합 \\(H\\)를 상상하자. 위상공간 \\((X,{\\cal T})\\)에서 임의의 \\(F\\)와 \\(p\\)에 대하여서도 두 열린집합 \\(G\\),\\(H\\)가 서로소가 되도록 선택할 수 있다면 그 위상공간은 regular 하다고 표현한다 (Lipschutz (1965), p. 140).\n이해를 위한 예시\n\n\\(F= [0,1]\\)\n\\(p = 1.01\\)\n\\(G= (-0.001, 1.001)\\)\n\\(H= (1.009,1.011)\\)\n\n- 예비학습2: 위상공간 \\((X,{\\cal T})\\)를 고려하자. \\(X\\)의 부분집합중 서로소인 닫힌집합 \\(F_1,F_2\\)를 상상하자.이제 닫힌집합 \\(F_1\\)를 포함하는 아주 작은 열린집합 \\(G\\)와 \\(F_2\\)를 포함하는 아주 작은 열린집합 \\(H\\)를 상상하자. 위상공간 \\((X,{\\cal T})\\)에서 임의의 \\(F_1\\)와 \\(F_2\\)에 대하여서도 두 열린집합 \\(G\\),\\(H\\)가 서로소가 되도록 선택할 수 있다면 그 위상공간은 normal 하다고 표현한다 (Lipschutz (1965), p. 141).\n5 \\({\\cal T}=\\{\\emptyset, X\\}\\)로 설정한다면 이 조건은 당연히 성립하지 않겠지?- 위상공간 \\((X,{\\cal T})\\) 에는 얼마나 많은 (혹은 다양한) 열린집합이 있을까? 위상공간 \\((X,{\\cal T}_1)\\)이 \\(T_1\\)-space라는 의미는 서로 다른 \\(a,b \\in X\\)에 대하여 \\(a\\)만 포함하는 열린집합 혹은 \\(b\\)만 포함하는 열린집합이 각각 존재한다는 의미이다.5 위상공간 \\((X,{\\cal T}_2)\\)이 \\(T_2\\)-space 혹은 Hausdorff space라는 의미는 \\(T_1\\)-space에서, \\(a\\)만 포함하는 열린집합과 \\(b\\)만 포함하는 열린집합이 서로 disjoint한 경우를 의미한다. 위상공간 \\((X,{\\cal T}_3)\\)이 \\(T_3\\)-space 라는 의미는 \\(T_1\\)이고 regular space 라는 의미이다. 위상공간 \\((X,{\\cal T}_4)\\)이 \\(T_4\\)-space 라는 의미는 \\(T_1\\)이고 normal space 라는 의미이다.\n- 포함관계: \\(T_1\\)-space는 \\(T_2\\)-space를 포함하고, \\(T_2\\)-space는 \\(T_3\\)-space를, \\(T_3\\)-space는 \\(T_4\\)-space를 포함한다 (Lipschutz (1965), p. 141 그림). 그리고 metric space는 \\(T_4\\) space에 포함된다.\n- \\(T_1\\) 위상공간에서는 “singleton = closed set” 이라고 주장할 수 있다. (Lipschutz (1965), Thm 10.1) \\(T_2\\) 위상공간에서는 “convergent sequence has a unique limit” 을 주장할 수 있다 (Lipschutz (1965), Thm 10.3). \\(T_3\\) 위상공간은 특별히 기억할만한 부분이 없어보인다. \\(T_4\\) 위상공간은 우리손레마 (urysohn lemma) 가 성립하는 공간이다 (Lipschutz (1965), Thm 10.7).\n- 우리손레마: \\(T_4\\) 위상공간 \\((X,{\\cal T}_4)\\)를 상상하자. 그리고 \\(X\\)의 부분집합 중 서로소인 닫힌 부분집합 \\(F_1,F_2\\)를 상상하자. 우리손레마에 의하면 어떠한 \\(F_1,F_2\\)에 대하여서도,\n\n\\(f(F_1)=\\{0\\}\\)\n\\(f(F_1)=\\{1\\}\\)\n\n를 만족하는 적당한 연속함수 \\(f:X \\to [0,1]\\)이 항상 존재함이 알려져 있다 (Lipschutz (1965), Thm 10.7). 이것은 \\(F_1,F_2\\)를 구분할 수 있는 어떠한 함수 \\(f\\)가 항상 존재함을 의미하는데 이는 \\((X,{\\cal T}_4)\\)를 거리공간화 하는데 이용할 수 있다. 구체적으로 위상공간 \\((X,{\\cal T}_4)\\)가 추가적으로 second coutable 조건을 만족한다면6 \\((X,{\\cal T}_4)\\) 는 거리공간으로 바꿀 수 있음이 알려져 있다 (Lipschutz (1965), Thm 10.8).\n\nLipschutz, Seymour. 1965. “Schaum’s Outline of Theory and Problems of General Topology.” (No Title).\n6 즉 \\((X,{\\cal T}_4)\\)가 countable한 base를 가진다면- 위상공간 \\((X,{\\cal T})\\)가 \\(T_4\\)-space이고 추가적으로 second coutable space 라면 \\((X,{\\cal T})\\)는 Hilbert cube 와 호모몰픽(homeomorphic) 하다는 것이 알려져 있다 (Lipschutz (1965), p. 142).\n\n\nChap 14: Complete Metric Spaces\n- Convergent sequence 은 단독으로 정의될 수 없으며 위상공간 \\((X,{\\cal T})\\) 와 묶어서 정의된다. 그리고 Cauchy sequence 역시 단독으로 정의될 수 없으며 메트릭스페이스 \\((X,d)\\) 와 묶어서 정의된다.\n- Convergent sequence 와 Cauchy sequence 는 비슷해보이지만 미묘하게 다른점이 있다.\n(1) 컨버전트-시컨트는 위상공간 \\((X,{\\cal T})\\) 만 있으면 정의할 수 있지만 코시수열은 그 위상공간이 메트릭스페이스 이어야 한다는 제약이 있다. 왜냐하면 컨버전트-시컨스의 정의에는 오픈셋만 필요하지만 코시수열은 볼이 필요하고 볼은 메트릭에 의해서만 정의되기 때문이다.\n(2) 컨버전트-시컨스와 코시수열 모두 열의 각 항이 \\(X\\)의 원소이어야 한다는 조건이 있다. 하지만 컨버전트-시컨스는 그 limit 까지 \\(X\\)의 원소이어야 하는데 코시수열은 그렇지 않다는 차이점이 있다.\n- \\(X=(0,1)\\) 위의 usual metric 에 의해서 유도되는 메트릭스페이스 \\((X,d)\\) 를 생각하자. 수열\n\\[\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{4},\\dots,\\right\\}\\]\n\\(X\\)에서 정의된 코시수열이지만 \\(X\\)에서 정의되는 컨버전트-시컨스는 아니다.\n- 내가 이해한 바는 아래와 같다.\n(1) 토폴로지 \\((X,{\\cal T})\\) 는 항상 컨버전트-시컨스를 정의할 준비가 되어있는 공간이다.\n(2) 위에서 정의가능한 컨버전트-시컨스는 코시수열과 아무런 관련이 없다. 그리고 우리가 통상적으로 고등학교때부터 다루어왔던 수열의 수렴의 개념과도 거리가 멀다.\n(3) 토폴로지 \\((X,{\\cal T})\\) 가 메트릭스페이스라면 컨버전트-시컨스는 코시수열과 어떤관계가 있으며 고등학교때부터 내가 다루어 왔던 상식적인 수렴하는 수열의 개념과도 관련이 있다.\n(4) \\((X,{\\cal T})\\) 가 메트릭스페이스 라고 가정하자. 그럼 아래가 만족한다고 생각할 수 있다.\n\n\\(\\{a_n\\}\\) converges on \\(X\\) \\(\\Longleftrightarrow\\) \\(\\{a_b\\}\\) is Cauchy sequence on \\(X\\) and \\(\\lim_{n\\to\\infty} a_n \\in X\\)\n\n즉 러프하게 말해서 \\(X\\)에서의 컨버전트-시컨스는 (i) \\(X\\)에서의 코시수열이면서 (ii) limit 이 \\(X\\)에 포함되는 수열이라고 말할 수 있다. 이런 정의로 치면 우리가 고등학교때부터 생각해왔던 소박한 정의의 수렴하는 수열은 사실 코시수열에 가깝고 컨버전트-시컨스는 고등학교때부터 배운 소박한 수렴을 하며 동시에 수렴값이 \\(X\\)에서 잘 정의되는 수열을 의미한다고 볼 수 있다. 앞으로는 소박한 수렴과 컨버전트-시컨스를 엄밀하게 구분하여 말하도록 하자. 즉 \\(\\{a_n\\}\\)이 코시수열이라는 말은 \\(\\{a_n\\}\\)이 소박한 수렴을 한다는 의미이고 \\(\\{a_n\\}\\)이 컨버전트-시컨스라는 의미는 \\(\\{a_n\\}\\)이 소박한수렴을 하며 동시에 그 극한값이 well-define 된다는 의미(=\\(\\{a_n\\}\\)의 수렴값이 \\(X\\)의 원소라는 의미)이다.\n- (proposition 14.1) 메트릭스페이스 한정으로, 컨버전트-시컨스는 모두 코시수열이다. (당연한 소리를.. 이런걸 proposition 이라고..)\n- 당연히 위 정리의 역은 성립하지 않는다. 즉 메트릭스페이스 \\((X,{\\cal T})\\) 에서 정의된 코시수열이 반드시 컨버전트-시컨스라는 보장은 없다. (이것도 당연한 소리.. 왜냐하면 수렴값이 \\(X\\)에 포함된다는 보장이 없기 때문) 하지만 그 메트릭스페이스가 complete 하다면 위 정리의 역도 성립한다.\n- 컴플리트하지 않은 메트릭스페이스 \\((X,d)\\)를 컴플리트한 메트릭스페이스 \\((X^* , d)\\) 로 바꿀 수 없을까? 유주얼메트릭(usual metric) \\(d\\) 와 \\(X=(0,1)\\) 로 만들어지는 메트릭스페이스는 컴플리트하지 않지만 \\(d\\) 와 \\(X^* =[0,1]\\) 로 만들어지는 메트릭스페이스는 컴플리트하다. 이런 경우 $(X^* ,d) $ 는 \\((X,d)\\) 의 completion 이라고 한다.\n- 즉 아래의 조건들을 만족하면 공간 $(X^* ,d) $ 는 공간 \\((X,d)\\) 의 completion 이라고 부른다.\n(1) \\(X\\subset X^*\\)\n(2) \\((X^* ,d)\\) is complete metric space\n(3) \\((X,d) \\overset{ism}{\\sim} (X^* ,d)\\).\n\n- 메트릭스페이스 \\((X,d)\\)에서 아래의 식을 만족하는 두 코시수열 \\(\\{a_n\\}\\), \\(\\{\\tilde a_n\\}\\) 을 생각하여보자.\n\\[\\lim_{n\\to\\infty} d(a_n,\\tilde a_n)=0 \\]\n이러한 코시수열들을\n\\[\\{a_n\\} \\overset{slim}{\\sim} \\{\\tilde a_n\\}\\]\n이라고 표현하자. 이때 관계 \\(\\overset{slim}{\\sim}\\) 은 \\(X\\)에서 정의가능한 모든 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 equivalence relation 이 된다고 한다. (증명은 알아서) 따라서 이걸 이용하면 거리공간에서 \\(slim\\) 의 관계를 가지는 임의의 두 수열은 같은 극한을 가진다는 결론이 나온다. (이것도 잘 따져보자.)\n- 잠시 (1) 바이너리-릴레이션(binary relation), (2) 이퀴배런스-릴레이션(equivalence relation), (3) 이퀴배런스-클래스(equivalence class) 그리고 (4) 코션트셋(quotient set)에 대하여 설명하고 넘어가겠다.\n(1) 집합 \\({\\cal C}_ X\\) 의 두 원소 \\(\\{a_n\\}\\), \\(\\{b_n\\}\\) 간 바이너리-릴레이션 \\(R\\)이 존재한다는 문장은 집합론적인 언어로 표현가능하다. 구체적으로는 \\(R\\)을 곱집합 \\({\\cal C}_ X \\times {\\cal C}_ X\\) 의 적당한 부분집합으로 설정하고 순서쌍 \\(\\big(\\{a_n\\},\\{b_n\\}\\big)\\) 이 \\(R\\) 의 원소라는 식으로 표현한다. 예를 들면 아래와 같은 식으로 말이다.\n$ {a_n} and {b_n} has arelation with~ R \\ ({a_n},{b_n}) R _ X _ X \\ {a_n} {b_n}$\n(2) 그리고 \\({\\cal C}_ X\\) 위에서의 바이너리-릴레이션 \\(R\\)이 (i) reflexivity (ii) symmetricity (iii) transitivity 를 만족하면 이 릴레이션을 특별히 이퀴배런스-릴레이션 이라고 말한다.\n(3) 그리고 아래와 같이 \\({\\cal C}_ X\\) 에서 \\(\\{a_n\\}\\) 과 이퀴배런스-릴레이션을 가지는 원소들을 모아놓은 집합을 생각할 수 있다. \\[\\big[\\{a_n\\}\\big]_ R:=\\big\\{ \\{x_n\\} : \\{x_n\\} \\overset{R}{\\sim} \\{a_n\\} ~and~ \\{x_n\\} \\in {\\cal C}_ X \\big\\}\\]\n이 집합을 \\(\\{a_n\\}\\)의 equivalence class on \\({\\cal C}_ X\\) by \\(R\\) 이라고 부른다. 보통은 \\(R\\)을 생략하여 \\(\\big[\\{a_n\\}\\big]\\)와 같이만 표현하지만 나는 기호의 명확성을 위해서 관계까지 명시하였다.\n(4) 이퀴배런스-클래스는 본질적으로 파티션과 밀접한 연관이 있다. 여기에서 클래스 \\({\\cal P}_ A\\) 가 집합 \\(A\\)의 파티션이란 의미는 클래스 \\({\\cal P}_ A\\) 에 속한 모든 원소의 합이 \\(A\\) 이며 클래스 \\({\\cal P}_ A\\) 의 각 원소는 서로 배타적이라는 의미이다. 이퀴배런스-클래스가 그럼 왜 파티션과 관련이 있을까? 그것은 어떠한 집합에서 이퀴배런스-릴레이션이 존재하면 그 집합을 배타적인 이퀴배런스-클래스의 합집합으로 표현가능하기 때문이다. 즉 이퀴배런스-릴레이션 혹은 이퀴배런-클래스의 존재는 파티션의 존재를 임플라이 한다. 그리고 이러한 파티션을 이퀴배런스-릴레이션 \\(R\\)에 의해 생성된 quotient set 혹은 quotient space 라고 한다. 관계 \\(R\\)에 의한 \\(A\\)의 코션트 셋은 기호로 \\(A ~\\overset{R}{\\sim}\\) 와 같이 쓴다. 예를들어 \\[\\begin{align}\n{\\cal C}_ X ~ /\\overset{slim}{\\sim}\n\\end{align}\\] 은 집합 \\(X\\) 상에서 존재하는 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 이퀴배런스-릴레이션 \\(slim\\) 에 의해서 생성된 코션트셋을 의미한다."
  },
  {
    "objectID": "posts/1_Essays/2019-04-26-퓨리에변환.html",
    "href": "posts/1_Essays/2019-04-26-퓨리에변환.html",
    "title": "[Essays] 퓨리에 변환",
    "section": "",
    "text": "About this doc\n- 이번에는 퓨리에 표현들을 정리하도록 하겠다. 내생각엔 퓨리에 표현들도 벡터의 미분만큼 복잡한 것 같다. 정의가 너무 많고 그게 그거 같아서 그렇다. 이번기회에 깔끔하게 정리하도록 하자. 참고한 문헌은 아래와 같다.\n\nHaykin, S., & Van Veen, B. (2007). Signals and systems. John Wiley & Sons.\n\n\n\n들어가며\n- 우선 신호와 하나의 신호값을 구분하는 notation을 생각하자. 우리가 다루는 신호 즉 데이터는 값들의 집합이다. 우리가 시계열자료를 다룬다면 데이터는 아래와 같이 표현한다.\n\n\\(\\{x_i: i \\in \\mathbb{Z}\\}\\)\n\n이와 유시하게 우리가 다루는 자료가 \\(t \\in \\mathbb{R}\\)인 연속신호라면 아래와 같이 표현한다.\n\n\\(\\{x(t): t \\in \\mathbb{R}\\}\\)\n\n우리가 모든 \\(i \\in \\mathbb{Z}\\) 혹은 모든 \\(t \\in \\mathbb{R}\\)에서 신호를 다룰 생각이 없다면 아래와 같은 표현도 얼마든지 가능하다.\n\n\\(\\{x_i: i=0,1,\\dots, \\xi-1 \\}\\)\n\\(\\{x(t):t \\in (0,\\zeta) \\}\\)\n\n- 위와 같이 집합의 표현 없이 단독으로 \\(x_i\\), \\(x(t)\\)와 같이 쓰면 하나의 고정된 값 \\(i,t\\)에 대한 \\(x_i\\), \\(x(t)\\)로 이해하자. 솔직히 이렇게 꼭 신호를 엄밀하게 집합으로 정의하는게 유별나 보일수도 있다. 일반적으로 사람들은 \\(\\{x(t): ~t \\in \\mathbb{R} \\}\\) 대신에 보통 \\(x(t)\\)로 간단하게 줄여서 쓰곤한다.1 하지만 이 포스팅에 한정하여 위와 같이 집합의 형태로 엄밀하게 구분해 쓰도록 하자. 처음에는 익숙하지 않지만 나중에는 편리하다.\n1 나도 그렇다.\n\n퓨리에표현들\n- 지금부터 우리가 고려하는 모든 신호들은 기본적으로 (1) infinity range에서 정의된 신호라고 가정한다. 즉 연속신호이면 \\(\\mathbb{R}\\)에서 정의된다고 가정하고 이산신호면 \\(\\mathbb{Z}\\)에서 정의된다고 가정한다. 또한 우리가 분석하고자 하는 신호는 (2) integrable 하다고 가정한다. 이건 퓨리에표현들이 적분 혹은 무한합의 형태로 표현된다는 것을 상기하면 타당하여 보인다.\n- 즉 우리가 고려하는 신호는 인피니티-레인지에서 정의되며 인피니티-레인지에서 적분값이 유한한 연속신호 혹은 이산신호 임을 알 수 있다. 이러한 신호는 구체적으로는 아래와 같이 쓸 수 있다.\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\}\\)\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\}\\)\n\n- 그런데 integrable 한 함수들만을 고려하다 보면 우리가 다룰 수 있는 신호의 범위가 확 줄어들게 된다. 가령 예를 들어서 아래와 같은 신호는 적분을 하면 무한대가 나오기 때문에 intergrable 하지 않다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in \\mathbb{R} \\right\\}\\)\n\n이것은 좀 불합리해 보이는데 위의 신호는 주기신호라서 한 주기의 패턴만 분석하면 될것 같이 보이기 때문이다. 위의 신호는 intergrable 하지않지만 아래의 신호는 intergrable 하다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in (0,2\\pi) \\right\\}\\)\n\n우리는 이런신호까지 분석하기로 한다. 이런신호를 분석할 수 있는 이유는 해석학 교재를 참고하면 된다.2\n2 사실 나도 잘 모름 (뭐 quotient group이런거 알아야 하는데 공부하려면 꽤 걸릴듯)- 아무튼 우리는 (1) 인피니티-레인지에서 정의되는 가지는 신호 (2) 인피니티-레인지에서 적분값이 잘 정의되는 신호, 혹은 한 주기만 적분해 보았을때 그 값이 잘 정의되는 주기신호 를 타겟팅해 분석한다. 즉 분석하는 신호는 구체적으로 아래의 4가지이다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt &lt;\\infty, ~ , x(t)=x(t+\\zeta), ~ t \\in \\mathbb{R}, \\right\\}.\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| &lt;\\infty,~ i, x_i=x_{i+\\xi},~ i \\in \\mathbb{Z} \\right\\}.\\)\n\n- 표현들을 정리하기에 앞서서 몇 가지 알아두어야 할 사항이 있다. (1) 시간축에서 연속인 신호는 주파수측에서는 비주기신호가 나온다. (2) 시간축에서 디스크릿한 신호는 주파수측에서는 주기신호이다. (3) 시간축에서 주기인 신호는 주파수에서는 디스크릿하다. (4) 시간축에서 비주기신호는 주파수에서 연속이다. 이 사실들을 종합하면 각각의 경우에 해당하는 퓨리에 표현들은 아래와 같은 특징을 가지고 있음을 알 수 있다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{-\\infty}^{\\infty} |\\hat x(\\omega)| d\\omega &lt;\\infty,~ \\omega \\in \\mathbb{R} \\right\\}\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt &lt;\\infty,~ x(t)=x(t+\\zeta),~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=-\\infty}^{\\infty} |\\hat x_k| &lt;\\infty,~ k \\in \\mathbb{Z} \\right\\}\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{0}^{2\\pi} |\\hat x(\\omega)| d\\omega &lt;\\infty,~ \\hat x(\\omega)=\\hat x(\\omega+2\\pi),~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| &lt;\\infty,~ x_i=x_{i+\\xi},~ i \\in \\mathbb{R} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=0}^{\\xi-1} |\\hat x_k| &lt;\\infty,~\\hat x_k = \\hat x_{k+\\xi} ,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 여기에서 \\(\\zeta\\)는 (시간축에서) 연속신호의 주기라고 정의하고 \\(\\xi\\)는 (시간축에서) 이산신호의 주기라고 약속하자. 주파수영역이 디스크릿하게 나오면 FS라고 부르고 주파수영역이 컨티뉴어스하게 나오면 FT라고 부른다. 특이한점은 비주기-이산신호에 대한 FS \\(\\hat x(\\omega)\\)는 주파수 영역에서 주기가 \\(2\\pi\\)임을 파악할 수 있다. 이유는 궁금해하지말자. (내생각에 그냥 \\(\\omega\\)를 적당히 스케일링하여 주기를 \\(2\\pi\\)로 맞췄을 거다.)\n- 이제 짜증나는 적분가능조건따위는 버리도록 하자. 대신에 각 경우에 퓨리에변환(혹은series)과 그 역이 어떻게 정의되는지 알아보자. 그리고 외우자. 각 신호가 어떠한 도메인에서 정의되는지만 잘 파악하면 의외로 외우기 쉽다.\ncase 1. 연속-비주기\n\n\\(\\left\\{x(t): x(t)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\int_{-\\infty}^{\\infty} x(t)e^{-j\\omega t} dt,~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\sum_{k=-\\infty}^{\\infty} \\hat x_k e^{j \\frac{2\\pi}{\\zeta} t} ,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 3. 이산-비주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega, ~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\sum_{i=-\\infty}^{\\infty}x_ie^{-j\\omega i}, ~\\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\sum_{k=0}^{\\xi-1} \\hat x_k e^{-j\\frac{2\\pi k}{\\xi}i},~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i},~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주기함수는 (그것이 이산이든 연속이든) 주파수영역에서의 값이 디스크릿하다. 즉 위에서 case2와 case4인 경우는 주파수영역에서 값이 디스크릿하다. 이것을 연속함수인것처럼 바꿔보면 아래와 같이 쓸 수 있다.\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\zeta}\\right), ~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega,,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i}\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\xi}\\right),~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주목할것은 주기가 \\(\\zeta\\) 혹은 \\(\\xi\\) 인 함수의 주파수 응답은 오로지\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\zeta}, k \\in \\mathbb{Z}\\right\\}\\)\n\n혹은\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\xi}, k \\in \\mathbb{Z}\\right\\}\\)\n\n에서만 존재한다는 점이다. 또한 이산신호의 경우 \\(x_i\\)의 주기가 \\(\\xi\\) 이면 \\(\\hat{x}(\\omega)\\)의 주기역시 \\(\\xi\\) 라는점 역시 주목할만한 부분이다.\n- 주파수영역에서 디스크릿한 함수를 연속인것처럼 표현했듯이 시간영역에서 디스크릿한 함수 역시 연속인것처럼 표현할 수 있다. 예를들면 \\(\\{x_i: x_i=x(iT),~i \\in \\mathbb{Z}\\}\\) 와 같은 관계가 있는 경우 아래와 같이 표현 가능하다.\n\\[x_{\\delta}(t)=\\sum_{i=-\\infty}^{\\infty}x_i\\delta(t-iT).\\]\n이거 엄청 중요하다."
  },
  {
    "objectID": "3_yechan.html",
    "href": "3_yechan.html",
    "title": "YECHAN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 24, 2023\n\n\n[HST] CommunityDetection\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_yeon.html",
    "href": "3_yeon.html",
    "title": "SEOYEONC",
    "section": "",
    "text": "https://github.com/seoyeonc\nhttps://seoyeonc.github.io/md/\nhttps://seoyeonc.github.io/blog/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연 \n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 1, 2023\n\n\n[SEOYEONC] type1 err, type2 err\n\n\n신록예찬 \n\n\n\n\nJan 12, 2023\n\n\n[SEOYEONC] 지수분포 평균검정\n\n\n신록예찬 \n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬 \n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_essays.html",
    "href": "1_essays.html",
    "title": "Essays",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 7, 2023\n\n\n[Essays] 다중척도방법\n\n\n신록예찬 \n\n\n\n\nJul 4, 2023\n\n\n[Essays] 토폴로지\n\n\n신록예찬 \n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬 \n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_cgsp.html",
    "href": "2_cgsp.html",
    "title": "CGSP",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬 \n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬 \n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬 \n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  }
]