[
  {
    "objectID": "2_pl.html",
    "href": "2_pl.html",
    "title": "PyTorch Lightning",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nThis blog was created for my personal research, study and lecture preparation. Therefore, the contents of the blog can be thought of as my practice notes. As a result, sometimes the content of a post may be left unstructured or unfinished. The blog is named after my favorite essay ‘신록예찬’, which is also a nickname I use informally. You can check the written article in the sidebar on the left. It is a great honor for me if these posts can help others to learn and research.\n\nSome links that help me\nblogs\n\nhttps://seoyeonc.github.io/md/\nhttps://seoyeonc.github.io/blog/\nhttps://boram-coco.github.io/coco/\nhttps://pinkocto.github.io/noteda/\nhttps://pinkocto.github.io/Quarto-Blog/\n\nfirst url bunch\n\nchatGPT: https://openai.com/blog/chatgpt/\nmatplotlib: https://matplotlib.org/stable/gallery/index.html\njupyterlab: https://jupyterlab.readthedocs.io/en/stable/index.html\npandas: https://pandas.pydata.org/docs/user_guide/index.html#user-guide\njulia: https://docs.julialang.org/en/v1/\nkeras: https://keras.io/examples/\njulia plots: https://docs.juliaplots.org/stable/\npytorch lightning: https://www.pytorchlightning.ai/\nplotly: https://plotly.com/graphing-libraries/\nquarto: https://quarto.org/\n\nsecond url bunch\n\nlatex: https://editor.codecogs.com/\ntable: https://www.tablesgenerator.com/\npytorch lightning (codes in book): https://github.com/PacktPublishing/Deep-Learning-with-PyTorch-Lightning\nPyG: https://github.com/rusty1s/pytorch_geometric\nrayshader: https://www.rayshader.com/reference/plot_gg.html\nSouth Korea (map): https://github.com/southkorea\nregexp: https://zvon.org/comp/m/regexp.html\nrpy2: https://rpy2.github.io/doc/v3.1.x/html/index.html\npywave: https://pywavelets.readthedocs.io/en/latest/ref/wavelets.html\nPyGSP: https://pygsp.readthedocs.io/en/stable/#\nlatex (neural networks): https://tikz.net/neural_networks/\nplotly overview: https://plotly.com/python/plotly-express/\nfastai (official): https://docs.fast.ai/\nfastai (lecture): https://course.fast.ai/#\nfastai (github codes): https://github.com/fastai/fastai/tree/master/dev_nbs/course\nGML (codes in book): https://github.com/PacktPublishing/Graph-Machine-Learning\nGMLKOR (codes in book): https://github.com/AcornPublishing/graph-ml\nmathNET https://gtribello.github.io/mathNET/index.html\n\nlectures notes\n\nhttp://personal.psu.edu/drh20/asymp/fall2006/lectures/\nhttps://web.ma.utexas.edu/users/gordanz/lecture_notes_page.html"
  },
  {
    "objectID": "3_boram.html",
    "href": "3_boram.html",
    "title": "BORAM",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 19, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬\n\n\n\n\nMay 19, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬\n\n\n\n\nMay 12, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬\n\n\n\n\nMay 12, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬\n\n\n\n\nMay 6, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Start\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_gml.html",
    "href": "2_gml.html",
    "title": "GML",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - Graph metrics\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - networkx로 그래프 이해하기\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - 밴치마크 및 저장소\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - AutoEncoder\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Graph Neural Network\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Structural Deep Network Embedding\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - Graph CNN\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 그래프 정규화 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 특징 기반 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap5: 응용문제 - 누락된 링크예측\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap5: 응용문제 - 커뮤니티 감지\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap6: 소셜네트워크 그래프\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Graph Neural Network Topic Classifier\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Shallow-Learning Topic Modelling\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Text Analytics and Natural Language Processing using Graphs\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap8: 신용카드 거래에 대한 그래프 분석\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap9: Graph Database Connection\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2099-01-01-notes1.html",
    "href": "posts/2099-01-01-notes1.html",
    "title": "연습장1",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n1. 묵찌빠 (150점)\nref: https://namu.wiki/w/묵찌빠\n묵찌빠는 가위바위보의 변형 놀이이다.\n보통 가위바위보의 게임과 연이어 진행되는데, 가위바위보 승부 이후 이긴 사람이 공격권을 가지고, 묵(바위)/찌(가위)/빠(보자기) 가운데 하나를 외치는 동시에 말한 것과 일치하도록 손 모양을 바꾼다. 공격권을 가진 사람의 손 모양이 상대(수비권을 가진 사람)의 손 모양과 일치하면 공격권을 가진 사람의 승리. 두 사람이 동시에 모양을 바꿔야 하기에 구호를 맞추어 하기도 한다. 구호로 쓰이는 것은 “하나, 둘”, “(자신의 손에 따라, 혹은 공격자 손에 따라) ‘묵, 묵’, ‘찌, 찌’, ‘빠, 빠’”, “(두번째 것의 변형으로) ‘묵에, 묵에, ’찌에, 찌에’, ‘빠에, 빠에’” 등이 있다. 구호를 외친 후 세번째 박자에 맞추어 바꾼다. 게임의 긴장감을 살리기 위해 불시에 공격이 들어가는 식으로, 아무런 구호도 없이 묵찌빠 중 공격자가 바꿀 손모양을 바로 외치는 경우도 있다.\n승부가 갈리지 않았을 경우에는 다시 가위바위보를 하는 것이 아니라, 현 상태에서 가위바위보 규칙 상 이긴 사람이 공격권을 가져가게 된다.\n(1) 아래는 RPS_BASE 클래스의 구현예시이다.\n\nclass RPS_BASE:\n    def __init__(self,candidate):\n        self.candidate = candidate\n        self.actions = list() \n    def __setitem__(self,index,val):\n        self.actions[index] = val\n    def __getitem__(self,item):\n        return self.actions[item]        \n    def __len__(self):\n        return len(self.actions)\n    def __eq__(self,other):\n        return self[-1] == other[-1]\n    def __gt__(self,other):\n        return [self[-1],other[-1]] in [['묵','찌'],['찌','빠'],['빠','묵']]\n    def __ge__(self,other):\n        return (self == other) or (self > other)\n    def __lt__(self,other):\n        return not (self >= other)\n    def __le__(self,other):\n        return (self == other) or (self < other)    \n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/>\n        기록: {}\n        \"\"\"        \n        return html_str.format(self.candidate,self.actions)       \n    def pick(self):\n        self.actions.append(np.random.choice(self.candidate))        \n\n이 클래스에서 아래와 같은 2명의 플레이어 인스턴스를 생성하라.\n\na: [‘묵’,‘찌’] 중에 하나를 랜덤으로 선택\nb: [‘찌’,‘빠’] 중에 하나를 랜덤으로 선택\n\n두 인스턴스를 100회 랜덤대결하고 결과를 기록하라. 어떠한 플레이어가 더 유리한가?\n\n답안은 100회중 a몇회 승리, b몇회 승리와 같은 숫자형식으로만 나오면 인정한다. (코드를 정리하거나 별도의 클래스를 만드는 것을 요구하지 않음)\n\n(2) RPS_BASE에서 아래와 같은 두명의 플레이어 인스턴스를 생성하라.\n\na: [‘묵’,‘찌’,‘빠’] 중 하나를 랜덤으로 선택\nb: [‘찌’,‘빠’] 중 하나를 랜덤으로 선택\n\n아래와 같은 a,b의 attribute을 변경하라.\n\na.modes = [None]\na.actions = [None] \nb.modes = [None]\nb.actions = [None]\n\n플레이어 a,b를 이용하여 반복적으로 가위바위보 대결을 수행하고 아래와 같이 대결결과에 따라 공격권을 결정하는 함수 mul을 만들어라.\n경우1: 플레이어 a가 승리할 경우\n\na가 공격모드, b가 수비모드가 된다.\na.modes에는 ‘Attack’ 이 b.modes에는 Defence가 추가된다.\na.actions와 b.actions는 각각의 플레이어가 선택한 패(묵,찌,빠)가 기록된다.\n\n경우2: 플레이어 b가 승리할 경우\n\nb가 공격모드, a가 수비모드가 된다.\nb.modes에는 ‘Attack’ 이 a.modes에는 Defence가 추가된다.\na.actions와 b.actions는 각각의 플레이어가 선택한 패(묵,찌,빠)가 기록된다.\n\n경우3: 비길경우\n\n공격권의 변화는 없다.\na.modes,b.modes는 각각 이전의 값이 추가된다.\na.actions와 b.actions는 각각의 플레이어가 선택한 패(묵,찌,빠)가 기록된다.\n\n아래는 함수 mul을 사용한 예시이다.\n시점1: 둘다 찌를 내어 공격권을 아무도 획득하지못함\n\nmul(a,b)\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌']\n        \n\n\n\na.modes\n\n[None, None]\n\n\n\nb.modes\n\n[None, None]\n\n\n시점2: 이번에도 둘다 찌를 내어 아무도 공격권을 획득하지 못함\n\nmul(a,b)\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌', '찌']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '찌']\n        \n\n\n\na.modes\n\n[None, None, None]\n\n\n\nb.modes\n\n[None, None, None]\n\n\n시점3: 이번에는 a가 공격권을 획득 (묵>찌)\n\nmul(a,b)\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌', '찌', '묵']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '찌', '찌']\n        \n\n\n\na.modes\n\n[None, None, None, 'Attack']\n\n\n\nb.modes\n\n[None, None, None, 'Defence']\n\n\n시점4: 이번에는 b가 공격권을 획득 (찌>빠)\n\nmul(a,b)\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌', '찌', '묵', '빠']\n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '찌', '찌', '찌']\n        \n\n\n\na.modes\n\n[None, None, None, 'Attack', 'Defence']\n\n\n\nb.modes\n\n[None, None, None, 'Defence', 'Attack']\n\n\n(3) RPS_BASE를 상속받아 MookjjibbaPlayer라는 새로운 클래스를 정의하라. MookjjibbaPlayer 클래스에서 아래의 메소드를 새롭게 정의 혹은 재정의하여\n\n__init__\n_repr_html_\n__mul__\nreset\n\n인스턴스가 아래와 같은 동작을 하도록 설계하라.\n시점0: 생성예시 (__init__)\n\na=MookjjibbaPlayer(['묵','찌','빠'])\nb=MookjjibbaPlayer(['묵','찌','빠'])\n\n\na.actions\n\n[None]\n\n\n\na.modes\n\n[None]\n\n\n\nb.actions\n\n[None]\n\n\n\nb.modes\n\n[None]\n\n\n시점0: 출력예시(_repr_html_)\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n시점1: 대결 및 결과출력 (__mul__, _repr_html_)\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵'] \n        모드: [None, None] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵'] \n        모드: [None, None] \n        \n\n\n시점2: 대결 및 결과출력 (__mul__, _repr_html_)\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵', '묵'] \n        모드: [None, None, 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵', '빠'] \n        모드: [None, None, 'Attack'] \n        \n\n\n시점3: 대결결과의 초기화 (reset)\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵', '묵'] \n        모드: [None, None, 'Defence'] \n        \n\n\n\na.reset()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\n(4) (3) 에서 생성된 MookjjibbaPlayer의 두 개의 인스턴스 a,b를 입력으로 받고 최초공격권을 결정하는 함수 jumpball을 설계하라.\n아래는 jumpball함수의 사용예시이다.\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\njumpball(a,b)\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌', '찌', '찌'] \n        모드: [None, None, None, 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '찌', '찌', '묵'] \n        모드: [None, None, None, 'Attack'] \n        \n\n\nhint: 아래의 코드를 관찰\n\na = [] \nb = [] \na.append(np.random.choice([0,1,2]))\nb.append(np.random.choice([0,1,2]))\nwhile a==b: \n    a.append(np.random.choice([0,1,2]))\n    b.append(np.random.choice([0,1,2]))\n\n\na\n\n[0, 0, 0, 1]\n\n\n\nb\n\n[0, 0, 0, 0]\n\n\n(5) 지금까지 코드를 바탕으로\n\na: [‘묵’,‘찌’] 중 하나를 랜덤으로 고르는 플레이어\nb: [‘찌’,‘빠’] 중 하나를 랜덤으로 고르는 플레이어\n\n를 설정하여 100회 가상대결을 진행하라. 100회 가상대결결과를 제시하라.\n(참고) – 아래는 제가 구현한 예시입니다. 참고용일 뿐이며 이와 같은 방식으로 구현할 필요는 없습니다.\n1. a,b 두명의 플레이어 생성\n\na=MookjjibbaPlayer(['묵','찌'])\nb=MookjjibbaPlayer(['찌','빠'])\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n2. a,b 두명의 플레이어를 입력으로 하여 게임1을 생성후 1회 게임진행\n\ngame = PlayMookjjibba(a,b)\n\n\ngame.play()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌'] \n        기록: [None, '찌', '묵', '묵', '찌'] \n        모드: [None, 'Attack', 'Defence', 'Defence', 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '빠', '빠', '빠', '찌'] \n        모드: [None, 'Defence', 'Attack', 'Attack', 'Attack'] \n        \n\n\n\n해설: a가 찌, b가 빠를 내어 최초 공격권을 a가 획득하였지만 이후 공격권을 상실한 상실함. 이후 4번째 가위바위보에서 b가 a가 동시에 찌를 내며 b의 승리로 마무리됨\n\n3. game.records 에 b의 승리가 기록되어 있음\n\ngame.records \n\n['b']\n\n\n4. 1회 대결기록을 삭제하고 또 다른 게임을 진행: 이번에는 a의 승리\n\ngame.reset_player_history()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None] \n        모드: [None] \n        \n\n\n\ngame.play()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌'] \n        기록: [None, '찌', '찌', '찌'] \n        모드: [None, None, 'Attack', 'Attack'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '빠', '찌'] \n        모드: [None, None, 'Defence', 'Defence'] \n        \n\n\n5. 이번에는 b의 승리가 기록됨\n\ngame.records\n\n['b', 'a']\n\n\n\n\n2. 기타문제 (50점)\n(1) LinearRegression 이라는 이름의 클래스를 만들고 아래의 기능을 넣어라.\n__init__: “클래스 \\(\\to\\) 인스턴스” 인 시점에 길이가 \\(n\\)인 numpy array \\({\\bf x}=(x_1,\\dots,x_n)\\), \\({\\bf y}=(y_1,\\dots,y_n)\\)을 입력으로 받아 내부에 저장한다.\nfit: fit은 내부에 저장된 \\({\\bf x}\\), \\({\\bf y}\\)를 이용하여 \\(\\hat{\\bf y}=(\\hat{y}_1,\\dots,\\hat{y}_n)\\)을 계산하는 역할을 한다. 계산은 아래의 수식을 이용한다. \\[\\hat{\\bf y}= {\\bf X}({\\bf X}^T {\\bf X})^{-1}{\\bf X}^T {\\bf y}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n \\end{bmatrix}\\]\nplot: plot은 \\((x_i,y_i)\\)와 \\((x_i,\\hat{y}_i)\\)를 시각화하는 역할을 한다.\n아래의 자료를 LinearRegression의 입력으로 받고 시각화하는 분석을 수행하라.\n\nx = np.linspace(0,1,100)\ny = 2*x + np.random.normal(size=100)\nplt.plot(x,y,'o')\n\n\n\n\n(2) 앞면과 뒷면이 나올 확률이 각각 1/2인 동전을 생각하자. 하니와 규빈은 이 동전을 연속으로 던져서 아래와 같은 룰을 정하여 내기를 하였다.\n\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,앞면) 이 나오면 하니의 승리\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,뒷면) 이 나오면 규빈의 승리\n\n이 내기는 하니가 유리한가? 규빈이 유리한가? 시뮬레이션을 통해 검증하라.\n\n\n\n\n\n\nNote\n\n\n\nhint: 똑같이 유리하다\n\n\n(3) 앞면과 뒷면이 나올 확률이 각각 1/2인 동전을 생각하자. 하니와 규빈은 이 동전을 연속으로 던져서 아래와 같은 룰을 정하여 내기를 하였다.\n\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (앞면,뒷면) 이 나오면 하니의 승리\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,뒷면) 이 나오면 규빈의 승리\n\n이 내기는 하니가 유리한가? 규빈이 유리한가? 시뮬레이션을 통해 검증하라.\n\n\n\n\n\n\nNote\n\n\n\nhint: 이 내기는 하니가 유리합니다. 저는 1000회 시뮬레이션 결과\n{'하니': 761, '규빈': 239} ## 하니가 761번 승리\n와 같이 결과가 나왔습니다.\n\n\n(4) Time을 상속받아 Init 클래스를 만들고 __repr__을 조작하여 아래와 같이 인스턴스 생성시점을 출력하는 기능을 구현하라.\n\nclass Time:\n    def time(self):\n        return datetime.datetime.now().strftime('%y-%m-%d %X')\n\n\na = Init()\n\n\na\n\n인스턴스생성시점: 23-06-16 10:27:19\n\n\n\nb = Init()\n\n\na,b\n\n(인스턴스생성시점: 23-06-16 10:27:19, 인스턴스생성시점: 23-06-16 10:27:28)\n\n\n(5) tuple 클래스와 아래의 Check를 상속받아 아래와 같은 역할을 하는 새로운 Tuple 클래스를 만들라.\n\nclass Check:\n    def ckeck(self):\n        return [l for l in dir(self) if l[0]!='_']\n\n\ntpl = Tuple('asdfassdfsasdf')\ntpl # 값과 함께 사용가능한 메소드가 함께 출력 \n\n('a', 's', 'd', 'f', 'a', 's', 's', 'd', 'f', 's', 'a', 's', 'd', 'f')\n\nmethods=['ckeck', 'count', 'freq', 'index']\n\n\n\ntpl.freq()\n\n{'d': 3, 'a': 3, 's': 5, 'f': 3}\n\n\n(6) 아래와 같은 클래스를 고려하자.\n\nclass Init(object):\n    def __init__(self,value):\n        self.value = value\n        \nclass Times2(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value * 2\n        \nclass Plus5(Init):\n    def __init__(self,value):\n        super().__init__(value)\n        self.value = self.value + 5\n\nPlus5와 Times2를 상속하여 적당한 클래스 Times2Plus5를 정의하고 생성과 동시에 \\(x \\to (x\\times 2)+5\\) 를 수행도록 하라.\n\nclass Times2Plus5(Plus5,Times2):\n    def __init__(self,value):\n        super().__init__(value)\n\n사용예시\n\na=Times2Plus5(0)\na.value\n\n5\n\n\n\na=Times2Plus5(1)\na.value\n\n7\n\n\n\na=Times2Plus5(5)\na.value\n\n15\n\n\n(7) 아래의 함수가 있다고 하자.\n\ndef f(x): \n    return np.sin(x)\n\n적당한 함수 derivate를 정의하여 함수를 입력으로 받으면 그 도함수를 출력으로 리턴하도록 하라. 아래의 코드를 이용하여 검증하라.\n\nx = np.linspace(-6,6,100)\nplt.plot(x,f(x),label=r'$f(x)=\\sin(x)$')\nplt.plot(x,(derivate(f))(x),label=r'$f\\'(x)=\\cos(x)$')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f236da92ee0>\n\n\n\n\n\n(8). Student 클래스를 생성지침 및 사용예시를 참고하여 설계하라.\n생성지침\nattributes\n\nname: 이름을 저장하는 변수\nage: 나이를 저장하는 변수\nsemester: 학기를 저장하는 변수\n\nmethods\n\n__init__: name, age, semester 세 가지 매개변수를 입력받아 인스턴스의 attribute로 저장\n__str__: 인스턴스의 정보(이름,나이,학기)를 문자열 형태로 반환\n\n사용예시\n\n# 사용 예시\nboram = Student(name='김보람', age=20, semester=1)\nprint(boram)\n\n이름: 김보람\n나이: 20\n학기: 1\n\n\n(9) 8의 클래스를 상속받아 Student2 만들라. __add__ 재정의하여 Student2의 인스턴스가 아래와 같이 동작하도록 하라.\n\nboram = Student2()\n\n입학을 축하합니다. 당신의 나이는 20.0이고 현재 학기는 0학기 입니다.\n\n\n\nboram + '등록'+ '휴학' + '등록' + '휴학'\nboram\n\n\n        나이: 22.0 \n        학기: 2 \n        \n\n\n(10) 적당한 클래스를 선언하여 \\(f(x)=x+{\\tt const}\\)를 수행하는 함수를 생성하도록 하라.\n사용예시1\n\nf = AddConstant(5) # f(x) = x+5 \n\n\nf(10)\n\n15\n\n\n사용예시2\n\nf = AddConstant(-3) # f(x) = x-3\n\n\nf(10)\n\n7\n\n\n\nclass PlayMookjjibba:\n    def __init__(self,a,b):\n        self.a = a\n        self.b = b\n        self.records = list()\n    def reset_player_history(self):\n        self.a.reset()\n        self.b.reset()\n    def record(self):\n        if self.a.modes[-1] == 'Attack':\n            self.records.append('a')\n        else:\n            self.records.append('b')\n    def jumpball(self):\n        self.a * self.b \n        while self.a == self.b:\n            self.a * self.b \n    def play(self):\n        self.jumpball()\n        while self.a != self.b:\n            self.a * self.b \n        self.record()\n\n\nclass MookjjibbaPlayer(RPS_BASE):\n    def __init__(self,candidate):\n        super().__init__(candidate)\n        self.modes = [None]\n        self.actions = [None]\n    def __mul__(self,other):\n        self.pick()\n        other.pick()\n        if self > other:\n            self.modes.append('Attack')\n            other.modes.append('Defence')\n        elif self < other:\n            self.modes.append('Defence')\n            other.modes.append('Attack')\n        else: \n            self.modes.append(self.modes[-1]) \n            other.modes.append(other.modes[-1])\n    def _repr_html_(self):\n        html_str = \"\"\"\n        낼 수 있는 패: {} <br/>\n        기록: {} <br/>\n        모드: {} <br/>\n        \"\"\"        \n        return html_str.format(self.candidate,self.actions,self.modes)\n    def reset(self):\n        self.__init__(self.candidate)\n\n\na=MookjjibbaPlayer(['묵','찌','빠'])\nb=MookjjibbaPlayer(['묵','찌','빠'])\n\n\na*b\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵'] \n        모드: [None, 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '빠'] \n        모드: [None, 'Attack'] \n        \n\n\n\ngood = [action for mode,action in zip(a.modes[1:],a.actions[1:]) if mode=='Attack']\ngood.count(\"찌\"),good.count(\"묵\"),good.count(\"빠\")\n\n(0, 1, 0)\n\n\n\nclass PlayMookjjibba:\n    def __init__(self,a,b):\n        self.a = a\n        self.b = b\n        self.records = list()\n    def reset_player_history(self):\n        self.a.reset()\n        self.b.reset()\n    def record(self):\n        if self.a.modes[-1] == 'Attack':\n            self.records.append('a')\n        else:\n            self.records.append('b')\n    def jumpball(self):\n        self.a * self.b \n        while self.a == self.b:\n            self.a * self.b \n    def play(self):\n        self.jumpball()\n        while self.a != self.b:\n            self.a * self.b \n        self.record()\n\n\na=MookjjibbaPlayer(['묵','찌'])\nb=MookjjibbaPlayer(['찌','빠'])\n\n\ngame1 = PlayMookjjibba(a,b)\n\n\ngame1.play()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌'] \n        기록: [None, '묵', '묵', '묵', '찌'] \n        모드: [None, 'Attack', 'Defence', 'Defence', 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '빠', '빠', '찌'] \n        모드: [None, 'Defence', 'Attack', 'Attack', 'Attack'] \n        \n\n\n\nfor _ in range(1000):\n    game1.play()\n\n\n{'a':game1.records.count('a'),'b':game1.records.count('b')}\n\n{'a': 680, 'b': 321}\n\n\n\na=MookjjibbaPlayer(['묵','찌','빠'])\nb=MookjjibbaPlayer(['찌','빠'])\n\n\ngame2 = PlayMookjjibba(a,b)\n\n\ngame2.play()\n\n\na\n\n\n        낼 수 있는 패: ['묵', '찌', '빠'] \n        기록: [None, '묵', '찌', '묵', '빠'] \n        모드: [None, 'Attack', 'Attack', 'Defence', 'Defence'] \n        \n\n\n\nb\n\n\n        낼 수 있는 패: ['찌', '빠'] \n        기록: [None, '찌', '빠', '빠', '빠'] \n        모드: [None, 'Defence', 'Defence', 'Attack', 'Attack'] \n        \n\n\n\nfor _ in range(1000):\n    game2.play()\n\n\n{'a':game2.records.count('a'),'b':game2.records.count('b')}\n\n{'a': 50519, 'b': 50482}\n\n\n\nclass TossCoin:\n    def __init__(self):\n        self.history = []\n        self.coin_history = [[]]\n    def toss(self):\n        self.coin_history[-1].append(np.random.rand() < 0.5)\n    def __iter__(self):\n        return self \n    def __next__(self):\n        self.toss() \n        if self.coin_history[-1][-2:] == [True,True]:\n            self.history.append('a')\n            self.coin_history.append([])\n            raise StopIteration\n        if self.coin_history[-1][-2:] == [False,True]:\n            self.history.append('b')\n            self.coin_history.append([])\n            raise StopIteration\n\n\nT, T,F\nF, T/F -> \nF, T -> \n\n\na=[True,False]\n\n\na[-2:] == [True,True]\n\nFalse\n\n\n\ncoin = TossCoin()\n\n\nfor i in range(10000):\n    for _ in coin:\n        pass\n\n\ncoin.history.count('a'),coin.history.count('b')\n\n(2422, 7578)\n\n\n\ncoin.history[:7]\n\n['b', 'b', 'b', 'b', 'b', 'b', 'b']"
  },
  {
    "objectID": "posts/2099-01-02-notes2.html",
    "href": "posts/2099-01-02-notes2.html",
    "title": "연습장2",
    "section": "",
    "text": "import numpy as np\n\n(3) 앞면과 뒷면이 나올 확률이 각각 1/2인 동전을 생각하자. 하니와 규빈은 이 동전을 연속으로 던져서 아래와 같은 룰을 정하여 내기를 하였다.\n\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (앞면,앞면) 이 나오면 하니의 승리\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,뒷면) 이 나오면 규빈의 승리\n\n예를들어 동전을 반복하여 던져 아래와 같이 나온다면 하니의 승리이다.\n\n(앞면, 뒷면, 앞면, 뒷면, 앞면, 앞면)\n(앞면, 뒷면, 앞면, 앞면)\n\n동전을 반복하여 던져 아래와 같이 나온다면 규빈의 승리이다.\n\n(앞면, 뒷면, 앞면, 뒷면, 뒷면)\n(뒷면, 앞면, 뒷면, 뒷면)\n\n이 내기는 하니가 유리한가? 규빈이 유리한가? 시뮬레이션을 통해 검증하라.\n\nclass TossCoin: \n    def __init__(self):\n        self.toss_results = [[]]\n        self.winner_history = []\n        self.hanis_winning_criteria  = [True,True]\n        self.guebin_winning_criteria = [False,False]\n    def __iter__(self):\n        return self \n    def toss(self):\n        rslt =  np.random.rand() < 0.5\n        self.toss_results[-1].append(rslt)   \n    def __next__(self):\n        self.toss()\n        if self.toss_results[-1][-2:] == self.hanis_winning_criteria:\n            self.winner_history.append('하니')\n            self.toss_results[-1].append([])\n            raise StopIteration\n        elif self.toss_results[-1][-2:] == self.guebin_winning_criteria: \n            self.winner_history.append('규빈')\n            self.toss_results[-1].append([])\n            raise StopIteration\n    def summary(self):\n        return {'하니':self.winner_history.count('하니'), \n                '규빈':self.winner_history.count('규빈')}\n\n\ncoin_iterator = TossCoin()\n\n\nfor i in range(1000):\n    for _ in coin_iterator:\n        pass\n\n\ncoin_iterator.summary()\n\n{'하니': 525, '규빈': 475}\n\n\n(4) 앞면과 뒷면이 나올 확률이 각각 1/2인 동전을 생각하자. 하니와 규빈은 이 동전을 연속으로 던져서 아래와 같은 룰을 정하여 내기를 하였다.\n\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,앞면) 이 나오면 하니의 승리\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,뒷면) 이 나오면 규빈의 승리\n\n이 내기는 하니가 유리한가? 규빈이 유리한가? 시뮬레이션을 통해 검증하라.\n\ncoin_iterator = TossCoin()\n\n\ncoin_iterator.hanis_winning_criteria = [False,True]\n\n\nfor i in range(1000):\n    for _ in coin_iterator:\n        pass\n\n\ncoin_iterator.summary()\n\n{'하니': 484, '규빈': 516}\n\n\n(5) 앞면과 뒷면이 나올 확률이 각각 1/2인 동전을 생각하자. 하니와 규빈은 이 동전을 연속으로 던져서 아래와 같은 룰을 정하여 내기를 하였다.\n\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (앞면,뒷면) 이 나오면 하니의 승리\n동전을 연속으로 반복하여 던진다. 최근 2회의 결과가 (뒷면,뒷면) 이 나오면 규빈의 승리\n\n이 내기는 하니가 유리한가? 규빈이 유리한가? 시뮬레이션을 통해 검증하라.\n\ncoin_iterator = TossCoin()\n\n\ncoin_iterator.hanis_winning_criteria = [True,False]\n\n\nfor i in range(1000):\n    for _ in coin_iterator:\n        pass\n\n\ncoin_iterator.summary()\n\n{'하니': 761, '규빈': 239}\n\n\n\ncoin_iterator\n\n<__main__.TossCoin at 0x7f2a3cfb2610>"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "",
    "text": "ref: https://www.jstatsoft.org/article/view/v012i08"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Ebayesthresh로 무엇을 할 수 있는가?",
    "text": "Ebayesthresh로 무엇을 할 수 있는가?\n아래와 같은 상황을 가정하자.\n\\[X_i = \\mu_i +\\epsilon_i.\\]\n여기에서 아래를 가정한다.\n\n\\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\)\neach \\(\\mu_i\\) is zero with probability \\((1−w)\\), while, with probability \\(w\\), \\(\\mu_i\\) is drawn from a symmetric heavy-tailed density \\(\\gamma\\).\n\n일반적으로 \\(w\\), 즉 \\(\\mu_i\\)가 0이 아닐 확률은 매우 작은값으로 설정된다. 따라서 위와 같은 구조로 \\(\\epsilon_i\\)와 \\(\\mu_i\\)를 생성하면 아래와 같이 된다.\n\n\\(\\epsilon_i\\): 절대값이 작은 신호들이 dense하게 있음.\n\\(\\mu_i\\): 절대값이 큰 신호들이 sparse하게 있음. (sparse한 이유는 \\(w\\)가 작으므로)\n\n따라서 \\(X_i\\)의 모양은 아래의 그림의 왼쪽과 같다.\n\n이 논문의 목표는 왼쪽의 그림 \\(X_i= \\mu_i +\\epsilon_i\\)로부터 오른쪽의 그림 \\(\\hat{\\mu}_i\\)을 구하는 것이다. 즉 작은 절대값의 노이즈 \\(\\epsilon_i\\)에서 큰 절대값의 신호 \\(\\mu_i\\)를 골라내는 일을 목표로 한다. 저자들은 이러한 작업을 “건초더미에서 바늘찾기”라는 말로 비유하였다. 이러한 “건초더미에서 바늘찾기”는 여러 분야에 응용될 수 있다. 구체적으로는 천문학, 이미지프로세싱, 데이터마이닝, 모형선택등에 사용될 수 있다고 한다. 언급한 분야에 대한 자세한 discussion은 Johnstone and Silverman (2004)에서 찾을 수 있다. 또한 “건초더미에서 바늘찾기”는 위에서 언급한 분야 이외에 퓨리에, 웨이블릿 혹은 다른 dictionaries에 의한 함수추정문제를 해결할 수 있다. 이는 퓨리에나 웨이블릿변환과 같은 multiscale trasnform이 원래 신호를 sparese한 구조로 바꾸기 때문이다. 즉 퓨리에변환 웨이블릿변환으로 underlying function을 추정할 수 있다는 의미이다. 우리는 이러한 접근법에 좀 더 초점을 맞추도록 하겠다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "간단한 사용법",
    "text": "간단한 사용법\nR을 이용하여 Ebayesthresh를 사용하는 간단한 방법을 살펴보도록 하자. 논문에 표현된 그림1을 재현하여 보자.\n\nlibrary(EbayesThresh)\n\n\nset.seed(1)\nx <- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\nplot(x,type='l',lwd=0.2)\n\n\n\n\n위와 같은 자료 \\(X_i\\)를 관측하였다고 가정하자. 이 신호에는 “건초(\\(\\epsilon_i\\))”더미에 25개의 “바늘(\\(\\mu_i\\))”이 섞여있다. 여기에서 “바늘”만 골라내는 코드는 아래와 같이 작성할 수 있다.\n\nmuhat <- ebayesthresh(x, sdev=1)\n\n결과를 시각화하면 아래와 같다.\n\nplot(x,type='l',lwd=0.2)\nlines(muhat,col=2,lwd=2)"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "arguments",
    "text": "arguments\n일반적으로 ebayesthresh 함수를 사용하는 방법은 아래와 같다.\n\nmuhat <- ebayesthresh(\n    x,\n    prior = \"laplace\", \n    a = 0.5, \n    bayesfac = FALSE, \n    sdev = NA, \n    verbose = FALSE, \n    threshrule = \"median\"\n)\n\nprior, a: \\(\\mu_i\\)의 density. 보통 \\(\\frac{1}{2}a \\exp(-a|u|)\\)라고 가정한다. parameter \\(a\\)는 Section 2.1에서 자시해 나옴.\nbayesfac, threshrule: Section 2.2, 2.3에 자세히 나온다.\nsdev: \\(\\epsilon_i\\)의 sd를 의미한다. 이 값을 알고 있다면 설정하면 되지만 보통은 이 값을 모른다고 가정한다. \\(\\epsilon_i\\)의 sd를 모르는 경우는 observed data로 부터 추정하는데 보통 \\({\\tt median}(|X_i|)\\)로 추정한다.\n\\(\\epsilon_i\\)의 sd를 \\({\\tt median}(|X_i|)\\)로 추정하는 motivation을 이해하는 것이 중요하다. 이는 sparse assumption of \\(\\mu_i\\)에서 시작한다. 신호 \\(\\mu_i\\)가 합리적인 수준에서 sparse하다면 median absolute value of \\(X_i\\)는 \\(\\mu_i\\)의 값들과 상관이 없을 것이다. 하지만 당연히 신호가 sparse하지 않다면 이러한 방식으로 sdev를 추정하는 것은 매우 조심스럽게 수행되어야 할 것이다.\n\nn <- 1000\nx <- rnorm(n) + sample(c(runif(25,-7,7), rep(0,n-25)))\nprint(sd(x))\nprint(median(abs(x)))\n\n[1] 1.117016\n[1] 0.6787613\n\n\n\n실제로는 잘 추론하지 못하는 것 같다?"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "원리",
    "text": "원리\n어떻게 \\(\\hat{\\mu}_i\\)를 추정할 수 있을까? 가장 간단한 방법은 thresholding이다.\n많은 실제예제에서 \\(\\mu_i\\)는 어떤 의미에서 (in some sense) sparse하다고 여길 수 있다. EbayesThresh 패키지는 이처럼 \\(\\mu_i\\)가 sparse하다는 구조 (혹은 가정)을 이용하여 \\(\\mu_i\\)를 적절하게 추정한다.\nSparsity를 이용하는 자연스러운 방법은 threshoding이다: 여기에서 threshold의 값 \\(t\\)를 너무 크게 잡으면 신호를 잡음으로 잘못 판단할 것이고 \\(t\\)의 값이 너무 작다면 잡음을 신호로 잘못 판단할 수 있다. 따라서 \\(t\\)의 선택은 이 양쪽 기준사이의 tradeoff가 있는데 EbayesThresh는 이러한 tradeoff를 자동으로 조정하는 효과가 있다.\n\n\\(\\mu_i\\)는 \\(w\\)의 확률로 0 이며 \\((1-w)\\)의 확률로 0이 아니다. \\(\\mu_i\\)가 0이 아닐경우에는 symmetric heavy-tailed density \\(\\gamma\\)에서 추출된다고 가정한다. 여기에서 prior에 대한 key parameter인 \\(w\\)는 데이터로부터 자동으로 추정된다. (marginal maximum likelihood 를 이용한다) 그리고 추정된 \\(w\\)는 Bayesian model로 다시 대입된다.\n\\(w\\)가 추정되면 Bayesian model은 thresholding procedure를 수행할 수 있다. 왜냐하면 \\(w\\)를 추정하면 \\(t(w)\\)를 선택한다는 말과 같은말이기 때문이다.\n\nargument"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "The Bayesian model",
    "text": "The Bayesian model\n\\[X_i \\sim N(\\mu_i,1)\\]\n\\(f_{\\text{prior}}(\\mu)=(1-w)\\delta_0(\\mu)+w \\gamma_a(\\mu), \\quad \\gamma_a(\\mu)=\\frac{1}{2}a\\exp(-a|\\mu|)\\)\n여기에서 \\(\\gamma_a(\\mu)\\)는 하나의 예시일 뿐이다. Ebayesthresh에 디폴트로 설정된 prior=\"laplace\"를 셋팅하면 \\(\\gamma_a(\\mu)\\)가 사용된다. \\(\\gamma\\)의 선택은 tail이 polynomial rates로 줄어드는 어떠한 분포를 사용해도 무방하다. 저자들은 quasi-Cauchy분포를 제안하였는데 이는 Johnstone and Sliverman이 만든 theoretical assumption을 만족하는 분포중 가장 꼬리가 두꺼운 분포이다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Thresholding rules",
    "text": "Thresholding rules\n모수 \\(\\mu\\)는 사전분포(prior distribution)를 가진다고 가정하고 \\(X \\sim N(\\mu,1)\\)이라고 가정하자. 이 경우 \\(X=x\\)가 given되었을 경우 \\(\\mu\\)의 사후분포(posterior distribution)를 구할 수 있다. (자세한 내용은 Section 6을 참고해야함) 사후분포의 중앙값을 \\(\\hat{\\mu}(x;w)\\)라고 하자. (사후분포의 중앙값이 \\(w\\)에 영향받는 이유는 사전분포가 \\(w\\)에 depend하기 때문이다. 여기에서 \\(w\\)는 marginal MLE로 적절히 추론한다고 가정한다)\n\\(X_i\\)는 독립이라고 가정한다. 여기에서 \\(X_i\\)가 독립이 아니라면 약간의 정보손실이 있을 수 있다. 하지만 \\(X_i\\) 사이에 너무 많은 dependency가 존재하는 경우가 아니라면 Ebayesthresh는 어느정도 합리적인 결과를 제공한다.\n만약에 bayesfac=TRUE를 사용하면 \\(\\mu\\)의 사후분포의 중앙값 대신에 Bayes factor threshold 를 쓸 수도 있다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Choosing the threshold",
    "text": "Choosing the threshold\n\\(X_i\\)의 marginal density는\n\\((1-w)\\phi(x) +w(\\gamma \\star \\phi)(x)\\)\n\\(l(w) = \\sum_{i=1}^{n}\\log \\big\\{(1-w)\\phi(X_i)+wg(X_i) \\big\\}\\)\n와 같이 정의가능하다. 단, 여기에서 \\(g:= \\gamma\\star \\phi\\) 이다.\n이제 우리는 아래의 식을 풀면된다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]\n여기에서 \\(\\sqrt{2\\log n}\\)은 흔히 말하는 universal threshold 이다.\n만약에 \\(w\\)이외에 \\(a\\)도 추정해야 한다면 아래와 같이 추정할 수 있다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html",
    "title": "[Python] Numpy",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#선언",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#선언",
    "title": "[Python] Numpy",
    "section": "선언",
    "text": "선언\n\na=np.array([1,2,3]) # list를 선언하고 np.array화 \nl=[1,2,3]"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#기본연산-브로드캐스팅",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#기본연산-브로드캐스팅",
    "title": "[Python] Numpy",
    "section": "기본연산 브로드캐스팅",
    "text": "기본연산 브로드캐스팅\n\na+1\n\narray([2, 3, 4])\n\n\n\nl+1\n\nTypeError: can only concatenate list (not \"int\") to list\n\n\n\na*2\n\n\nl*2\n\n\na/2\n\narray([0.5, 1. , 1.5])\n\n\n\nl/2\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'\n\n\n\na-2\n\narray([-1,  0,  1])\n\n\n\nl-2\n\nTypeError: unsupported operand type(s) for -: 'list' and 'int'\n\n\n\na**2\n\narray([1, 4, 9])\n\n\n\nl**2\n\nTypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n\n\n\na%2 # %2 = 2로 나눈 나머지를 리턴\n\narray([1, 0, 1])\n\n\n\nl%2\n\nTypeError: unsupported operand type(s) for %: 'list' and 'int'"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#기타수학연산지원",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#기타수학연산지원",
    "title": "[Python] Numpy",
    "section": "기타수학연산지원",
    "text": "기타수학연산지원\n\nnp.sqrt(a), np.sqrt(l)\n\n(array([1.        , 1.41421356, 1.73205081]),\n array([1.        , 1.41421356, 1.73205081]))\n\n\n\nnp.log(a), np.log(l)\n\n(array([0.        , 0.69314718, 1.09861229]),\n array([0.        , 0.69314718, 1.09861229]))\n\n\n\nnp.exp(a), np.exp(l)\n\n(array([ 2.71828183,  7.3890561 , 20.08553692]),\n array([ 2.71828183,  7.3890561 , 20.08553692]))\n\n\n\nnp.sin(a), np.sin(l)\n\n(array([0.84147098, 0.90929743, 0.14112001]),\n array([0.84147098, 0.90929743, 0.14112001]))"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-1차원",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-1차원",
    "title": "[Python] Numpy",
    "section": "인덱싱 1차원",
    "text": "인덱싱 1차원\n- 선언\n\nl=[11,22,33,44,55,66]\na=np.array(l)\n\n- 인덱스로 접근\n\nl[0],l[1],l[2],l[3],l[4],l[5]\n\n(11, 22, 33, 44, 55, 66)\n\n\n\na[0],a[1],a[2],a[3],a[4],a[5]\n\n(11, 22, 33, 44, 55, 66)\n\n\n- :이용 (슬라이싱)\n\nl[2:4] ## index=2에서 출발해서 index=4에서 멈춘다. (4는 포함하지 않음) \n\n[33, 44]\n\n\n\na[2:4]\n\narray([33, 44])\n\n\n- 정수배열에 의한 인덱싱\n\na\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\na[[0,2,4]]\n\narray([11, 33, 55])\n\n\n- 부울값에 의한 인덱싱\n\na\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\na[[True,False,True,False,True,False]]\n\narray([11, 33, 55])\n\n\n응용하면?\n\na < 33\n\narray([ True,  True, False, False, False, False])\n\n\n\na[a<33]\n\narray([11, 22])\n\n\n리스트는 불가능\n\nl\n\n[11, 22, 33, 44, 55, 66]\n\n\n\nl<33 ## 여기서부터 에러남 \n\nTypeError: '<' not supported between instances of 'list' and 'int'"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-2차원",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-2차원",
    "title": "[Python] Numpy",
    "section": "인덱싱 2차원",
    "text": "인덱싱 2차원\n- 중첩리스트와 2차원 np.array 선언\n\nA = [[1,2,3,4],[-1,-2,-3,-4],[5,6,7,8],[-5,-6,-7,-8]]\nA2 = np.array(A)\n\n\nA\n\n[[1, 2, 3, 4], [-1, -2, -3, -4], [5, 6, 7, 8], [-5, -6, -7, -8]]\n\n\n\nA2\n\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4],\n       [ 5,  6,  7,  8],\n       [-5, -6, -7, -8]])\n\n\n- A에서 원소인덱싱\n\nA[0][0] # (1,1)의 원소 \n\n1\n\n\n\nA[1][2] # (2,3)의 원소 \n\n-3\n\n\n\nA[-1][0] # (4,1)의 원소 \n\n-5\n\n\n- A2에서 원소인덱싱\n\nA2[0][0] # (1,1)의 원소 \n\n1\n\n\n\nA2[1][2] # (2,3)의 원소 \n\n-3\n\n\n\nA2[-1][0] # (4,1)의 원소 \n\n-5\n\n\n- 이렇게 해도된다? (넘파이에서 제시하는 신기술, R에서는 기본탑재기능, 이중 list는 불가능)\n\nA2[0,0] # (1,1)의 원소 \n\n1\n\n\n\nA2[1,2] # (2,3)의 원소 \n\n-3\n\n\n\nA2[-1,0] # (4,1)의 원소\n\n-5\n\n\n- 슬라이싱!\n\nA2\n\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4],\n       [ 5,  6,  7,  8],\n       [-5, -6, -7, -8]])\n\n\n\nA2[0,:] ## 1행\n\narray([1, 2, 3, 4])\n\n\n\nA2[0] ## 1행\n\narray([1, 2, 3, 4])\n\n\n\nA2[[0,2],:] # 1,3행 \n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nA2[[0,2]] # 1,3 행 \n\narray([[1, 2, 3, 4],\n       [5, 6, 7, 8]])\n\n\n\nA2[:,0] # 1열 \n\narray([ 1, -1,  5, -5])\n\n\n\nA2[:,[0,2]] # 1,3열 \n\narray([[ 1,  3],\n       [-1, -3],\n       [ 5,  7],\n       [-5, -7]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열의-선언",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열의-선언",
    "title": "[Python] Numpy",
    "section": "1차원 배열의 선언",
    "text": "1차원 배열의 선언\n- 리스트나 튜플을 선언하고 형변환\n\nnp.array((1,2,3))\n\narray([1, 2, 3])\n\n\n\nnp.array([1,2,3])\n\narray([1, 2, 3])\n\n\n- range()를 이용하여 만들고 형변환\n\nnp.array(range(12))\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n\n\n- np.zeros, np.ones\n\nnp.zeros(3)\n\narray([0., 0., 0.])\n\n\n\nnp.ones(3)\n\narray([1., 1., 1.])\n\n\n- np.linspace\n\nnp.linspace(0,1,12)\n\narray([0.        , 0.09090909, 0.18181818, 0.27272727, 0.36363636,\n       0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182,\n       0.90909091, 1.        ])\n\n\n- np.arange\n\nnp.arange(5)\n\narray([0, 1, 2, 3, 4])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#reshape",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#reshape",
    "title": "[Python] Numpy",
    "section": "reshape",
    "text": "reshape\n- reshape: ndarray의 특수한 기능\n\nl=[11,22,33,44,55,66]\na=np.array(l)\n\n\na ## 길이가 6인 벡터\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\na.reshape(2,3)\n\narray([[11, 22, 33],\n       [44, 55, 66]])\n\n\n\na ## reshape은 a자체를 변화시키진 않음\n\narray([11, 22, 33, 44, 55, 66])\n\n\n\nb = a.reshape(3,2)\nb\n\narray([[11, 22],\n       [33, 44],\n       [55, 66]])\n\n\n- 다시 b를 a처럼 바꾸고싶다면?\n\nb\n\narray([[11, 22],\n       [33, 44],\n       [55, 66]])\n\n\n\nb.reshape(6)\n\narray([11, 22, 33, 44, 55, 66])\n\n\n- reshape with -1\n\na=np.array(range(24))\na\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.reshape(2,-1)\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(3,-1)\n\narray([[ 0,  1,  2,  3,  4,  5,  6,  7],\n       [ 8,  9, 10, 11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(4,-1)\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n\na.reshape(6,-1)\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23]])\n\n\n\na.reshape(8,-1)\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14],\n       [15, 16, 17],\n       [18, 19, 20],\n       [21, 22, 23]])\n\n\n\na.reshape(12,-1)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [ 6,  7],\n       [ 8,  9],\n       [10, 11],\n       [12, 13],\n       [14, 15],\n       [16, 17],\n       [18, 19],\n       [20, 21],\n       [22, 23]])\n\n\n\na=a.reshape(12,-1)\na\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [ 6,  7],\n       [ 8,  9],\n       [10, 11],\n       [12, 13],\n       [14, 15],\n       [16, 17],\n       [18, 19],\n       [20, 21],\n       [22, 23]])\n\n\n\na.reshape(-1)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열의-선언-1",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열의-선언-1",
    "title": "[Python] Numpy",
    "section": "2차원 배열의 선언",
    "text": "2차원 배열의 선언\n\nnp.zeros((3,3)) # np.zeros([3,3])\n\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\nnp.ones((3,3))\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\nnp.eye(3)\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nnp.diag([1,2,3]) # np.diag((1,2,3))\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#랜덤으로-생성",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#랜덤으로-생성",
    "title": "[Python] Numpy",
    "section": "랜덤으로 생성",
    "text": "랜덤으로 생성\n\nnp.random.randn(10) # 표준정규분포에서 10개 뽑음\n\narray([ 1.08324716,  0.62817097,  0.12847717, -0.04525161,  0.96142135,\n       -1.21223986, -0.17343561,  0.5136161 , -1.26171776,  0.80209933])\n\n\n\nnp.random.rand(10) # 0~1사이에서 10개 뽑음 \n\narray([2.51475291e-05, 4.73976026e-01, 7.39149130e-01, 7.14419133e-01,\n       6.97771860e-01, 3.97994936e-01, 6.77282056e-01, 7.05790725e-01,\n       2.88589932e-01, 7.57068891e-01])\n\n\n\nnp.random.randn(4).reshape(2,2)\n\narray([[ 0.32819241,  0.93190348],\n       [-0.06517115, -0.83736631]])\n\n\n\nnp.random.randn(6).reshape(3,2)\n\narray([[-1.23749937, -0.58528095],\n       [ 1.14685968, -1.30696566],\n       [ 1.38417905, -0.94795928]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#행렬",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#행렬",
    "title": "[Python] Numpy",
    "section": "행렬",
    "text": "행렬\n\nA= np.array(range(4)).reshape(2,2)\nA\n\narray([[0, 1],\n       [2, 3]])\n\n\n\nA.T # 전치행렬 \n\narray([[0, 2],\n       [1, 3]])\n\n\n\nnp.linalg.inv(A) # 역행렬 \n\narray([[-1.5,  0.5],\n       [ 1. ,  0. ]])\n\n\n\nA@np.linalg.inv(A) # 행렬곱 \n\narray([[1., 0.],\n       [0., 1.]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열과-연립1차방정식",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원-배열과-연립1차방정식",
    "title": "[Python] Numpy",
    "section": "2차원 배열과 연립1차방정식",
    "text": "2차원 배열과 연립1차방정식\n- 아래의 연립방정식 고려\n\\(\\begin{cases} y + z + w= 3 \\\\ x + z + w= 3 \\\\ x + y + w= 3 \\\\ x + y + z= 3 \\\\ \\end{cases}\\)\n- 행렬표현은?\n\\(\\begin{bmatrix} 0 & 1 & 1 & 1 \\\\ 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 0 \\\\ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\\\ 3 \\end{bmatrix}\\)\n- 풀이\n\nA = np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])\nA\n\narray([[0, 1, 1, 1],\n       [1, 0, 1, 1],\n       [1, 1, 0, 1],\n       [1, 1, 1, 0]])\n\n\n\nb=np.array([3,3,3,3]).reshape(4,1)\nb\n\narray([[3],\n       [3],\n       [3],\n       [3]])\n\n\n\nnp.linalg.inv(A) @ b\n\narray([[1.],\n       [1.],\n       [1.],\n       [1.]])\n\n\n- 다른풀이\nb를 아래와 같이 만들어도 된다.\n\nb=np.array([3,3,3,3])\nb\n\narray([3, 3, 3, 3])\n\n\n\nnp.linalg.inv(A) @ b\n\narray([1., 1., 1., 1.])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#의-유연성",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#의-유연성",
    "title": "[Python] Numpy",
    "section": "@의 유연성",
    "text": "@의 유연성\n- 엄밀하게는 아래의 행렬곱이 가능하다. - (2,2) @ (2,1) => (2,1) - (1,2) @ (2,2) => (1,2)\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(2,1) \nA@b\n\narray([[ 5],\n       [11]])\n\n\n\nA.shape, b.shape, (A@b).shape\n\n((2, 2), (2, 1), (2, 1))\n\n\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(1,2) \nb@A\n\narray([[ 7, 10]])\n\n\n\nA.shape, b.shape, (b@A).shape\n\n((2, 2), (1, 2), (1, 2))\n\n\n- 당연히 아래는 성립안한다.\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(2,1) \nb@A\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)\n\n\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2]).reshape(1,2) \nA@b\n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)\n\n\n- 아래는 어떨까? 계산가능할까? \\(\\to\\) 모두 계산가능! - (2,) @ (2,2) => (2,) - (2,2) @ (2,) => (2,)\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2])\nA@b\n\narray([ 5, 11])\n\n\n\nA.shape, b.shape, (A@b).shape\n\n((2, 2), (2,), (2,))\n\n\n\nb를 마치 (2,1)처럼 본다.\n\n\nA = np.array([1,2,3,4]).reshape(2,2) \nb = np.array([1,2])\nb@A\n\narray([ 7, 10])\n\n\n\nb.shape, A.shape, (b@A).shape\n\n((2,), (2, 2), (2,))\n\n\n\nb를 마치 (1,2)처럼 보는 느낌\n\n- 아래는 어떠할까?\n\nb1=np.array([1,2,3,4])\nb2=np.array([1,2,3,4])\nb1@b2\n\n30\n\n\n\nb1.shape,b2.shape,(b1@b2).shape\n\n((4,), (4,), ())\n\n\n\n(1,4) @ (4,1) 로 생각함\n\n\nb1=np.array([1,2,3,4])\nb2=np.array([1,2,3,4])\nb2@b1\n\n30\n\n\n\nb2.shape,b1.shape,(b2@b1).shape\n\n((4,), (4,), ())\n\n\n\n(1,4) @ (4,1) 로 생각함\n\n- 즉 마치 아래와 같이 해석한다.\n\nb1=np.array([1,2,3,4]).reshape(1,4)\nb2=np.array([1,2,3,4]).reshape(4,1)\nb1@b2\n\narray([[30]])\n\n\n\nb1.shape,b2.shape,(b1@b2).shape\n\n((1, 4), (4, 1), (1, 1))\n\n\n- 때로는 (4,1)@(1,4)와 같은 계산결과를 얻고 싶을 수 있는데 이때는 차원을 명시해야함\n\nb1=np.array([1,2,3,4]).reshape(4,1)\nb2=np.array([1,2,3,4]).reshape(1,4)\nb1@b2\n\narray([[ 1,  2,  3,  4],\n       [ 2,  4,  6,  8],\n       [ 3,  6,  9, 12],\n       [ 4,  8, 12, 16]])\n\n\n\nb1.shape, b2.shape, (b1@b2).shape\n\n((4, 1), (1, 4), (4, 4))\n\n\n- 아래도 가능\n\nb1=np.array([1,2,3,4])\nb2=np.array([1,2,3,4])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#차원",
    "title": "[Python] Numpy",
    "section": "차원",
    "text": "차원\n- 아래는 모두 미묘하게 다르다.\n\na= np.array(1) # 스칼라, 0d array\na, a.shape\n\n(array(1), ())\n\n\n\na= np.array([1]) #벡터, 1d array\na, a.shape\n\n(array([1]), (1,))\n\n\n\na= np.array([[1]]) #메트릭스, 2d array\na, a.shape\n\n(array([[1]]), (1, 1))\n\n\n\na= np.array([[[1]]]) #텐서, 3d array\na, a.shape\n\n(array([[[1]]]), (1, 1, 1))"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.concatenate",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.concatenate",
    "title": "[Python] Numpy",
    "section": "np.concatenate",
    "text": "np.concatenate\n- 기본예제\n\na=np.array([1,2])\nb=-a \n\n\nnp.concatenate([a,b])\n\narray([ 1,  2, -1, -2])\n\n\n- 응용\n\na=np.array([1,2])\nb=-a\nc=2*a\n\n\nnp.concatenate([a,b,c])\n\narray([ 1,  2, -1, -2,  2,  4])\n\n\n\n여기까진 딱히 칸캐터네이트의 메리트가 없어보이죠?\n리스트였다면 a+b+c 하면 되는 기능이니까?\n\n- 2array 에 적용해보자\n\na=np.array(range(4)).reshape(2,2) \nb=-a\n\n\nnp.concatenate([a,b])\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n- 옆으로 붙일려면?\n\nnp.concatenate([a,b],axis=1)\n\narray([[ 0,  1,  0, -1],\n       [ 2,  3, -2, -3]])\n\n\n- axis=1이 뭐지? axis=0,2 등을 치면 어떻게 될까?\n\nnp.concatenate([a,b],axis=0)\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 0, -1],\n       [-2, -3]])\n\n\n\n이건 그냥 np.concatenate([a,b])와 같다.\nnp.concatenate([a,b])는 np.concatenate([a,b],axis=0) 의 생략버전이군?\n\n\nnp.concatenate([a,b],axis=2)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n이런건 없다.\n\n- axis의 의미가 뭔지 좀 궁금함. 좀더 예제를 살펴보자.\n\na=np.array(range(2*3*4)).reshape(2,3,4)\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\nb=-a\nb\n\narray([[[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=1)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\nnp.concatenate([a,b],axis=2)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\n이번에는 2까지 된다?\n\n\nnp.concatenate([a,b],axis=3)\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n3까지는 안된다?\n\n- 뭔가 나름의 방식으로 합쳐지는데 원리가 뭘까?\n(분석1) np.concatenate([a,b],axis=0)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\na.shape,b.shape, np.concatenate([a,b],axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\n첫번째차원이 바뀌었다 => 첫번째 축이 바뀌었다 => axis=0 (파이썬은 0부터 시작하니까!)\n\n(분석2) np.concatenate([a,b],axis=1)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\na.shape,b.shape, np.concatenate([a,b],axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n두번째차원이 바뀌었다 => 두번째 축이 바뀌었다 => axis=1\n\n(분석3) np.concatenate([a,b],axis=2)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\na.shape,b.shape, np.concatenate([a,b],axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n세번째차원이 바뀌었다 => 세번째 축이 바뀌었다 => axis=2\n\n(분석4) np.concatenate([a,b],axis=3)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\na.shape,b.shape, np.concatenate([a,b],axis=3).shape\n\nAxisError: axis 3 is out of bounds for array of dimension 3\n\n\n\n네번째 차원은 없다 => 네번째 축도 없다 => axis=3으로 하면 에러가 난다.\n\n(보너스) np.concatenate([a,b],axis=-1)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\nnp.concatenate([a,b],axis=-1)\n\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15],\n        [ 16,  17,  18,  19, -16, -17, -18, -19],\n        [ 20,  21,  22,  23, -20, -21, -22, -23]]])\n\n\n\na.shape,b.shape,np.concatenate([a,b],axis=-1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n\n마지막 차원이 바뀌었다 => 마지막 축이 바뀌었다 => axis= -1\n\n(보너스2) np.concatenate([a,b],axis=-2)\n\na=np.array(range(2*3*4)).reshape(2,3,4)\nb=-a\n\n\nnp.concatenate([a,b],axis=-2)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]]])\n\n\n\na.shape,b.shape,np.concatenate([a,b],axis=-2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\n마지막에서 두번째 차원이 바뀌었다 => 마지막에서 두번째 축이 바뀌었다 => axis=-2\n\n- 0차원은 축이 없으므로 concatenate를 쓸 수 없다.\n\na=np.array(1)\nb=np.array(-1) \n\n\na.shape,b.shape\n\n((), ())\n\n\n\nnp.concatenate([a,b])\n\nValueError: zero-dimensional arrays cannot be concatenated\n\n\n- 꼭 a,b가 같은 차원일 필요는 없다.\n\na=np.array(range(4)).reshape(2,2)\nb=np.array(range(2)).reshape(2,1) \n\n\nnp.concatenate([a,b],axis=1)\n\narray([[0, 1, 0],\n       [2, 3, 1]])\n\n\n\na.shape,b.shape,np.concatenate([a,b],axis=1).shape\n\n((2, 2), (2, 1), (2, 3))"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.stack",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.stack",
    "title": "[Python] Numpy",
    "section": "np.stack",
    "text": "np.stack\n- 혹시 아래가 가능할까? - (3,) 결합 (3,) => (3,2)\n\na=np.array([1,2,3])\nb=-a\n\n\na,b\n\n(array([1, 2, 3]), array([-1, -2, -3]))\n\n\n\nnp.concatenate([a,b],axis=1)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n불가능\n\n- 아래와 같이 하면 가능하다.\n\na=np.array([1,2,3])\nb=-a\n\n\nnp.concatenate([a.reshape(3,1), b.reshape(3,1)],axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n\n분석: (3) (3) => (3,1) (3,1) => (3,1) concat (3,1)\n\n- 이걸 줄여서 아래와 같이 할 수 있다.\n\nnp.stack([a, b],axis=1)\n\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3]])\n\n\n- 아래도 가능\n\nnp.stack([a,b],axis=0)\n\narray([[ 1,  2,  3],\n       [-1, -2, -3]])\n\n\n- 분석해보고 외우자\n(분석1)\n\na=np.array([1,2,3])\nb=-a\n\n\na.shape,b.shape,np.stack([a,b],axis=0).shape\n\n((3,), (3,), (2, 3))\n\n\n\n\n\n=> 첫위치에 축을 추가 (axis=0) => (1,3) (1,3) => (2,3)\n\n\n\n(분석2)\n\na=np.array([1,2,3])\nb=-a\n\n\na.shape,b.shape,np.stack([a,b],axis=1).shape\n\n((3,), (3,), (3, 2))\n\n\n\n\n\n=> 두번째 위치에 축을 추가 (axis=1) => (3,1) (3,1) => (3,2)\n\n\n\n- 고차원예제\n\na=np.arange(3*4*5).reshape(3,4,5)\nb=-a\n\n\na.shape,b.shape\n\n((3, 4, 5), (3, 4, 5))\n\n\n\nnp.stack([a,b],axis=0).shape # (3,4,5) => (1,3,4,5) // 첫위치에 축이 추가\n\n(2, 3, 4, 5)\n\n\n\nnp.stack([a,b],axis=1).shape # (3,4,5) => (3,1,4,5) // 두번쨰 위치에 축이 추가\n\n(3, 2, 4, 5)\n\n\n\nnp.stack([a,b],axis=2).shape # (3,4,5) => (3,4,1,5) // axis=2 \n\n(3, 4, 2, 5)\n\n\n\nnp.stack([a,b],axis=3).shape # (3,4,5) => (3,4,5,1) // axis=3\n\n(3, 4, 5, 2)\n\n\n\nnp.stack([a,b],axis=-1).shape # (3,4,5) => (3,4,5,1) // axis=-1 <=> axis=3 \n\n(3, 4, 5, 2)\n\n\nnp.concatenate 는 축의 총 갯수를 유지하면서 결합, np.stack은 축의 갯수를 하나 증가시키며 결합"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#sum",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#sum",
    "title": "[Python] Numpy",
    "section": "sum",
    "text": "sum\n- 1차원\n\na=np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\na.sum()\n\n6\n\n\n\na.sum(axis=0)\n\n6\n\n\n- 2차원\n\na=np.array(range(6)).reshape(2,3)\na\n\narray([[0, 1, 2],\n       [3, 4, 5]])\n\n\n\na.sum(axis=0)\n\narray([3, 5, 7])\n\n\n\na.sum(axis=1)\n\narray([ 3, 12])\n\n\n- 2차원 결과 분석\n\na.shape, a.sum(axis=0).shape \n\n((2, 3), (3,))\n\n\n\n첫번째 축이 삭제됨\n\n\na.shape, a.sum(axis=1).shape \n\n((2, 3), (2,))\n\n\n\n두번째 축이 삭제됨\n\n- 연습\n\na=np.array(range(10)).reshape(5,2)\na\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n\n(문제1) 1열의 합, 2열의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => (2,) 로 나와야 한다 (그럼 첫번째 축이 삭제되어야하네?)\n\na.sum(axis=0)\n\narray([20, 25])\n\n\n(문제2) 1행의 합, 2행의 합, …, 5행의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => (5,)로 나와야 한다 (그럼 두번째 축이 삭제되어야하네?)\n\na.sum(axis=1)\n\narray([ 1,  5,  9, 13, 17])\n\n\n(문제3) a의 모든원소의 합을 계산하고 싶다면?\n(풀이) 차원이 (5,2) => () 로 나와야 한다 (첫번째축, 두번째축 모두 삭제되어야 하네?)\n\na.sum(axis=(0,1))\n\n45\n\n\n\na.sum() # 즉 a.sum(axis=(0,1))이 디폴트값임 \n\n45"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#mean-max-min-prod",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#mean-max-min-prod",
    "title": "[Python] Numpy",
    "section": "mean, max, min, prod",
    "text": "mean, max, min, prod\n- 모두 sum과 유사한논리\n\na=np.array(range(10)).reshape(5,2)\na\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n\n\na.mean(axis=0), a.std(axis=0), a.max(axis=0), a.min(axis=0), a.prod(axis=0)\n\n(array([4., 5.]),\n array([2.82842712, 2.82842712]),\n array([8, 9]),\n array([0, 1]),\n array([  0, 945]))\n\n\n\na.mean(axis=1), a.std(axis=1), a.max(axis=1), a.min(axis=1), a.prod(axis=1)\n\n(array([0.5, 2.5, 4.5, 6.5, 8.5]),\n array([0.5, 0.5, 0.5, 0.5, 0.5]),\n array([1, 3, 5, 7, 9]),\n array([0, 2, 4, 6, 8]),\n array([ 0,  6, 20, 42, 72]))\n\n\n- 참고로 std는 분모를 n으로 나눈다.\n\na=np.array([1,2,3,4])\na.std()\n\n1.118033988749895\n\n\n\nnp.sqrt(sum((a-a.mean())**2)/4)\n\n1.118033988749895\n\n\n- 분모를 n-1로 나눌려면?\n\na=np.array([1,2,3,4])\na.std(ddof=1)\n\n1.2909944487358056\n\n\n\nnp.sqrt(sum((a-a.mean())**2)/3)\n\n1.2909944487358056"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#argmax-argmin",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#argmax-argmin",
    "title": "[Python] Numpy",
    "section": "argmax, argmin",
    "text": "argmax, argmin\n- 1차원\n\na=np.array([1,-2,3,10,4])\na\n\narray([ 1, -2,  3, 10,  4])\n\n\n\na.argmax()\n\n3\n\n\n\na.argmax(axis=0)\n\n3\n\n\n\na.argmin()\n\n1\n\n\n\na.argmin(axis=0)\n\n1\n\n\n- 2차원\n\nnp.random.seed(1)\na=np.random.randn(4*5).reshape(4,5)\na\n\narray([[ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763],\n       [-2.3015387 ,  1.74481176, -0.7612069 ,  0.3190391 , -0.24937038],\n       [ 1.46210794, -2.06014071, -0.3224172 , -0.38405435,  1.13376944],\n       [-1.09989127, -0.17242821, -0.87785842,  0.04221375,  0.58281521]])\n\n\n\na.argmin(axis=0)\n\narray([1, 2, 3, 0, 1])\n\n\n\na.argmax(axis=1)\n\narray([0, 1, 0, 4])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#cumprod-cumsum",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#cumprod-cumsum",
    "title": "[Python] Numpy",
    "section": "cumprod, cumsum",
    "text": "cumprod, cumsum\n- 1차원\n\na=np.array([1,2,3,4])\n\n\na.cumsum(), a.cumprod()\n\n(array([ 1,  3,  6, 10]), array([ 1,  2,  6, 24]))\n\n\n\na.cumsum(axis=0), a.cumprod(axis=0)\n\n(array([ 1,  3,  6, 10]), array([ 1,  2,  6, 24]))\n\n\n- 2차원\n\na=np.array(range(3*4)).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\na.cumsum(axis=0), a.cumprod(axis=0)\n\n(array([[ 0,  1,  2,  3],\n        [ 4,  6,  8, 10],\n        [12, 15, 18, 21]]),\n array([[  0,   1,   2,   3],\n        [  0,   5,  12,  21],\n        [  0,  45, 120, 231]]))\n\n\n\na.cumsum(axis=1), a.cumprod(axis=1)\n\n(array([[ 0,  1,  3,  6],\n        [ 4,  9, 15, 22],\n        [ 8, 17, 27, 38]]),\n array([[   0,    0,    0,    0],\n        [   4,   20,  120,  840],\n        [   8,   72,  720, 7920]]))"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#diff",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#diff",
    "title": "[Python] Numpy",
    "section": "diff",
    "text": "diff\n- 1차차분\n\na=np.array([1,2,4,6,7])\na\n\narray([1, 2, 4, 6, 7])\n\n\n\nnp.diff(a)\n\narray([1, 2, 2, 1])\n\n\n- 2차차분\n\nnp.diff(np.diff(a))\n\narray([ 1,  0, -1])\n\n\n\nnp.diff(a,n=2)\n\narray([ 1,  0, -1])\n\n\n- prepend, append\n\na\n\narray([1, 2, 4, 6, 7])\n\n\n\nnp.diff(a,prepend=100) # [100, a] 와 같은 리스트를 차분 \n\narray([-99,   1,   2,   2,   1])\n\n\n\nnp.diff(a,append=100) # [a, 100] 와 같은 리스트를 차분 \n\narray([ 1,  2,  2,  1, 93])\n\n\n\nnp.diff(a,prepend=a[0]) # [a[0], a] 와 같은 리스트를 차분 \n\narray([0, 1, 2, 2, 1])\n\n\n\nnp.diff(a,append=a[-1]) # [a, a[-1]] 와 같은 리스트를 차분\n\narray([1, 2, 2, 1, 0])\n\n\n- 2차원 array의 차분\n\na=np.arange(24).reshape(4,6)\na\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n\n\n\nnp.diff(a,axis=0)\n\narray([[6, 6, 6, 6, 6, 6],\n       [6, 6, 6, 6, 6, 6],\n       [6, 6, 6, 6, 6, 6]])\n\n\n\nnp.diff(a,axis=1)\n\narray([[1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#splitver1",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#splitver1",
    "title": "[Python] Numpy",
    "section": "split(ver1)",
    "text": "split(ver1)\n- 1차원\n\na=np.array([1,2,3,4,5,6])\na\n\narray([1, 2, 3, 4, 5, 6])\n\n\n\nnp.split(a,2)\n\n[array([1, 2, 3]), array([4, 5, 6])]\n\n\n\nnp.split(a,3)\n\n[array([1, 2]), array([3, 4]), array([5, 6])]\n\n\n\nnp.split(a,6)\n\n[array([1]), array([2]), array([3]), array([4]), array([5]), array([6])]\n\n\n- 2차원\n\nb=np.stack([a,a])\nb\n\narray([[1, 2, 3, 4, 5, 6],\n       [1, 2, 3, 4, 5, 6]])\n\n\n\nb.shape\n\n(2, 6)\n\n\n\nnp.split(b,2,axis=0)\n\n[array([[1, 2, 3, 4, 5, 6]]), array([[1, 2, 3, 4, 5, 6]])]\n\n\n\nnp.split(b,2,axis=1)\n\n[array([[1, 2, 3],\n        [1, 2, 3]]),\n array([[4, 5, 6],\n        [4, 5, 6]])]\n\n\n\nnp.split(b,3,axis=1)\n\n[array([[1, 2],\n        [1, 2]]),\n array([[3, 4],\n        [3, 4]]),\n array([[5, 6],\n        [5, 6]])]"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#splitver2",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#splitver2",
    "title": "[Python] Numpy",
    "section": "split(ver2)",
    "text": "split(ver2)\n- 1차원: 리스트입력\n\nnp.random.seed(43052)\na=np.random.rand(12)\na\n\narray([0.81768226, 0.04953212, 0.83868626, 0.61977707, 0.12254052,\n       0.11712779, 0.8795562 , 0.97941543, 0.90986893, 0.96667407,\n       0.59164493, 0.84014933])\n\n\n\nnp.split(a,(2,5)) # 처음2, 그다음3(=합쳐서5), 나머지 \n\n[array([0.81768226, 0.04953212]),\n array([0.83868626, 0.61977707, 0.12254052]),\n array([0.11712779, 0.8795562 , 0.97941543, 0.90986893, 0.96667407,\n        0.59164493, 0.84014933])]\n\n\n\nnp.split(a,(1,3,5)) # :1, 1:3, 3:5, 5: \n\n[array([0.81768226]),\n array([0.04953212, 0.83868626]),\n array([0.61977707, 0.12254052]),\n array([0.11712779, 0.8795562 , 0.97941543, 0.90986893, 0.96667407,\n        0.59164493, 0.84014933])]\n\n\n- 2차원: 리스트입력\n\na= np.arange(30).reshape(6,5)\na\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14],\n       [15, 16, 17, 18, 19],\n       [20, 21, 22, 23, 24],\n       [25, 26, 27, 28, 29]])\n\n\n\nnp.split(a,[3,4],axis=0) # :3, 3:4, 4: \n\n[array([[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14]]),\n array([[15, 16, 17, 18, 19]]),\n array([[20, 21, 22, 23, 24],\n        [25, 26, 27, 28, 29]])]\n\n\n\nnp.split(a,[1,3,4],axis=1) # :1, 1:3, 3:4, 4: \n\n[array([[ 0],\n        [ 5],\n        [10],\n        [15],\n        [20],\n        [25]]),\n array([[ 1,  2],\n        [ 6,  7],\n        [11, 12],\n        [16, 17],\n        [21, 22],\n        [26, 27]]),\n array([[ 3],\n        [ 8],\n        [13],\n        [18],\n        [23],\n        [28]]),\n array([[ 4],\n        [ 9],\n        [14],\n        [19],\n        [24],\n        [29]])]"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.apply_along_axis",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.apply_along_axis",
    "title": "[Python] Numpy",
    "section": "np.apply_along_axis",
    "text": "np.apply_along_axis\n\na=np.array(range(12)).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\ndef mysum(arr): \n    return sum(arr)+1 \n\n\nnp.apply_along_axis(mysum,0,a) # np.apply_along_axis(mysum,axis=0,a) 은 에러발생 순서외어야함 \n\narray([13, 16, 19, 22])\n\n\n\nnp.apply_along_axis(mysum,1,a)\n\narray([ 7, 23, 39])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.apply_over_axes",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.apply_over_axes",
    "title": "[Python] Numpy",
    "section": "np.apply_over_axes",
    "text": "np.apply_over_axes\n- 뭔가 쓰기 어렵다..\n\na=np.array(range(12)).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\ndef mysum(arr): \n    return np.sum(arr)+1 \n\n\nnp.apply_over_axes(mysum,a,[0,1])\n\nTypeError: mysum() takes 1 positional argument but 2 were given\n\n\n\nnp.apply_over_axes?\n\n\nSignature: np.apply_over_axes(func, a, axes)\nDocstring:\nApply a function repeatedly over multiple axes.\n`func` is called as `res = func(a, axis)`, where `axis` is the first\nelement of `axes`.  The result `res` of the function call must have\neither the same dimensions as `a` or one less dimension.  If `res`\nhas one less dimension than `a`, a dimension is inserted before\n`axis`.  The call to `func` is then repeated for each axis in `axes`,\nwith `res` as the first argument.\nParameters\n----------\nfunc : function\n    This function must take two arguments, `func(a, axis)`.\na : array_like\n    Input array.\naxes : array_like\n    Axes over which `func` is applied; the elements must be integers.\nReturns\n-------\napply_over_axis : ndarray\n    The output array.  The number of dimensions is the same as `a`,\n    but the shape can be different.  This depends on whether `func`\n    changes the shape of its output with respect to its input.\nSee Also\n--------\napply_along_axis :\n    Apply a function to 1-D slices of an array along the given axis.\nNotes\n-----\nThis function is equivalent to tuple axis arguments to reorderable ufuncs\nwith keepdims=True. Tuple axis arguments to ufuncs have been available since\nversion 1.7.0.\nExamples\n--------\n>>> a = np.arange(24).reshape(2,3,4)\n>>> a\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nSum over axes 0 and 2. The result has same number of dimensions\nas the original array:\n>>> np.apply_over_axes(np.sum, a, [0,2])\narray([[[ 60],\n        [ 92],\n        [124]]])\nTuple axis arguments to ufuncs are equivalent:\n>>> np.sum(a, axis=(0,2), keepdims=True)\narray([[[ 60],\n        [ 92],\n        [124]]])\nFile:      ~/anaconda3/envs/py310/lib/python3.10/site-packages/numpy/lib/shape_base.py\nType:      function"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.where-np.argwhere",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.where-np.argwhere",
    "title": "[Python] Numpy",
    "section": "np.where, np.argwhere",
    "text": "np.where, np.argwhere\n- 1차원\n\na=np.array([0,0,0,1,0])\na\n\narray([0, 0, 0, 1, 0])\n\n\n\nnp.where(a==1) # 조건을 만족하는 인덱스가 (3,) 이라는 의미\n\n(array([3]),)\n\n\n\nnp.argwhere(a==1) \n\narray([[3]])\n\n\n- 2차원\n\nnp.random.seed(43052)\na=np.random.randn(12).reshape(3,4)\na\n\narray([[ 0.38342049,  1.0841745 ,  1.14277825,  0.30789368],\n       [ 0.23778744,  0.35595116, -1.66307542, -1.38277318],\n       [-1.92684484, -1.4862163 ,  0.00692519, -0.03488725]])\n\n\n\nnp.where(a<0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 \n\n(array([1, 1, 2, 2, 2]), array([2, 3, 0, 1, 3]))\n\n\n\nnp.argwhere(a<=0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 \n\narray([[1, 2],\n       [1, 3],\n       [2, 0],\n       [2, 1],\n       [2, 3]])\n\n\n\na[np.where(a<=0)] # 조건을 만족하는 원소를 모두 출력 => 1차원 array로 나온다. \n\narray([-1.66307542, -1.38277318, -1.92684484, -1.4862163 , -0.03488725])\n\n\n\na[np.argwhere(a<=0)] # 출력불가 \n\nIndexError: index 3 is out of bounds for axis 0 with size 3\n\n\n\na[np.argwhere(a<=0)[0]] # 출력불가 \n\narray([[ 0.23778744,  0.35595116, -1.66307542, -1.38277318],\n       [-1.92684484, -1.4862163 ,  0.00692519, -0.03488725]])\n\n\n\na[np.argwhere(a<=0)[0][0],np.argwhere(a<=0)[0][1]] # 어거지로 출력가능하긴함 \n\n-1.6630754187023522\n\n\n- np.where의 특수기능\n\nnp.random.seed(43052)\na=np.random.randn(12).reshape(3,4)\na\n\narray([[ 0.38342049,  1.0841745 ,  1.14277825,  0.30789368],\n       [ 0.23778744,  0.35595116, -1.66307542, -1.38277318],\n       [-1.92684484, -1.4862163 ,  0.00692519, -0.03488725]])\n\n\n\nnp.where(a<0,0,a) # a<0을 체크 => 조건에 맞으면 0 => 조건에 안맞으면 a \n\narray([[0.38342049, 1.0841745 , 1.14277825, 0.30789368],\n       [0.23778744, 0.35595116, 0.        , 0.        ],\n       [0.        , 0.        , 0.00692519, 0.        ]])\n\n\n\nnp.where(a<0,0,1) # a<0을 체크 => 조건에 맞으면 0 => 조건에 안맞으면 1\n\narray([[1, 1, 1, 1],\n       [1, 1, 0, 0],\n       [0, 0, 1, 0]])\n\n\n- 요약 - np.where: 인덱스의 좌표읽는게 가독성이 썩 좋지는 않음, 그런데 조건에 맞는 원소를 출력하거나 처리하는 (특수기능) 목적으로는 좋은 함수 - np.argwhere: 인덱스의 좌표읽는 가독성은 좋은편임. 그런데 조건에 맞는 원소를 출력하거나 처리하는 기능은 떨어짐"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-고급",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#인덱싱-고급",
    "title": "[Python] Numpy",
    "section": "인덱싱 고급",
    "text": "인덱싱 고급\n\na=np.arange(12).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n- 원래배열과 인덱싱결과를 보고 축의개수를 비교해보자.\n\na[0,:] # 인덱싱의 결과 축의 개수가 바뀐다! 2d array -> 1d array \n\narray([0, 1, 2, 3])\n\n\n\na[[0,1],:] # 이것은 축의 개수가 유지된다. 2d array -> 2d array \n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\n\n- 하나의 행, 혹은 열을 뽑을때 축의 개수를 유지하려면 아래와 같이 하면 된다.\n\na[[0],:]\n\narray([[0, 1, 2, 3]])\n\n\n\na[:,[0]]\n\narray([[0],\n       [4],\n       [8]])\n\n\n- 이것은 아래와 미묘하게 다르다는 점을 다시한번 기억하자.\n\na[0,:]\n\narray([0, 1, 2, 3])\n\n\n\na[:,0]\n\narray([0, 4, 8])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.ix_",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.ix_",
    "title": "[Python] Numpy",
    "section": "np.ix_",
    "text": "np.ix_\n- 아래의 인덱싱을 비교하자.\n\na=np.arange(12).reshape(3,4)\na\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\na[0:2,0:2]\n\narray([[0, 1],\n       [4, 5]])\n\n\n\na[[0,1],0:2]\n\narray([[0, 1],\n       [4, 5]])\n\n\n\na[0:2,[0,1]]\n\narray([[0, 1],\n       [4, 5]])\n\n\n- 언뜻생각하면 위의 결과와 아래의 결과가 동일하게 나올것 같다.\n\na[[0,1],[0,1]]\n\narray([0, 5])\n\n\n\n실제로는 [a[0,0], a[1,1]] 이 array로 나옴\n\n- 사실 np.where에서 이미 관찰하였다.\n\nnp.where(a % 5==0)\n\n(array([0, 1, 2]), array([0, 1, 2]))\n\n\n\na[np.where(a % 5==0)]\n\narray([ 0,  5, 10])\n\n\n\na[0:3,0:3] 이 출력되는것이 아니라 a[0,0], a[1,1], a[2,2] 가 출력된다.\n\n- a[[0,1],[0,1]]이 a[0:2,0:2]를 의미하게 하려면 아래와 같이 하면 된다.\n\na[np.ix_([0,1],[0,1])] # 유용해보이지만 생각보다 잘 쓰진 않더라.. \n\narray([[0, 1],\n       [4, 5]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.rand",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.rand",
    "title": "[Python] Numpy",
    "section": "np.random.rand()",
    "text": "np.random.rand()\n- 0~1사이의 난수 10개 생성\n\nnp.random.rand(10)\n\narray([0.8852783 , 0.31328711, 0.6141936 , 0.36838019, 0.08044368,\n       0.47142422, 0.43324944, 0.22441988, 0.01174913, 0.91587271])\n\n\n- 0~2사이의 난수 10개 생성\n\nnp.random.rand(10)*2 \n\narray([1.76650136, 0.65414205, 0.91517695, 1.10990737, 1.11690026,\n       1.50037557, 0.59895898, 1.71776826, 1.20931097, 0.01302267])\n\n\n- 1~2사이의 난수 10개 생성\n\nnp.random.rand(10)+1\n\narray([1.80605888, 1.06988897, 1.76763953, 1.72438164, 1.06247252,\n       1.97571034, 1.76681327, 1.12138996, 1.14946193, 1.08540348])\n\n\n- 1~3사이의 난수 10개 생성\n\nnp.random.rand(10)*2+1\n\narray([1.83316001, 2.82392447, 2.12983406, 2.70590512, 1.91501147,\n       1.52545303, 2.94666317, 2.67759399, 1.61667643, 2.80011097])\n\n\n\n(0,1) -> (0,2) -> (1,3) 사이의 난수생성"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.randn",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.randn",
    "title": "[Python] Numpy",
    "section": "np.random.randn()",
    "text": "np.random.randn()\n- N(0,1)에서 10개 추출\n\nnp.random.randn(10)  \n\narray([-0.25065063, -1.15695441, -1.29293855, -0.63070219, -0.69898838,\n        1.26693336,  0.84153043, -1.46458729, -0.16847185,  1.29054265])\n\n\n- N(1,1)에서 10개 추출\n\nnp.random.randn(10)+1 # N(1,1)에서 추출 \n\narray([ 1.36478515,  0.90052556,  0.17122775,  0.34652966,  1.07046089,\n       -1.46535548,  2.3927757 ,  0.94466987,  0.15876552,  0.62279321])\n\n\n- N(0,4)에서 10개 추출\n\nnp.random.randn(10)*2 # N(0,4)에서 추출 \n\narray([ 2.66361951,  0.34052965, -1.0283997 , -2.54081612,  3.4852053 ,\n       -0.7170628 , -0.36915341,  1.24489513,  0.90095895, -0.79682218])\n\n\n- N(3,4)에서 10개 추출\n\nnp.random.randn(10)*2+3 # N(3,4)에서 추출 \n\narray([ 0.86608514,  2.63310847,  0.15161244,  1.62661581,  4.47154709,\n        7.37582762,  0.70315947,  2.44134572,  1.99277107, -0.32804188])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.randint",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.randint",
    "title": "[Python] Numpy",
    "section": "np.random.randint()",
    "text": "np.random.randint()\n- [0,10)의 범위에서 하나의 정수생성\n\nnp.random.randint(10) # [0,10) 의 범위에서 하나의 정수 생성 \n\n8\n\n\n- [0,10)의 범위에서 20개의 난수 생성\n\nnp.random.randint(10, size=(20,)) \n\narray([5, 8, 7, 1, 4, 8, 1, 0, 4, 7, 1, 5, 3, 4, 2, 5, 8, 9, 8, 9])\n\n\n- [0,10)의 범위에서 (2,2) shape의 정수생성\n\nnp.random.randint(10, size=(2,2)) \n\narray([[1, 9],\n       [3, 3]])\n\n\n- 위와 같은코드. 즉 [0,10)의 범위에서 (2,2) shape의 정수생성\n\nnp.random.randint(low=10,size=(2,2)) # 왜 low지?\n\narray([[8, 4],\n       [0, 4]])\n\n\n- [10,20)의 범위에서 (2,2) shape의 정수생성\n\nnp.random.randint(low=10,high=20,size=(5,5)) \n\narray([[14, 11, 11, 13, 18],\n       [14, 12, 14, 14, 13],\n       [19, 10, 11, 13, 11],\n       [13, 17, 16, 10, 14],\n       [11, 12, 17, 13, 19]])\n\n\n- 의문: np.random.randint(low=10,size=(2,2))는 np.random.randint(high=10,size=(2,2))이어야 맞는거 아닌가?\n저도 그런것 같아요, 그런데 구현을 이상하게 한듯합니다\n\nnp.random.randint?\n\n\nDocstring:\nrandint(low, high=None, size=None, dtype=int)\nReturn random integers from `low` (inclusive) to `high` (exclusive).\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n.. note::\n    New code should use the ``integers`` method of a ``default_rng()``\n    instance instead; please see the :ref:`random-quick-start`.\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n    .. versionadded:: 1.11.0\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nGenerator.integers: which should be used for new code.\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\nGenerate a 1 x 3 array with 3 different upper bounds\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\nGenerate a 1 by 3 array with 3 different lower bounds\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)\nType:      builtin_function_or_method\n\n\n\n\n\nnote: Return random integers from the “discrete uniform” distribution of the specified dtype in the “half-open” interval [low, high). If high is None (the default), then results are from [0, low)."
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.choice",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#np.random.choice",
    "title": "[Python] Numpy",
    "section": "np.random.choice()",
    "text": "np.random.choice()\n- ver1\n\nnp.random.choice(5,20) # [0,5)에서 20개 뽑음, 중복허용 \n\narray([0, 0, 4, 2, 0, 0, 4, 0, 1, 4, 4, 2, 0, 1, 0, 1, 2, 2, 2, 2])\n\n\n\nnp.random.randint(5,size=(20,)) 와 동일\n\n\nnp.random.choice(5,5,replace=False) # [0,5)에서 5개 뽑음, 중복허용X => 제비뽑기 \n\narray([4, 1, 2, 0, 3])\n\n\n- ver2\n\nnp.random.choice([0,1,2,3],20) # [0,1,2,3]에서 20개 뽑음, 중복허용 \n\narray([3, 3, 0, 3, 1, 1, 2, 0, 0, 1, 0, 0, 3, 2, 2, 0, 1, 2, 1, 2])\n\n\n\nnp.random.choice([\"apple\",\"banana\",\"orange\"],20) # [\"apple\",\"banana\",\"orange\"]에서 20개 뽑음, 중복허용 \n\narray(['banana', 'orange', 'apple', 'banana', 'apple', 'orange', 'banana',\n       'apple', 'banana', 'apple', 'orange', 'banana', 'banana', 'orange',\n       'apple', 'apple', 'banana', 'banana', 'orange', 'banana'],\n      dtype='<U6')"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#통계분포",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#통계분포",
    "title": "[Python] Numpy",
    "section": "통계분포",
    "text": "통계분포\n\nnp.random.binomial(n=10,p=0.2,size=(5,)) # X1,...,X5 ~ B(10,0.2) \n\narray([4, 1, 1, 1, 0])\n\n\n\nnp.random.normal(loc=0,scale=2,size=(5,)) # X1,...,X5 ~ N(0,4) \n\narray([ 3.08322663,  1.7758872 ,  0.39141069,  2.36938406, -1.78970012])\n\n\n\nnp.random.uniform(low=2,high=4,size=(5,)) # X1,...,X5 ~ U(2,4)\n\narray([2.25126784, 2.78749105, 3.86552351, 3.12696917, 3.65084364])\n\n\n\nnp.random.poisson(lam=5,size=(5,)) # X1,...,X5 ~ Poi(5) \n\narray([6, 5, 4, 3, 3])\n\n\n\nnp.random.exponential(scale=2, size=(5,)) # X1,...,X5 ~ 평균이 2인 지수분포 \n\narray([3.93952652, 4.07955895, 1.7005346 , 0.40383429, 1.05525183])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-1-메소드-도움말-확인하기",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-1-메소드-도움말-확인하기",
    "title": "[Python] Numpy",
    "section": "note 1: 메소드 도움말 확인하기",
    "text": "note 1: 메소드 도움말 확인하기\n- 파이썬에서 함수적용하는 2가지 방식 - sum(a) - a.sum()\n\na = np.array([1,2,3,4,5])\na\n\narray([1, 2, 3, 4, 5])\n\n\n\na.sum()\n\n15\n\n\n\nnp.sum(a)\n\n15\n\n\n- a.sum()과 같이 함수를 쓰는 방식을 메소드라고 한다. np.sum()은 그냥 np폴더에 어딘가에 정의된 sum()함수\n- 넘파이: 대부분은 np.sum()등과 같은 함수를 만들고 a.sum()이라는 메소드가 np.sum(a)과 동일한 역할을 하도록 추가 설계하는 식임. \\(\\to\\) 도움말은 np.sum()에 자세히 나와있음\n\na.sum?\n\n\nDocstring:\na.sum(axis=None, dtype=None, out=None, keepdims=False, initial=0, where=True)\nReturn the sum of the array elements over the given axis.\nRefer to `numpy.sum` for full documentation.\nSee Also\n--------\nnumpy.sum : equivalent function\nType:      builtin_function_or_method\n\n\n\n\n\nnp.sum?\n\n\nSignature:\nnp.sum(\n    a,\n    axis=None,\n    dtype=None,\n    out=None,\n    keepdims=<no value>,\n    initial=<no value>,\n    where=<no value>,\n)\nDocstring:\nSum of array elements over a given axis.\nParameters\n----------\na : array_like\n    Elements to sum.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a sum is performed.  The default,\n    axis=None, will sum all of the elements of the input array.  If\n    axis is negative it counts from the last to the first axis.\n    .. versionadded:: 1.7.0\n    If axis is a tuple of ints, a sum is performed on all of the axes\n    specified in the tuple instead of a single axis or all the axes as\n    before.\ndtype : dtype, optional\n    The type of the returned array and of the accumulator in which the\n    elements are summed.  The dtype of `a` is used by default unless `a`\n    has an integer dtype of less precision than the default platform\n    integer.  In that case, if `a` is signed then the platform integer\n    is used while if `a` is unsigned then an unsigned integer of the\n    same precision as the platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output, but the type of the output\n    values will be cast if necessary.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `sum` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\ninitial : scalar, optional\n    Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n    .. versionadded:: 1.15.0\nwhere : array_like of bool, optional\n    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n    .. versionadded:: 1.17.0\nReturns\n-------\nsum_along_axis : ndarray\n    An array with the same shape as `a`, with the specified\n    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n    is returned.  If an output array is specified, a reference to\n    `out` is returned.\nSee Also\n--------\nndarray.sum : Equivalent method.\nadd.reduce : Equivalent functionality of `add`.\ncumsum : Cumulative sum of array elements.\ntrapz : Integration of array values using the composite trapezoidal rule.\nmean, average\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\nThe sum of an empty array is the neutral element 0:\n>>> np.sum([])\n0.0\nFor floating point numbers the numerical precision of sum (and\n``np.add.reduce``) is in general limited by directly adding each number\nindividually to the result causing rounding errors in every step.\nHowever, often numpy will use a  numerically better approach (partial\npairwise summation) leading to improved precision in many use-cases.\nThis improved precision is always provided when no ``axis`` is given.\nWhen ``axis`` is given, it will depend on which axis is summed.\nTechnically, to provide the best speed possible, the improved precision\nis only used when the summation is along the fast axis in memory.\nNote that the exact precision may vary depending on other parameters.\nIn contrast to NumPy, Python's ``math.fsum`` function uses a slower but\nmore precise approach to summation.\nEspecially when summing a large number of lower precision floating point\nnumbers, such as ``float32``, numerical errors can become significant.\nIn such cases it can be advisable to use `dtype=\"float64\"` to use a higher\nprecision for the output.\nExamples\n--------\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\nIf the accumulator is too small, overflow occurs:\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\nYou can also start the sum with a value other than zero:\n>>> np.sum([10], initial=5)\n15\nFile:      ~/anaconda3/envs/py310/lib/python3.10/site-packages/numpy/core/fromnumeric.py\nType:      function"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-2-hstack-vstack",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-2-hstack-vstack",
    "title": "[Python] Numpy",
    "section": "note 2: hstack, vstack",
    "text": "note 2: hstack, vstack\n- hstack, vstack 를 쓰는 사람도 있다.\n\na=np.arange(6) \nb=-a\n\n\nnp.vstack([a,b])\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 0, -1, -2, -3, -4, -5]])\n\n\n\nnp.stack([a,b],axis=0)\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [ 0, -1, -2, -3, -4, -5]])\n\n\n\nnp.hstack([a,b])\n\narray([ 0,  1,  2,  3,  4,  5,  0, -1, -2, -3, -4, -5])\n\n\n\nnp.concatenate([a,b],axis=0)\n\narray([ 0,  1,  2,  3,  4,  5,  0, -1, -2, -3, -4, -5])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-3-append",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-3-append",
    "title": "[Python] Numpy",
    "section": "note 3: append",
    "text": "note 3: append\n- 기능1: reshape(-1) + concat\n\na=np.arange(30).reshape(5,6)\nb= -np.arange(8).reshape(2,2,2)\n\n\na.shape, b.shape\n\n((5, 6), (2, 2, 2))\n\n\n\nnp.append(a,b)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  0, -1, -2, -3,\n       -4, -5, -6, -7])\n\n\n\nnp.concatenate([a.reshape(-1),b.reshape(-1)])\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  0, -1, -2, -3,\n       -4, -5, -6, -7])\n\n\n- 기능2: concat\n\na=np.arange(2*3*4).reshape(2,3,4)\nb=-a\n\n\na.shape,b.shape, np.append(a,b,axis=0).shape\n\n((2, 3, 4), (2, 3, 4), (4, 3, 4))\n\n\n\na.shape,b.shape, np.append(a,b,axis=1).shape\n\n((2, 3, 4), (2, 3, 4), (2, 6, 4))\n\n\n\na.shape,b.shape, np.append(a,b,axis=2).shape\n\n((2, 3, 4), (2, 3, 4), (2, 3, 8))\n\n\n- concat과의 차이?\n\na=np.arange(2*3*4).reshape(2,3,4)\nb=-a\nc=2*a\n\n\nnp.append(a,b,c,axis=0)\n\nTypeError: _append_dispatcher() got multiple values for argument 'axis'\n\n\n\nnp.concatenate([a,b,c],axis=0)\n\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-4-ravel-flatten",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-4-ravel-flatten",
    "title": "[Python] Numpy",
    "section": "note 4: ravel, flatten",
    "text": "note 4: ravel, flatten\n\na=np.arange(2*3*4).reshape(2,3,4)\na\n\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\n\n\n\na.reshape(-1)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.ravel()\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])\n\n\n\na.flatten()\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-5-기타-통계함수들",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-5-기타-통계함수들",
    "title": "[Python] Numpy",
    "section": "note 5: 기타 통계함수들",
    "text": "note 5: 기타 통계함수들\n- 평균, 중앙값, 표준편차, 분산,\n\na=np.random.normal(loc=0,scale=2,size=(100,))\na\n\narray([ 1.10482395, -0.86877641, -2.87354381, -1.07857919,  0.03703613,\n       -1.14739291,  1.87565381,  2.15846406,  0.49726646, -0.00810987,\n        2.53348503,  0.23407604, -0.70033057,  0.82176462, -1.52795024,\n        1.53364238,  2.06437879,  0.80876024, -0.26983189, -1.53527448,\n       -1.28588465, -0.49564791, -2.91531584, -0.18212024,  0.64680382,\n       -1.5480415 ,  1.31911602, -1.58918923, -5.21076055,  0.56745112,\n       -3.6658885 ,  0.5856616 ,  0.5748427 ,  0.56718898, -1.4937509 ,\n       -1.83666952,  0.80465047,  0.97184222,  0.24572426, -1.40343521,\n        1.48031211, -1.79697954,  2.0767591 , -0.88344071,  0.63964122,\n        1.41092588, -0.05452613, -0.29799167,  0.46962945, -2.8253509 ,\n       -1.41275472,  1.27618038, -1.41329545,  0.42832797,  3.36681894,\n       -1.84457945,  1.22956994,  0.81007832,  0.06983388,  0.80544143,\n       -0.13307524,  2.030119  , -4.06015169,  0.2349643 , -0.48281504,\n       -0.35643278, -1.18831743, -2.3565089 ,  0.66814577,  0.21763942,\n       -2.07618811,  1.80628638,  0.78098654,  1.58641403, -1.71025858,\n       -0.29232917,  0.21923226, -1.62321184, -0.44817688,  0.63777853,\n       -1.6351924 ,  2.60108463,  1.15300474, -0.50451115, -2.1568106 ,\n       -1.13993511, -3.32948725, -0.5059582 ,  0.66831111, -1.51265995,\n       -0.7356817 , -0.5188582 ,  0.73946266, -0.55988812,  0.19881875,\n       -1.8650296 , -0.62325849,  1.65669507, -0.45051465, -0.02321452])\n\n\n\nnp.mean(a)\n\n-0.23238783083421655\n\n\n\nnp.median(a)\n\n-0.09380068380510138\n\n\n\nnp.std(a)\n\n1.5287396635800397\n\n\n\nnp.var(a)\n\n2.337044959002813\n\n\n- corr matrix, cov matrix\n\nnp.random.seed(43052)\nx=np.random.randn(10000)\ny=np.random.randn(10000)*2\nz=np.random.randn(10000)*0.5\n\n\nnp.corrcoef([x,y,z]).round(2)\n\narray([[ 1.  , -0.01,  0.01],\n       [-0.01,  1.  ,  0.  ],\n       [ 0.01,  0.  ,  1.  ]])\n\n\n\nnp.cov([x,y,z]).round(2)\n\narray([[ 0.99, -0.02,  0.  ],\n       [-0.02,  4.06,  0.  ],\n       [ 0.  ,  0.  ,  0.25]])"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-6-dtype",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-6-dtype",
    "title": "[Python] Numpy",
    "section": "note 6: dtype",
    "text": "note 6: dtype\n- np.array는 항상 dtype이 있다.\n\na= np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\na.dtype\n\ndtype('int64')\n\n\n\na= np.array([1.0,2.0,3.0])\na\n\narray([1., 2., 3.])\n\n\n\na.dtype\n\ndtype('float64')\n\n\n- 같은 int라도 int16, int32, int64로 나누어진다.\n\na = np.array([1,2,3],dtype=np.int32) # 자료형을 특정하여 선언\na\n\narray([1, 2, 3], dtype=int32)\n\n\n\na.dtype\n\ndtype('int32')\n\n\n- float도 float16, float32, float64가 있다.\n\na = np.array([1,2,3],dtype=np.float32)\n\n\na\n\narray([1., 2., 3.], dtype=float32)\n\n\n- 데이터타입은 아래와 같은 방법으로 변환시킬 수 있다.\n\na = np.array([1,2,3],dtype=np.int32)\na\n\narray([1, 2, 3], dtype=int32)\n\n\n\na=a.astype(dtype=np.int64)\n\n\na.dtype\n\ndtype('int64')\n\n\n- 문자열의 경우\n\na = np.array(['a','b','c'])\na.dtype\n\ndtype('<U1')\n\n\n\na = np.array(['ab','b','c'])\na.dtype\n\ndtype('<U2')\n\n\n\na = np.array(['abc','b','c'])\na.dtype\n\ndtype('<U3')\n\n\n- 문자열 + 숫자혼합 => 문자열로 통일\n\na = np.array(['a',1])\na\n\narray(['a', '1'], dtype='<U21')\n\n\n\na = np.array(['a',1.0])\na\n\narray(['a', '1.0'], dtype='<U32')\n\n\n- 숫자를 문자열로 전환: 필요가 있을까..\n\na=np.array([1,2,3])\na\n\narray([1, 2, 3])\n\n\n\na.astype(np.str_)\n\narray(['1', '2', '3'], dtype='<U21')"
  },
  {
    "objectID": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-7-브로드캐스팅과-시간측정",
    "href": "posts/2_Studies/Python/2000-04-05-넘파이.html#note-7-브로드캐스팅과-시간측정",
    "title": "[Python] Numpy",
    "section": "note 7: 브로드캐스팅과 시간측정",
    "text": "note 7: 브로드캐스팅과 시간측정\n\nimport time \n\n\nt1=time.time()\n\n\nt2=time.time()\nt2-t1\n\n0.17961955070495605\n\n\n\nx=np.arange(10**4)\ny=np.arange(10**4)\n\n\narr1=x.reshape(10**4,1)\narr2=y.reshape(1,10**4)\n\n\ndist = (arr1-arr2)**2\n\n\ndist = np.zeros([10**4,10**4])\n\n\nfor i in range(10**4):\n    for j in range(10**4):\n        dist[i,j] = (i-j)**2"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "using LinearAlgebra, FFTW"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#cyclic-shfit-operator-bf-b",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#cyclic-shfit-operator-bf-b",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Cyclic shfit operator \\({\\bf B}\\)",
    "text": "Cyclic shfit operator \\({\\bf B}\\)\nThe matrix \\({\\bf B}\\) representing the periodic shift is\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0]\n\n5×5 Matrix{Int64}:\n 0  0  0  0  1\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n\n\nThis matrix is the cyclic shift.\nnote: \\({\\bf B}\\) is orthogonal matrix.\n\nB'B\n\n5×5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n(ex1) Define \\({\\bf s}\\) as\n\ns = [1,2,3,4,5]\ns\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nObserve that\n\nB*s\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\n\nB^3*s\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 2\n\n\nThus we can interprete the matrix \\({\\bf B}\\) as cyclic shift operator such that\n\\[\n{\\bf B}s_n =s_{n-1}\n\\]\nfor \\(n=1,\\dots, N-1\\) and \\({\\bf B}s_0 =s_N\\).\nnote: \\({\\bf B}\\)는 시계열에서 다루는 backshift operator 와 비슷함."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\nThe matrix \\({\\bf B}\\) can be expressed as\n\\({\\bf B}={\\bf DFT}^\\ast \\cdot {\\bf \\Lambda} \\cdot {\\bf DFT}\\)\nwhere \\({\\bf DFT}\\) is unitary and symmetric matrix and \\(\\bf \\Lambda\\) is diagonal matrix.\n\nλ, Ψ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5×5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  …  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB ≈ Ψ * Diagonal(λ) * Ψ'\n\ntrue\n\n\nDefine \\({\\boldsymbol \\Psi}^\\ast={\\bf DFT}\\).\n\nDFT = Ψ'\n\n5×5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  …  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\nNote that the eigenvalues are not ordered in julia.\n\nλ[5], exp(-im* 2π/5 * 0)\n\n(0.9999999999999998 + 0.0im, 1.0 - 0.0im)\n\n\n\nλ[3], exp(-im* 2π/5 * 1)\n\n(0.30901699437494734 - 0.9510565162951536im, 0.30901699437494745 - 0.9510565162951535im)\n\n\n\nλ[1], exp(-im* 2π/5 * 2)\n\n(-0.8090169943749472 - 0.5877852522924725im, -0.8090169943749473 - 0.5877852522924732im)\n\n\n\nλ[2], exp(-im* 2π/5 * 3)\n\n(-0.8090169943749472 + 0.5877852522924725im, -0.8090169943749475 + 0.587785252292473im)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#spectral-components-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#spectral-components-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral components and Frequencies",
    "text": "Spectral components and Frequencies\nWe remark:\n(1) Spectral components: For \\(k = 0,1,2,\\dots, N-1\\), the \\(k\\)-th column of \\({\\bf DFT}^\\ast\\) is defined by\n\\[\\Psi_k:=\\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 \\\\ e^{j\\frac{2\\pi}{N}k} \\\\ e^{j\\frac{2\\pi}{N}2k} \\\\ e^{j\\frac{2\\pi}{N}3k} \\\\  \\dots \\\\ e^{j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}.\\]\nNote that \\(\\Psi_k\\) can be also interpreted as \\(\\ell\\)-th eigenvector of \\({\\bf A}\\) correspoding \\(\\lambda_\\ell = e^{-j\\frac{2\\pi}{N}k}\\). Those eigenvectors\n\\[\\big\\{{\\bf 1},\\Psi_1,\\Psi_2, \\dots, \\Psi_{N-1}\\big\\}\\]\nform a complete orthonomal basis of \\(\\mathbb{C}^N\\). These vectors are called spectral components.\n(2) Frequencies: The diagonal entries of \\({\\bf \\Lambda}\\) are the eigenvalues of the time shift \\({\\bf B}\\). In Physics and in operator theory, these eigenvalues are the frequencies of the signal. In DSP it is more common to call frequencies\n\\[\\Omega_k=\\frac{-1}{2\\pi j}\\ln\\lambda_k=\\frac{-1}{2\\pi j}\\ln e^{-j \\frac{2\\pi}{N}k}=\\frac{k}{N}, \\quad k=0,1,2,\\dots,N-1.\\]\n\nThe \\(N\\) (time) frequencies \\(\\Omega_k\\) are all distinct, positive, equally spaced, and increasing from \\(0\\) to \\(\\frac{N-1}{N}\\). The spectral components are the complex exponential sinusiodal functions. For example, corresponding to the zero frequency is the DC spectral component (a vector whose entries are constant and all equal to \\(\\frac{1}{\\sqrt{N}}\\))."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft-1",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft-1",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\n일반적으로 우리가 알고있는 DFT1는 아래와 같다. (이 그림은 위키피디아에서 캡쳐한 것이다)\n\n\n\n그림1: 위키에서 긁어온 DFT의 정의\n\n\n즉 DFT는 임의의 신호 \\(\\{{\\bf x}_n\\}:=x_0,x_1,\\dots,x_{N-1}\\)를 적당한 규칙2에 따라서 \\(\\{{\\bf X}_k\\}:=X_0,X_1,\\dots,X_{N-1}\\)로 바꾸는 변환을 이라고 이해할 수 있다. 이때 사용되는 적당한 규칙은 구체적으로 아래의 수식을 의미한다.\n\\[X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\]\n그런데 매트릭스를 활용하면 위의 수식을 아래와 같이 표현할 수 있다.\n\\[\\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\dots \\\\ X_{N-1} \\end{bmatrix}\n=\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\dots \\\\ x_{N-1} \\end{bmatrix}\\]\n편의상 \\({\\bf X}\\)와 \\({\\bf x}\\)를 \\(N \\times 1\\) col-vec이라고 생각하고 DFT를 아래와 같은 matrix로 정의하자.\n\\[{\\bf DFT} = \\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n그러면\n\\[{\\bf X} = {\\bf DFT} \\cdot {\\bf x}\\]\n와 같이 표현할 수 있고 \\({\\bf x}\\)에서 \\({\\bf X}\\)로 바꾸는 과정을 단순히 \\({\\bf DFT}\\)행렬을 \\({\\bf x}\\)의 왼쪽에 곱하는 과정으로 이해할 수 있다.\n(참고) 사실 아래와 같이 \\({\\bf DFT}\\)를 정의하는 버전도 있다. (둘이 혼용해서 쓰인다)\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\n예제1 아래는 위키에서 긁어온 예제이다. 이 예제를 따라가보자.\n\n\n\n그림2: 위키에서 긁어온 예제이미지\n\n\n예제를 풀기위해서 우선 아래와 같은 벡터를 선언하다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n(풀이1)\n\\(4\\times 4\\)의 크기를 가지는 DFT행렬을 선언한다.\n(step1) 아래의 매트릭스 생성\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4×4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n(step2) _DFT의 각 원소에 함수 \\(f: x \\to \\exp(-i\\frac{2\\pi}{4}x)\\)를 취함\n\nf = x -> exp(-im * (2π/4) * x)\nDFT = _DFT .|> f\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n이제 \\({\\bf X}\\)를 구하면 아래와 같다.\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n위키의 답이 잘 나옴\n(풀이2)\n참고로 아래와 같이 패키지를 이용하여 구할 수도 있다.\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#inverse-dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#inverse-dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Inverse DFT",
    "text": "Inverse DFT\n앞으로는 \\({\\bf DFT}\\)를 아래와 같이 정의하자.\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\\({\\bf DFT}\\)행렬에는 몇 가지 특징이 있다.\n특징1: 유니터리행렬이다. 즉 \\({\\bf DFT}^\\ast \\cdot {\\bf DFT} = {\\bf DFT}^\\ast \\cdot{\\bf DFT} = {\\bf I}\\) 이다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nf = x -> exp(-im * (2π/4) * x)\nDFT = _DFT .|> f\nDFT # 아까의 예제의 DFT!\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT = (1/√4)*DFT # 새로운 DFT의 정의 \nDFT'DFT .|> round # 유니터리행렬임을 확인!\n\n4×4 Matrix{ComplexF64}:\n  1.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   1.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   1.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   1.0+0.0im\n\n\n특징2: \\({\\bf DFT}\\)는 대칭행렬이다. 따라서 이 행렬의 켤레전치는 DFT의 각 원소에서 단순히 \\(i=\\sqrt{-1}\\) 대신에 \\(-i\\) 를 넣은 것과 같다.\n특징1-2를 조합하면 아래와 같이 \\({\\bf DFT}\\)에서 \\(i\\) 대신에 \\(-i\\)를 넣은 행렬이 변환 DFT를 취소시킬 수 있음을 이해할 수 있다. 3\n\\[\\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 1} & e^{i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 2} & e^{i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n행렬 \\({\\bf DFT}\\)를 discrete Fourier transform으로 생각했듯이 위의 행렬을 inverse discrete Fourier transform으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft의-또-다른-정의",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#dft의-또-다른-정의",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT의 또 다른 정의",
    "text": "DFT의 또 다른 정의\n이번에는 \\({\\bf DFT}\\)에 대한 다른 정의를 생각해보자. 우선 아래와 같은 행렬 \\({\\bf B}\\)를 고려하자.\n\nB= [0 0 0 1 \n    1 0 0 0 \n    0 1 0 0\n    0 0 1 0]\n\n4×4 Matrix{Int64}:\n 0  0  0  1\n 1  0  0  0\n 0  1  0  0\n 0  0  1  0\n\n\n이것은 길이가 4인 임의의 column vector를 아래로 한칸씩 이동시키는 매트릭스이다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\nB*x # 아래로 한칸이동 \n\n4-element Vector{Complex{Int64}}:\n -1 + 2im\n  1 + 0im\n  2 - 1im\n  0 - 1im\n\n\n\nB^2*x # 아래로 두칸이동, B^2*x = B*(Bx) 이므로 \n\n4-element Vector{Complex{Int64}}:\n  0 - 1im\n -1 + 2im\n  1 + 0im\n  2 - 1im\n\n\n한편 이 매트릭스 \\({\\bf B}\\)는 아래와 같이 고유분해가 가능하다.\n\\[ {\\bf B} = {\\bf \\Psi} {\\bf \\Lambda} {\\bf \\Psi}^\\ast\\]\n\n\\({\\bf \\Psi}\\): make \\(\\frac{1}{\\sqrt{N}}[e^{\\sqrt{-1} \\frac{2\\pi}{N} ij}~\\text{ for }~ i=0,1,2,\\dots,N-1~\\text{ for }~j=0,1,2,\\dots,N-1]\\) and apply reshape function with \\((N,N)\\).\n\\({\\bf \\Lambda}\\): make \\([e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}~\\text{ for }~ i=0,1,2\\dots,N-1]\\) and apply Diagonal function.\n\n\nN = 4 \nλ = [exp(-im * (2π/N) *i) for i in 0:(N-1)]\nΛ = Diagonal(λ)\n_Ψ = 1/√N *[exp(im * (2π/N) * i*j) for i in 0:(N-1) for j in 0:(N-1)]\nΨ = reshape(_Ψ, (N,N))\nB ≈ Ψ * Λ * Ψ'\n\ntrue\n\n\n그런데 위에서 정의된 \\({\\bf \\Psi}^\\ast\\)는 우리가 그전에 정의하였던 \\({\\bf DFT}\\)의 행렬과 같다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nDFT = _DFT .|> (x -> exp(-im * (2π/4) * x)) \nDFT = DFT * 1/√N\n\n4×4 Matrix{ComplexF64}:\n 0.5-0.0im           0.5-0.0im          …           0.5-0.0im\n 0.5-0.0im   3.06162e-17-0.5im             -9.18485e-17+0.5im\n 0.5-0.0im          -0.5-6.12323e-17im             -0.5-1.83697e-16im\n 0.5-0.0im  -9.18485e-17+0.5im              2.75546e-16-0.5im\n\n\n\nΨ' == DFT \n\ntrue\n\n\n결국 요약하면 길이가 \\(N\\)인 신호의 \\({\\bf DFT}\\)행렬은 아래의 과정으로 구할 수 있음을 알 수 있다.\n\nForward operator \\({\\bf A}\\)를 정의한다.\n\\({\\bf A}\\)의 고유벡터행렬 \\({\\bf \\Psi}\\)을 구한다. 4\n\\({\\bf \\Psi}\\)의 conjugate transpose matrix \\({\\bf \\Psi}^\\ast\\) 를 구한다. 이것이 \\({\\bf DFT}\\) matrix 이다. 5"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#spectral-component-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap 8.3.html#spectral-component-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral component and Frequencies",
    "text": "Spectral component and Frequencies\n\\({\\bf A}\\)의 고유벡터 \\({\\bf \\Psi}\\)의 각 column을 spectral component라고 부른다.\n\nψ₁ = Ψ[:,1] # ψ₁ is first spectral component \nψ₂ = Ψ[:,2] # ψ₂ is seconde spectral component \nψ₃ = Ψ[:,3] # ψ₃ is third spectral component \nψ₄ = Ψ[:,4] # ψ₄ is last spectral component\n\n그리고 아래와 같은 수열을 \\(\\Omega_{k}=\\frac{k}{N}\\)을 frequency 라고 부른다.\n\nN=4 \nΩ = [k/N for k in 0:(N-1)]\nΩ\n\n4-element Vector{Float64}:\n 0.0\n 0.25\n 0.5\n 0.75"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Kronecker product",
    "text": "Kronecker product\n크로네커곱의 정의는 아래와 같다.\n\\[{\\bf A} \\otimes {\\bf B}\n=\\begin{bmatrix}\na_{11}{\\bf B} & a_{12}{\\bf B} & \\dots & a_{1m}{\\bf B} \\\\\na_{21}{\\bf B} & a_{22}{\\bf B} & \\dots & a_{2m}{\\bf B} \\\\\n\\dots & \\dots & \\dots & \\dots \\\\\na_{n1}{\\bf B} & a_{n2}{\\bf B} & \\dots & a_{nm}{\\bf B} \\\\\n\\end{bmatrix}\\]\n두 행렬 \\({\\bf A}_{m\\times n}\\), \\({\\bf B}_{p\\times q}\\)의 크로네커곱 \\({\\bf A}\\otimes {\\bf B}\\)의 차원은 \\(mp \\times nq\\) 가 된다. 계산예시는 아래와 같다.\n\n\n\n위키에서 긁은 예제, 글씨가 좀 작음\n\n\n크로네커곱에 대한 성질들이 위키에 많이 있으니 참고하면 좋다.\n(예제1)\n\nA= [1 2\n    3 4]\nB= [0 5\n    6 7]\nC = kron(A, B)\n\n4×4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n(예제2)\n\nA= [1 -4 7; -2 3 3]\nB= [8 -9 -6 -5; 1 -3 -4 7; 2 8 -8 -3; 1 2 -5 -1]\nC = kron(A, B)\n\n8×12 Matrix{Int64}:\n   8   -9  -6   -5  -32   36   24   20  56  -63  -42  -35\n   1   -3  -4    7   -4   12   16  -28   7  -21  -28   49\n   2    8  -8   -3   -8  -32   32   12  14   56  -56  -21\n   1    2  -5   -1   -4   -8   20    4   7   14  -35   -7\n -16   18  12   10   24  -27  -18  -15  24  -27  -18  -15\n  -2    6   8  -14    3   -9  -12   21   3   -9  -12   21\n  -4  -16  16    6    6   24  -24   -9   6   24  -24   -9\n  -2   -4  10    2    3    6  -15   -3   3    6  -15   -3"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Khatri–Rao product",
    "text": "Khatri–Rao product\n카트리-라오곱은 매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 같은 차원의 블락매트릭스로 정의될때 각 서브매트릭스의 크로네커 곱으로 정의된다. 정의와 계산예시는 아래와 같다.\n\n\n\n예시1: 위키에서 긁은 그림\n\n\n또 다른 계산예시는 아래와 같다. 이 예제는 중요하니까 구현해보자.\n\n\n\n예시2: 위키에서 긁은 그림\n\n\n(예제1)\n\nC= [1 2 3 \n    4 5 6 \n    7 8 9] \nD= [1 4 7\n    2 5 8\n    3 6 9]\n\n3×3 Matrix{Int64}:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:3]...)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81\n\n\n이건 자주 쓸일이 있을것 같으니까 함수로 저장하자.\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#181 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프 표현",
    "text": "그래프 표현\n아래의 그림을 살펴보자.\n\n\n\n그래프의 개념을 이해하는 필요한 그림, 일단 오른쪽의 \\({\\bf S}\\)는 무시할 것\n\n\n오른쪽의 \\({\\bf S}\\)는 무시하고 왼쪽의 그래프만 살펴보자. 이 그림에는 6개의 노드가 있고 각각의 노드는 저 마다의 연결구조를 가진다. 이러한 연결구조는 \\({\\bf G}=({\\bf N},{\\bf E})\\) 으로 표현할 수 있는데 여기에서 \\({\\bf N}\\)은 노드들의 집합이고 \\({\\bf E}\\)는 엣지들의 집합이다.1 보통 \\({\\cal E}\\)는 복잡하므로 연결정보를 매트릭스 \\({\\bf E}\\)로 표현하는데 이러한 \\({\\bf E}\\)를 인접행렬이라고 부른다. 인접행렬의 각 원소는 \\(E_{ij}= \\begin{cases} 1 & (i,j) \\in {\\cal E} \\\\ 0 & o.w \\end{cases}\\) 와 같이 정의한다. 이 그림의 경우 \\({\\cal N}\\) 와 \\({\\cal E}\\), \\({\\bf E}\\) 는 아래와 같다.\n\n\\({\\cal N}=\\{1,2,3,4,5,6\\}\\)\n\\({\\bf E}=\\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 \\\\ 1 & 0 & 1 & 0 & 1 & 0\\\\ 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\({\\cal E} = \\{(i,j) : E_{ij}=1 \\}\\)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "GSO",
    "text": "GSO\n후에 자세히 서술하겠지만 전통적인 시계열분석기법을 그래프신호로 확장하기 위해서는 단지 퓨리에변환 대신에 그래프퓨리에 변환을 사용하면 된다. 즉 퓨리에변환을 일반화한 그래프퓨리에변환을 잘 정의하면 된다.\n전통적인 신호처리 영역에서의 퓨리에변환은 시계열자료의 인접성을 의미하는 행렬 \\({\\bf B}\\)2의 고유행렬의 켤레전치로 정의할 수 있다. 이를 이용하면 그래프 퓨리에변환은 그래프자료의 인접성을 의미하는 행렬3의 고유행렬의 켤레전치로 정의할 수 있음을 유추할 수 있다. 즉 비유클리드 자료에서도 \\({\\bf B}\\)에 대응하는 어떠한 매트릭스가 정의되어야 하는데 (그리고 이 매트릭스는 그래프자료의 인접성에 대한 정보가 있어야 한다) 이 매트릭스를 \\({\\bf S}\\)라고 정의하고 grahp shift operator (GSO) 라고 이름 붙인다.\n주어진 그래프 \\({\\cal G}=({\\cal N},{\\cal E})\\) 에 대하여 GSO \\({\\bf S}\\)는 \\({\\bf E}+{\\bf I}\\)의 값이 1인 영역에만 값이 있는 어떠한 행렬이다. 다시 아래의 그림을 생각하여 보자.\n\n\n\nGSO의 개념을 이해하는데 필요한 그림\n\n\n왼쪽그래프의 GSO는 오른쪽과 같은 행렬 \\({\\bf S}\\)가 된다. 이제 \\({\\bf S}\\) 의 고유벡터행렬을 구한 뒤에 그것의 켤레전치를 \\({\\bf GFT}\\) 행렬로 정의하면 될 것 같다. 문제는 “\\({\\bf S}\\)의 고유벡터행렬이 항상 존재하는가?” 인데, 사실 이게 항상 존재한다는 보장이 없다. 즉 \\({\\bf S}\\)의 고유벡터 행렬이 존재 안할 수도 있다. 따라서 GSO \\({\\bf S}\\)가 고유분해가능하다는 조건이 추가적으로 필요한데 이러한 조건을 만족하는 GSO를 normal GSO라고 부른다. 우리는 당연히 normal GSO에 대해서만 관심이 있으므로 앞으로 특별한 언급이 없는한 GSO는 모두 normal GSO라고 가정한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Periodogram, correlogram, and LS estimator",
    "text": "Periodogram, correlogram, and LS estimator\nFrom \\({\\bf C}_{\\tilde{\\bf x}}:= \\mathbb{E}\\left[\\tilde{\\bf x}\\tilde{\\bf x}^H \\right]=\\mathbb{E}\\left[({\\bf V}^H{\\bf x})({\\bf V}^H{\\bf x})^H \\right]=\\text{diag}({\\bf p})\\) it follows that one may express the PSD as \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\). That is, the PSD is given by the expected value of the squared frequency components of the random process. This leads to a natural approach for the estimation of \\({\\bf p}\\) from a finite set of \\(R\\) realizations of the process \\({\\bf x}\\). Indeed, we compute the \\({\\bf GFT} \\tilde{\\bf x}_r = {\\bf V}^H{\\bf x}_r\\) of each observed signal \\({\\bf x}_r\\) and estimate \\({\\bf p}\\) as\n\\[\n\\hat{\\bf p}_{pg}:= \\frac{1}{R}\\sum_{r=1}^R|\\tilde{\\bf x}_r|^2=\\frac{1}{R}\\sum_{r=1}^{R}|{\\bf V}^H{\\bf x}_{r}|^2.\n\\]\nThe estimator \\(\\hat{\\bf p}_{pg}\\) is termed periodogram due to its evident similarity with its homonym5 in classical estimation. It is simple to show that \\({\\bf p}_{pg}\\) is an unbiased estimator, that is, \\(\\mathbb{E}[\\hat{\\bf p}_{pg}]= {\\bf p}\\). A more detailed analysis of the performance of \\(\\hat{\\bf p}_{pg}\\), for the case where the observations are Gaussian, is given in Proposition 12.1.6\nAn alternative nonparametric estimation scheme, denominated correlogram, can be devised by starting from the definition of \\({\\bf p}\\) in\n\\[{\\bf p}:=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big).\\]\nNamely, one may substitute \\({\\bf C}_{\\bf x}\\) in above equation by the sample covariance \\(\\hat{\\bf C}_{\\bf x} = \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\) computed based on the available observations to obtain\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\nNotice that the matrix \\({\\bf V}^H\\hat{\\bf C}_{\\bf x}{\\bf V}\\) is in general, not diagonal because the eigenbasis of \\(\\hat{\\bf C}_{\\bf x}\\) differs from \\({\\bf V}\\), the eigenbasis of \\({\\bf C}_{\\bf x}\\). Nonetheless, we keep only the diagonal elements \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x}{\\bf v}_i\\) for \\(i = 1, \\dots , N\\) as our PSD estimator. It can be shown that the correlogram \\({\\bf p}_{cg}\\) and the periodogram \\({\\bf p}_{pg}\\) lead to identical estimators, as is the case in classical signal processing.\nThe correlogram can also be interpreted as an LS estimator. The decomposition in \\({\\bf C}_{\\bf x}={\\bf V}\\text{diag}({\\bf p}){\\bf V}^H\\) allows a linear parameterization of the covariance matrix \\({\\bf C}_{\\bf x}\\) as\n\\[\n{\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H.\n\\]\nThis linear parametrization will also be useful for the sampling schemes developed in Section 12.4. Vectorizing \\({\\bf C}_{\\bf x}\\) in \\({\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H\\) results in a set of \\(N^2\\) equations in \\({\\bf p}\\)\n\\[\n{\\bf c}_{\\bf x} = \\text{vec}({\\bf C}_{\\bf x})=\\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf G}_{np}{\\bf p},\n\\]\nwhere \\(\\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf v}_i^\\ast \\otimes {\\bf v}_i\\). Relying on the Khatri-Rao product, we then form the \\(N^2 \\times N\\) matrix \\({\\bf G}_{np}\\) as\n\\[\n{\\bf G}_{np}:= \\left[{\\bf v}_1^\\ast \\otimes {\\bf v}_1, \\dots, {\\bf v}_N^\\ast \\otimes {\\bf v}_N \\right] = {\\bf V}^\\ast \\odot {\\bf V}.\n\\]\n\nHere \\(\\otimes\\) denote the Kronecker matrix product and \\(\\odot\\) denote the Khatri-Rao matrix product.\n\nUsing the sample covariance matrix \\(\\hat{\\bf C}_{\\bf x}\\) as an estimate of \\({\\bf C}_{\\bf x}\\), we can match the estimated covariance vector \\(\\hat{\\bf c}_{\\bf x}=\\text{vec}(\\hat{\\bf C}_{\\bf x})\\) to the true covariance vector \\({\\bf c}_{\\bf x}\\) in the LS sense as\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\nIn other words, the LS estimator minimizes the squared error \\(\\text{tr}\\left[\\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)^T \\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)\\right]\\). From expression \\(\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\) it can be shown that the \\(i\\)th element of \\(\\hat{\\bf p}_{ls}\\) is \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x} {\\bf v}_i\\). Combining this with Eq.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right]\\]\nwe get that the LS estimator \\(\\hat{\\bf p}_{ls}\\) and the correlogram \\(\\hat{\\bf p}_{cg}\\) —and hence the periodogram as well— are all identical estimators. The estimators derived in this subsection do not assume any data distribution and are well suited for cases where the data probability density function is not available. In what follows, we provide performance bounds for these estimators under the condition that the observed signals are Gaussian."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD",
    "text": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD\n\n전통적인 분석방법\n클래식한 정상시계열은 유한차수의 ARMA로 근사할 수 있음이 알려져 있다7. 유한차수의 ARMA의 계수 \\(p\\),\\(q\\)를 적절하게 추정하기 위해서는 시계열 \\({\\bf x}\\)를 SACF plot 혹은 SPACF plot 을 이용하면 된다. 이때 SACF 혹은 SPACF 의 그림을 살펴보고 적절한 모형을 선택하기 위해서는 유한차수 ARMA의 이론적 ACF의 모양을 알면 되는데,8 이를 바꾸어서 말하면 결국 정상시계열 \\({\\bf x}\\)의 모든 정보는 ACF에 들어있다는 의미가 된다. 즉 정상시계열은 ACF만 잘 추정하면 모든 것이 해결된다.\n그런데 ACF의 모든 정보는 다시 아래의 행렬에 들어있다.\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^T]\\]\n여기에서 \\({\\bf x}\\)는 realization이 아니라 확률벡터를 의미함을 유의하자.9 따라서 정상시계열의 경우 \\({\\bf C}_{\\bf x}\\)를 잘 추정하면 모든것이 해결된다고 생각하면 된다.\n\n참고: 정상시계열의 경우 ACF 만 정확하게 알아도 (반대로 PACF만 정확하게 알아도) 이론상 모든 모형을 특정할 수 있다. 즉 정상시계열의 모형을 특정하기 위해서는 ACF plot, PACF plot 하나만 있어도 충분하다. (Wold’s Thm은 떠올리면 모든 정상시계열은 무한MA로 유니크하게 표현할 수 있는데, 이는 PACF plot을 가지고 모든 정상시계열을 유니크하게 특정할 수 있다는 것을 의미한다) 다만 좀 더 모형을 특정하는 과정을 용이하게 하기 위해서 실전에서는 SACF plot 과 SPACF plot 을 함께 보는 것이 유리하다.\n\n(예제) AR(1) 모형\n왜 ACF의 모든정보를 \\({\\bf C}_{\\bf x}\\)로 부터 알수 있는지 코드를 통하여 실습하여 보자. (바로 이해된다면 사실 이 예제는 스킵해도 무방함) 아래와 같은 모형을 가정하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n여기에서 \\(\\epsilon_t\\)는 서로 독립인 표준정규분포를 따른다. 이 모형에서 길이가 100인 시계열을 임의로 발생시키자.\n\nx = zeros(100*1000)\nx[1] = randn()\nfor t in 2:100\n    x[t] = 0.5*x[t-1] + randn()\nend\n\n모형에서 생성된 하나의 시계열을 시각화 하면 아래와 같다.\n\nplot(x) # 그냥 그려본것임. 별 의미는 없음\n\n\n\n\nlag=1일 경우 이 시계열의 SACF를 계산하면 아래와 같다.\n\nx[1:99] .* x[2:100]\n\n99-element Vector{Float64}:\n  1.587897526021493\n  1.130306190921068\n  0.5698214432110668\n  0.4648189302568683\n  0.3099446153360606\n  0.36362604534744775\n  0.8191871414624922\n -0.1720390842292145\n -0.06301214708310766\n  0.026414715508855904\n -0.007988283356933327\n -0.04178812545299474\n  0.22453267567940685\n  ⋮\n  3.931333581073927\n  1.315564948810858\n  0.9096080102581454\n  0.5410986320348997\n  0.29627801400693676\n  1.0673283524686212\n -1.0394649044573636\n  2.80195248208142\n  4.152973765526384\n  2.316315764368524\n  0.978758337765867\n -0.5840281943972468\n\n\n\n이 계산결과는 각 \\(t\\)에 대하여 \\(x_{t-1}x_t\\) 를 계산한 것과 같다.\n\n이 수열들의 평균은 아래와 같다.\n\nx[1:99] .* x[2:100] |> mean\n\n0.5835563885014224\n\n\n\n이 계산결과는 \\(\\frac{1}{99}\\sum_{t=2}^{100} x_{t-1}x_t\\)를 계산한 것과 같다.\n\n이론적인 값인 0.5 근처의 값이 잘 나옴을 알 수 있다.\nlag=2일 경우도 마찬가지로 구할 수 있다.\n\nx[1:98] .* x[3:100] |> mean\n\n0.38420263596668275\n\n\n이러한 숫자들은 그런데 \\({\\bf x}{\\bf x}^T\\)를 이용하여서도 구할 수 있다.10\n\nx*x'\n\n100×100 Matrix{Float64}:\n  0.760108    1.5879      0.541064   …  -1.57394    -0.472676    0.939172\n  1.5879      3.31719     1.13031       -3.28802    -0.987441    1.96197\n  0.541064    1.13031     0.385143      -1.12037    -0.336463    0.668527\n  0.800507    1.67229     0.569821      -1.65759    -0.497799    0.989089\n  0.441361    0.922022    0.314172      -0.913915   -0.274462    0.545336\n  0.533784    1.1151      0.379961   …  -1.10529    -0.331936    0.659531\n  0.517803    1.08171     0.368586      -1.0722     -0.321998    0.639786\n  1.20252     2.51212     0.855987      -2.49003    -0.747794    1.48581\n -0.108745   -0.227173   -0.0774074      0.225175    0.0676234  -0.134363\n  0.440444    0.920106    0.313519      -0.912016   -0.273892    0.544203\n  0.0455859   0.0952309   0.0324492  …  -0.0943935  -0.0283478   0.0563249\n -0.133198   -0.278257   -0.0948139      0.27581     0.0828298  -0.164577\n  0.238468    0.498169    0.169748      -0.493789   -0.148292    0.294646\n  ⋮                                  ⋱                          \n  2.04697     4.2762      1.45708       -4.2386     -1.27291     2.52919\n  0.488514    1.02053     0.347736      -1.01155    -0.303784    0.603596\n  1.41531     2.95665     1.00746    …  -2.93065    -0.880119    1.74873\n  0.290602    0.60708     0.206858      -0.601742   -0.180712    0.359062\n  0.774954    1.61891     0.551632      -1.60468    -0.481908    0.957516\n  1.04688     2.18698     0.745197      -2.16775    -0.651007    1.2935\n -0.754723   -1.57665    -0.537231       1.56279     0.469328   -0.932519\n -2.82194    -5.89516    -2.00873    …   5.84333     1.75484    -3.48673\n -1.11863    -2.33686    -0.796269       2.31632     0.695624   -1.38215\n -1.57394    -3.28802    -1.12037        3.25911     0.978758   -1.94472\n -0.472676   -0.987441   -0.336463       0.978758    0.293936   -0.584028\n  0.939172    1.96197     0.668527      -1.94472    -0.584028    1.16042\n\n\n여기에서 각 원소들이 의미하는 바는 아래와 같다.\n\n대각선의 원소: \\(x_t^2,~ t=1,2,\\dots,100\\) 을 의미\n대각선 한칸 위, 혹은 한칸 아래: \\(x_{t-1} x_t~ t=2,3,\\dots,100\\) 을 의미\n대각선 두칸 위, 혹은 두칸 아래: \\(x_{t-2} x_t~ t=3,4,\\dots,100\\) 을 의미\n\n\n\n\nx*x'의 계산결과를 캡쳐한 그림, 이것은 \\(\\hat{\\bf C}_{\\bf x}\\)를 의미함\n\n\n확인해보자.\nlag=1, 스크린샷의 노란색\n\n(x[1:99] .* x[2:100])[1:5]\n\n5-element Vector{Float64}:\n 1.587897526021493\n 1.130306190921068\n 0.5698214432110668\n 0.4648189302568683\n 0.3099446153360606\n\n\n\nlag1에 해당하는 숫자들임. 이는 스크린샷에서 노란색으로 표현된 1.589, 1.13031, 0.569821 … 등과 일치한다.\n\nlag=2, 스크린샷의 빨간색\n\n(x[1:98] .* x[3:100])[1:5]\n\n5-element Vector{Float64}:\n 0.5410642277088621\n 1.6722932576420804\n 0.3141719983177106\n 0.5621541352252872\n 0.30066534927151267\n\n\n\nlag2에 해당하는 숫자들임. 이는 스크린샷에서 빨간색으로 표현된 숫자들인 0.54164, 1.67229, 0.31417 … 등과 일치한다.\n\n\n\n스펙트럼 방법\n지금까지는 정상시계열일 경우 ACF를 이용한 간단한 분석방법을 다시 복습했다. 그리고 \\({\\bf C}_{\\bf x}\\)가 ACF를 구함에 필요한 모든정보를 가지고 있음을 이해했다. 한편 \\({\\bf C}_{\\bf x}\\)은 positive definite matrix 이므로 아래와 같이 분해가능하다.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식표현을 잘 해석하면 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf V}\\)와 \\({\\bf p}\\)에 담겨있다는 사실을 이해할 수 있다. 그런데 정상시계열일 경우 한정하여 \\({\\bf C}_{x}\\)의 고유벡터행렬은 \\({\\bf B}\\)의 고유벡터행렬과 일치한다는 사실을 알고 있다. 따라서 \\({\\bf V}\\)는 \\({\\bf B}\\)로 부터 그냥 알 수 있는 정보이다. 따라서 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf p}\\)에 담겨있다는 사실을 알 수 있다. 이는 적절한 \\({\\bf p}\\)를 추정하는 일은 적절한 \\({\\bf C}_{\\bf x}\\)를 추정하는 것과 같다는 사실을 알려준다.\n요약하면 아래와 같다.\n\n임의의 정상시계열은 이론적인 ACF (혹은 PACF)를 잘 추정하면 유니크하게 특정할 수 있다. (Wold’s Thm)\nACF를 잘 추정한다는 말은 \\({\\bf C}_{\\bf x}\\)를 잘 추정한다는 의미이다.\n그런데 \\({\\bf p}\\)를 잘 추정하면 \\({\\bf C}_{\\bf x}\\)를 잘 추정하는 일이 된다.\n따라서 임의의 정상시계열은 \\({\\bf p}\\)를 잘 추정하면 유니크하게 특정할 수 있다는 결론을 얻는다.\n\n여기에서 \\({\\bf p}\\)를 power spectral density 라고 부른다. 일반적으로 정상시계열을 분석하기 위해서는 \\({\\bf C}_{\\bf x}\\)를 특정하거나, \\({\\bf p}\\)를 특정하면 되는데 여기에서 \\({\\bf p}\\)를 특정한뒤 \\({\\bf p}\\)로 부터 \\({\\bf C}\\)를 역으로 해석하는 방법론을 spectral analysis라고 부른다. 경우에 따라서 \\({\\bf C}_{\\bf x}\\)를 특정하는 것이 용이할 수도 있지만 \\({\\bf p}\\)를 특정하고 해석하는 것이 용이할 때도 있다.\n그렇다면 주어진 시계열 \\({\\bf x}\\)에 대하여 \\({\\bf p}\\)를 어떻게 구할까? 직관적으로 생각하면 단순히 아래의 알고리즘으로 구하면 된다는 것을 알 수 있다.\n\n\\({\\bf C}_{\\bf x}\\)를 알아낸다.\n\\({\\bf C}_{\\bf x}\\)를 고유분해하여 \\({\\bf p}\\)를 구한다.\n\n또 다른 방법으로는 교재에 소개된 바 있는 아래의 수식을 이용하는 것이다.11\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\]\n이것을 이용하면 아래와 같은 알고리즘을 떠올릴 수 있다.\n\n\\({\\bf B}\\)의 고유벡터행렬 \\({\\bf V}\\)를 구하고 \\({\\bf V}^H{\\bf x}\\)를 계산한다.\n계산된 결과를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n그런데 \\({\\bf V}^H{\\bf x}= {\\bf DFT} \\cdot {\\bf x}\\) 이므로 1의 과정을 아래와 같이 바꾸어 서술할 수 있다.\n\n\\({\\bf x}\\)를 퓨리에변환하여 \\(\\tilde{\\bf x} = {\\bf DFT} \\cdot {\\bf x}\\) 를 계산한다.\n\\(\\tilde{\\bf x}\\)를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n즉 임의의 시계열을 퓨리에변환한 뒤 제곱하면 \\({\\bf p}\\)를 얻을 수 있다.\n(예제2) – 하나의 realization에서 \\(\\hat{\\bf p}\\)를 구해보자.\n(예제1에 이어서) 아래의 모형에서 생성된 \\({\\bf x}\\)를 다시 고려하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n\nplot(x)\n\n\n\n\n이 자료의 PSD \\({\\bf p}\\)는 아래와 같이 구할 수 있다.\n단계1: \\({\\bf x}\\)의 DFT를 계산\n\nx̃ = fft(x) \n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\\({\\bf B}\\)를 설정하고 고유값분해 하기 귀찮아서 그냥 DFT해주는 패키지 사용함\n\n단계2: \\(\\hat{\\bf p}\\)를 계산\n\np̂ = abs.(x̃).^2\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n참고\nfft(x) 대신에 아래의 코드를 이용해도 된다.\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\nV'x\n\n100-element Vector{ComplexF64}:\n  -5.756917285643587 + 0.0im\n -19.082672090492103 - 1.0178306444775291im\n   14.23050682476898 - 11.867854578090007im\n  3.8980118254428824 + 1.2603018602424476im\n  -16.15797305318818 + 27.48824632227092im\n   12.32574209329044 - 1.5134316695905325im\n  3.9542122497256385 + 15.369129638224617im\n    9.51693811050782 + 19.371467179753516im\n  -19.38292930624826 + 9.495062886234233im\n -7.8539348514784155 + 4.134711886071595im\n -14.072349901900417 - 5.945064076174276im\n -14.596266922162371 + 3.447776409279244im\n   5.857720447482956 + 5.7388951128385735im\n                     ⋮\n   5.857720447482839 - 5.738895112838781im\n -14.596266922162307 - 3.4477764092792627im\n  -14.07234990190023 + 5.945064076174198im\n  -7.853934851478599 - 4.134711886071242im\n -19.382929306248577 - 9.49506288623372im\n   9.516938110507212 - 19.371467179753736im\n  3.9542122497250025 - 15.369129638224603im\n  12.325742093290597 + 1.5134316695903638im\n  -16.15797305318867 - 27.488246322270854im\n  3.8980118254424903 - 1.2603018602428118im\n  14.230506824769146 + 11.867854578089572im\n   -19.0826720904922 + 1.0178306444775123im\n\n\n진짜 똑같은지 확인\n\nfft(x)\n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\n전통적인 방법과 스펙트럼 방법의 비교\n시계열자료의 전통적인 분석과 spectral analysis는 대충 아래의 과정으로 비교 설명할 수 있다.\n\n\n\n\n\n\n\n\n단계\n전통적인 방법\n스펙트럴 분석\n\n\n\n\n1\n\\({\\bf x}\\)의 plot을 그려봄\n\\({\\bf x}\\)의 plot을 그려봄\n\n\n2\nSACF plot, SPACF plot 을 그려봄\nPSD plot을 그려봄\n\n\n3\nACF를 추정 (=ARMA(\\(p\\),\\(q\\))에 대응하는 파라메터를 추정)\n\\({\\bf p}\\)를 추정\n\n\n4\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n\n\n\n눈여겨 볼 점은 PSD plot의 존재이다. 전통적인 시계열에서 SACF plot 과 비슷하게 스펙트럼 방법에서 시계열을 분석하기 위해 필요한 매우 중요한 시각화 이다. 간단하게 비교를 하면 아래와 같다.\nSACF plot\n\nx축: lag=0, lag=1, ….\ny축: lag에 대응하는 상관계수값\n\nPSD plot\n\nx축: \\(\\Omega=\\big\\{\\frac{k}{N}:~ \\text{for}~ k=0,\\dots, N-1\\big\\}\\), 정규화된 freq를 의미함\ny축: 주파수에 대응하는 power값\n\n전통적인 방법에 비하여 스펙트럴 분석이 가지는 장점은 위의 표에서 소개한 일반적인 분석루틴이 시계열이 아닌 그래프신호로 쉽게 확장가능 하다는 점이다12. 따라서 앞으로는 전통적인 시계열 분석방법 대신 스펙트럴 분석만을 다룰 것이다. 스펙트럴 분석의 핵심적인 부분은 \\({\\bf p}\\)를 추정하는 방법과 추정량의 점근적 성질들을 파악하는 것이다. 이 포스트에서는 \\({\\bf p}\\)를 추정하는 방법만을 다룬다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프신호에서의 PSD의 추정",
    "text": "그래프신호에서의 PSD의 추정\n이제 그래프 신호에서 \\({\\bf p}\\)를 추정하는 방법에 대하여 살펴보자. 그래프이동변환 (Graph Shift Operator, GSO)13 \\({\\bf S}={\\bf V}{\\bf \\Lambda}{\\bf V}^H\\)에 대하여 정상인 시계열 \\({\\bf x}\\)를 고려한다. 이 신호의 그래프퓨리에 변환14은 아래와 같이 구할 수 있다.\n\\[\\tilde{\\bf x}={\\bf GFT} {\\bf x} = {\\bf V}^H{\\bf x}\\]\n여기에서 \\(\\tilde{\\bf x}\\)를 \\({\\bf x}\\)의 주파수응답(frequency representation)이라고 부른다.15 우리는 아래의 수식에서 \\({\\bf p}\\)의 값에 관심이 있다.\n\\[{\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\]\n여기에서 \\({\\bf p}\\)를 PSD (power spectrum density) 라고 한다. \\({\\bf p}\\)가 포함된 표현식은 위의 수식 이외에도 2개가 더 있다. 이를 모두 요약하면 아래와 같다16\n\n\\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)17\n\\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n\\({\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\)\n\n위의 표현중 3.에서 \\({\\bf c}_{\\bf x}\\)은 \\({\\bf C}_x\\)를 벡터화한 것이며 \\({\\bf G}_{np}\\)는 \\({\\bf V}^\\ast\\) 와 \\({\\bf V}\\)를 열별-크로네커곱 (column-wise Kronecker product) 이다. 이때 \\({\\bf G}_{np}\\)의 정의가 조금 생소하니 한번 계산하여 보자.\n(예제) 아래와 같은 GSO \\({\\bf B}\\)를 고려하자.\n\nB= [0 1 0 0 \n    0 0 1 0 \n    0 0 0 1 \n    1 0 0 0]\n\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  1  0\n 0  0  0  1\n 1  0  0  0\n\n\n이러한 GSO에 대하여 \\({\\bf G}_{np}\\)는 아래와 같이 구할 수 있다.\n(1) \\({\\bf V}\\)를 정의\n\nV = [i*j for i in 0:3 for j in 0:3] |> \n    x -> reshape(x,(4,4)) .|> \n    x -> exp(im * (2π/4) * x) \n\n4×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n16×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n\n\n위에서 언급한 표현식 1,2,3 을 이용하면 \\({\\bf p}\\)를 추정하는 세 가지 방법을 각각 정의할 수 있다. 하나씩 살펴보자.\n\n1. \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 수식 \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)를 적당히 변형하면 아래를 얻을 수 있다.\n\\[{\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\]\n여기에서\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^H]\\approx \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_t{\\bf x}_r^H\\]\n이므로 이 수식에 근거하여 \\({\\bf p}\\)을 추정한다면 아래와 같이 할 수 있다.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면18, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H{\\bf x}_r{\\bf x}_r^H{\\bf V} \\right].\\]\n\n주의: 여기에서 \\({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V}\\) 는 항상 대각행렬이지만 \\({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V}\\) 은 대각행렬이 아닐수도 있음을 유의하자. 즉 이론적인 모수는 대각행렬이지만 sample version은 대각행렬이 아닐 수 있다. 대각선이 아닌 원소는 버리면 된다.)\n\n\n아이디어: 혹시 대각선이 아닌 원소들을 이용하여 오차항 \\(\\epsilon_t\\)의 분산을 추정할 수도 있지 않을까? 이미 연구가 있겠지?\n\n(예제)\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n\np̂ = diag(V' * (x*x') * V)\n\n100-element Vector{ComplexF64}:\n 33.142096633741986 + 0.0im\n 365.18435333408354 + 1.5376069362644531e-13im\n  343.3532967764883 + 6.904176529646917e-14im\n 16.782856970223083 - 3.5538396658301444e-14im\n 1016.6837790613963 + 5.475049904926759e-15im\n 154.21439356883144 + 6.4512443306088e-14im\n  251.8459403524346 + 2.1316282072803006e-14im\n 465.82585169550384 + 1.816929057526117e-13im\n 465.85416770456044 + 4.1584439183295984e-14im\n    78.780135032089 + 1.3472456770553478e-14im\n 233.37481863133462 + 6.315728724701355e-14im\n 224.93817023139385 - 3.472109560086835e-14im\n  67.24780595702241 + 7.105427357601002e-14im\n                    ⋮\n   67.2478059570233 + 6.384723798533952e-14im\n 224.93817023139195 + 1.9727655769954595e-14im\n 233.37481863132837 - 2.1872689567834747e-14im\n    78.780135032089 + 1.917599080404094e-14im\n 465.85416770456294 + 4.808950231511622e-14im\n 465.82585169550094 - 4.890486289860305e-14im\n  251.8459403524291 + 2.0146681724568905e-14im\n  154.2143935688347 - 1.0948596967617507e-13im\n 1016.6837790614081 + 1.2114814701286432e-13im\n  16.78285697022108 + 2.376159104534641e-14im\n 343.35329677648286 + 1.1310381241837407e-14im\n 365.18435333408746 + 4.574214786667376e-14im\n\n\n\n\n2. \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\approx \\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n따라서 \\(\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2\\) 를 PSD \\({\\bf p}\\)에 대한 추정량이라고 생각할 수 있다. 이러한 추정량을 기호로 \\(\\hat{\\bf p}_{pg}\\)라고 정의하고 periodogram이라고 부른다. 즉\n\\[\\hat{\\bf p}_{pg}=\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n\\[\\hat{\\bf p}_{pg}=|{\\bf V}^H {\\bf x}_r|^2 \\]\n즉 이 경우 \\(\\hat{\\bf p}_{pg}\\)는 단순히 관측시계열 \\({\\bf x}_r\\)의 그래프 퓨리에 변환 \\(\\tilde{\\bf x}={\\bf V}^H{\\bf x}_r\\) 결과에 절대값을 취하고 제곱한 것과 같다.\n(예제)\n스펙트럼방법챕터 예제2에서 이미 보여준 적 있다. 주어진 시계열 \\({\\bf x}\\)에 대하여 \\(\\hat{\\bf p}_{pg}\\)를 구하는 방법을 요약하면 아래와 같다.\n\nx̃ = fft(x) # 단계1: GFT, 이 신호는 시계열이라서 GFT대신에 DFT를 써도 된다.\np̂ = abs.(x̃).^2 # 단계2: hat p\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n\n\n3. \\({\\bf c}_{\\bf x} = {\\bf G}_{np} {\\bf p}\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식으로부터 아래를 얻을 수 있다.\n\\[{\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\]\n여기에서 \\({\\bf c}_{\\bf x}\\) 대신에 \\(\\hat{\\bf c}_{\\bf x}\\) 를 대입하면 아래와 같이 생각할 수 있다.\n\\[\\hat{\\bf c}_{\\bf x} \\approx  {\\bf G}_{np} {\\bf p}\\]\n이 문제는 아래와 같은 회귀모형으로 생각할 수 있다.\n\n\n\n\n\n\n\n\n\n회귀모형\n우리의 문제\n\n\n\n\n모형\n\\({\\bf y} \\approx {\\bf X}{\\boldsymbol \\beta}\\)\n\\(\\hat{\\bf c}_{\\bf x} \\approx {\\bf G}_{np}{\\bf p}\\)\n\n\n설명변수\n\\({\\bf X}\\)19\n\\({\\bf G}_{np}\\)20\n\n\n반응변수\n\\({\\bf y}\\)21\n\\(\\hat{\\bf c}_{\\bf x}\\)22\n\n\n추정하고 싶은 파라메터\n\\({\\boldsymbol \\beta}\\)23\n\\(\\hat{\\bf p}\\)24\n\n\n오차항\n대부분 정규분포를 가정\n??? 모르겠는데??\n\n\n\n회귀분석에서 아래의 수식이 익숙하다면\n\\[\n\\hat{\\boldsymbol \\beta}_{ls} = \\underset{\\boldsymbol \\beta}{\\operatorname{argmin}} \\|{\\bf y}-{\\bf X}{\\boldsymbol \\beta}\\|_2^2=({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}.\n\\]\n\\({\\bf p}\\)를 추정하기 위한 아래의 수식도 쉽게 이해할 수 있다. (의문: 그런데 왜 MSE를 손실함수로 쓰고 있는 거야? 오차항이 설마 정규분포?)\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\n(예제)\n(1) \\({\\bf V}\\)를 정의\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n10000×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im      0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im   …  0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im      0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im    …  0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n 1.0+0.0im       1.0+0.0im                1.0+0.0im\n\n\n(3) \\(\\hat{\\bf p}_{ls}=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\)\n\nĉₓ = vec(x*x')\np̂ = inv(Gₙₚ' * Gₙₚ) * Gₙₚ' * ĉₓ \n\n100-element Vector{ComplexF64}:\n  0.003314209663374193 - 2.7356277964988863e-19im\n   0.03651843533340838 - 4.01518191768058e-18im\n   0.03433532967764885 + 2.515448157755484e-17im\n 0.0016782856970223292 - 1.0070028487673847e-17im\n   0.10166837790613971 + 3.1129277935880596e-18im\n  0.015421439356883134 + 9.403422807142065e-18im\n  0.025184594035243472 - 3.993782799800785e-18im\n   0.04658258516955039 - 1.850761436988587e-18im\n   0.04658541677045607 + 1.1559103895961936e-17im\n  0.007878013503208905 + 3.559698092088507e-18im\n  0.023337481863133468 + 2.6204945155857973e-18im\n   0.02249381702313939 + 5.304406111488559e-18im\n  0.006724780595702225 - 1.655564138463681e-17im\n                       ⋮\n  0.006724780595702329 + 1.8121162053534517e-18im\n  0.022493817023139205 - 1.0461976779111972e-17im\n   0.02333748186313285 - 6.792203007975684e-18im\n  0.007878013503208907 - 2.3575339315335667e-18im\n  0.046585416770456294 + 1.5392042695643853e-17im\n  0.046582585169550106 - 1.123245521985718e-17im\n  0.025184594035242928 + 1.1628578774983873e-18im\n  0.015421439356883466 + 5.864828990948797e-18im\n   0.10166837790614085 + 2.2712943512935246e-17im\n 0.0016782856970221013 + 4.829637376114682e-18im\n   0.03433532967764831 + 3.3208196889839756e-19im\n  0.036518435333408754 + 1.3795822112205515e-17im\n\n\n\n?? 뭔가 스케일이 안맞음\n\n\nN^2 * p̂\n\n100-element Vector{ComplexF64}:\n  33.14209663374193 - 2.7356277964988864e-15im\n 365.18435333408377 - 4.0151819176805797e-14im\n  343.3532967764885 + 2.515448157755484e-13im\n 16.782856970223293 - 1.0070028487673847e-13im\n 1016.6837790613971 + 3.1129277935880596e-14im\n 154.21439356883135 + 9.403422807142065e-14im\n 251.84594035243472 - 3.9937827998007846e-14im\n  465.8258516955039 - 1.850761436988587e-14im\n  465.8541677045607 + 1.1559103895961937e-13im\n  78.78013503208905 + 3.559698092088507e-14im\n 233.37481863133468 + 2.6204945155857973e-14im\n 224.93817023139388 + 5.304406111488559e-14im\n  67.24780595702225 - 1.655564138463681e-13im\n                    ⋮\n  67.24780595702329 + 1.8121162053534517e-14im\n 224.93817023139206 - 1.0461976779111972e-13im\n  233.3748186313285 - 6.792203007975684e-14im\n  78.78013503208906 - 2.3575339315335666e-14im\n 465.85416770456294 + 1.5392042695643854e-13im\n 465.82585169550106 - 1.123245521985718e-13im\n 251.84594035242927 + 1.1628578774983874e-14im\n 154.21439356883465 + 5.864828990948797e-14im\n 1016.6837790614085 + 2.2712943512935246e-13im\n  16.78285697022101 + 4.8296373761146824e-14im\n  343.3532967764831 + 3.3208196889839758e-15im\n  365.1843533340875 + 1.3795822112205514e-13im\n\n\n\n\\(N^2\\)를 곱해주니까 아까부터 구하던 값이 그대로 잘 나옴. (\\({\\bf DFT}\\) 혹은 \\({\\bf GFT}\\)를 정의할때 \\(\\frac{1}{\\sqrt N}\\)으로 스케일링 하느냐 마느냐 차이때문에 생기는 현상임)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "의문점",
    "text": "의문점\n아래의 그림을 살펴보자.\n\n\n\n그림12.3(교재에서 긁어온 그림): Power spectral density estimation. All estimators are based on the same random process defined on the Karate club network (Zachary 1977). (A) Periodogram estimation with different numbers of observations. (B) Windowed average periodogram from a single realization and a different number of windows. (C) Windowed average periodogram for four windows and a varying number of realizations. (D) Parametric MA estimation for 1 and 10 realizations.\n\n\n이 그림은 다양한 방법으로 true PSD \\({\\bf p}\\)를 추정한 결과를 나타내는 PSD plot 이다25. 우리가 적용한 방법은 (A)에서 \\(R=1\\)일 경우이다. 보는것 처럼 true PSD 를 놀라울 정도로 제대로 추정하지 못한다26. 만약에 우리가 모형에서 하나의 시계열이 아니라 1000개의 정도의 시계열을 관측하였다면 좀 더 합리적으로 추정할 수 있다. 그런데 사실 하나의 모형에서 1000개씩이나 되는 시계열을 관측하는 일은 현실적으로 불가능하다27 따라서 우리는 비교적 적은 \\(R\\)에서 합리적인 PSD의 추정치를 이끌어내야 한다. 그림 (B),(C)는 상대적으로 적은 \\(R\\)에 대해 \\({\\bf p}\\)를 추정하는 windowed periodogram 을 이용하여 PSD를 추정한 결과이다. (C)를 살펴보면 \\(R=1\\) 일경우 \\({\\bf p}\\)를 추정한 값들이 나와있는데 (A)와 비교하면 꽤 합리적으로 보인다.\n문제는 (A)-(C)에서 제안된 방법 모두가 (D)에 제시된 전통적인 방법에 비하여 퍼포먼스가 떨어진다는 것이다. (D)는 parametric 모형을 사용한 결과이다. 파라메트릭 방법이므로 특정 모델을 한정하고 거기에 대응하는 한두개의 모수만 추정하면 되므로 추정이 잘 된다.28 반면 (A)-(C)의 경우 한 두개의 파라메터가 아니라 \\({\\bf p}\\)의 모든 원소를 추정해야하므로 추정할 파라메터가 데이터의 수 \\(N\\)과 같다29. 따라서 추정치의 분산이 크다. 사실 이것은 파라메트릭 방법과 세미파라메트릭 방법이라는 구조적인 차이때문에 어쩔 수 없는 것 같다. 그래도 세미파라메트릭 방법은 머리아프게 모델링을 할 필요가 없고30 내가 적합한 모델이 맞는지 확인할 필요도 없다31는 장점이 있다.\n아래는 나름 PSD를 추정하는 신기술인 것 같다.\n\n\n\n그림12.4(교재에서 긁어온 그림): PSD estimation from a subset of nodes. Estimators are based on a random process defined on the Karate club network (Zachary 1977). (A) Graph sampling for nonparametric PSD estimation. Here, 20 out of 34 nodes are observed. The sampled nodes are highlighted by the circles around the nodes. (B) Nonparametric PSD estimation based on observations from 20 nodes and 100 data snapshots. (C) Graph sampling for parametric MA PSD estimation. Here, 4 out of 34 nodes are observed. (D) Parametric MA PSD estimation based on observations from 4 nodes and 100 data snapshots.\n\n\n그래프신호의 sub-sampling을 이용하는 것 같은데 교재의 뒤쪽에 서술되어있다. \\(R=100\\)임을 고려하여도 퍼포먼스가 좋은 편인듯 하다32."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "href": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "title": "[CGSP] Chap 12.4: Node Subsampling for PSD Estimation",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics\n\n\ncolumnwise_kron = \n(C,D) -> hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#49 (generic function with 1 method)\n\n\n\n12.4.1 The Sampling Problem\n아래와 같이 길이가 \\(N=10\\) 인 신호 \\({\\bf x}\\)를 고려하자.\n\nx = rand(10)\n\n10-element Vector{Float64}:\n 0.03235208758206609\n 0.5069925854414447\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n 0.24116013388795854\n 0.8439116925218157\n 0.6362602319916778\n 0.386069828675059\n 0.5313655894235898\n\n\n여기에서 1,3,4,5 번째 원소만 추출하여길이가 \\(K=4\\) 인 신호 \\({\\bf y}\\)를 만들고 싶다.\n\ny = x[[1,3,4,5]]\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n이 과정은 아래와 같이 수행할 수도 있다.\n\nΦ= [1 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0]\n\n4×10 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0\n 0  0  0  1  0  0  0  0  0  0\n 0  0  0  0  1  0  0  0  0  0\n\n\n\nΦ*x\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n즉 적당한 \\(K\\times N\\) selection matrix를 선언하여 subsampling을 수행할 수 있다. 이때 매트릭스 \\({\\bf \\Phi}\\)를 subsampling matrix 혹은 sparse sampling matrix 라고 부른다.\n\n\n12.4.2 Compressed LS Estimator\n\nN = 10\nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |> \n    x -> reshape(x,(N,N)) .|> \n    x -> exp(im * (2π/N) * x) \n\n10×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n\n\n\nG = columnwise_kron(conj(V),V)\n\n100×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im   0.809017-0.587785im     …   0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n    ⋮                                ⋱  \n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.809017+0.587785im     …   0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0-1.11022e-16im          -1.0+2.27596e-15im\n 1.0+0.0im  -0.809017-0.587785im     …  -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n\n\n- 방법1\n\nĉx = vec(x*x')\np̂ = inv(G' * G) * G' * ĉx\n\n10-element Vector{ComplexF64}:\n    0.25854107856772546 + 2.245922875954761e-20im\n   0.004743491121735806 - 1.3138893409553828e-18im\n   0.006946482731189413 - 9.791191432641327e-19im\n   0.001721693617954179 - 1.9827974128203887e-18im\n   0.011344167525098774 + 2.6827005818057562e-19im\n 0.00012662617844242917 - 3.748573865136995e-20im\n   0.011344167525098762 + 2.7448152053954017e-18im\n  0.0017216936179541913 - 9.35534609073096e-19im\n   0.006946482731189404 + 1.954408900185458e-18im\n   0.004743491121735756 - 2.561030398375897e-18im\n\n\n- 방법2\n\nĉy = vec(y*y')\np̂ = (kron(Φ,Φ)*G)' * ĉy\n\n10-element Vector{ComplexF64}:\n   3.759462826821233 + 0.0im\n   2.765185174577697 - 2.0816681711721685e-17im\n   1.077337414764992 + 2.7755575615628914e-17im\n 0.11594812606807317 + 2.0816681711721685e-17im\n 0.08838298603932843 + 3.903127820947816e-17im\n 0.32863702713833354 + 4.622231866529366e-33im\n 0.08838298603932859 + 9.540979117872439e-18im\n  0.1159481260680729 - 2.0816681711721685e-17im\n  1.0773374147649915 + 0.0im\n  2.7651851745776965 - 2.0816681711721685e-17im"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "using LinearAlgebra, DSP"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Simultaneously Diagonalizable",
    "text": "Simultaneously Diagonalizable\n매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 대각화 가능하다는 것은 아래의 표현을 만족하는 적당한 invertible matrix \\({\\bf \\Psi}_A\\), \\({\\bf \\Psi}_B\\)와 대각행렬 \\({\\bf \\Lambda}_A\\), \\({\\bf \\Lambda}_B\\)가 존재한다는 의미가 된다.\n\\[{\\bf A} = {\\bf V}_{A} {\\bf \\Lambda}_A {\\bf V}_{A}^{-1}\\]\n\\[{\\bf B} = {\\bf V}_{B} {\\bf \\Lambda}_B {\\bf V}_{B}^{-1}\\]\n그리고 만약에 \\({\\bf V}_{A}={\\bf V}_{B}\\)이라면 즉\n\\[{\\bf A} = {\\bf V} {\\bf \\Lambda}_A {\\bf V}^{-1}\\]\n\\[{\\bf B} = {\\bf V} {\\bf \\Lambda}_B {\\bf V}^{-1}\\]\n이라면 \\(\\{{\\bf A},{\\bf B}\\}\\)가 simultaneously diagonalzable 하다고 표현한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Commute",
    "text": "Commute\n두 matrix \\({\\bf A}\\)와 \\({\\bf B}\\)에 대하여\n\\[{\\bf A}{\\bf B}= {\\bf B}{\\bf A}\\]\n인 관계가 성립하면 두 매트릭스가 commute 한다고 표현한다. 그런데 \\({\\bf A}{\\bf B}={\\bf A}{\\bf B}\\)의 조건은 \\({\\bf A}, {\\bf B}\\)가 동시대각화가능할 (simultaneously diagonalzable) 조건과 같다. 1 따라서 simultaneously diagonalzable 는 commute와 같은 말이라 생각해도 무방하다.\n\n참고: 위키피디아.."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Shift Invariant Filter",
    "text": "Shift Invariant Filter\n\nref: Djuric and Richard (2018) Chap 8.3 의 내용 중 일부\n\nDefine the matrix \\({\\bf B}\\) as periodic shift matrix such that\n\\[\n{\\bf B} = \\begin{bmatrix}\n0 & 0 & 0 & \\dots  & 0 & 1 \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\n0 & 0 & \\dots & 1 & 0 & 0 \\\\\n0 & 0 & \\dots & 0 & 1 & 0 \\\\\n\\end{bmatrix}.\\]\nA generic filter \\({\\boldsymbol h}\\) is given by its \\(z\\)-transform\n\\[h(z)=h_0z^0+h_1z^{-1}+\\cdots +h_{N-1}z^{-(N-1)}\\]\nwhere \\(s_{n-1}=z^{-1}s_n\\). In vector notation, and with respect to the standard basis \\({\\bf I}\\), the filter is represented by the matrix \\({\\bf H}\\), a polynomial in the cyclic shift\n\\[{\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+\\cdots+h_{N-1}{\\bf B}^{N-1}.\\]\nFilters are shift invariant iff\n\\[z\\cdot h(z) = h(z)\\cdot z\\]\nor from the matrix representation\n\\[{\\bf B}h({\\bf B})=h({\\bf B}){\\bf B}.\\]\nExample\nLet \\({\\bf B}\\) as\n\nB= [0 1 0 0 0 0 0\n    0 0 1 0 0 0 0 \n    0 0 0 1 0 0 0 \n    0 0 0 0 1 0 0 \n    0 0 0 0 0 1 0 \n    0 0 0 0 0 0 1 \n    1 0 0 0 0 0 0]\n\n7×7 Matrix{Int64}:\n 0  1  0  0  0  0  0\n 0  0  1  0  0  0  0\n 0  0  0  1  0  0  0\n 0  0  0  0  1  0  0\n 0  0  0  0  0  1  0\n 0  0  0  0  0  0  1\n 1  0  0  0  0  0  0\n\n\nDefine \\({\\boldsymbol h}\\) as\n\nh = [1/3,1/3,1/3]\n\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\n\nFurthermore define \\({\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+h_2{\\bf B}^2\\)\n\nH = (1/3)*B^0 + (1/3)*B^1 + (1/3)*B^2 \n\n7×7 Matrix{Float64}:\n 0.333333  0.333333  0.333333  0.0       0.0       0.0       0.0\n 0.0       0.333333  0.333333  0.333333  0.0       0.0       0.0\n 0.0       0.0       0.333333  0.333333  0.333333  0.0       0.0\n 0.0       0.0       0.0       0.333333  0.333333  0.333333  0.0\n 0.0       0.0       0.0       0.0       0.333333  0.333333  0.333333\n 0.333333  0.0       0.0       0.0       0.0       0.333333  0.333333\n 0.333333  0.333333  0.0       0.0       0.0       0.0       0.333333\n\n\nObserve following:\n\nB*H == H*B \n\ntrue\n\n\nThus, filter \\({\\boldsymbol h}\\) is shift invariant filter and matrix \\({\\bf H}\\) is shift invariant operator.\nnote: \\({\\boldsymbol h}\\) is moving average filter.\nnote: for any \\({\\bf x}\\), \\({\\bf H}{\\bf x}\\) is definded by\n\\[\\left[\\frac{x_{n-1}+x_n+x_1}{3},\\frac{x_n+x_1+x_2}{3},\\dots,\\frac{x_{n-3}+x_{n-2}+x_n}{3}\\right].\\]\n\nx = [1,1,1,1,2,2,2]\nH*x\n\n7-element Vector{Float64}:\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666665\n 2.0\n 1.6666666666666665\n 1.3333333333333333\n\n\nnote: In some sense, the matrix \\({\\bf H}{\\bf x}\\) can be thought as generalized version of \\({\\boldsymbol h}\\star {\\bf x}\\) where \\(\\star\\) is convolution up to shift\n\nconv(h,x)\n\n9-element Vector{Float64}:\n 0.3333333333333334\n 0.6666666666666667\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666667\n 2.0\n 1.3333333333333333\n 0.6666666666666667\n\n\nFinally, we observe that, from the Cayley-Hamilton Theorem, \\({\\bf B}\\) satisfies its characteristic polynomial \\(\\Delta({\\bf B})\\), where \\(\\Delta(\\lambda)\\) is the determinant of \\(\\lambda{\\bf I}-{\\bf B}\\). The characteristic polynomial \\(\\Delta({\\bf B})\\) has degree \\(N\\), so, in DSP, as described so far, linear filters are (matrix) polynomial with degree at most \\(N-1\\).\n\n이 부분은 책에 써있길래 가져오긴 했는데, 무슨 의미인지 모르겠음"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Coexisting Approaches",
    "text": "Coexisting Approaches\nStationary graph processes were first defined and analyzed in (Girault 2015). The fundamental problem identified there is that GSOs do not preserve energy in general and therefore cannot be isometric (Gavili and Zhang 2017). This problem is addressed in (Girault, Gonçalves, and Fleury 2015) with the definition of an isometric graph shift that preserves the eigenvector space of the Laplacian GSO but modifies its eigenvalues.\nA stationary graph process is then defined as one whose probability distributions are invariant with respect to multiplications with the isometric shift. One drawback of this approach is that the isometric shift is a complex-valued operator and has a sparsity structure (if any) different from \\({\\bf S}\\). By contrast, the vertex-based definition in\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf x}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf x}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf x}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf x} \\Big)^H \\bigg]\\]\nis based on the original GSO \\({\\bf S}\\), which is local and real-valued. As a result, above Eq. provides intuition on the relations between stationarity and locality, which can be leveraged to develop stationarity tests or estimation schemes that work with local information. Graph stationarity was also studied in (Perraudin and Vandergheynst 2017) where the requirement of having a covariance matrix diagonalizable by the eigenvectors of the Laplacian GSO is adopted as a definition. This condition is shown to be equivalent to statistical invariance with respect to the translation operator introduced in (Shuman, Ricaud, and Vandergheynst 2016). When the shift \\({\\bf S}\\) coincides with the Laplacian of the graph and the eigenvalues of \\({\\bf S}\\) are all distinct, Definitions 12.1 and 12.2 are equivalent to those in Perraudin and Vandergheynst (2017). Hence, the definitions presented here differ from (Perraudin and Vandergheynst 2017) in that we consider general normal shifts instead of Laplacians and that we see Definition 12.1 as a definition, not a property. These are mathematically minor differences that are important in practice though; see Segarra et al. (2017) for more details."
  },
  {
    "objectID": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "href": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "title": "[PL] Lesson1: 단순선형회귀",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport pytorch_lightning as pl \n\n\nref\nref: https://guebin.github.io/DL2022/posts/II.%20DNN/2022-09-20-3wk-2.html\n\n\nRegression 1: CPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1)\n\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1767: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n  category=PossibleUserWarning,\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nRegression 2: GPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1, accelerator='gpu', devices=1)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html",
    "href": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html",
    "title": "[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec",
    "section": "",
    "text": "import matplotlib.pyplot as plt\ndef draw_graph(G, pos_nodes, node_names={}, node_size=50, plot_weight=False):\n    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray', arrowsize=30)\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n    \n    \n    if plot_weight:\n        pos_attrs = {}\n        for node, coords in pos_nodes.items():\n            pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n        nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n        edge_labels=dict([((a,b,),d[\"weight\"]) for a,b,d in G.edges(data=True)])\n        nx.draw_networkx_edge_labels(G, pos_nodes, edge_labels=edge_labels)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#node2vec-example",
    "href": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#node2vec-example",
    "title": "[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec",
    "section": "Node2Vec example",
    "text": "Node2Vec example\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=7, m2=4)\ndraw_graph(G, nx.spring_layout(G))\n\nnode2vec = Node2Vec(G, dimensions=2)\nmodel = node2vec.fit(window=10)\n\nComputing transition probabilities: 100%|██████████| 18/18 [00:00<00:00, 1941.31it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:00<00:00, 13.25it/s]\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = model.wv.get_vector(str(x))\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#edge2vec-example",
    "href": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#edge2vec-example",
    "title": "[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec",
    "section": "Edge2Vec example",
    "text": "Edge2Vec example\n\nfrom node2vec.edges import HadamardEmbedder\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#graph2vec-example",
    "href": "posts/2_Studies/GML/Chapter02/01_embedding_examples.html#graph2vec-example",
    "title": "[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec",
    "section": "Graph2Vec Example",
    "text": "Graph2Vec Example\n\nimport random\nimport matplotlib.pyplot as plt\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)\n    k = random.randint(5, n)\n    p = random.uniform(0, 1)\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10)\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter08/01_Credit_card_edges_classification.html",
    "href": "posts/2_Studies/GML/Chapter08/01_Credit_card_edges_classification.html",
    "title": "[GML] Chap8: 신용카드 거래에 대한 그래프 분석",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"/Users/claudiostamile/Downloads/fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf\n\n\ndf[\"is_fraud\"].value_counts()\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x:node_id for node_id,x in enumerate(set(df[\"cc_num\"].values.tolist() + df[\"merchant\"].values.tolist()))}\n    df[\"from\"] = df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"to\"] = df[\"merchant\"].apply(lambda x: mapping[x])\n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from', 'to']).agg({\"is_fraud\": \"sum\", \"amt\": \"sum\"}).reset_index()\n    df[\"is_fraud\"] = df[\"is_fraud\"].apply(lambda x: 1 if x>0 else 0)\n    \n    G = nx.from_edgelist(df[[\"from\", \"to\"]].values, create_using=graph_type)\n    \n    nx.set_node_attributes(G,{x:1 for x in df[\"from\"].unique()}, \"bipartite\")\n    nx.set_node_attributes(G,{x:2 for x in df[\"to\"].unique()}, \"bipartite\")\n    \n    nx.set_edge_attributes(G, \n                       {(int(x[\"from\"]), int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \n                       \"label\")\n\n    nx.set_edge_attributes(G, \n                       {(int(x[\"from\"]), int(x[\"to\"])):x[\"amt\"] for idx, x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \n                       \"weight\")\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x:node_id for node_id,x in enumerate(set(df.index.values.tolist() + \n                                                        df[\"cc_num\"].values.tolist() + \n                                                        df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"] = df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"] = df[\"merchant\"].apply(lambda x: mapping[x])\n\n    G = nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\n                         [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], \n                         create_using=graph_type)\n\n    nx.set_node_attributes(G,{x[\"in_node\"]:1 for idx,x in df.iterrows()}, \"bipartite\")\n    nx.set_node_attributes(G,{x[\"out_node\"]:2 for idx,x in df.iterrows()}, \"bipartite\")\n    nx.set_node_attributes(G,{mapping[idx]:3 for idx, x in df.iterrows()}, \"bipartite\")\n\n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n\n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    return G\n\n\nG = build_graph_bipartite(df, nx.Graph())\n\n\nfrom networkx.algorithms import bipartite\nbipartite.is_bipartite(G)\n\n\nplt.figure(figsize=(10,10))\ntop = nx.bipartite.sets(G)[0]\npos = nx.bipartite_layout(G, top)\nnx.draw(G, pos=pos, with_labels=False, node_color=default_node_color, edge_color=default_edge_color)\nplt.show()\n\n\nplt.axis(\"off\")\nplt.figure(figsize=(10,10))\n\nnx.draw_networkx(G, pos=spring_pos, node_color=default_node_color, \n                 edges_color=default_edge_color, with_labels=False, node_size=15)\n\n\n\n\n\nprint(nx.info(G))\n\n\nplt.figure(figsize=(10,10))\ndegrees = pd.Series({k: v for k, v in nx.degree(G)})\ndegrees.plot.hist()\nplt.yscale(\"log\")\n\n\nallEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in G.edges(data=True)})\nnp.quantile(allEdgesWeights.values,[0.10,0.50,0.70,0.9,1.0])\n\n\nquant_dist = np.quantile(allEdgesWeights.values,[0.10,0.50,0.70,0.9])\nquant_dist\n\n\nallEdgesWeightsFiltered = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in G.edges(data=True) \n                                     if d[2][\"weight\"] < quant_dist[-1]})\n\n\nplt.figure(figsize=(10,10))\nallEdgesWeightsFiltered.plot.hist(bins=40)\nplt.yscale(\"log\")\n\n\nplt.figure(figsize=(10,10))\nbC = nx.betweenness_centrality(G)\nbc_distr = pd.Series(bC)\nbc_distr.plot.hist()\nplt.yscale(\"log\")\n\n\nnp.mean(list(bC.values()))\n\n\n# degree centrality\nplt.figure(figsize=(10,10))\ndeg_C = nx.degree_centrality(G)\ndegc_distr = pd.Series(deg_C)\ndegc_distr.plot.hist()\n\n\n# closeness centrality\nplt.figure(figsize=(10,10))\nclos_C = nx.closeness_centrality(G)\nclosc_distr = pd.Series(clos_C)\nclosc_distr.plot.hist()\n\n\nnp.mean(list(clos_C.values()))\n\n\n# assortativity\nnx.degree_pearson_correlation_coefficient(G)\n\n\n\n\nimport community\n\nparts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\ncommunities.value_counts().sort_values(ascending=False)\n\n\ngraphs = []\nd = {}\nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n\n\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter07/02_supervised_classification-embeddings.html",
    "href": "posts/2_Studies/GML/Chapter07/02_supervised_classification-embeddings.html",
    "title": "[GML] Chap7: Shallow-Learning Topic Modelling",
    "section": "",
    "text": "Shallow-Learning Topic Modelling\nIn the following we will show you how to create a topic model, using a shallow-learning approach. Here we will use the results and the embeddings obtained from the document-document projection of the bipartite graph.\nNOTE: This Notebook can only be run after the 01_nlp_graph_creation notebook, as some of the results computed in the first notebook will be here reused.\n\nLoad Dataset\n\nimport pandas as pd\n\n\ncorpus = pd.read_pickle(\"corpus.p\")\n\n\nfrom collections import Counter\ntopics = Counter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common(10)\n\n\ntopics\n\n[('earn', 3964),\n ('acq', 2369),\n ('money-fx', 717),\n ('grain', 582),\n ('crude', 578),\n ('trade', 485),\n ('interest', 478),\n ('ship', 286),\n ('wheat', 283),\n ('corn', 237)]\n\n\n\ntopicsList = [topic[0] for topic in topics]\ntopicsSet = set(topicsList)\ndataset = corpus[corpus[\"label\"].apply(lambda x: len(topicsSet.intersection(x))>0)]\n\nCreate a class to “simulate” the training of the embeddings\n\nfrom sklearn.base import BaseEstimator\n\nclass EmbeddingsTransformer(BaseEstimator):\n    \n    def __init__(self, embeddings_file):\n        self.embeddings_file = embeddings_file\n        \n    def fit(self, *args, **kwargs):\n        self.embeddings = pd.read_pickle(self.embeddings_file)\n        return self\n        \n    def transform(self, X):\n        return self.embeddings.loc[X.index]\n    \n    def fit_transform(self, X, y):\n        return self.fit().transform(X)\n\n\n\nfrom glob import glob \nfiles = glob(\"./embeddings/*\")\n\n\ngraphEmbeddings = EmbeddingsTransformer(files[0]).fit()\n\nTrain/Test split\n\ndef get_labels(corpus, topicsList=topicsList):\n    return corpus[\"label\"].apply(\n        lambda labels: pd.Series({label: 1 for label in labels}).reindex(topicsList).fillna(0)\n    )[topicsList]\n\n\ndef get_features(corpus):\n    return corpus[\"parsed\"] #graphEmbeddings.transform(corpus[\"parsed\"])\n\n\ndef get_features_and_labels(corpus):\n    return get_features(corpus), get_labels(corpus)\n\n\ndef train_test_split(corpus):\n    graphIndex = [index for index in corpus.index if index in graphEmbeddings.embeddings.index]\n    \n    train_idx = [idx for idx in graphIndex if \"training/\" in idx]\n    test_idx = [idx for idx in graphIndex if \"test/\" in idx]\n    return corpus.loc[train_idx], corpus.loc[test_idx]\n\n\ntrain, test = train_test_split(dataset)\n\nBuild the model and cross-validation\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.multioutput import MultiOutputClassifier\n\n\nmodel = MultiOutputClassifier(RandomForestClassifier())\n\n\npipeline = Pipeline([\n    (\"embeddings\", graphEmbeddings),\n    (\"model\", model)\n])\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nfiles\n\n['./bipartiteGraphEmbeddings_20_10.p',\n './bipartiteGraphEmbeddings_10.p',\n './bipartiteGraphEmbeddings_20_30.p',\n './bipartiteGraphEmbeddings_20.p',\n './bipartiteGraphEmbeddings_20_20.p',\n './bipartiteGraphEmbeddings_30.p']\n\n\n\nparam_grid = {\n    \"embeddings__embeddings_file\": files,\n    \"model__estimator__n_estimators\": [50, 100], \n    \"model__estimator__max_features\": [0.2,0.3, \"auto\"], \n    #\"model__estimator__max_depth\": [3, 5]\n}\n\n\nfeatures, labels = get_features_and_labels(train)\n\n\nfrom sklearn.metrics import f1_score \n\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1, \n                           scoring=lambda y_true, y_pred: f1_score(y_true, y_pred,average='weighted'))\n\n\nmodel = grid_search.fit(features, labels)\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n  category=UserWarning\n\n\n\nmodel\n\nGridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('embeddings',\n                                        EmbeddingsTransformer(embeddings_file='bipartiteGraphEmbeddings_20.p')),\n                                       ('model',\n                                        MultiOutputClassifier(estimator=RandomForestClassifier(class_weight='balanced')))]),\n             n_jobs=-1,\n             param_grid={'embeddings__embeddings_file': ['./bipartiteGraphEmbeddings_20_10.p',\n                                                         './bipartiteGraphEmbeddings_10.p',\n                                                         './bipartiteGraphEmbeddings_20_30.p',\n                                                         './bipartiteGraphEmbeddings_20.p',\n                                                         './bipartiteGraphEmbeddings_20_20.p',\n                                                         './bipartiteGraphEmbeddings_30.p'],\n                         'model__estimator__max_features': [0.2, 0.3, 'auto'],\n                         'model__estimator__n_estimators': [50, 100]},\n             scoring=<function <lambda> at 0x14af7ee60>)\n\n\n\nmodel.best_params_\n\n{'embeddings__embeddings_file': './bipartiteGraphEmbeddings_20_10.p',\n 'model__estimator__max_features': 0.2,\n 'model__estimator__n_estimators': 50}\n\n\nEvaluate performance\n\ndef get_predictions(model, features):\n    return pd.DataFrame(\n        model.predict(features), \n        columns=topicsList, \n        index=features.index\n    )\n\n\npreds = get_predictions(model, get_features(test))\nlabels = get_labels(test)\n\n\nerrors = 1 - (labels - preds).abs().sum().sum() / labels.abs().sum().sum()\n\n\nerrors\n\n0.6702547542160029\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint(classification_report(labels, preds))\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.94      0.95      1087\n           1       0.93      0.74      0.83       719\n           2       0.79      0.45      0.57       179\n           3       0.96      0.64      0.77       149\n           4       0.95      0.59      0.73       189\n           5       0.95      0.45      0.61       117\n           6       0.87      0.41      0.56       131\n           7       0.83      0.21      0.34        89\n           8       0.69      0.34      0.45        71\n           9       0.61      0.25      0.35        56\n\n   micro avg       0.94      0.72      0.81      2787\n   macro avg       0.85      0.50      0.62      2787\nweighted avg       0.92      0.72      0.79      2787\n samples avg       0.76      0.75      0.75      2787\n\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter07/03_supervised_classsification_graphSAGE-TFIDF.html",
    "href": "posts/2_Studies/GML/Chapter07/03_supervised_classsification_graphSAGE-TFIDF.html",
    "title": "[GML] Chap7: Graph Neural Network Topic Classifier",
    "section": "",
    "text": "In the following we will focus on building a model for topic classification based on a Graph Neural Network approach.\nIn particular in the following we will show you how to:\n\nCreate a TF-IDF representation of the corpus, that will be used as node features in the Graph Neural Network model\nBuild, train a Graph Neural Network model and identify the best threshold for classifying documents\nTest the performance of the model in a out-of-sample tests, following a truly inductive approach\n\nNOTE: This Notebook can only be run after the 01_nlp_graph_creation notebook, as some of the results computed in the first notebook will be here reused.\n\n\n\nimport nltk \n\n\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\n\n\ncorpus = pd.read_pickle(\"corpus.p\")\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      label\n      clean_text\n      parsed\n      language\n    \n    \n      id\n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      [trade]\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U.S.-JA...\n      en\n    \n    \n      test/14828\n      [grain]\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...\n      en\n    \n    \n      test/14829\n      [crude, nat-gas]\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...\n      en\n    \n    \n      test/14832\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...\n      en\n    \n    \n      test/14833\n      [palm-oil, veg-oil]\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...\n      en\n    \n  \n\n\n\n\n\nfrom collections import Counter\ntopics = Counter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common(10)\n\n\ntopics\n\n[('earn', 3964),\n ('acq', 2369),\n ('money-fx', 717),\n ('grain', 582),\n ('crude', 578),\n ('trade', 485),\n ('interest', 478),\n ('ship', 286),\n ('wheat', 283),\n ('corn', 237)]\n\n\n\ntopicsList = [topic[0] for topic in topics]\ntopicsSet = set(topicsList)\ndataset = corpus[corpus[\"label\"].apply(lambda x: len(topicsSet.intersection(x))>0)]\n\n\ndef get_labels(corpus, topicsList=topicsList):\n    return corpus[\"label\"].apply(\n        lambda labels: pd.Series({label: 1 for label in labels}).reindex(topicsList).fillna(0)\n    )[topicsList]\n\n\nlabels = get_labels(dataset)\n\n\nlabels.head()\n\n\n\n\n\n  \n    \n      \n      earn\n      acq\n      money-fx\n      grain\n      crude\n      trade\n      interest\n      ship\n      wheat\n      corn\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/14828\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/14829\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/14832\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      test/14839\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ndef get_features(corpus):\n    return corpus[\"parsed\"]\n\n\ndef get_features_and_labels(corpus):\n    return get_features(corpus), get_labels(corpus)\n\n\ndef train_test_split(corpus):\n    train_idx = [idx for idx in corpus.index if \"training/\" in idx]\n    test_idx = [idx for idx in corpus.index if \"test/\" in idx]\n    return corpus.loc[train_idx], corpus.loc[test_idx]\n\n\ntrain, test = train_test_split(dataset)\n\n\ndef my_spacy_tokenizer(pos_filter=[\"NOUN\", \"VERB\", \"PROPN\"]):\n    def tokenizer(doc):\n        return [token.lemma_ for token in doc if (pos_filter is None) or (token.pos_ in pos_filter)] \n    return tokenizer\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\ncntVectorizer = TfidfVectorizer(\n    analyzer=my_spacy_tokenizer(),\n    max_df = 0.25, min_df = 2, max_features = 10000\n)\n\n\ntrainFeatures, _ = get_features_and_labels(train)\ntestFeatures, _ = get_features_and_labels(test)\n\n\ntrainedTransformed = cntVectorizer.fit_transform(trainFeatures)\ntestTransformed = cntVectorizer.transform(testFeatures)\n\n\nfeatures = pd.concat([\n    pd.DataFrame.sparse.from_spmatrix(trainedTransformed, index=trainFeatures.index), \n    pd.DataFrame.sparse.from_spmatrix(testTransformed, index=testFeatures.index)\n])\n\n\nfeatures.shape\n\n(9034, 10000)\n\n\nCreating the Graph\n\nimport stellargraph as sg\nfrom stellargraph import StellarGraph, IndexedArray\nfrom stellargraph.mapper import GraphSAGENodeGenerator\nfrom stellargraph.layer import GraphSAGE\n\nfrom tensorflow.keras import layers, optimizers, losses, metrics, Model\n\n\nedges = pd.read_pickle(\"bipartiteEdges.p\")\n\n\nentityTypes = {entity: ith for ith, entity in enumerate(edges[\"type\"].unique())}\n\n\nentityTypes\n\n{'keywords': 0, 'GPE': 1, 'ORG': 2, 'PERSON': 3}\n\n\n\ndocumentFeatures = features.loc[set(corpus.index).intersection(features.index)] #.assign(document=1, entity=0)\n\n\ndocumentFeatures.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      9990\n      9991\n      9992\n      9993\n      9994\n      9995\n      9996\n      9997\n      9998\n      9999\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      training/9238\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/15296\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/15287\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      training/5938\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/21465\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows × 10000 columns\n\n\n\n\nentities = edges.groupby([\"target\", \"type\"])[\"source\"].count().groupby(level=0).apply(\n    lambda s: s.droplevel(0).reindex(entityTypes.keys()).fillna(0)\n).unstack(level=1)\n\n\nentityFeatures = (entities.T / entities.sum(axis=1)).T.assign(document=0, entity=1)\n\n\nnodes = {\"entity\": entityFeatures, \n         \"document\": documentFeatures}\n\n\nstellarGraph = StellarGraph(nodes, \n                            edges[edges[\"source\"].isin(documentFeatures.index)], \n                            edge_type_column=\"type\")\n\n\nprint(stellarGraph.info())\n\nStellarGraph: Undirected multigraph\n Nodes: 23998, Edges: 86849\n\n Node types:\n  entity: [14964]\n    Features: float32 vector, length 6\n    Edge types: entity-GPE->document, entity-ORG->document, entity-PERSON->document, entity-keywords->document\n  document: [9034]\n    Features: float32 vector, length 10000\n    Edge types: document-GPE->entity, document-ORG->entity, document-PERSON->entity, document-keywords->entity\n\n Edge types:\n    document-keywords->entity: [78838]\n        Weights: range=[0.0827011, 1], mean=0.258464, std=0.0898612\n        Features: none\n    document-ORG->entity: [4129]\n        Weights: range=[2, 22], mean=3.24122, std=2.30508\n        Features: none\n    document-GPE->entity: [2943]\n        Weights: range=[2, 25], mean=3.25926, std=2.07008\n        Features: none\n    document-PERSON->entity: [939]\n        Weights: range=[2, 14], mean=2.97444, std=1.65956\n        Features: none\n\n\n\nfrom stellargraph.data import EdgeSplitter\n\n\nsplitter = EdgeSplitter(stellarGraph)\n\n\ngraphTest, samplesTest, labelsTest = splitter.train_test_split(p=0.2)\n\n** Sampled 17369 positive and 17369 negative edges. **\n\n\n\nprint(stellarGraph.info())\n\nStellarGraph: Undirected multigraph\n Nodes: 23998, Edges: 86849\n\n Node types:\n  entity: [14964]\n    Features: float32 vector, length 6\n    Edge types: entity-GPE->document, entity-ORG->document, entity-PERSON->document, entity-keywords->document\n  document: [9034]\n    Features: float32 vector, length 10000\n    Edge types: document-GPE->entity, document-ORG->entity, document-PERSON->entity, document-keywords->entity\n\n Edge types:\n    document-keywords->entity: [78838]\n        Weights: range=[0.0827011, 1], mean=0.258464, std=0.0898612\n        Features: none\n    document-ORG->entity: [4129]\n        Weights: range=[2, 22], mean=3.24122, std=2.30508\n        Features: none\n    document-GPE->entity: [2943]\n        Weights: range=[2, 25], mean=3.25926, std=2.07008\n        Features: none\n    document-PERSON->entity: [939]\n        Weights: range=[2, 14], mean=2.97444, std=1.65956\n        Features: none\n\n\n\nprint(graphTest.info())\n\nStellarGraph: Undirected multigraph\n Nodes: 23998, Edges: 69480\n\n Node types:\n  entity: [14964]\n    Features: float32 vector, length 6\n    Edge types: entity-GPE->document, entity-ORG->document, entity-PERSON->document, entity-keywords->document\n  document: [9034]\n    Features: float32 vector, length 10000\n    Edge types: document-GPE->entity, document-ORG->entity, document-PERSON->entity, document-keywords->entity\n\n Edge types:\n    document-keywords->entity: [63057]\n        Weights: range=[0.0827011, 1], mean=0.258427, std=0.0899773\n        Features: none\n    document-ORG->entity: [3296]\n        Weights: range=[2, 22], mean=3.21572, std=2.2592\n        Features: none\n    document-GPE->entity: [2360]\n        Weights: range=[2, 19], mean=3.24237, std=2.01535\n        Features: none\n    document-PERSON->entity: [767]\n        Weights: range=[2, 14], mean=3, std=1.69163\n        Features: none\n\n\n\n\n\nWe start by splitting the data into train, validation and test\n\ntargets = labels.reindex(documentFeatures.index).fillna(0)\n#documentFeatures.drop([\"entity\", \"document\"], axis=1)\n\n\ntargets.head()\n\n\n\n\n\n  \n    \n      \n      earn\n      acq\n      money-fx\n      grain\n      crude\n      trade\n      interest\n      ship\n      wheat\n      corn\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/16678\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      test/15913\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      training/12032\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      training/8366\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      training/10454\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ndef train_test_split(corpus):\n    graphIndex = [index for index in corpus.index]\n    \n    train_idx = [idx for idx in graphIndex if \"training/\" in idx]\n    test_idx = [idx for idx in graphIndex if \"test/\" in idx]\n    return corpus.loc[train_idx], corpus.loc[test_idx]\n\n\nsampled, hold_out = train_test_split(targets)\n\n\nallNeighbors = np.unique([n for node in sampled.index for n in stellarGraph.neighbors(node)])\n\n\nsubgraph = stellarGraph.subgraph(set(sampled.index).union(allNeighbors))\n\n\nprint(subgraph.info())\n\nStellarGraph: Undirected multigraph\n Nodes: 16927, Edges: 62454\n\n Node types:\n  entity: [10438]\n    Features: float32 vector, length 6\n    Edge types: entity-GPE->document, entity-ORG->document, entity-PERSON->document, entity-keywords->document\n  document: [6489]\n    Features: float32 vector, length 10000\n    Edge types: document-GPE->entity, document-ORG->entity, document-PERSON->entity, document-keywords->entity\n\n Edge types:\n    document-keywords->entity: [56647]\n        Weights: range=[0.0918226, 1], mean=0.25739, std=0.0888008\n        Features: none\n    document-ORG->entity: [3032]\n        Weights: range=[2, 22], mean=3.20877, std=2.21143\n        Features: none\n    document-GPE->entity: [2104]\n        Weights: range=[2, 25], mean=3.25808, std=2.08119\n        Features: none\n    document-PERSON->entity: [671]\n        Weights: range=[2, 14], mean=2.97615, std=1.66958\n        Features: none\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain, leftOut = train_test_split(\n    sampled,\n    train_size=0.1,\n    test_size=None,\n    random_state=42,\n)\n\nvalidation, test = train_test_split(\n    leftOut, train_size=0.2, test_size=None, random_state=100,\n)\n\n\nvalidation = validation[validation.sum(axis=1) > 0]\ntest = test[test.sum(axis=1) > 0]\n\n\nprint(f\"Validation: {validation.shape}\")\nprint(f\"Test: {test.shape}\")\n\nValidation: (1168, 10)\nTest: (4673, 10)\n\n\n\n\nWe start by creating the model\n\nbatch_size = 50\nnum_samples = [10, 5]\n\n\nfrom stellargraph.mapper import HinSAGENodeGenerator\n\ngenerator = HinSAGENodeGenerator(subgraph, batch_size, num_samples, head_node_type=\"document\")\n\n\nfrom stellargraph.layer import HinSAGE\n\ngraphsage_model = HinSAGE(\n    layer_sizes=[32, 32], generator=generator, bias=True, dropout=0.5,\n)\n\n\nx_inp, x_out = graphsage_model.in_out_tensors()\nprediction = layers.Dense(units=train.shape[1], activation=\"sigmoid\")(x_out)\n\n\nprediction.shape\n\nTensorShape([None, 10])\n\n\n\nmodel = Model(inputs=x_inp, outputs=prediction)\nmodel.compile(\n    optimizer=optimizers.Adam(lr=0.005),\n    loss=losses.binary_crossentropy,\n    metrics=[\"acc\"],\n)\n\nWe now train the model\n\ntrain_gen = generator.flow(train.index, train, shuffle=True)\n\n\nval_gen = generator.flow(validation.index, validation)\n\n\nhistory = model.fit(\n    train_gen, epochs=50, validation_data=val_gen, verbose=1, shuffle=False\n)\n\nEpoch 1/50\n13/13 [==============================] - 215s 17s/step - loss: 0.6139 - acc: 0.1365 - val_loss: 0.4780 - val_acc: 0.4401\nEpoch 2/50\n13/13 [==============================] - 169s 13s/step - loss: 0.4675 - acc: 0.4323 - val_loss: 0.4001 - val_acc: 0.4401\nEpoch 3/50\n13/13 [==============================] - 162s 13s/step - loss: 0.3973 - acc: 0.4319 - val_loss: 0.3486 - val_acc: 0.4401\nEpoch 4/50\n13/13 [==============================] - 153s 12s/step - loss: 0.3447 - acc: 0.4604 - val_loss: 0.3124 - val_acc: 0.4401\nEpoch 5/50\n13/13 [==============================] - 144s 11s/step - loss: 0.3090 - acc: 0.4997 - val_loss: 0.2853 - val_acc: 0.4932\nEpoch 6/50\n13/13 [==============================] - 159s 13s/step - loss: 0.2886 - acc: 0.5484 - val_loss: 0.2639 - val_acc: 0.6045\nEpoch 7/50\n13/13 [==============================] - 187s 15s/step - loss: 0.2612 - acc: 0.6354 - val_loss: 0.2453 - val_acc: 0.6387\nEpoch 8/50\n13/13 [==============================] - 203s 16s/step - loss: 0.2509 - acc: 0.6294 - val_loss: 0.2307 - val_acc: 0.6404\nEpoch 9/50\n13/13 [==============================] - 178s 14s/step - loss: 0.2370 - acc: 0.6489 - val_loss: 0.2160 - val_acc: 0.6789\nEpoch 10/50\n13/13 [==============================] - 190s 15s/step - loss: 0.2155 - acc: 0.6836 - val_loss: 0.2046 - val_acc: 0.7029\nEpoch 11/50\n13/13 [==============================] - 172s 14s/step - loss: 0.2047 - acc: 0.7310 - val_loss: 0.1938 - val_acc: 0.7260\nEpoch 12/50\n13/13 [==============================] - 145s 12s/step - loss: 0.2009 - acc: 0.7208 - val_loss: 0.1846 - val_acc: 0.7509\nEpoch 13/50\n13/13 [==============================] - 167s 13s/step - loss: 0.1834 - acc: 0.7843 - val_loss: 0.1755 - val_acc: 0.7860\nEpoch 14/50\n13/13 [==============================] - 208s 17s/step - loss: 0.1787 - acc: 0.7943 - val_loss: 0.1679 - val_acc: 0.8005\nEpoch 15/50\n13/13 [==============================] - 216s 17s/step - loss: 0.1718 - acc: 0.8123 - val_loss: 0.1598 - val_acc: 0.8365\nEpoch 16/50\n13/13 [==============================] - 201s 16s/step - loss: 0.1619 - acc: 0.8612 - val_loss: 0.1531 - val_acc: 0.8416\nEpoch 17/50\n13/13 [==============================] - 173s 14s/step - loss: 0.1609 - acc: 0.8378 - val_loss: 0.1470 - val_acc: 0.8502\nEpoch 18/50\n13/13 [==============================] - 157s 12s/step - loss: 0.1496 - acc: 0.8471 - val_loss: 0.1412 - val_acc: 0.8690\nEpoch 19/50\n13/13 [==============================] - 155s 12s/step - loss: 0.1471 - acc: 0.8600 - val_loss: 0.1379 - val_acc: 0.8604\nEpoch 20/50\n13/13 [==============================] - 154s 12s/step - loss: 0.1366 - acc: 0.8801 - val_loss: 0.1318 - val_acc: 0.8767\nEpoch 21/50\n13/13 [==============================] - 155s 12s/step - loss: 0.1362 - acc: 0.8708 - val_loss: 0.1285 - val_acc: 0.8664\nEpoch 22/50\n13/13 [==============================] - 156s 12s/step - loss: 0.1361 - acc: 0.8546 - val_loss: 0.1259 - val_acc: 0.8682\nEpoch 23/50\n13/13 [==============================] - 154s 12s/step - loss: 0.1197 - acc: 0.9104 - val_loss: 0.1231 - val_acc: 0.8733\nEpoch 24/50\n13/13 [==============================] - 146s 11s/step - loss: 0.1240 - acc: 0.8834 - val_loss: 0.1175 - val_acc: 0.8844\nEpoch 25/50\n13/13 [==============================] - 131s 10s/step - loss: 0.1145 - acc: 0.9165 - val_loss: 0.1165 - val_acc: 0.8853\nEpoch 26/50\n13/13 [==============================] - 131s 10s/step - loss: 0.1216 - acc: 0.8844 - val_loss: 0.1155 - val_acc: 0.8784\nEpoch 27/50\n13/13 [==============================] - 132s 11s/step - loss: 0.1084 - acc: 0.9093 - val_loss: 0.1111 - val_acc: 0.8878\nEpoch 28/50\n13/13 [==============================] - 127s 10s/step - loss: 0.1039 - acc: 0.9156 - val_loss: 0.1095 - val_acc: 0.8853\nEpoch 29/50\n13/13 [==============================] - 128s 10s/step - loss: 0.1066 - acc: 0.9175 - val_loss: 0.1095 - val_acc: 0.8818\nEpoch 30/50\n13/13 [==============================] - 194s 16s/step - loss: 0.0987 - acc: 0.9199 - val_loss: 0.1089 - val_acc: 0.8784\nEpoch 31/50\n13/13 [==============================] - 194s 16s/step - loss: 0.0995 - acc: 0.9164 - val_loss: 0.1047 - val_acc: 0.8827\nEpoch 32/50\n13/13 [==============================] - 206s 16s/step - loss: 0.0938 - acc: 0.9322 - val_loss: 0.1030 - val_acc: 0.8818\nEpoch 33/50\n13/13 [==============================] - 199s 16s/step - loss: 0.0907 - acc: 0.9205 - val_loss: 0.1014 - val_acc: 0.8853\nEpoch 34/50\n13/13 [==============================] - 213s 17s/step - loss: 0.0918 - acc: 0.9208 - val_loss: 0.0990 - val_acc: 0.8887\nEpoch 35/50\n13/13 [==============================] - 264s 21s/step - loss: 0.0887 - acc: 0.9342 - val_loss: 0.0978 - val_acc: 0.8878\nEpoch 36/50\n13/13 [==============================] - 378s 30s/step - loss: 0.0875 - acc: 0.9170 - val_loss: 0.0956 - val_acc: 0.8955\nEpoch 37/50\n13/13 [==============================] - 247s 19s/step - loss: 0.0856 - acc: 0.9363 - val_loss: 0.0969 - val_acc: 0.8896\nEpoch 38/50\n13/13 [==============================] - 224s 17s/step - loss: 0.0777 - acc: 0.9312 - val_loss: 0.0938 - val_acc: 0.8921\nEpoch 39/50\n13/13 [==============================] - 201s 16s/step - loss: 0.0837 - acc: 0.9205 - val_loss: 0.0930 - val_acc: 0.8938\nEpoch 40/50\n13/13 [==============================] - 201s 16s/step - loss: 0.0844 - acc: 0.9180 - val_loss: 0.0917 - val_acc: 0.8938\nEpoch 41/50\n13/13 [==============================] - 197s 16s/step - loss: 0.0731 - acc: 0.9353 - val_loss: 0.0917 - val_acc: 0.8938\nEpoch 42/50\n13/13 [==============================] - 210s 17s/step - loss: 0.0732 - acc: 0.9220 - val_loss: 0.0908 - val_acc: 0.8861\nEpoch 43/50\n13/13 [==============================] - 236s 19s/step - loss: 0.0718 - acc: 0.9440 - val_loss: 0.0923 - val_acc: 0.8896\nEpoch 44/50\n13/13 [==============================] - 186s 15s/step - loss: 0.0711 - acc: 0.9581 - val_loss: 0.0912 - val_acc: 0.8861\nEpoch 45/50\n13/13 [==============================] - 169s 13s/step - loss: 0.0704 - acc: 0.9449 - val_loss: 0.0893 - val_acc: 0.8887\nEpoch 46/50\n13/13 [==============================] - 183s 15s/step - loss: 0.0768 - acc: 0.9366 - val_loss: 0.0897 - val_acc: 0.8887\nEpoch 47/50\n13/13 [==============================] - 196s 16s/step - loss: 0.0723 - acc: 0.9305 - val_loss: 0.0861 - val_acc: 0.8990\nEpoch 48/50\n13/13 [==============================] - 154s 12s/step - loss: 0.0733 - acc: 0.9289 - val_loss: 0.0873 - val_acc: 0.8964\nEpoch 49/50\n13/13 [==============================] - 228s 18s/step - loss: 0.0691 - acc: 0.9568 - val_loss: 0.0878 - val_acc: 0.8998\nEpoch 50/50\n13/13 [==============================] - 211s 17s/step - loss: 0.0625 - acc: 0.9409 - val_loss: 0.0864 - val_acc: 0.8896\n\n\n\nsg.utils.plot_history(history)\n\n\n\n\n\nhistory = model.fit(\n    train_gen, epochs=50, validation_data=val_gen, verbose=1, shuffle=False\n)\n\n\nsg.utils.plot_history(history)\n\n\n\n\n\ntest_gen = generator.flow(test.index, test)\n\n\ntest_metrics = model.evaluate(test_gen)\nprint(\"\\nTest Set Metrics:\")\nfor name, val in zip(model.metrics_names, test_metrics):\n    print(\"\\t{}: {:0.4f}\".format(name, val))\n\n94/94 [==============================] - 391s 4s/step - loss: 0.0933 - acc: 0.8795\n\nTest Set Metrics:\n    loss: 0.0933\n    acc: 0.8795\n\n\n\ntest_predictions = pd.DataFrame(model.predict(test_gen), index=test.index, columns=test.columns)\n\n\ntest_results = pd.concat({\n    \"target\": test, \n    \"preds\": test_predictions\n}, axis=1)\n\n\nfrom sklearn.metrics import f1_score, classification_report\n\n\nf1s = {}\n\nfor th in [0.01,0.05,0.1,0.2,0.3,0.4,0.5]:\n    f1s[th] = f1_score(test_results[\"target\"], 1.0*(test_results[\"preds\"]>th), average=\"macro\")\n    \npd.Series(f1s).plot()\n\n<AxesSubplot:>\n\n\n\n\n\nAs it can be seen, with a threshold of about 0.2 we obtain the best performances. We thus use this value for producing the classification report\n\nprint(classification_report(test_results[\"target\"], 1.0*(test_results[\"preds\"]>0.2)))\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94      2075\n           1       0.85      0.96      0.90      1200\n           2       0.65      0.90      0.75       364\n           3       0.83      0.95      0.89       305\n           4       0.86      0.68      0.76       296\n           5       0.74      0.56      0.63       269\n           6       0.60      0.80      0.69       245\n           7       0.62      0.10      0.17       150\n           8       0.49      0.95      0.65       149\n           9       0.44      0.88      0.58       129\n\n   micro avg       0.80      0.89      0.84      5182\n   macro avg       0.70      0.78      0.70      5182\nweighted avg       0.82      0.89      0.84      5182\n samples avg       0.83      0.90      0.85      5182\n\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\nWe now provide a prediction truly inductive, thus we will be using the full graph and we will also use the threshold of 0.2 we have identified above as the one providing the top f1-score.\n\ngenerator = HinSAGENodeGenerator(stellarGraph, batch_size, num_samples, head_node_type=\"document\")\n\n\nhold_out = hold_out[hold_out.sum(axis=1) > 0]\n\n\nhold_out_gen = generator.flow(hold_out.index, hold_out)\n\n\nhold_out_predictions = model.predict(hold_out_gen)\n\n\npreds = pd.DataFrame(1.0*(hold_out_predictions > 0.2), index=hold_out.index, columns=hold_out.columns)\n\n\nresults = pd.concat({\n    \"target\": hold_out, \n    \"preds\": preds\n}, axis=1)\n\n\nprint(classification_report(results[\"target\"], results[\"preds\"]))\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96      1087\n           1       0.90      0.97      0.93       719\n           2       0.64      0.92      0.76       179\n           3       0.82      0.95      0.88       149\n           4       0.85      0.62      0.72       189\n           5       0.74      0.50      0.59       117\n           6       0.60      0.79      0.68       131\n           7       0.43      0.03      0.06        89\n           8       0.50      0.96      0.66        71\n           9       0.39      0.86      0.54        56\n\n   micro avg       0.82      0.89      0.85      2787\n   macro avg       0.68      0.76      0.68      2787\nweighted avg       0.83      0.89      0.84      2787\n samples avg       0.84      0.90      0.86      2787\n\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter07/01_nlp_graph_creation.html",
    "href": "posts/2_Studies/GML/Chapter07/01_nlp_graph_creation.html",
    "title": "[GML] Chap7: Text Analytics and Natural Language Processing using Graphs",
    "section": "",
    "text": "In the following we will focus on analyzing textual documents and leverage on graph analysis in order to identify insight and extract relevant information.\nIn particular in the following we will show you how to:\n\nExtract structured information from text by using NLP techniques and models\nBuild different type of graphs starting from the information extracted in the previous point\nAnalyze the graph\n\n\n\n\nimport nltk \n\n\nimport numpy as np\n\nimport pandas as pd\nimport networkx as nx\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\n\nfrom nltk.corpus import reuters\n\n\ncorpus = pd.DataFrame([\n    {\"id\": _id, \"clean_text\": reuters.raw(_id).replace(\"\\n\", \"\"), \"label\": reuters.categories(_id)}\n    for _id in reuters.fileids()\n]).set_index(\"id\")\n\n\ncorpus.iloc[10][\"clean_text\"]\n\n'SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION  Mines and Energy Minister Subroto  confirmed Indonesian support for an extension of the sixth  International Tin Agreement (ITA), but said a new pact was not  necessary.      Asked by Reuters to clarify his statement on Monday in  which he said the pact should be allowed to lapse, Subroto said  Indonesia was ready to back extension of the ITA.      \"We can support extension of the sixth agreement,\" he said.  \"But a seventh accord we believe to be unnecessary.\"      The sixth ITA will expire at the end of June unless a  two-thirds majority of members vote for an extension.  '\n\n\n\nfrom collections import Counter\nlen(Counter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common())\n\n90\n\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n    \n    \n      id\n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n    \n  \n\n\n\n\n\n\n\n\nimport langdetect\n\n\nimport numpy as np\n\ndef getLanguage(text: str):\n    try:\n        return langdetect.detect(text)\n    except: \n        return np.nan\n\n\ncorpus[\"language\"] = corpus[\"clean_text\"].apply(getLanguage)\n\n\ncorpus[\"language\"].value_counts().head(10)\n\nen    9899\nsv     432\nde     371\nsw      29\nso      23\npt       7\nnl       7\nvi       6\net       5\nca       2\nName: language, dtype: int64\n\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n      language\n    \n    \n      id\n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n      en\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n      en\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n      en\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      en\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n      en\n    \n  \n\n\n\n\nUsing fasttext\n\n!curl -w GET https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz > lid.176.ftz\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  916k  100  916k    0     0   547k      0  0:00:01  0:00:01 --:--:--  548k\n\n\n\nimport fasttext\n\nm = fasttext.load_model(\"lid.176.ftz\")\ndef getLanguage(text: str):\n    return m.predict(text)[0][0].replace(\"__label__\", \"\")\n\nWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n\n\n\ncorpus[\"language\"] = corpus[\"clean_text\"].apply(getLanguage)\n\n\ncorpus[\"language\"].value_counts().head(10)\n\nen    10278\nde       90\nja       73\nit       67\nsv       52\nzh       48\nes       31\nfr       27\neu       20\neo       12\nName: language, dtype: int64\n\n\n\ncorpus[corpus[\"language\"]==\"ja\"].iloc[5][\"clean_text\"]\n\n'USDA - U.S. 1986/87 ENDING CORN STOCKS  5,240 MLN BU, WHEAT 1,848 MLN, SOYBEANS 610 MLN  USDA - U.S. 1986/87 ENDING CORN STOCKS  5,240 MLN BU, WHEAT 1,848 MLN, SOYBEANS 610 MLN  '\n\n\n\n\n\n\nimport spacy\n\nIn order to download the model from the Spacy library, please issue the following command in a shell\n\npython -m spacy download en_core_web_md\n\n\nnlp = spacy.load('en_core_web_md')\n\n\ncorpus[\"parsed\"] = corpus[\"clean_text\"].apply(nlp)\n\n\ncorpus.loc[\"test/14832\"][\"clean_text\"]\n\n\"THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Thailand's trade deficit widened to 4.5  billion baht in the first quarter of 1987 from 2.1 billion a  year ago, the Business Economics Department said.      It said Janunary/March imports rose to 65.1 billion baht  from 58.7 billion. Thailand's improved business climate this  year resulted in a 27 pct increase in imports of raw materials  and semi-finished products.      The country's oil import bill, however, fell 23 pct in the  first quarter due to lower oil prices.      The department said first quarter exports expanded to 60.6  billion baht from 56.6 billion.      Export growth was smaller than expected due to lower  earnings from many key commodities including rice whose  earnings declined 18 pct, maize 66 pct, sugar 45 pct, tin 26  pct and canned pineapples seven pct.      Products registering high export growth were jewellery up  64 pct, clothing 57 pct and rubber 35 pct.  \"\n\n\n\nfrom spacy import displacy\n\n\ndisplacy.render(corpus.loc[\"test/14832\"][\"parsed\"], style='ent', jupyter=True)\n\nTHAI TRADE DEFICIT WIDENS IN \n\n    FIRST QUARTER\n    DATE\n\n  \n\n    Thailand\n    GPE\n\n's trade deficit widened to \n\n    4.5  billion baht\n    MONEY\n\n in \n\n    the first quarter of 1987\n    DATE\n\n from \n\n    2.1 billion\n    MONEY\n\n a  year ago, \n\n    the Business Economics Department\n    ORG\n\n said.      It said \n\n    Janunary\n    GPE\n\n/March imports rose to \n\n    65.1 billion baht\n    MONEY\n\n  from \n\n    58.7 billion\n    MONEY\n\n. \n\n    Thailand\n    GPE\n\n's improved business climate \n\n    this  year\n    DATE\n\n resulted in a \n\n    27 pct\n    MONEY\n\n increase in imports of raw materials  and semi-finished products.      The country's oil import bill, however, fell \n\n    23 pct\n    MONEY\n\n in \n\n    the  first quarter\n    DATE\n\n due to lower oil prices.      The department said \n\n    first quarter\n    DATE\n\n exports expanded to \n\n    60.6  billion baht\n    MONEY\n\n from \n\n    56.6 billion\n    MONEY\n\n.      Export growth was smaller than expected due to lower  earnings from many key commodities including rice whose  earnings declined \n\n    18 pct\n    MONEY\n\n, maize \n\n    66 pct\n    MONEY\n\n, sugar \n\n    45 pct\n    MONEY\n\n, tin \n\n    26  pct\n    MONEY\n\n and canned pineapples \n\n    seven pct\n    MONEY\n\n.      Products registering high export growth were jewellery up  64 pct, clothing \n\n    57 pct\n    MONEY\n\n and rubber \n\n    35 pct\n    MONEY\n\n.  \n\n\n\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n      language\n      parsed\n      triplets\n      keywords\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n      en\n      (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U.S.-JA...\n      [(EXPORTERS, (FEAR, False), DAMAGE), (Japan, (...\n      [(trading, 0.461513063953854), (said, 0.315985...\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n      en\n      (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...\n      [(VERMIN, (EAT, False), STOCKS), (vermin, (con...\n      [(vermin, 0.3120614380287176), (daily, 0.26110...\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n      en\n      (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...\n      [(JAPAN, (REVISE, False), DEMAND), (Industry, ...\n      [(energy, 0.3857636092660117), (demand, 0.3479...\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      en\n      (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...\n      [(Products, (registering, False), growth), (Pr...\n      [(pct, 0.5457455609144312), (export, 0.2656069...\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n      en\n      (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...\n      [(INDONESIA, (SEES, False), PRICE), (Indonesia...\n      [(indonesia, 0.2410428235502938), (harahap, 0....\n    \n  \n\n\n\n\n\ncorpus[[\"clean_text\", \"label\", \"language\", \"parsed\"]].to_pickle(\"corpus.p\")\n\n\n\n\n\nIn the following, we will show you how to create two different kind of graphs out of a corpus of documents:\n\nKnowledge base graphs, where the subject-verb-object relation will be encoded to build a semantic graph\nBipartite graphs, linking documents with the entities/keywords appearing therein\n\n\n\n\nfrom subject_object_extraction import findSVOs\n\n\ncorpus[\"triplets\"] = corpus[\"parsed\"].apply(lambda x: findSVOs(x, output=\"obj\"))\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n      language\n      parsed\n      triplets\n    \n    \n      id\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n      en\n      (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U.S.-JA...\n      [(EXPORTERS, (FEAR, False), DAMAGE), (Japan, (...\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n      en\n      (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...\n      [(VERMIN, (EAT, False), STOCKS), (vermin, (con...\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n      en\n      (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...\n      [(JAPAN, (REVISE, False), DEMAND), (Industry, ...\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      en\n      (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...\n      [(Products, (registering, False), growth), (Pr...\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n      en\n      (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...\n      [(INDONESIA, (SEES, False), PRICE), (Indonesia...\n    \n  \n\n\n\n\n\nedge_list = [\n    {\"id\": _id, \"source\": source.lemma_.lower(), \"target\": target.lemma_.lower(), \"edge\": edge.lemma_.lower()}\n    for _id, triplets in corpus[\"triplets\"].iteritems()\n    for (source, (edge, neg), target) in triplets\n]\n\n\nedges = pd.DataFrame(edge_list)\n\n\nedges[\"edge\"].value_counts().head(10)\n\nbe         7620\nhave       2675\ninclude    2010\ntell       1729\nbuy        1464\nsell       1385\nsay        1216\ntake       1172\nmake       1151\ngive       1029\nName: edge, dtype: int64\n\n\n\nimport networkx as nx\n\n\nG=nx.from_pandas_edgelist(edges, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\n\nlen(G.nodes)\n\n7576\n\n\n\ndef plotDistribution(serie: pd.Series, nbins: int, minValue=None, maxValue=None):\n    _minValue=int(np.floor(np.log10(minValue if minValue is not None else serie.min())))\n    _maxValue=int(np.ceil(np.log10(maxValue if maxValue is not None else serie.max())))\n    bins = [0] + list(np.logspace(_minValue, _maxValue, nbins)) + [np.inf]\n    serie.hist(bins=bins)\n    plt.xscale(\"log\")\n\n\ndef graphSummary(graph, bins=10):\n    print(nx.info(graph))\n    plt.figure(figsize=(20, 8))\n    plt.subplot(1,2,1)\n    degrees = pd.Series({k: v for k, v in nx.degree(graph)})\n    plt.yscale(\"log\")\n    plotDistribution(degrees, bins)\n    try:\n        plt.subplot(1,2,2)\n        allEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in graph.edges(data=True)})\n        plotDistribution(allEdgesWeights, bins)\n        plt.yscale(\"log\")\n    except:\n        pass\n\n\nprint(nx.info(G))\n\nName: \nType: MultiDiGraph\nNumber of nodes: 7576\nNumber of edges: 72263\nAverage in degree:   9.5384\nAverage out degree:   9.5384\n\n\n\ngraphSummary(G, bins=15)\n\nName: \nType: MultiDiGraph\nNumber of nodes: 7576\nNumber of edges: 72263\nAverage in degree:   9.5384\nAverage out degree:   9.5384\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\n\n\n\n\nimport numpy as np\nnp.log10(pd.Series({k: v for k, v in nx.degree(G)}).sort_values(ascending=False)).hist()\nplt.yscale(\"log\")\n\n\n\n\nAnalysis of a certain relation\n\nedges.head()\n\n\n\n\n\n  \n    \n      \n      id\n      source\n      target\n      edge\n    \n  \n  \n    \n      0\n      test/14826\n      exporter\n      damage\n      fear\n    \n    \n      1\n      test/14826\n      japan\n      fear\n      raise\n    \n    \n      2\n      test/14826\n      row\n      damage\n      inflict\n    \n    \n      3\n      test/14826\n      they\n      correspondent\n      tell\n    \n    \n      4\n      test/14826\n      they\n      u.s.\n      tell\n    \n  \n\n\n\n\n\ne = edges[(edges[\"source\"]!=\" \") & (edges[\"target\"]!=\" \") & (edges[\"edge\"]==\"lend\")]\n\n\nG=nx.from_pandas_edgelist(e, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\n\nimport os\n\nplt.figure(figsize=(13, 6))\n\npos = nx.spring_layout(G, k=1.2) # k regulates the distance between nodes\n\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos, font_size=12)\n\n# plt.show()\n# plt.savefig(os.path.join(\".\", \"KnowledgeGraph.png\"), dpi=300, format=\"png\")\n\n\n\n\n\n\n\n\nLet’s start by extracting the keywords from the documents\n\nimport gensim\n\n\nfrom gensim.summarization import keywords \n\n\ntext = corpus[\"clean_text\"][0]\nkeywords(text, words=10, split=True, scores=True, pos_filter=('NN', 'JJ'), lemmatize=True)\n\n[('trading', 0.4615130639538529),\n ('said', 0.3159855693494515),\n ('export', 0.2691553824958079),\n ('import', 0.17462010006456888),\n ('japanese electronics', 0.1360932626379031),\n ('industry', 0.1286043740379779),\n ('minister', 0.12229815662000462),\n ('japan', 0.11434500812642447),\n ('year', 0.10483992409352465)]\n\n\n\ncorpus[\"keywords\"] = corpus[\"clean_text\"].apply(\n    lambda text: keywords(text, words=10, split=True, scores=True, pos_filter=('NN', 'JJ'), lemmatize=True)\n)\n\n\ncorpus.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n      language\n      parsed\n      triplets\n      keywords\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n      en\n      (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U.S.-JA...\n      [(EXPORTERS, (FEAR, False), DAMAGE), (Japan, (...\n      [(trading, 0.461513063953854), (said, 0.315985...\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n      en\n      (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...\n      [(VERMIN, (EAT, False), STOCKS), (vermin, (con...\n      [(vermin, 0.3120614380287176), (daily, 0.26110...\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n      en\n      (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...\n      [(JAPAN, (REVISE, False), DEMAND), (Industry, ...\n      [(energy, 0.3857636092660117), (demand, 0.3479...\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      en\n      (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...\n      [(Products, (registering, False), growth), (Pr...\n      [(pct, 0.5457455609144312), (export, 0.2656069...\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n      en\n      (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...\n      [(INDONESIA, (SEES, False), PRICE), (Indonesia...\n      [(indonesia, 0.2410428235502938), (harahap, 0....\n    \n  \n\n\n\n\n\ndef extractEntities(ents, minValue=1, typeFilters=[\"GPE\", \"ORG\", \"PERSON\"]):\n    entities = pd.DataFrame([\n        {\"lemma\": e.lemma_, \"lower\": e.lemma_.lower(), \"type\": e.label_}\n        for e in ents if hasattr(e, \"label_\")\n    ])\n\n    if len(entities)==0:\n        return pd.DataFrame()\n    \n    g = entities.groupby([\"type\", \"lower\"])\n\n    summary = pd.concat({\n        \"alias\": g.apply(lambda x: x[\"lemma\"].unique()), \n        \"count\": g[\"lower\"].count()\n    }, axis=1)\n    \n    return summary[summary[\"count\"]>1].loc[pd.IndexSlice[typeFilters, :, :]]\n\ndef getOrEmpty(parsed, _type):\n    try:\n        return list(parsed.loc[_type][\"count\"].sort_values(ascending=False).to_dict().items())\n    except:\n        return []\n\ndef toField(ents):\n    typeFilters=[\"GPE\", \"ORG\", \"PERSON\"]\n    parsed = extractEntities(ents, 1, typeFilters)\n    return pd.Series({_type: getOrEmpty(parsed, _type) for _type in typeFilters})\n    \n\n\nentities = corpus[\"parsed\"].apply(lambda x: toField(x.ents))\n\n\nmerged = pd.concat([corpus, entities], axis=1) \n\n\nmerged.head()\n\n\n\n\n\n  \n    \n      \n      clean_text\n      label\n      language\n      parsed\n      triplets\n      keywords\n      GPE\n      ORG\n      PERSON\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      test/14826\n      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n      [trade]\n      en\n      (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U.S.-JA...\n      [(EXPORTERS, (FEAR, False), DAMAGE), (Japan, (...\n      [(trading, 0.461513063953854), (said, 0.315985...\n      [(u.s., 13), (japan, 12), (taiwan, 3), (tokyo,...\n      []\n      []\n    \n    \n      test/14828\n      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n      [grain]\n      en\n      (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...\n      [(VERMIN, (EAT, False), STOCKS), (vermin, (con...\n      [(vermin, 0.3120614380287176), (daily, 0.26110...\n      [(china, 2)]\n      []\n      []\n    \n    \n      test/14829\n      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n      [crude, nat-gas]\n      en\n      (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...\n      [(JAPAN, (REVISE, False), DEMAND), (Industry, ...\n      [(energy, 0.3857636092660117), (demand, 0.3479...\n      [(japan, 2)]\n      []\n      []\n    \n    \n      test/14832\n      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER  Th...\n      [corn, grain, rice, rubber, sugar, tin, trade]\n      en\n      (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...\n      [(Products, (registering, False), growth), (Pr...\n      [(pct, 0.5457455609144312), (export, 0.2656069...\n      [(thailand, 2)]\n      []\n      []\n    \n    \n      test/14833\n      INDONESIA SEES CPO PRICE RISING SHARPLY  Indon...\n      [palm-oil, veg-oil]\n      en\n      (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...\n      [(INDONESIA, (SEES, False), PRICE), (Indonesia...\n      [(indonesia, 0.2410428235502938), (harahap, 0....\n      [(indonesia, 4), (malaysia, 2)]\n      [(cpo, 2)]\n      []\n    \n  \n\n\n\n\nWe finally create the bipartite graph\n\nedges = pd.DataFrame([\n    {\"source\": _id, \"target\": keyword, \"weight\": score, \"type\": _type}\n    for _id, row in merged.iterrows()\n    for _type in [\"keywords\", \"GPE\", \"ORG\", \"PERSON\"] \n    for (keyword, score) in row[_type]\n])\n\n\nG = nx.Graph()\nG.add_nodes_from(edges[\"source\"].unique(), bipartite=0)\nG.add_nodes_from(edges[\"target\"].unique(), bipartite=1)\nG.add_edges_from([\n    (row[\"source\"], row[\"target\"])\n    for _, row in edges.iterrows()\n])\n\n\ndocument_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0}\nentity_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 1}\n\n\nnodes_with_low_degree = {n for n, d in nx.degree(G, nbunch=entity_nodes) if d<5}\n\n\nprint(nx.info(G))\n\nName: \nType: Graph\nNumber of nodes: 25752\nNumber of edges: 100311\nAverage degree:   7.7905\n\n\n\nfrom networkx.algorithms.bipartite.projection import overlap_weighted_projected_graph\n\n### Entity-Entity Graph Projection\n\nsubGraph = G.subgraph(set(G.nodes) - nodes_with_low_degree)\n\n\nentityGraph = overlap_weighted_projected_graph(\n    subGraph, \n    {n for n, d in subGraph.nodes(data=True) if d[\"bipartite\"] == 1}\n)\n\n\nlen(entityGraph.nodes())\n\n2386\n\n\n\ndegrees = pd.Series({k: v for k, v in nx.degree(entityGraph)})\n\n\nplotDistribution(degrees, 100)\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\n\n\n\n\nprint(nx.info(entityGraph))\n\nName: \nType: Graph\nNumber of nodes: 2386\nNumber of edges: 120198\nAverage degree: 100.7527\n\n\n\nallEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in entityGraph.edges(data=True)})\n\n\nplotDistribution(allEdgesWeights, 100)\nplt.yscale(\"log\")\n\n\n\n\n\nfilteredEntityGraph = entityGraph.edge_subgraph(\n    [edge for edge in entityGraph.edges if entityGraph.edges[edge][\"weight\"]>0.05]\n)\n\n\nprint(nx.info(filteredEntityGraph))\n\nName: \nType: Graph\nNumber of nodes: 2265\nNumber of edges: 8082\nAverage degree:   7.1364\n\n\n\n\n\nglobalKpis = [{\n    \"shortest_path\": nx.average_shortest_path_length(_graph),\n    \"clustering_coefficient\": nx.average_clustering(_graph),\n    \"global_efficiency\": nx.global_efficiency(_graph)\n} for components in nx.connected_components(filteredEntityGraph) \n    for _graph in [nx.subgraph(filteredEntityGraph, components)]]\n\n\npd.concat([\n    pd.DataFrame(globalKpis), \n    pd.Series([len(c) for c in nx.connected_components(filteredEntityGraph)])\n], axis=1)\n\n\n\n\n\n  \n    \n      \n      shortest_path\n      clustering_coefficient\n      global_efficiency\n      0\n    \n  \n  \n    \n      0\n      4.715074\n      0.211563\n      0.227356\n      2254\n    \n    \n      1\n      1.000000\n      0.000000\n      1.000000\n      2\n    \n    \n      2\n      1.500000\n      0.000000\n      0.750000\n      4\n    \n    \n      3\n      1.333333\n      0.000000\n      0.833333\n      3\n    \n    \n      4\n      1.000000\n      0.000000\n      1.000000\n      2\n    \n  \n\n\n\n\n\npd.Series([len(c) for c in nx.connected_components(filteredEntityGraph)]).sum()\n\n2265\n\n\n\nglobalKpis[0]\n\n{'shortest_path': 4.715073779178782,\n 'clustering_coefficient': 0.21156314975836948,\n 'global_efficiency': 0.2273555107741054}\n\n\n\n# nx.write_gexf(filteredEntityGraph, \"filteredEntityGraph.gexf\")\n\n\nbetweeness = nx.betweenness_centrality(filteredEntityGraph)\n\n\n_betweeness = pd.Series(betweeness)\n\n\nplotDistribution(_betweeness[_betweeness>0], 100)\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\n\n\n\n\npageRanks = pd.Series(nx.pagerank(filteredEntityGraph))\n\n\ndegrees = pd.Series({k: v for k, v in nx.degree(filteredEntityGraph)})\n\n\nkpis = pd.concat({\n    \"pageRank\": pageRanks, \n    \"degrees\": degrees, \n    \"betweeness\": _betweeness\n}, axis=1)\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1,2,1)\nplt.title(\"Page rank vs degrees\")\nplt.plot(kpis[\"pageRank\"], kpis[\"degrees\"], '.', color=\"tab:blue\")\nplt.xlabel(\"page rank\")\nplt.ylabel(\"degree\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\nplt.subplot(1,2,2)\nplt.title(\"Page rank vs betweeness\")\nplt.plot(kpis[\"pageRank\"], kpis[\"betweeness\"], '.', color=\"tab:blue\")\nplt.xlabel(\"page rank\")\nplt.ylabel(\"betweeness\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.ylim([1E-5, 2E-2])\n\n(1e-05, 0.02)\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\nplt.plot(kpis[\"pageRank\"], kpis[\"betweeness\"], 'b.')\nplt.xlabel(\"page rank\")\nplt.ylabel(\"betweeness\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n\n\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1,2,1)\nplotDistribution(degrees, 13)\nplt.yscale(\"log\")\nplt.title(\"Degree Distribution\")\n\nplt.subplot(1,2,2)\nplotDistribution(allEdgesWeights, 20)\nplt.xlim([1E-2, 10])\nplt.yscale(\"log\")\nplt.title(\"Edge Weight Distribution\")\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\nText(0.5, 1.0, 'Edge Weight Distribution')\n\n\n\n\n\n\nallEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in filteredEntityGraph.edges(data=True)})\n\n\nplotDistribution(allEdgesWeights, 20)\nplt.xlim([1E-2, 10])\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\n#Create network layout for visualizations\nspring_pos = nx.spring_layout(filteredEntityGraph)\n\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nplt.axis(\"off\")\nnx.draw_networkx(filteredEntityGraph, pos=spring_pos, node_color=default_node_color, \n                 edge_color=default_edge_color, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\nimport community\n\n\ncommunities = pd.Series(community.best_partition(filteredEntityGraph))\n\n\ncommunities.value_counts().sort_values(ascending=False).plot(kind=\"bar\", figsize=(12, 5))\nplt.xlabel(\"Community\")\nplt.ylabel(\"# Members\")\n\nText(0, 0.5, '# Members')\n\n\n\n\n\n\ncommunities.loc[\"turkish\"]\n\n16\n\n\n\nnodes = communities[communities==17].index\n\n\nnodes\n\nIndex(['pharmaceutical', 'worth', 'american motors', 'parts', 'auditors',\n       'qualified', 'midland', 'salomon', 'consolidated', 'taft', 'goldman',\n       'rejects', 'plants', 'wednesday', 'tvx', 'miami', 'jersey', 'broadcast',\n       'dudley taft', 'earn', 'audit', 'opinion', 'closing', 'directors',\n       'liquidating', 'stations', 'controls', 'radio', 'chrysler',\n       'statements', 'gets', 'motors', 'year ending', 'aluminum', 'beverage',\n       'near', 'employs', 'renault', 'kentucky', 'bass', 'marine', 'semi',\n       'staff', 'share payable', 'brand', 'adding', 'broadcasting', 'car',\n       'financing', 'smelter', 'guinness', 'bidder', 'henderson', 'houston',\n       'extended', 'david', 'amc', 'mitsui', 'toledo', 'alcan', 'importer',\n       'institutional'],\n      dtype='object')\n\n\n\nsmallGrap = nx.subgraph(filteredEntityGraph, nbunch=nodes)\n\n\nplt.figure(figsize=(10,10))\n\npos = nx.spring_layout(smallGrap) # k regulates the distance between nodes\n\nnx.draw(smallGrap, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n\n# plt.show()\n# plt.savefig(os.path.join(\".\", \"CloseUp.png\"), dpi=300, format=\"png\")\n\n\n\n\nHere we show a Bipartite Closeup of the cluster\n\nbipartiteCloseup = subGraph.edge_subgraph(\n    {e for e in subGraph.edges() if len(set(e).intersection(nodes))>0}\n)\n\ndeg = nx.degree(bipartiteCloseup)\n\nsmallGrap = nx.subgraph(bipartiteCloseup, {n for n, d in bipartiteCloseup.nodes(data=True) if d[\"bipartite\"]==1 or deg[n]>1})\n\n\nlen([n for n, d in bipartiteCloseup.nodes(data=True) if d[\"bipartite\"]==0])\n\n480\n\n\n\nlen(nodes)\n\n62\n\n\n\nplt.figure(figsize=(10,10))\n\npos = nx.kamada_kawai_layout(smallGrap) # k regulates the distance between nodes\n\nnode_color = [\"skyblue\" if d[\"bipartite\"]==1 else \"red\" for n, d in smallGrap.nodes(data=True)]\n\nnx.draw(smallGrap, with_labels=False, node_color=node_color, #'skyblue', \n        node_size=150, edge_cmap=plt.cm.Blues, pos = pos)\n\n\n# plt.show()\n# plt.savefig(os.path.join(\".\", \"BipartiteCloseUp.png\"), dpi=300, format=\"png\")\n\n\n\n\n\n\n\nUsing TSNE\n\nfrom node2vec import Node2Vec\n\nnode2vec = Node2Vec(filteredEntityGraph, dimensions=5) \nmodel = node2vec.fit(window=10) \nembeddings = model.wv \n\nComputing transition probabilities: 100%|██████████| 2265/2265 [00:07<00:00, 321.05it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [01:45<00:00, 10.55s/it]\n\n\n\nfrom sklearn.manifold import TSNE\ntsne=TSNE(n_components=2)\nembedding2d=tsne.fit_transform(embeddings.vectors)\n\n\nplt.plot(embedding2d[:, 0], embedding2d[:, 1], 'o')\n\n\n\n\nUsing Node2Vec\nNode2Vec allows also to compute a similarity between entities\n\nembeddings.most_similar(positive=[\"turkey\"])\n\n[('turkish', 0.9921346306800842),\n ('lira', 0.987409234046936),\n ('debts', 0.9794315099716187),\n ('coastal', 0.9783217906951904),\n ('athens', 0.9770432710647583),\n ('greece', 0.9727554321289062),\n ('benefits', 0.9630903601646423),\n ('carolina', 0.962989330291748),\n ('sharp', 0.9628170728683472),\n ('jones', 0.9522427320480347)]\n\n\n\n\n\n\nfrom networkx.algorithms.bipartite.projection import overlap_weighted_projected_graph\n\n\ndocumentGraph = overlap_weighted_projected_graph(\n    G, \n    {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0}\n)\n\n\nfrom matplotlib import pyplot as plt\n\n\nprint(nx.info(documentGraph))\n\nName: \nType: Graph\nNumber of nodes: 10788\nNumber of edges: 12994465\nAverage degree: 2409.0591\n\n\n\ndegrees = pd.Series({k: v for k, v in nx.degree(documentGraph)})\n\n\nplotDistribution(degrees, 100)\nplt.yscale(\"log\")\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\n\n\n\n\nallEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in documentGraph.edges(data=True)})\n\n\nplotDistribution(allEdgesWeights, 100)\nplt.yscale(\"log\")\nplt.xlim([1E-2, 1])\n\n(0.01, 1)\n\n\n\n\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1,2,1)\nplotDistribution(degrees, 13)\nplt.yscale(\"log\")\nplt.title(\"Degree Distribution\")\n\nplt.subplot(1,2,2)\nplotDistribution(allEdgesWeights, 20)\nplt.xlim([1E-2, 10])\nplt.yscale(\"log\")\nplt.title(\"Edge Weight Distribution\")\n\nText(0.5, 1.0, 'Edge Weight Distribution')\n\n\n\n\n\n\nfilteredDocumentGraph = documentGraph.edge_subgraph(\n    allEdgesWeights[(allEdgesWeights>0.6)].index.tolist()\n)\n\n\nprint(nx.info(filteredDocumentGraph))\n\nName: \nType: Graph\nNumber of nodes: 1958\nNumber of edges: 7884\nAverage degree:   8.0531\n\n\n\n\n\ndegrees = pd.Series({k: v for k, v in nx.degree(filteredDocumentGraph)})\n\n\nplotDistribution(degrees, 100)\nplt.yscale(\"log\")\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-7/lib/python3.7/site-packages/matplotlib/axes/_axes.py:6694: RuntimeWarning: invalid value encountered in multiply\n  boffset = -0.5 * dr * totwidth * (1 - 1 / nx)\n\n\n\n\n\n\nallEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in filteredDocumentGraph.edges(data=True)})\n\n\nplotDistribution(allEdgesWeights, 100)\nplt.yscale(\"log\")\nplt.xlim([1E-1, 1])\n\n(0.1, 1)\n\n\n\n\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1,2,1)\nplotDistribution(degrees, 13)\nplt.yscale(\"log\")\nplt.title(\"Degree Distribution\")\n\nplt.subplot(1,2,2)\nplotDistribution(allEdgesWeights, 20)\nplt.xlim([1E-2, 10])\nplt.yscale(\"log\")\nplt.title(\"Edge Weight Distribution\")\n\nText(0.5, 1.0, 'Edge Weight Distribution')\n\n\n\n\n\n\n\n\n\n#Create network layout for visualizations\nspring_pos = nx.spring_layout(filteredDocumentGraph)\n\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nplt.axis(\"off\")\nnx.draw_networkx(filteredDocumentGraph, pos=spring_pos, node_color=default_node_color, \n                 edge_color=default_edge_color, with_labels=False, node_size=15)\n\n\n\n\n\ncomponents = pd.Series({ith: component \n              for ith, component in enumerate(nx.connected_components(filteredDocumentGraph))})\n\n\nplotDistribution(components.apply(len), nbins=20)\nplt.yscale(\"log\")\n\n\n\n\n\ncoreDocumentGraph = nx.subgraph(\n    filteredDocumentGraph,\n    [node for nodes in components[components.apply(len)>8].values for node in nodes]\n)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html",
    "href": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html",
    "title": "[GML] Chap1: 시작하기 - networkx로 그래프 이해하기",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#weighted-directed-graph",
    "href": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#weighted-directed-graph",
    "title": "[GML] Chap1: 시작하기 - networkx로 그래프 이해하기",
    "section": "Weighted Directed Graph",
    "text": "Weighted Directed Graph\n\nG = nx.MultiDiGraph()\nV = {'Paris', 'Dublin','Milan', 'Rome'}\nE = [ ('Paris','Dublin', 11), ('Paris','Milan', 8),\n     ('Milan','Rome', 5),('Milan','Dublin', 19)]\nG.add_nodes_from(V)\nG.add_weighted_edges_from(E)\nplot(G)\n\n\n\n\n\nprint(nx.to_pandas_edgelist(G))\nprint(nx.to_pandas_adjacency(G))\n\n  source  target  weight\n0  Milan    Rome       5\n1  Milan  Dublin      19\n2  Paris  Dublin      11\n3  Paris   Milan       8\n        Dublin  Rome  Milan  Paris\nDublin     0.0   0.0    0.0    0.0\nRome       0.0   0.0    0.0    0.0\nMilan     19.0   5.0    0.0    0.0\nParis     11.0   0.0    8.0    0.0"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#bipartite-graph",
    "href": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#bipartite-graph",
    "title": "[GML] Chap1: 시작하기 - networkx로 그래프 이해하기",
    "section": "Bipartite Graph",
    "text": "Bipartite Graph\n\nn_nodes = 10\nn_edges = 12\nbottom_nodes = [ith for ith in range(n_nodes) if ith % 2 ==0]\ntop_nodes = [ith for ith in range(n_nodes) if ith % 2 ==1]\niter_edges = zip(\n    np.random.choice(bottom_nodes, n_edges),  \n    np.random.choice(top_nodes, n_edges))\nedges = pd.DataFrame([\n    {\"source\": a, \"target\": b} for a, b in iter_edges])\nB = nx.Graph()\nB.add_nodes_from(bottom_nodes, bipartite=0)\nB.add_nodes_from(top_nodes, bipartite=1)\nB.add_edges_from([tuple(x) for x in edges.values])\n\n\nfrom networkx.drawing.layout import bipartite_layout\npos = bipartite_layout(B, bottom_nodes)\nnx.draw_networkx(B, pos=pos,node_size=2500,font_weight=900,font_size=25,alpha=0.8,arrowsize=20)\nfig=plt.gcf()\nfig.set_figheight(8)\nfig.set_figwidth(15)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#multi-graph",
    "href": "posts/2_Studies/GML/Chapter01/01_Introduction_Networkx.html#multi-graph",
    "title": "[GML] Chap1: 시작하기 - networkx로 그래프 이해하기",
    "section": "Multi Graph",
    "text": "Multi Graph\n\ndirected_multi_graph = nx.MultiDiGraph()\nV = {'Dublin', 'Paris', 'Milan', 'Rome'}\nE = [('Milan','Dublin'), ('Milan','Dublin'), ('Paris','Milan'), ('Paris','Dublin'), ('Milan','Rome'), ('Milan','Rome')]\ndirected_multi_graph.add_nodes_from(V)\ndirected_multi_graph.add_edges_from(E)\n\n[0, 1, 0, 0, 0, 1]\n\n\n\nplot(G)\n\n\n\n\n\n??"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/03_Graphs_Benchmarks.html",
    "href": "posts/2_Studies/GML/Chapter01/03_Graphs_Benchmarks.html",
    "title": "[GML] Chap1: 시작하기 - 밴치마크 및 저장소",
    "section": "",
    "text": "from matplotlib import pyplot as plt\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\noutput_dir = \"./figures\"\n\n\nimport os\n\n\ndef draw_graph(G, node_names={}, filename=None, node_size=50, layout = None):\n    pos_nodes = nx.spring_layout(G) if layout is None else layout(G)\n    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray')\n  \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n  \n    nx.draw_networkx_labels(G, pos_attrs, labels=node_names, font_family='serif')\n  \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n  \n    if filename:\n        plt.savefig(os.path.join(output_dir, filename), format=\"png\")\n\n\n# draw enhanced path on the graph\ndef draw_enhanced_path(G, path_to_enhance, node_names={}, filename=None, layout=None):\n    path_edges = list(zip(path,path[1:]))\n    pos_nodes = nx.spring_layout(G) if layout is None else layout(G)\n    \n    plt.figure(figsize=(5,5),dpi=300)\n    pos_nodes = nx.spring_layout(G)\n    nx.draw(G, pos_nodes, with_labels=False, node_size=50, edge_color='gray')\n  \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n  \n    nx.draw_networkx_labels(G, pos_attrs, labels=node_names, font_family='serif')\n    nx.draw_networkx_edges(G,pos_nodes,edgelist=path_edges, edge_color='#cc2f04', style='dashed', width=2.0)\n  \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n  \n    if filename:\n        plt.savefig(os.path.join(output_dir, filename), format=\"png\")\n\n### Simple Example of Graphs\nWe start with some simple graphs\n\ncomplete = nx.complete_graph(n=7)\nlollipop = nx.lollipop_graph(m=7, n=3)\nbarbell = nx.barbell_graph(m1=7, m2=4)\n\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,3,1)\ndraw_graph(complete)\nplt.title(\"Complete\")\nplt.subplot(1,3,2)\nplt.title(\"Lollipop\")\ndraw_graph(lollipop)\nplt.subplot(1,3,3)\nplt.title(\"Barbell\")\ndraw_graph(barbell)\n\n\n\n\n\ncomplete = nx.relabel_nodes(nx.complete_graph(n=7), lambda x: x + 0)\nlollipop = nx.relabel_nodes(nx.lollipop_graph(m=7, n=3), lambda x: x+100)\nbarbell = nx.relabel_nodes(nx.barbell_graph(m1=7, m2=4), lambda x: x+200)\n\n\ndef get_random_node(graph):\n    return np.random.choice(graph.nodes)\n\n\nimport numpy as np\n\n\n\n\nallGraphs = nx.compose_all([complete, barbell, lollipop])\nallGraphs.add_edge(get_random_node(lollipop), get_random_node(lollipop))\nallGraphs.add_edge(get_random_node(complete), get_random_node(barbell))\n\n\ndraw_graph(allGraphs, layout=nx.kamada_kawai_layout)\n\n\n\n\n\n\nIn the following we create and analyse some simple graph generated by the Barabasi-Albert model\n\nBA_graph_small = nx.extended_barabasi_albert_graph(n=20,m=1,p=0,q=0)\n\n\ndraw_graph(BA_graph_small, layout=nx.circular_layout)\n\n\n\n\nWe analyse large Barabasi-Albert graphs to investigate their ability to generate power-law distribution for the degree of node\n\nn = 1E5\nbag = nx.extended_barabasi_albert_graph(n,m=1,p=0,q=0)\n\n\ndegree = dict(nx.degree(bag)).values()\n\n\nbins = np.round(np.logspace(np.log10(min(degree)), np.log10(max(degree)), 10))\n\n\nfrom collections import Counter\ncnt = Counter(np.digitize(np.array(list(degree)), bins))\n\n\nplt.figure(figsize=(15,6))\nplt.subplot(1,2,1)\ndraw_graph(BA_graph_small, layout=nx.circular_layout)\nplt.subplot(1,2,2)\nx, y = list(zip(*[(bins[k-1], v/n) for k, v in cnt.items()]))\nplt.plot(x, y, 'o'); plt.xscale(\"log\"); plt.yscale(\"log\")\nplt.xlabel(\"Degree k\")\nplt.ylabel(\"P(k)\")\n\nText(0, 0.5, 'P(k)')\n\n\n\n\n\n\nplt.figure(figsize=(15, 6))\n\nplt.hist(degree, bins=bins)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n\n\n\nOther simple graph Benchmarks\n\nimport pandas as pd\n\n\ngraph = nx.florentine_families_graph()\n\n\nnx.draw_kamada_kawai(graph, with_labels=True, node_size=20, font_size=14)\nplt.savefig(\"Florentine.png\")\n\n\n\n\n\n\n\nThis dataset (and other) can be downloaded from http://networkrepository.com/. The datasets are generally in the MTX file format that has been described in the book.\nIn particular the dataset here presented is taken from the collaboration network of Arxiv Astro Physics, that can be downloaded from http://networkrepository.com/ca-AstroPh.php.\nAs better explained in the book, some of the files that can be downloaded from that source are somewhat non-standard and needs small fixes. Please make sure the header of the file has the following\n%%MatrixMarket matrix coordinate pattern symmetric\nwith a double %\n\nfrom scipy.io import mmread\n\n\nfile = \"ca-AstroPh.mtx\"\nadj_matrix = mmread(file)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'ca-AstroPh.mtx'\n\n\n\ngraph = nx.from_scipy_sparse_matrix(adj_matrix)\n\nAttributeError: module 'networkx' has no attribute 'from_scipy_sparse_matrix'\n\n\n\ndegrees = dict(nx.degree(graph))\n\n\nci = nx.clustering(graph)\n\n\ncentrality = nx.centrality.eigenvector_centrality(graph)\n\n\nstats = pd.DataFrame({\n    \"centrality\": centrality, \n    \"C_i\": ci, \n    \"degree\": degrees\n})\n\n\nstats.head()\n\n\n\n\n\n  \n    \n      \n      centrality\n      C_i\n      degree\n    \n  \n  \n    \n      Acciaiuoli\n      0.132157\n      0.000000\n      1\n    \n    \n      Medici\n      0.430315\n      0.066667\n      6\n    \n    \n      Castellani\n      0.259020\n      0.333333\n      3\n    \n    \n      Peruzzi\n      0.275722\n      0.666667\n      3\n    \n    \n      Strozzi\n      0.355973\n      0.333333\n      4\n    \n  \n\n\n\n\nHere we provide some simple analysis of the DataFrame we generated to see correlations between centrality, clustering coefficient and degree.\n\nplt.plot(stats[\"centrality\"], stats[\"degree\"], 'o')\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n\n\n\n\nplt.plot(stats[\"centrality\"], stats[\"C_i\"], 'o')\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n\n\n\n\n\n\nHere we plot the ego-network of the most-connected node, that has id 6933. However, even this network looks a bit messy since it has hundreds of nodes. We therefore sample randomly or based on centrality/clustering coefficient in order to plot a relevant subgraph.\n\nneighbors = [n for n in nx.neighbors(graph, 6933)]\n\nNetworkXError: The node 6933 is not in the graph.\n\n\n\nsampling = 0.1\n\n\nnTop = round(len(neighbors)*sampling)\n\nNameError: name 'neighbors' is not defined\n\n\n\nidx = {\n    \"random\": stats.loc[neighbors].sort_index().index[:nTop], \n    \"centrality\": stats.loc[neighbors].sort_values(\"centrality\", ascending=False).index[:nTop],\n    \"C_i\": stats.loc[neighbors].sort_values(\"C_i\", ascending=False).index[:nTop]\n}\n\nNameError: name 'neighbors' is not defined\n\n\n\ndef plotSubgraph(graph, indices, center = 6933):\n    draw_graph(\n        nx.subgraph(graph, list(indices) + [center]),\n        layout = nx.kamada_kawai_layout\n    )\n\n\nplt.figure(figsize=(15,6))\nfor ith, title in enumerate([\"random\", \"centrality\", \"C_i\"]):\n    plt.subplot(1,3,ith+1)\n    plotSubgraph(graph, idx[title])\n    plt.title(title)\nplt.savefig(os.path.join(output_dir, \"PhAstro\"))\n\nNameError: name 'idx' is not defined\n\n\n\n\n\n\n\n\nOtherwise, we could also export the data from networkx in order to plot it and analyse it using the Gephi software.\n\nnx.write_gexf(graph, 'ca-AstroPh.gexf')\n\n\n\n\n\nn=10\n\nn_nodes = 10\nn_edges = 12\n\nbottom_nodes = [ith for ith in range(n) if int(ith) % 2 == 0]\ntop_nodes = [ith for ith in range(n) if int(ith) % 2 == 1.0]\n\n\niter_edges = zip(np.random.choice(bottom_nodes, n_edges), np.random.choice(top_nodes, n_edges))\n\nedges = pd.DataFrame([{\"source\": a, \"target\": b} for a, b in iter_edges])\n\n\n\n\nB = nx.Graph()\n\nB.add_nodes_from(bottom_nodes, bipartite=0)\nB.add_nodes_from(top_nodes, bipartite=1)\n\n\nB.add_edges_from([tuple(x) for x in edges.values])\n\n\nfrom networkx.drawing.layout import bipartite_layout\n\n\npos = bipartite_layout(B, bottom_nodes)\n\nnx.draw_networkx(B, pos=pos)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/02_Graph_metrics.html",
    "href": "posts/2_Studies/GML/Chapter01/02_Graph_metrics.html",
    "title": "[GML] Chap1: 시작하기 - Graph metrics",
    "section": "",
    "text": "import networkx as nx\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\n# draw a simple graph\ndef draw_graph(G, node_names={}, filename=None, node_size=50):\n    pos_nodes = nx.spring_layout(G)\n    nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray')\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    nx.draw_networkx_labels(G, pos_attrs, labels=node_names, font_family='serif')\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    \n    if filename:\n        plt.savefig(filename, format=\"png\")\n\n\n# draw enhanced path on the graph\ndef draw_enhanced_path(G, path_to_enhance, node_names={}, filename=None):\n    path_edges = list(zip(path,path[1:]))\n    pos_nodes = nx.spring_layout(G)\n\n    plt.figure(figsize=(5,5),dpi=300)\n    pos_nodes = nx.spring_layout(G)\n    nx.draw(G, pos_nodes, with_labels=False, node_size=50, edge_color='gray')\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    nx.draw_networkx_labels(G, pos_attrs, labels=node_names, font_family='serif')\n    nx.draw_networkx_edges(G,pos_nodes,edgelist=path_edges, edge_color='#cc2f04', style='dashed', width=2.0)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    \n    if filename:\n        plt.savefig(filename, format=\"png\")"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter01/02_Graph_metrics.html#통합측정지표",
    "href": "posts/2_Studies/GML/Chapter01/02_Graph_metrics.html#통합측정지표",
    "title": "[GML] Chap1: 시작하기 - Graph metrics",
    "section": "통합측정지표",
    "text": "통합측정지표\n- 노드가 서로 상호 연결되는 경향을 측정\n\n최단경로, 최단경로길이\n- setup\n\nG = nx.Graph()\nV = {1:'Dublin',2:'Paris',3:'Milan',4:'Rome',5:'Naples',6:'Moscow',7:'Seoul'}\nG.add_nodes_from(V.keys())\nG.add_edges_from([(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(6,7),(7,5)])\n\n\nG.nodes\n\nNodeView((1, 2, 3, 4, 5, 6, 7))\n\n\n\nG.edges\n\nEdgeView([(1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 7)])\n\n\n\nnx.draw_networkx(G)\nprint(V)\n\n{1: 'Dublin', 2: 'Paris', 3: 'Milan', 4: 'Rome', 5: 'Naples', 6: 'Moscow', 7: 'Seoul'}\n\n\n\n\n\nFigure 1: 도시간의 연결을 나타내는 무향그래프\n\n\n\n\n- shortest path\n\npath = nx.shortest_path(G,source=1,target=7)\npath\n\n[1, 3, 4, 5, 7]\n\n\n\npath = nx.shortest_path(G,source=1,target=4)\npath\n\n[1, 3, 4]\n\n\n\npath = nx.shortest_path(G,source=4,target=6)\npath\n\n[4, 5, 6]\n\n\n- shortest path lenghth\n최단경로길이는 아래와 같이 구할수 있음\n\npath = nx.shortest_path(G,source=1,target=7)\nlen(path)\n\n5\n\n\n\n\n특성경로길이 (= 평균최단경로길이)\n- characteristic path length = average of shortest path lenght\n가능한 모든 노드 쌍 사이의 모든 최단경로 길이의 평균으로 정의한다.\n\\[\\text{characteristic path length}=\\frac{1}{|V|(|V|-1)}\\sum_{i \\in V} l_i\\]\n여기에서 \\(l_i\\)는 노드 \\(i\\)와 다른 모든 노드사이의 평균경로 길이로 정의한다. 특성경로길이가 더 짧은 네트워크는 정보를 더 빠르게 전송할 수 있다고 해석할 수 있다.\n\nG1 = nx.Graph()\nG2 = nx.Graph()\nV = {1,2,3,4}\nE1 = [(1,2),(2,3),(3,4),(4,1)]\nE2 = [(1,2),(1,3),(1,4),(2,3),(2,4),(3,4)]\nG1.add_nodes_from(V)\nG2.add_nodes_from(V)\nG1.add_edges_from(E1)\nG2.add_edges_from(E2)\n\n\nnx.draw_networkx(G1)\n\n\n\n\nFigure 2: (G1) 링형태의 그래프\n\n\n\n\n\nnx.draw_networkx(G2)\n\n\n\n\nFigure 3: (G2) 완전연결된 그래프\n\n\n\n\n- 두 그래프 \\({\\cal G}_1\\)1와 \\({\\cal G}_2\\)2의 특성경로길이를 각각 계산하면 아래와 같다.\n\nnx.average_shortest_path_length(G1)\n\n1.3333333333333333\n\n\n\nnx.average_shortest_path_length(G2)\n\n1.0\n\n\n\n\n대역효율성\n모든 노드 쌍에 대한 역최단경로 (inverse shortest path) 길의의 평균을 의미이다. Figure 2, Figure 3 의 그래프에서 이 값을 계산하면 아래와 같다.\n- 예제1\n\nnx.draw_networkx(G1)\nprint(\"대역효율성:\", nx.global_efficiency(G1))\n\n대역효율성: 0.8333333333333334\n\n\n\n\n\nFigure 4: (G1) 링형태의 그래프 + 대역효율성\n\n\n\n\n- 예제2\n\nnx.draw_networkx(G2)\nprint(\"대역효율성:\",nx.global_efficiency(G2))\n\n대역효율성: 1.0\n\n\n\n\n\nFigure 5: (G2) 완전저연결된 그래프 + 대역효율성\n\n\n\n\n- 예제3\n\nG = nx.Graph()\nnodes = {1:'Dublin',2:'Paris',3:'Milan',4:'Rome',5:'Naples',6:'Moscow',7:'Tokyo'}\nG.add_nodes_from(nodes.keys())\nG.add_edges_from([(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(6,7),(7,5)])\n\nprint(nx.global_efficiency(G))\nprint(nx.local_efficiency(G))\n\n0.611111111111111\n0.6666666666666667\n\n\n\n# higher efficiency\nG = nx.complete_graph(n=7)\nnodes = {0:'Dublin',1:'Paris',2:'Milan',3:'Rome',4:'Naples',5:'Moscow',6:'Tokyo'}\n\nge = round(nx.global_efficiency(G),2)\n\n# place the text box in axes coords\nax = plt.gca()\nax.text(-.4, -1.3, \"Global Efficiency:{}\".format(ge), fontsize=14, ha='left', va='bottom');\n\ndraw_graph(G,node_names=nodes,filename='efficiency.png')\n\n\n\n\n\n# lower efficiency\nG = nx.cycle_graph(n=7)\nnodes = {0:'Dublin',1:'Paris',2:'Milan',3:'Rome',4:'Naples',5:'Moscow',6:'Tokyo'}\n\nle = round(nx.global_efficiency(G),2)\n\n# place the text box in axes coords\nax = plt.gca()\nax.text(-.4, -1.3, \"Global Efficiency:{}\".format(le), fontsize=14, ha='left', va='bottom');\n\ndraw_graph(G, node_names=nodes,filename='less_efficiency.png')\n\n\n\n\n\n\nClustering coefficient\n\nG = nx.Graph()\nnodes = {1:'Dublin',2:'Paris',3:'Milan',4:'Rome',5:'Naples',6:'Moscow',7:'Tokyo'}\nG.add_nodes_from(nodes.keys())\nG.add_edges_from([(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(6,7),(7,5)])\n\n\nnx.average_clustering(G)\n\n0.6666666666666667\n\n\n\nnx.clustering(G)\n\n{1: 1.0,\n 2: 1.0,\n 3: 0.3333333333333333,\n 4: 0,\n 5: 0.3333333333333333,\n 6: 1.0,\n 7: 1.0}\n\n\n\ncc = nx.clustering(G)\nnode_size=[(v + 0.1) * 200 for v in cc.values()]\ndraw_graph(G, node_names=nodes, node_size=node_size,filename='clustering.png')\n\n\n\n\n\n\nCentrality\n\nG = nx.Graph()\nnodes = {1:'Dublin',2:'Paris',3:'Milan',4:'Rome',5:'Naples',6:'Moscow',7:'Tokyo'}\nG.add_nodes_from(nodes.keys())\nG.add_edges_from([(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(6,7),(7,5)])\n\n\nnx.degree_centrality(G)\n\n{1: 0.3333333333333333,\n 2: 0.3333333333333333,\n 3: 0.5,\n 4: 0.3333333333333333,\n 5: 0.5,\n 6: 0.3333333333333333,\n 7: 0.3333333333333333}\n\n\n\ndc = nx.degree_centrality(G)\nnode_size=[(v + 0.01) * 400 for v in dc.values()]\ndraw_graph(G, node_names=nodes, node_size=node_size,filename='deg_centr.png')\n\ndf = pd.DataFrame(dc,index=['Degree centrality'])\ndf.columns = nodes.values()\ndf\n\n\n\n\n\n  \n    \n      \n      Dublin\n      Paris\n      Milan\n      Rome\n      Naples\n      Moscow\n      Tokyo\n    \n  \n  \n    \n      Degree centrality\n      0.333333\n      0.333333\n      0.5\n      0.333333\n      0.5\n      0.333333\n      0.333333\n    \n  \n\n\n\n\n\n\n\n\nnx.closeness_centrality(G)\n\n{1: 0.4,\n 2: 0.4,\n 3: 0.5454545454545454,\n 4: 0.6,\n 5: 0.5454545454545454,\n 6: 0.4,\n 7: 0.4}\n\n\n\ndc = nx.closeness_centrality(G)\nnode_size=[(v + 0.1) * 400 for v in dc.values()]\ndraw_graph(G, node_names=nodes, node_size=node_size,filename='clos_centr.png')\n\ndf = pd.DataFrame(dc,index=['Closeness centrality'])\ndf.columns = nodes.values()\ndf\n\n\n\n\n\n  \n    \n      \n      Dublin\n      Paris\n      Milan\n      Rome\n      Naples\n      Moscow\n      Tokyo\n    \n  \n  \n    \n      Closeness centrality\n      0.4\n      0.4\n      0.545455\n      0.6\n      0.545455\n      0.4\n      0.4\n    \n  \n\n\n\n\n\n\n\n\nnx.betweenness_centrality(G)\n\n{1: 0.0,\n 2: 0.0,\n 3: 0.5333333333333333,\n 4: 0.6,\n 5: 0.5333333333333333,\n 6: 0.0,\n 7: 0.0}\n\n\n\ndc = nx.betweenness_centrality(G)\nnode_size=[(v + 0.1) * 400 for v in dc.values()]\ndraw_graph(G, node_names=nodes, node_size=node_size,filename='bet_centrality.png')\n\ndf = pd.DataFrame(dc,index=['Betweenness centrality'])\ndf.columns = nodes.values()\ndf\n\n\n\n\n\n  \n    \n      \n      Dublin\n      Paris\n      Milan\n      Rome\n      Naples\n      Moscow\n      Tokyo\n    \n  \n  \n    \n      Betweenness centrality\n      0.0\n      0.0\n      0.533333\n      0.6\n      0.533333\n      0.0\n      0.0\n    \n  \n\n\n\n\n\n\n\n\n\nAssortativity\n\nnx.degree_pearson_correlation_coefficient(G)\n\n-0.6\n\n\n\nG = nx.Graph()\nnodes = {1:'user1', 2:'user2', 3:'Football player', 4:'Fahsion blogger', 5:'user3', 6:'user4',\n         7:'user5', 8:'user6'}\nG.add_nodes_from(nodes.keys())\nG.add_edges_from([(1,3),(2,3),(7,3),(3,4),(5,4),(6,4),(8,4)])\n\ndraw_graph(G, node_names=nodes,filename='assortativity.png')\n\n\n\n\n\nnx.degree_pearson_correlation_coefficient(G)\n\n-0.7500000000000001\n\n\n\n\nModularity\n\nimport networkx.algorithms.community as nx_comm\n\nG = nx.Graph()\nnodes = {1:'Dublin',2:'Paris',3:'Milan',4:'Rome',5:'Naples',6:'Moscow',7:'Tokyo'}\nG.add_nodes_from(nodes.keys())\nG.add_edges_from([(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(6,7),(7,5)])\n\n# partitions can be provided manually\nprint(nx_comm.modularity(G, communities=[{1,2,3,4},{5,6,7}]))\n\n# or automatically computed using networkx\nprint(nx_comm.modularity(G, nx_comm.label_propagation_communities(G)))\n\n0.3671875\n0.3671875\n\n\n\n\nTransitivity\n\nnx.transitivity(G)\n\n0.5454545454545454"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter09/01_Neo4j_bindings.html",
    "href": "posts/2_Studies/GML/Chapter09/01_Neo4j_bindings.html",
    "title": "[GML] Chap9: Graph Database Connection",
    "section": "",
    "text": "Graph Database Connection\nIn the following, we will show you how to connect and query data on Neo4j, using python.\nIMPORTANT NOTE\nThis notebook requires that you have access to a working version of Neo4j. In order to install Neo4j locally, we advise you to refer to the Neo4j webpage (https://neo4j.com/download/) or to use docker (https://hub.docker.com/_/neo4j).\n\nwith open(\"./dataset/movieCreationQuery.txt\", \"rb\") as fid:\n    lines = fid.readlines()\n\n\nquery = \" \".join([line.decode(\"utf-8\").replace(\"\\n\", \"\") for line in lines])\n\n\nfrom neo4j import GraphDatabase\n\n\nuri = \"neo4j://localhost:7687\"\ndriver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"neo5j\"))\n\n\ndef run_query(tx, query):\n    return list(tx.run(query))\n\n\nwith driver.session() as session:\n    session.write_transaction(run_query, query)\n\nQuery\n\nquery = \"MATCH (n) RETURN count(*)\"\n\n\nwith driver.session() as session:\n    result = session.read_transaction(run_query, query)\n[r for r in result]\n\n[<Record count(*)=342>]\n\n\nDelete\n\nwith driver.session() as session:\n    result = session.write_transaction(run_query, \"MATCH (n)-[e]-() DELETE n, e\")"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\ndef draw_graph(G, node_names={}, nodes_label=[], node_size=900):\n    pos_nodes = nx.spring_layout(G)\n    \n    col = {0:\"steelblue\",1:\"red\",2:\"green\"}\n    \n    colors = [col[x] for x in nodes_label]\n    \n    nx.draw(G, pos_nodes, with_labels=True, node_color=colors, node_size=node_size, edge_color='gray', \n            arrowsize=30)\n    \n    \n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    plt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#degree-matrix",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#degree-matrix",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Degree matrix",
    "text": "Degree matrix\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#proximity-matrix",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#proximity-matrix",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Proximity matrix",
    "text": "Proximity matrix\n\nA = inv(D)*nx.to_numpy_matrix(G)\nA"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-propagation-implemenation",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-propagation-implemenation",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Label propagation implemenation",
    "text": "Label propagation implemenation\n\nimport numpy as np\nimport networkx as nx\nfrom numpy.linalg import inv\nfrom abc import ABCMeta, abstractmethod\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.multiclass import check_classification_targets\nfrom sklearn.utils.validation import check_is_fitted, _deprecate_positional_args\n\nclass GraphLabelPropagation(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3):\n\n        self.max_iter = max_iter\n        self.tol = tol\n\n    def predict(self, X):\n        \"\"\"Performs inductive inference across the model.\n        Parameters\n        ----------\n        X : A networkx array.\n            The data matrix.\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Predictions for input data.\n        \"\"\"\n        probas = self.predict_proba(X)\n        return self.classes_[np.argmax(probas, axis=1)].ravel()\n\n    def predict_proba(self, X):\n        \"\"\"Predict probability for each possible outcome.\n        Compute the probability estimates for each single node in X\n        and each possible outcome seen during training (categorical\n        distribution).\n        Parameters\n        ----------\n        X : A networkx array.\n        Returns\n        -------\n        probabilities : ndarray of shape (n_samples, n_classes)\n            Normalized probability distributions across\n            class labels.\n        \"\"\"\n        check_is_fitted(self)\n        \n        return self.label_distributions_\n    \n    def _validate_data(self, X, y):\n        if not isinstance(X, nx.Graph):\n            raise ValueError(\"Input should be a networkX graph\")\n        if not len(y) == len(X.nodes()):\n            raise ValueError(\"Label data input shape should be equal to the number of nodes in the graph\")\n        return X, y\n    \n    @staticmethod\n    def build_label(x,classes):\n        tmp = np.zeros((classes))\n        tmp[x] = 1\n        return tmp\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        A = inv(D)*nx.to_numpy_matrix(G)\n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it < self.max_iter & c_tool > self.tol:\n            Y = A*Y_prev\n            #force labeled nodes\n            Y[labeled_index] = Y0[labeled_index]\n            \n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            \n            Y_prev = Y\n            \n        self.label_distributions_ = Y\n        return self"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-propagation-execution",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-propagation-execution",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Label propagation execution",
    "text": "Label propagation execution\n\nglp = GraphLabelPropagation()\ny = np.array([-1 for x in range(len(G.nodes()))])\ny[0] = 1\ny[6] = 0\nglp.fit(G,y)\ntmp = glp.predict(G)\nprint(glp.predict_proba(G))\n\ndraw_graph(G, nodes_label=tmp+1, node_size=1200)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#degree-matrix-1",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#degree-matrix-1",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Degree matrix",
    "text": "Degree matrix\n\nimport numpy as np\nfrom numpy.linalg import inv\n\nD = [G.degree(n) for n in G.nodes()]\nD = np.diag(D)\nD"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#normalized-graph-laplacian-matrix",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#normalized-graph-laplacian-matrix",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Normalized graph Laplacian matrix",
    "text": "Normalized graph Laplacian matrix\n\nfrom scipy.linalg import fractional_matrix_power\nD_inv = fractional_matrix_power(D, -0.5)\nL = D_inv*nx.to_numpy_matrix(G)*D_inv\nL"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-spreading-implementation",
    "href": "posts/2_Studies/GML/Chapter04/02_Shallow_embeddings.html#label-spreading-implementation",
    "title": "[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법",
    "section": "Label spreading implementation",
    "text": "Label spreading implementation\n\nimport numpy as np\nimport networkx as nx\nfrom sklearn.preprocessing import normalize\nfrom scipy.linalg import fractional_matrix_power\nfrom sklearn.utils.multiclass import check_classification_targets\n\nclass GraphLabelSpreading(GraphLabelPropagation):\n    \"\"\"Graph label propagation module.\n    Parameters\n    ----------\n    max_iter : int, default=30\n        Change maximum number of iterations allowed.\n    tol : float, default=1e-3\n        Convergence tolerance: threshold to consider the system at steady\n        state.\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self, max_iter=30, tol=1e-3, alpha=0.6):\n\n        self.alpha = alpha\n        super().__init__(max_iter, tol)\n    \n    def fit(self, X, y):\n        \"\"\"Fit a semi-supervised label propagation model based\n        on the input graph G and corresponding label matrix y with a dedicated marker value for\n        unlabeled samples.\n        Parameters\n        ----------\n        X : A networkX array.\n        y : array-like of shape (n_samples,)\n            `n_labeled_samples` (unlabeled points are marked as -1)\n            All unlabeled samples will be transductively assigned labels.\n        Returns\n        -------\n        self : object\n        \"\"\"\n        X, y = self._validate_data(X, y)\n        self.X_ = X\n        check_classification_targets(y)\n\n        D = [X.degree(n) for n in X.nodes()]\n        D = np.diag(D)\n        D_inv = np.matrix(fractional_matrix_power(D,-0.5))\n        L = D_inv*nx.to_numpy_matrix(G)*D_inv\n        \n        # label construction\n        # construct a categorical distribution for classification only\n        unlabeled_index = np.where(y==-1)[0]\n        labeled_index = np.where(y!=-1)[0]\n        unique_classes = np.unique(y[labeled_index])\n        \n        self.classes_ = unique_classes\n        \n        Y0 = np.array([self.build_label(y[x], len(unique_classes)) \n                                 if x in labeled_index else np.zeros(len(unique_classes)) for x in range(len(y))])\n        \n        Y_prev = Y0\n        it = 0\n        c_tool = 10\n        \n        while it < self.max_iter & c_tool > self.tol:\n            Y = self.alpha*(L*Y_prev)+((1-self.alpha)*Y0)\n\n            it +=1\n            c_tol = np.sum(np.abs(Y-Y_prev))\n            Y_prev = Y\n        self.label_distributions_ = Y\n        return self"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/01_Feature_based_methods.html",
    "href": "posts/2_Studies/GML/Chapter04/01_Feature_based_methods.html",
    "title": "[GML] Chap4: 지도 그래프 학습 - 특징 기반 방법",
    "section": "",
    "text": "Feature based methods\nIn this notebook we will exploring a very naive (yet powerful) approach for solving graph-based supervised machine learning. The idea rely on the classic machine learning approach of handcrafted feature extraction.\nIn Chapter 1 you learned how local and global graph properties can be extracted from graphs. Those properties represent the graph itself and bring important informations which can be useful for classification.\nIn this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph\n\nfrom stellargraph import datasets\nfrom IPython.display import display, HTML\n\ndataset = datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\nTo compute the graph metrics, one way is to retrieve the adjacency matrix representation of each graph.\n\n# convert graphs from StellarGraph format to numpy adj matrices\nadjs = [graph.to_adjacency_matrix().A for graph in graphs]\n# convert labes fom Pandas.Series to numpy array\nlabels = graph_labels.to_numpy(dtype=int)\n\n\nimport numpy as np\nimport networkx as nx\n\nmetrics = []\nfor adj in adjs:\n  G = nx.from_numpy_matrix(adj)\n  # basic properties\n  num_edges = G.number_of_edges()\n  # clustering measures\n  cc = nx.average_clustering(G)\n  # measure of efficiency\n  eff = nx.global_efficiency(G)\n\n  metrics.append([num_edges, cc, eff])\n\n\nWe can now exploit scikit-learn utilities to create a train and test set. In our experiments, we will be using 70% of the dataset as training set and the remaining as testset\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)\n\nAs commonly done in many Machine Learning workflows, we preprocess features to have zero mean and unit standard deviation\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nIt’s now time for training a proper algorithm. We chose a support vector machine for this task\n\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclf = svm.SVC()\nclf.fit(X_train_scaled, y_train)\n\ny_pred = clf.predict(X_test_scaled)\n\nprint('Accuracy', accuracy_score(y_test,y_pred))\nprint('Precision', precision_score(y_test,y_pred))\nprint('Recall', recall_score(y_test,y_pred))\nprint('F1-score', f1_score(y_test,y_pred))"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/04_Graph_Neural_Networks.html",
    "href": "posts/2_Studies/GML/Chapter04/04_Graph_Neural_Networks.html",
    "title": "[GML] Chap4: 지도 그래프 학습 - Graph CNN",
    "section": "",
    "text": "In this notebook we will exploring a very naive (yet powerful) approach for solving graph-based supervised machine learning. The idea rely on the classic machine learning approach of handcrafted feature extraction.\nIn Chapter 1 you learned how local and global graph properties can be extracted from graphs. Those properties represent the graph itself and bring important informations which can be useful for classification.\n\n!pip install stellargraph\n\nUninstalling stellargraph-1.2.1:\n  Successfully uninstalled stellargraph-1.2.1\n\n\nIn this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph\n\nfrom stellargraph import datasets\nfrom IPython.display import display, HTML\n\ndataset = datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\nTo compute the graph metrics, one way is to retrieve the adjacency matrix representation of each graph.\n\n# convert graphs from StellarGraph format to numpy adj matrices\nadjs = [graph.to_adjacency_matrix().A for graph in graphs]\n# convert labes fom Pandas.Series to numpy array\nlabels = graph_labels.to_numpy(dtype=int)\n\n\nimport numpy as np\nimport networkx as nx\n\nmetrics = []\nfor adj in adjs:\n    G = nx.from_numpy_matrix(adj)\n    # basic properties\n    num_edges = G.number_of_edges()\n    # clustering measures\n    cc = nx.average_clustering(G)\n    # measure of efficiency\n    eff = nx.global_efficiency(G)\n\n    metrics.append([num_edges, cc, eff])\n\n\nWe can now exploit scikit-learn utilities to create a train and test set. In our experiments, we will be using 70% of the dataset as training set and the remaining as testset\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)\n\nAs commonly done in many Machine Learning workflows, we preprocess features to have zero mean and unit standard deviation\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nIt’s now time for training a proper algorithm. We chose a support vector machine for this task\n\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nclf = svm.SVC()\nclf.fit(X_train_scaled, y_train)\n\ny_pred = clf.predict(X_test_scaled)\n\nprint('Accuracy', accuracy_score(y_test,y_pred))\nprint('Precision', precision_score(y_test,y_pred))\nprint('Recall', recall_score(y_test,y_pred))\nprint('F1-score', f1_score(y_test,y_pred))\n\nAccuracy 0.7455089820359282\nPrecision 0.7709251101321586\nRecall 0.8413461538461539\nF1-score 0.8045977011494253"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/04_Graph_Neural_Networks.html#supervised-node-representation-learning-using-graphsage",
    "href": "posts/2_Studies/GML/Chapter04/04_Graph_Neural_Networks.html#supervised-node-representation-learning-using-graphsage",
    "title": "[GML] Chap4: 지도 그래프 학습 - Graph CNN",
    "section": "Supervised node representation learning using GraphSAGE",
    "text": "Supervised node representation learning using GraphSAGE\n\nfrom stellargraph import datasets\nfrom IPython.display import display, HTML\n\ndataset = datasets.Cora()\ndisplay(HTML(dataset.description))\nG, nodes = dataset.load()\n\nThe Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n\n\nLet’s split the dataset into training and testing set\n\nfrom sklearn.model_selection import train_test_split\ntrain_nodes, test_nodes = train_test_split(\n    nodes, train_size=0.1, test_size=None, stratify=nodes\n)\n\nSince we are performing a categorical classification, it is useful to represent each categorical label in its one-hot encoding\n\nfrom sklearn import preprocessing, feature_extraction, model_selection\nlabel_encoding = preprocessing.LabelBinarizer()\ntrain_labels = label_encoding.fit_transform(train_nodes)\ntest_labels = label_encoding.transform(test_nodes)\n\nIt’s now time for creating the mdoel. It will be composed by two GraphSAGE layers followed by a Dense layer with softmax activation for classification\n\nfrom stellargraph.mapper import GraphSAGENodeGenerator\nbatchsize = 50\nn_samples = [10, 5, 7]\ngenerator = GraphSAGENodeGenerator(G, batchsize, n_samples)\n\n\nfrom stellargraph.layer import GraphSAGE\nfrom tensorflow.keras.layers import Dense\n\ngraphsage_model = GraphSAGE(\n    layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6,\n)\n\n\ngnn_inp, gnn_out = graphsage_model.in_out_tensors()\noutputs = Dense(units=train_labels.shape[1], activation=\"softmax\")(gnn_out)\n\n\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\nmodel = Model(inputs=gnn_inp, outputs=outputs)\nmodel.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=[\"acc\"],)\n\nWe will use the flow function of the generator for feeding the model with the train and the test set.\n\ntrain_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)\ntest_gen = generator.flow(test_nodes.index, test_labels)\n\nFinally, let’s train the model!\n\nhistory = model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter04/03_Graph_regularization_graph_neural_training.html",
    "href": "posts/2_Studies/GML/Chapter04/03_Graph_regularization_graph_neural_training.html",
    "title": "[GML] Chap4: 지도 그래프 학습 - 그래프 정규화 방법",
    "section": "",
    "text": "Neural Graph Learning and Graph Regularization\nIn this tutorial, we will be creating a graph regularized version for a topic classification task. The task is to classify paper depending on their content. However in order to do so, we will also use the information encoded in the citation network that relates documents among each other. Of course, we do know that this kind of information is indeed powerful as papers belonging to the same subject tend to reference each other.\n\nLoad Dataset\nFor this tutorial we will be using the Cora dataset available in the stellargraph library\n\nfrom stellargraph import datasets\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\nWe now create the Dataset object where we will both include information of the targeted sample (node) and its neighbors. In the following we will also allow to control the number of labelling instances to be used, in order to reproduce and evaluate the classification performance in a semi-supervised setting.\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)\n\nWe split the dataset into a training set and a test set\n\ntrainingSet, testSet = semisupervisedDataset(G, labels)\n\n\nimport tensorflow as tf\n\n\nfrom tensorflow.data import Dataset\n\n\nvocabularySize = 1433\n\n\nneighbors=2\ndefaultWord = tf.constant(0, dtype=tf.float32, shape=[vocabularySize])\n\ndef parseExample(example, training=True):\n    schema = {\n        'words': tf.io.FixedLenFeature([vocabularySize], tf.float32, default_value=defaultWord),\n        'label': tf.io.FixedLenFeature((), tf.int64, default_value=-1)\n    }\n    \n    if training is True:\n        for i in range(neighbors):\n            name = f\"{GRAPH_PREFIX}_{i}\"\n            schema[f\"{name}_weight\"] = tf.io.FixedLenFeature([1], tf.float32, default_value=[0.0])\n            schema[f\"{name}_words\"] = tf.io.FixedLenFeature([vocabularySize], tf.float32, default_value=defaultWord)\n    \n    features = tf.io.parse_single_example(example, schema)\n    \n    label = features.pop(\"label\")\n    return features, label\n\n\ndef sampleGenerator(dataset):\n    def wrapper():\n        for example in dataset:\n            yield example\n    return wrapper\n            \nmyTrain = Dataset \\\n    .from_generator(sampleGenerator(trainingSet), output_types=tf.string, output_shapes=()) \\\n    .map(lambda x: parseExample(x, True))\n\nmyTest = Dataset \\\n    .from_generator(sampleGenerator(testSet), output_types=tf.string, output_shapes=()) \\\n    .map(lambda x: parseExample(x, False))\n\n\nfor features, labels in myTrain.batch(10).take(1):\n    print(features)\n    print(labels)\n\n{'NL_nbr_0_weight': <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[2.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [2.],\n       [1.],\n       [1.]], dtype=float32)>, 'NL_nbr_0_words': <tf.Tensor: shape=(10, 1433), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_1_weight': <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]], dtype=float32)>, 'NL_nbr_1_words': <tf.Tensor: shape=(10, 1433), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'words': <tf.Tensor: shape=(10, 1433), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>}\ntf.Tensor([1 1 4 1 0 5 2 1 6 3], shape=(10,), dtype=int64)\n\n\n\nfor features, labels in myTest.batch(10).take(1):\n    print(features)\n    print(labels)\n\n{'words': <tf.Tensor: shape=(10, 1433), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>}\ntf.Tensor([1 1 3 3 2 4 5 3 2 6], shape=(10,), dtype=int64)\n\n\n\n\nCreating the model\nWe now create the model that we will use to classify the documents\n\nlayers = [50, 50]\n\n\n\"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\ndef create_model(num_units):\n    inputs = tf.keras.Input(\n          shape=(vocabularySize,), dtype='float32', name='words'\n    )\n\n    # outputs = tf.keras.layers.Dense(len(label_index), activation='softmax')(inputs)\n\n    cur_layer =  inputs\n\n    for num_units in layers:\n        cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n        cur_layer = tf.keras.layers.Dropout(0.8)(cur_layer)\n\n    outputs = tf.keras.layers.Dense(len(label_index), activation='softmax')(cur_layer)\n\n    return tf.keras.Model(inputs, outputs=outputs)\n\n\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\nVanilla Model\nWe first train a simple, vanilla version that does not use the citation network information\n\nmodel = create_model([50, 50])\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])\n\n\nmodel.summary()\n\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nwords (InputLayer)           [(None, 1433)]            0         \n_________________________________________________________________\ndense (Dense)                (None, 50)                71700     \n_________________________________________________________________\ndropout (Dropout)            (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                2550      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 357       \n=================================================================\nTotal params: 74,607\nTrainable params: 74,607\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.fit(myTrain.batch(128), epochs=200, verbose=1, validation_data=myTest.batch(128),\n          callbacks=[TensorBoard(log_dir='/tmp/noRegularization')])\n\nEpoch 1/200\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-4/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['NL_nbr_0_weight', 'NL_nbr_0_words', 'NL_nbr_1_weight', 'NL_nbr_1_words'] which did not match any model input. They will be ignored by the model.\n  [n for n in tensors.keys() if n not in ref_input_names])\n\n\n5/5 [==============================] - 2s 323ms/step - loss: 1.9820 - accuracy: 0.1390 - val_loss: 1.9311 - val_accuracy: 0.2022\nEpoch 2/200\n5/5 [==============================] - 0s 114ms/step - loss: 1.9699 - accuracy: 0.1779 - val_loss: 1.9219 - val_accuracy: 0.2373\nEpoch 3/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.9632 - accuracy: 0.1964 - val_loss: 1.9135 - val_accuracy: 0.2742\nEpoch 4/200\n5/5 [==============================] - 0s 106ms/step - loss: 2.0003 - accuracy: 0.1981 - val_loss: 1.9071 - val_accuracy: 0.2886\nEpoch 5/200\n5/5 [==============================] - 1s 117ms/step - loss: 1.9681 - accuracy: 0.1810 - val_loss: 1.9010 - val_accuracy: 0.2941\nEpoch 6/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.8860 - accuracy: 0.2400 - val_loss: 1.8951 - val_accuracy: 0.3010\nEpoch 7/200\n5/5 [==============================] - 1s 124ms/step - loss: 1.8809 - accuracy: 0.2603 - val_loss: 1.8888 - val_accuracy: 0.3052\nEpoch 8/200\n5/5 [==============================] - 0s 111ms/step - loss: 1.8710 - accuracy: 0.2564 - val_loss: 1.8819 - val_accuracy: 0.3038\nEpoch 9/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.8745 - accuracy: 0.2519 - val_loss: 1.8754 - val_accuracy: 0.3015\nEpoch 10/200\n5/5 [==============================] - 0s 111ms/step - loss: 1.8847 - accuracy: 0.2621 - val_loss: 1.8691 - val_accuracy: 0.3024\nEpoch 11/200\n5/5 [==============================] - 1s 117ms/step - loss: 1.8810 - accuracy: 0.2581 - val_loss: 1.8633 - val_accuracy: 0.3029\nEpoch 12/200\n5/5 [==============================] - 1s 124ms/step - loss: 1.8815 - accuracy: 0.2402 - val_loss: 1.8580 - val_accuracy: 0.3024\nEpoch 13/200\n5/5 [==============================] - 0s 109ms/step - loss: 1.8592 - accuracy: 0.2660 - val_loss: 1.8530 - val_accuracy: 0.3024\nEpoch 14/200\n5/5 [==============================] - 0s 106ms/step - loss: 1.8818 - accuracy: 0.2764 - val_loss: 1.8481 - val_accuracy: 0.3024\nEpoch 15/200\n5/5 [==============================] - 0s 112ms/step - loss: 1.8602 - accuracy: 0.2645 - val_loss: 1.8436 - val_accuracy: 0.3029\nEpoch 16/200\n5/5 [==============================] - 0s 110ms/step - loss: 1.8200 - accuracy: 0.2913 - val_loss: 1.8386 - val_accuracy: 0.3024\nEpoch 17/200\n5/5 [==============================] - 0s 105ms/step - loss: 1.7878 - accuracy: 0.2694 - val_loss: 1.8328 - val_accuracy: 0.3024\nEpoch 18/200\n5/5 [==============================] - 0s 104ms/step - loss: 1.8208 - accuracy: 0.2823 - val_loss: 1.8262 - val_accuracy: 0.3019\nEpoch 19/200\n5/5 [==============================] - 0s 116ms/step - loss: 1.8273 - accuracy: 0.2808 - val_loss: 1.8200 - val_accuracy: 0.3019\nEpoch 20/200\n5/5 [==============================] - 1s 122ms/step - loss: 1.8076 - accuracy: 0.2861 - val_loss: 1.8137 - val_accuracy: 0.3019\nEpoch 21/200\n5/5 [==============================] - 0s 112ms/step - loss: 1.7817 - accuracy: 0.2773 - val_loss: 1.8071 - val_accuracy: 0.3019\nEpoch 22/200\n5/5 [==============================] - 1s 148ms/step - loss: 1.7817 - accuracy: 0.2879 - val_loss: 1.7996 - val_accuracy: 0.3019\nEpoch 23/200\n5/5 [==============================] - 0s 106ms/step - loss: 1.8022 - accuracy: 0.2694 - val_loss: 1.7920 - val_accuracy: 0.3019\nEpoch 24/200\n5/5 [==============================] - 0s 113ms/step - loss: 1.7664 - accuracy: 0.2857 - val_loss: 1.7837 - val_accuracy: 0.3019\nEpoch 25/200\n5/5 [==============================] - 1s 118ms/step - loss: 1.7413 - accuracy: 0.3139 - val_loss: 1.7750 - val_accuracy: 0.3019\nEpoch 26/200\n5/5 [==============================] - 0s 111ms/step - loss: 1.7423 - accuracy: 0.2957 - val_loss: 1.7673 - val_accuracy: 0.3019\nEpoch 27/200\n5/5 [==============================] - 0s 104ms/step - loss: 1.7182 - accuracy: 0.3222 - val_loss: 1.7600 - val_accuracy: 0.3019\nEpoch 28/200\n5/5 [==============================] - 0s 109ms/step - loss: 1.7282 - accuracy: 0.3140 - val_loss: 1.7521 - val_accuracy: 0.3019\nEpoch 29/200\n5/5 [==============================] - 1s 132ms/step - loss: 1.7342 - accuracy: 0.2928 - val_loss: 1.7451 - val_accuracy: 0.3019\nEpoch 30/200\n5/5 [==============================] - 0s 116ms/step - loss: 1.6762 - accuracy: 0.3217 - val_loss: 1.7368 - val_accuracy: 0.3019\nEpoch 31/200\n5/5 [==============================] - 0s 106ms/step - loss: 1.7440 - accuracy: 0.2937 - val_loss: 1.7286 - val_accuracy: 0.3019\nEpoch 32/200\n5/5 [==============================] - 1s 119ms/step - loss: 1.7002 - accuracy: 0.2902 - val_loss: 1.7214 - val_accuracy: 0.3024\nEpoch 33/200\n5/5 [==============================] - 0s 106ms/step - loss: 1.6856 - accuracy: 0.3105 - val_loss: 1.7143 - val_accuracy: 0.3029\nEpoch 34/200\n5/5 [==============================] - 1s 120ms/step - loss: 1.6947 - accuracy: 0.2990 - val_loss: 1.7079 - val_accuracy: 0.3033\nEpoch 35/200\n5/5 [==============================] - 0s 107ms/step - loss: 1.6785 - accuracy: 0.3159 - val_loss: 1.7024 - val_accuracy: 0.3038\nEpoch 36/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.6167 - accuracy: 0.3351 - val_loss: 1.6960 - val_accuracy: 0.3052\nEpoch 37/200\n5/5 [==============================] - 0s 114ms/step - loss: 1.6379 - accuracy: 0.3163 - val_loss: 1.6889 - val_accuracy: 0.3056\nEpoch 38/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.6286 - accuracy: 0.3426 - val_loss: 1.6814 - val_accuracy: 0.3066\nEpoch 39/200\n5/5 [==============================] - 1s 118ms/step - loss: 1.6328 - accuracy: 0.3559 - val_loss: 1.6738 - val_accuracy: 0.3084\nEpoch 40/200\n5/5 [==============================] - 1s 118ms/step - loss: 1.6194 - accuracy: 0.3266 - val_loss: 1.6667 - val_accuracy: 0.3126\nEpoch 41/200\n5/5 [==============================] - 1s 117ms/step - loss: 1.5999 - accuracy: 0.3031 - val_loss: 1.6612 - val_accuracy: 0.3181\nEpoch 42/200\n5/5 [==============================] - 0s 111ms/step - loss: 1.6033 - accuracy: 0.3178 - val_loss: 1.6555 - val_accuracy: 0.3246\nEpoch 43/200\n5/5 [==============================] - 0s 111ms/step - loss: 1.6016 - accuracy: 0.3283 - val_loss: 1.6490 - val_accuracy: 0.3338\nEpoch 44/200\n5/5 [==============================] - 0s 107ms/step - loss: 1.5466 - accuracy: 0.3435 - val_loss: 1.6403 - val_accuracy: 0.3430\nEpoch 45/200\n5/5 [==============================] - 0s 107ms/step - loss: 1.5700 - accuracy: 0.3411 - val_loss: 1.6300 - val_accuracy: 0.3500\nEpoch 46/200\n5/5 [==============================] - 1s 152ms/step - loss: 1.5677 - accuracy: 0.3146 - val_loss: 1.6208 - val_accuracy: 0.3587\nEpoch 47/200\n5/5 [==============================] - 1s 163ms/step - loss: 1.5848 - accuracy: 0.3527 - val_loss: 1.6121 - val_accuracy: 0.3652\nEpoch 48/200\n5/5 [==============================] - 1s 159ms/step - loss: 1.5458 - accuracy: 0.3726 - val_loss: 1.6040 - val_accuracy: 0.3703\nEpoch 49/200\n5/5 [==============================] - 1s 128ms/step - loss: 1.5457 - accuracy: 0.3176 - val_loss: 1.5965 - val_accuracy: 0.3758\nEpoch 50/200\n5/5 [==============================] - 1s 143ms/step - loss: 1.5226 - accuracy: 0.3776 - val_loss: 1.5889 - val_accuracy: 0.3804\nEpoch 51/200\n5/5 [==============================] - 0s 113ms/step - loss: 1.5445 - accuracy: 0.3343 - val_loss: 1.5813 - val_accuracy: 0.3869\nEpoch 52/200\n5/5 [==============================] - 1s 127ms/step - loss: 1.5416 - accuracy: 0.3672 - val_loss: 1.5740 - val_accuracy: 0.3938\nEpoch 53/200\n5/5 [==============================] - 0s 113ms/step - loss: 1.4925 - accuracy: 0.3735 - val_loss: 1.5666 - val_accuracy: 0.3984\nEpoch 54/200\n5/5 [==============================] - 1s 120ms/step - loss: 1.4956 - accuracy: 0.3615 - val_loss: 1.5591 - val_accuracy: 0.4035\nEpoch 55/200\n5/5 [==============================] - 0s 108ms/step - loss: 1.5306 - accuracy: 0.3399 - val_loss: 1.5520 - val_accuracy: 0.4090\nEpoch 56/200\n5/5 [==============================] - 0s 115ms/step - loss: 1.4512 - accuracy: 0.3820 - val_loss: 1.5445 - val_accuracy: 0.4155\nEpoch 57/200\n5/5 [==============================] - 1s 131ms/step - loss: 1.4307 - accuracy: 0.3891 - val_loss: 1.5374 - val_accuracy: 0.4183\nEpoch 58/200\n5/5 [==============================] - 1s 120ms/step - loss: 1.4430 - accuracy: 0.3657 - val_loss: 1.5300 - val_accuracy: 0.4247\nEpoch 59/200\n5/5 [==============================] - 1s 143ms/step - loss: 1.4438 - accuracy: 0.3667 - val_loss: 1.5221 - val_accuracy: 0.4280\nEpoch 60/200\n5/5 [==============================] - 0s 112ms/step - loss: 1.4592 - accuracy: 0.3633 - val_loss: 1.5149 - val_accuracy: 0.4326\nEpoch 61/200\n5/5 [==============================] - 1s 182ms/step - loss: 1.4056 - accuracy: 0.3988 - val_loss: 1.5073 - val_accuracy: 0.4367\nEpoch 62/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.4253 - accuracy: 0.3948 - val_loss: 1.5000 - val_accuracy: 0.4414\nEpoch 63/200\n5/5 [==============================] - 1s 149ms/step - loss: 1.3744 - accuracy: 0.4412 - val_loss: 1.4925 - val_accuracy: 0.4460\nEpoch 64/200\n5/5 [==============================] - 1s 133ms/step - loss: 1.3908 - accuracy: 0.4052 - val_loss: 1.4845 - val_accuracy: 0.4478\nEpoch 65/200\n5/5 [==============================] - 1s 119ms/step - loss: 1.3819 - accuracy: 0.4087 - val_loss: 1.4763 - val_accuracy: 0.4538\nEpoch 66/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.3798 - accuracy: 0.4137 - val_loss: 1.4688 - val_accuracy: 0.4566\nEpoch 67/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.3816 - accuracy: 0.4319 - val_loss: 1.4618 - val_accuracy: 0.4580\nEpoch 68/200\n5/5 [==============================] - 1s 122ms/step - loss: 1.3548 - accuracy: 0.4387 - val_loss: 1.4549 - val_accuracy: 0.4594\nEpoch 69/200\n5/5 [==============================] - 1s 144ms/step - loss: 1.3364 - accuracy: 0.4546 - val_loss: 1.4484 - val_accuracy: 0.4608\nEpoch 70/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.3729 - accuracy: 0.4436 - val_loss: 1.4427 - val_accuracy: 0.4612\nEpoch 71/200\n5/5 [==============================] - 1s 142ms/step - loss: 1.3252 - accuracy: 0.4525 - val_loss: 1.4366 - val_accuracy: 0.4658\nEpoch 72/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.3500 - accuracy: 0.4421 - val_loss: 1.4302 - val_accuracy: 0.4695\nEpoch 73/200\n5/5 [==============================] - 1s 121ms/step - loss: 1.2927 - accuracy: 0.4529 - val_loss: 1.4238 - val_accuracy: 0.4718\nEpoch 74/200\n5/5 [==============================] - 1s 129ms/step - loss: 1.2843 - accuracy: 0.4799 - val_loss: 1.4185 - val_accuracy: 0.4760\nEpoch 75/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.2724 - accuracy: 0.5005 - val_loss: 1.4128 - val_accuracy: 0.4829\nEpoch 76/200\n5/5 [==============================] - 1s 119ms/step - loss: 1.3129 - accuracy: 0.4651 - val_loss: 1.4083 - val_accuracy: 0.4848\nEpoch 77/200\n5/5 [==============================] - 1s 121ms/step - loss: 1.2618 - accuracy: 0.4697 - val_loss: 1.4047 - val_accuracy: 0.4861\nEpoch 78/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.2413 - accuracy: 0.5125 - val_loss: 1.4003 - val_accuracy: 0.4885\nEpoch 79/200\n5/5 [==============================] - 1s 118ms/step - loss: 1.2087 - accuracy: 0.5341 - val_loss: 1.3955 - val_accuracy: 0.4903\nEpoch 80/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.2851 - accuracy: 0.4639 - val_loss: 1.3900 - val_accuracy: 0.4935\nEpoch 81/200\n5/5 [==============================] - 1s 155ms/step - loss: 1.2209 - accuracy: 0.5177 - val_loss: 1.3837 - val_accuracy: 0.4986\nEpoch 82/200\n5/5 [==============================] - 1s 165ms/step - loss: 1.1996 - accuracy: 0.5319 - val_loss: 1.3780 - val_accuracy: 0.5005\nEpoch 83/200\n5/5 [==============================] - 1s 136ms/step - loss: 1.2368 - accuracy: 0.4949 - val_loss: 1.3740 - val_accuracy: 0.5046\nEpoch 84/200\n5/5 [==============================] - 1s 128ms/step - loss: 1.2231 - accuracy: 0.5320 - val_loss: 1.3710 - val_accuracy: 0.5069\nEpoch 85/200\n5/5 [==============================] - 1s 124ms/step - loss: 1.2342 - accuracy: 0.5022 - val_loss: 1.3667 - val_accuracy: 0.5106\nEpoch 86/200\n5/5 [==============================] - 1s 121ms/step - loss: 1.2375 - accuracy: 0.4890 - val_loss: 1.3614 - val_accuracy: 0.5125\nEpoch 87/200\n5/5 [==============================] - 1s 126ms/step - loss: 1.2203 - accuracy: 0.5061 - val_loss: 1.3577 - val_accuracy: 0.5162\nEpoch 88/200\n5/5 [==============================] - 1s 120ms/step - loss: 1.2521 - accuracy: 0.5327 - val_loss: 1.3570 - val_accuracy: 0.5185\nEpoch 89/200\n5/5 [==============================] - 0s 117ms/step - loss: 1.1961 - accuracy: 0.5341 - val_loss: 1.3572 - val_accuracy: 0.5194\nEpoch 90/200\n5/5 [==============================] - 1s 123ms/step - loss: 1.1892 - accuracy: 0.5204 - val_loss: 1.3551 - val_accuracy: 0.5226\nEpoch 91/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.1887 - accuracy: 0.5302 - val_loss: 1.3544 - val_accuracy: 0.5268\nEpoch 92/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.1729 - accuracy: 0.5289 - val_loss: 1.3553 - val_accuracy: 0.5254\nEpoch 93/200\n5/5 [==============================] - 1s 129ms/step - loss: 1.1671 - accuracy: 0.5114 - val_loss: 1.3555 - val_accuracy: 0.5277\nEpoch 94/200\n5/5 [==============================] - 1s 121ms/step - loss: 1.1611 - accuracy: 0.5413 - val_loss: 1.3543 - val_accuracy: 0.5277\nEpoch 95/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.1913 - accuracy: 0.5282 - val_loss: 1.3525 - val_accuracy: 0.5259\nEpoch 96/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.1435 - accuracy: 0.5480 - val_loss: 1.3524 - val_accuracy: 0.5263\nEpoch 97/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.1390 - accuracy: 0.5413 - val_loss: 1.3516 - val_accuracy: 0.5295\nEpoch 98/200\n5/5 [==============================] - 1s 120ms/step - loss: 1.1598 - accuracy: 0.5492 - val_loss: 1.3472 - val_accuracy: 0.5328\nEpoch 99/200\n5/5 [==============================] - 1s 122ms/step - loss: 1.1754 - accuracy: 0.4923 - val_loss: 1.3409 - val_accuracy: 0.5360\nEpoch 100/200\n5/5 [==============================] - 1s 126ms/step - loss: 1.1441 - accuracy: 0.5521 - val_loss: 1.3364 - val_accuracy: 0.5397\nEpoch 101/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.1213 - accuracy: 0.5697 - val_loss: 1.3323 - val_accuracy: 0.5416\nEpoch 102/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.1414 - accuracy: 0.5274 - val_loss: 1.3311 - val_accuracy: 0.5420\nEpoch 103/200\n5/5 [==============================] - 1s 151ms/step - loss: 1.0863 - accuracy: 0.5567 - val_loss: 1.3315 - val_accuracy: 0.5439\nEpoch 104/200\n5/5 [==============================] - 1s 132ms/step - loss: 1.0917 - accuracy: 0.5921 - val_loss: 1.3338 - val_accuracy: 0.5439\nEpoch 105/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.1084 - accuracy: 0.5622 - val_loss: 1.3379 - val_accuracy: 0.5416\nEpoch 106/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.0470 - accuracy: 0.5800 - val_loss: 1.3419 - val_accuracy: 0.5425\nEpoch 107/200\n5/5 [==============================] - 1s 124ms/step - loss: 1.0953 - accuracy: 0.5640 - val_loss: 1.3420 - val_accuracy: 0.5429\nEpoch 108/200\n5/5 [==============================] - 1s 133ms/step - loss: 1.0905 - accuracy: 0.5702 - val_loss: 1.3417 - val_accuracy: 0.5434\nEpoch 109/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.1185 - accuracy: 0.5661 - val_loss: 1.3425 - val_accuracy: 0.5457\nEpoch 110/200\n5/5 [==============================] - 1s 129ms/step - loss: 1.0886 - accuracy: 0.5817 - val_loss: 1.3417 - val_accuracy: 0.5466\nEpoch 111/200\n5/5 [==============================] - 1s 132ms/step - loss: 1.0223 - accuracy: 0.6096 - val_loss: 1.3427 - val_accuracy: 0.5476\nEpoch 112/200\n5/5 [==============================] - 1s 130ms/step - loss: 1.0619 - accuracy: 0.5801 - val_loss: 1.3440 - val_accuracy: 0.5485\nEpoch 113/200\n5/5 [==============================] - 1s 125ms/step - loss: 1.0970 - accuracy: 0.5693 - val_loss: 1.3435 - val_accuracy: 0.5489\nEpoch 114/200\n5/5 [==============================] - 1s 122ms/step - loss: 1.0307 - accuracy: 0.5842 - val_loss: 1.3434 - val_accuracy: 0.5503\nEpoch 115/200\n5/5 [==============================] - 1s 128ms/step - loss: 1.0729 - accuracy: 0.5749 - val_loss: 1.3409 - val_accuracy: 0.5512\nEpoch 116/200\n5/5 [==============================] - 1s 132ms/step - loss: 1.0652 - accuracy: 0.5892 - val_loss: 1.3405 - val_accuracy: 0.5526\nEpoch 117/200\n5/5 [==============================] - 1s 146ms/step - loss: 1.0331 - accuracy: 0.5950 - val_loss: 1.3448 - val_accuracy: 0.5531\nEpoch 118/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.0661 - accuracy: 0.6041 - val_loss: 1.3518 - val_accuracy: 0.5536\nEpoch 119/200\n5/5 [==============================] - 1s 127ms/step - loss: 1.0243 - accuracy: 0.6106 - val_loss: 1.3588 - val_accuracy: 0.5512\nEpoch 120/200\n5/5 [==============================] - 1s 129ms/step - loss: 0.9959 - accuracy: 0.6434 - val_loss: 1.3643 - val_accuracy: 0.5508\nEpoch 121/200\n5/5 [==============================] - 1s 127ms/step - loss: 1.0377 - accuracy: 0.6012 - val_loss: 1.3699 - val_accuracy: 0.5503\nEpoch 122/200\n5/5 [==============================] - 1s 129ms/step - loss: 1.0587 - accuracy: 0.5726 - val_loss: 1.3722 - val_accuracy: 0.5512\nEpoch 123/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.0212 - accuracy: 0.5867 - val_loss: 1.3694 - val_accuracy: 0.5536\nEpoch 124/200\n5/5 [==============================] - 1s 189ms/step - loss: 1.0011 - accuracy: 0.6239 - val_loss: 1.3711 - val_accuracy: 0.5522\nEpoch 125/200\n5/5 [==============================] - 1s 141ms/step - loss: 0.9889 - accuracy: 0.6348 - val_loss: 1.3728 - val_accuracy: 0.5536\nEpoch 126/200\n5/5 [==============================] - 1s 181ms/step - loss: 1.0428 - accuracy: 0.5786 - val_loss: 1.3751 - val_accuracy: 0.5522\nEpoch 127/200\n5/5 [==============================] - 1s 135ms/step - loss: 0.9908 - accuracy: 0.6201 - val_loss: 1.3732 - val_accuracy: 0.5545\nEpoch 128/200\n5/5 [==============================] - 1s 181ms/step - loss: 1.0277 - accuracy: 0.5775 - val_loss: 1.3710 - val_accuracy: 0.5572\nEpoch 129/200\n5/5 [==============================] - 1s 165ms/step - loss: 0.9862 - accuracy: 0.6151 - val_loss: 1.3746 - val_accuracy: 0.5568\nEpoch 130/200\n5/5 [==============================] - 1s 167ms/step - loss: 0.9637 - accuracy: 0.6345 - val_loss: 1.3762 - val_accuracy: 0.5572\nEpoch 131/200\n5/5 [==============================] - 1s 140ms/step - loss: 0.9107 - accuracy: 0.6454 - val_loss: 1.3763 - val_accuracy: 0.5591\nEpoch 132/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.0057 - accuracy: 0.6081 - val_loss: 1.3782 - val_accuracy: 0.5605\nEpoch 133/200\n5/5 [==============================] - 1s 128ms/step - loss: 0.9786 - accuracy: 0.6209 - val_loss: 1.3788 - val_accuracy: 0.5623\nEpoch 134/200\n5/5 [==============================] - 1s 123ms/step - loss: 1.0011 - accuracy: 0.5990 - val_loss: 1.3778 - val_accuracy: 0.5651\nEpoch 135/200\n5/5 [==============================] - 1s 125ms/step - loss: 0.9755 - accuracy: 0.6250 - val_loss: 1.3808 - val_accuracy: 0.5660\nEpoch 136/200\n5/5 [==============================] - 1s 126ms/step - loss: 0.9770 - accuracy: 0.6175 - val_loss: 1.3842 - val_accuracy: 0.5660\nEpoch 137/200\n5/5 [==============================] - 1s 127ms/step - loss: 0.9876 - accuracy: 0.6137 - val_loss: 1.3843 - val_accuracy: 0.5679\nEpoch 138/200\n5/5 [==============================] - 1s 136ms/step - loss: 0.9466 - accuracy: 0.6355 - val_loss: 1.3863 - val_accuracy: 0.5683\nEpoch 139/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.9569 - accuracy: 0.6377 - val_loss: 1.3873 - val_accuracy: 0.5679\nEpoch 140/200\n5/5 [==============================] - 1s 129ms/step - loss: 0.9659 - accuracy: 0.6065 - val_loss: 1.3893 - val_accuracy: 0.5693\nEpoch 141/200\n5/5 [==============================] - 1s 139ms/step - loss: 0.9756 - accuracy: 0.6249 - val_loss: 1.3956 - val_accuracy: 0.5674\nEpoch 142/200\n5/5 [==============================] - 1s 131ms/step - loss: 0.9219 - accuracy: 0.6481 - val_loss: 1.4022 - val_accuracy: 0.5674\nEpoch 143/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.9725 - accuracy: 0.6032 - val_loss: 1.4084 - val_accuracy: 0.5683\nEpoch 144/200\n5/5 [==============================] - 1s 138ms/step - loss: 0.9968 - accuracy: 0.6080 - val_loss: 1.4106 - val_accuracy: 0.5693\nEpoch 145/200\n5/5 [==============================] - 1s 129ms/step - loss: 0.9467 - accuracy: 0.6306 - val_loss: 1.4139 - val_accuracy: 0.5688\nEpoch 146/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.9845 - accuracy: 0.5994 - val_loss: 1.4185 - val_accuracy: 0.5683\nEpoch 147/200\n5/5 [==============================] - 1s 127ms/step - loss: 0.9523 - accuracy: 0.6219 - val_loss: 1.4186 - val_accuracy: 0.5702\nEpoch 148/200\n5/5 [==============================] - 1s 129ms/step - loss: 0.8707 - accuracy: 0.6598 - val_loss: 1.4205 - val_accuracy: 0.5716\nEpoch 149/200\n5/5 [==============================] - 1s 123ms/step - loss: 0.8485 - accuracy: 0.6838 - val_loss: 1.4247 - val_accuracy: 0.5716\nEpoch 150/200\n5/5 [==============================] - 1s 135ms/step - loss: 0.9404 - accuracy: 0.6376 - val_loss: 1.4308 - val_accuracy: 0.5702\nEpoch 151/200\n5/5 [==============================] - 1s 128ms/step - loss: 0.9460 - accuracy: 0.6284 - val_loss: 1.4369 - val_accuracy: 0.5697\nEpoch 152/200\n5/5 [==============================] - 1s 132ms/step - loss: 0.9460 - accuracy: 0.6174 - val_loss: 1.4409 - val_accuracy: 0.5725\nEpoch 153/200\n5/5 [==============================] - 1s 128ms/step - loss: 0.9012 - accuracy: 0.6378 - val_loss: 1.4443 - val_accuracy: 0.5716\nEpoch 154/200\n5/5 [==============================] - 1s 128ms/step - loss: 0.9226 - accuracy: 0.6226 - val_loss: 1.4550 - val_accuracy: 0.5720\nEpoch 155/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.9105 - accuracy: 0.6375 - val_loss: 1.4706 - val_accuracy: 0.5697\nEpoch 156/200\n5/5 [==============================] - 1s 157ms/step - loss: 0.8782 - accuracy: 0.6690 - val_loss: 1.4826 - val_accuracy: 0.5683\nEpoch 157/200\n5/5 [==============================] - 1s 129ms/step - loss: 0.9039 - accuracy: 0.6218 - val_loss: 1.4903 - val_accuracy: 0.5674\nEpoch 158/200\n5/5 [==============================] - 1s 135ms/step - loss: 0.9291 - accuracy: 0.6337 - val_loss: 1.4911 - val_accuracy: 0.5697\nEpoch 159/200\n5/5 [==============================] - 1s 122ms/step - loss: 0.9176 - accuracy: 0.6366 - val_loss: 1.4898 - val_accuracy: 0.5693\nEpoch 160/200\n5/5 [==============================] - 1s 134ms/step - loss: 0.8564 - accuracy: 0.6516 - val_loss: 1.4861 - val_accuracy: 0.5706\nEpoch 161/200\n5/5 [==============================] - 1s 127ms/step - loss: 0.8857 - accuracy: 0.6542 - val_loss: 1.4837 - val_accuracy: 0.5739\nEpoch 162/200\n5/5 [==============================] - 1s 128ms/step - loss: 0.8963 - accuracy: 0.6515 - val_loss: 1.4879 - val_accuracy: 0.5748\nEpoch 163/200\n5/5 [==============================] - 1s 125ms/step - loss: 0.8920 - accuracy: 0.6548 - val_loss: 1.4925 - val_accuracy: 0.5753\nEpoch 164/200\n5/5 [==============================] - 1s 123ms/step - loss: 0.8300 - accuracy: 0.6915 - val_loss: 1.5003 - val_accuracy: 0.5753\nEpoch 165/200\n5/5 [==============================] - 1s 133ms/step - loss: 0.9243 - accuracy: 0.6382 - val_loss: 1.5114 - val_accuracy: 0.5720\nEpoch 166/200\n5/5 [==============================] - 1s 122ms/step - loss: 0.8374 - accuracy: 0.6702 - val_loss: 1.5212 - val_accuracy: 0.5716\nEpoch 167/200\n5/5 [==============================] - 1s 127ms/step - loss: 0.8366 - accuracy: 0.6680 - val_loss: 1.5251 - val_accuracy: 0.5702\nEpoch 168/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.8265 - accuracy: 0.6612 - val_loss: 1.5247 - val_accuracy: 0.5734\nEpoch 169/200\n5/5 [==============================] - 1s 138ms/step - loss: 0.8960 - accuracy: 0.6386 - val_loss: 1.5208 - val_accuracy: 0.5757\nEpoch 170/200\n5/5 [==============================] - 1s 144ms/step - loss: 0.8747 - accuracy: 0.6373 - val_loss: 1.5191 - val_accuracy: 0.5771\nEpoch 171/200\n5/5 [==============================] - 1s 214ms/step - loss: 0.8566 - accuracy: 0.6471 - val_loss: 1.5159 - val_accuracy: 0.5803\nEpoch 172/200\n5/5 [==============================] - 1s 143ms/step - loss: 0.8520 - accuracy: 0.6547 - val_loss: 1.5122 - val_accuracy: 0.5799\nEpoch 173/200\n5/5 [==============================] - 1s 241ms/step - loss: 0.8766 - accuracy: 0.6567 - val_loss: 1.5082 - val_accuracy: 0.5822\nEpoch 174/200\n5/5 [==============================] - 1s 219ms/step - loss: 0.7819 - accuracy: 0.7044 - val_loss: 1.5063 - val_accuracy: 0.5822\nEpoch 175/200\n5/5 [==============================] - 1s 184ms/step - loss: 0.8579 - accuracy: 0.6566 - val_loss: 1.5071 - val_accuracy: 0.5822\nEpoch 176/200\n5/5 [==============================] - 1s 186ms/step - loss: 0.8649 - accuracy: 0.6656 - val_loss: 1.5106 - val_accuracy: 0.5813\nEpoch 177/200\n5/5 [==============================] - 1s 193ms/step - loss: 0.8462 - accuracy: 0.6377 - val_loss: 1.5143 - val_accuracy: 0.5826\nEpoch 178/200\n5/5 [==============================] - 1s 175ms/step - loss: 0.8405 - accuracy: 0.6676 - val_loss: 1.5171 - val_accuracy: 0.5845\nEpoch 179/200\n5/5 [==============================] - 1s 183ms/step - loss: 0.8065 - accuracy: 0.6713 - val_loss: 1.5199 - val_accuracy: 0.5831\nEpoch 180/200\n5/5 [==============================] - 1s 162ms/step - loss: 0.8850 - accuracy: 0.6369 - val_loss: 1.5232 - val_accuracy: 0.5831\nEpoch 181/200\n5/5 [==============================] - 1s 216ms/step - loss: 0.8796 - accuracy: 0.6546 - val_loss: 1.5259 - val_accuracy: 0.5826\nEpoch 182/200\n5/5 [==============================] - 1s 162ms/step - loss: 0.7898 - accuracy: 0.6730 - val_loss: 1.5311 - val_accuracy: 0.5826\nEpoch 183/200\n5/5 [==============================] - 1s 159ms/step - loss: 0.7982 - accuracy: 0.6828 - val_loss: 1.5380 - val_accuracy: 0.5836\nEpoch 184/200\n5/5 [==============================] - 1s 156ms/step - loss: 0.8291 - accuracy: 0.6792 - val_loss: 1.5468 - val_accuracy: 0.5836\nEpoch 185/200\n5/5 [==============================] - 1s 144ms/step - loss: 0.8375 - accuracy: 0.6788 - val_loss: 1.5529 - val_accuracy: 0.5840\nEpoch 186/200\n5/5 [==============================] - 1s 143ms/step - loss: 0.8144 - accuracy: 0.6788 - val_loss: 1.5538 - val_accuracy: 0.5840\nEpoch 187/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.8369 - accuracy: 0.6873 - val_loss: 1.5555 - val_accuracy: 0.5836\nEpoch 188/200\n5/5 [==============================] - 1s 130ms/step - loss: 0.8426 - accuracy: 0.6679 - val_loss: 1.5574 - val_accuracy: 0.5849\nEpoch 189/200\n5/5 [==============================] - 1s 144ms/step - loss: 0.7954 - accuracy: 0.6879 - val_loss: 1.5557 - val_accuracy: 0.5863\nEpoch 190/200\n5/5 [==============================] - 1s 131ms/step - loss: 0.7880 - accuracy: 0.6742 - val_loss: 1.5611 - val_accuracy: 0.5854\nEpoch 191/200\n5/5 [==============================] - 1s 172ms/step - loss: 0.8489 - accuracy: 0.6390 - val_loss: 1.5656 - val_accuracy: 0.5863\nEpoch 192/200\n5/5 [==============================] - 1s 175ms/step - loss: 0.8292 - accuracy: 0.6736 - val_loss: 1.5664 - val_accuracy: 0.5854\nEpoch 193/200\n5/5 [==============================] - 1s 151ms/step - loss: 0.7881 - accuracy: 0.6958 - val_loss: 1.5735 - val_accuracy: 0.5854\nEpoch 194/200\n5/5 [==============================] - 1s 145ms/step - loss: 0.8304 - accuracy: 0.6515 - val_loss: 1.5778 - val_accuracy: 0.5845\nEpoch 195/200\n5/5 [==============================] - 1s 217ms/step - loss: 0.7969 - accuracy: 0.6696 - val_loss: 1.5797 - val_accuracy: 0.5840\nEpoch 196/200\n5/5 [==============================] - 1s 183ms/step - loss: 0.7856 - accuracy: 0.6726 - val_loss: 1.5763 - val_accuracy: 0.5854\nEpoch 197/200\n5/5 [==============================] - 1s 170ms/step - loss: 0.7966 - accuracy: 0.6883 - val_loss: 1.5783 - val_accuracy: 0.5859\nEpoch 198/200\n5/5 [==============================] - 1s 147ms/step - loss: 0.8072 - accuracy: 0.6626 - val_loss: 1.5819 - val_accuracy: 0.5854\nEpoch 199/200\n5/5 [==============================] - 1s 135ms/step - loss: 0.7893 - accuracy: 0.6858 - val_loss: 1.5884 - val_accuracy: 0.5849\nEpoch 200/200\n5/5 [==============================] - 1s 154ms/step - loss: 0.7798 - accuracy: 0.6795 - val_loss: 1.5948 - val_accuracy: 0.5873\n\n\n<tensorflow.python.keras.callbacks.History at 0x14e4a9290>\n\n\n\n\nGraph Regularized Version\nWe now create the graph-regularized version that uses the citation network information\n\nbase_model = create_model([50, 50])\n\n\nimport neural_structured_learning as nsl\n\n\ngraph_reg_config = nsl.configs.make_graph_reg_config(\n    max_neighbors=2,\n    multiplier=0.1,\n    distance_type=nsl.configs.DistanceType.L2,\n    sum_over_axis=-1)\ngraph_reg_model = nsl.keras.GraphRegularization(base_model,\n                                                graph_reg_config)\ngraph_reg_model.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])\n#graph_reg_model.fit(train_dataset, epochs=200, verbose=1)\n\n\ngraph_reg_model.fit(myTrain.batch(128), epochs=200, verbose=1, validation_data=myTest.batch(128),\n          callbacks=[TensorBoard(log_dir='/tmp/regularization')])\n\nEpoch 1/200\n\n\n/Users/deusebio/.pyenv/versions/3.7.6/envs/ml-book-4/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/GraphRegularization/graph_loss/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/GraphRegularization/graph_loss/Reshape:0\", shape=(None, 7), dtype=float32), dense_shape=Tensor(\"gradient_tape/GraphRegularization/graph_loss/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n  \"shape. This may consume a large amount of memory.\" % value)\n\n\n5/5 [==============================] - 4s 328ms/step - loss: 2.1052 - accuracy: 0.1049 - scaled_graph_loss: 0.0056 - val_loss: 1.9455 - val_accuracy: 0.1440\nEpoch 2/200\n5/5 [==============================] - 1s 179ms/step - loss: 2.0342 - accuracy: 0.1722 - scaled_graph_loss: 0.0047 - val_loss: 1.9367 - val_accuracy: 0.1934\nEpoch 3/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.9881 - accuracy: 0.1603 - scaled_graph_loss: 0.0039 - val_loss: 1.9293 - val_accuracy: 0.2355\nEpoch 4/200\n5/5 [==============================] - 1s 178ms/step - loss: 1.9918 - accuracy: 0.1816 - scaled_graph_loss: 0.0035 - val_loss: 1.9224 - val_accuracy: 0.2770\nEpoch 5/200\n5/5 [==============================] - 1s 162ms/step - loss: 1.9551 - accuracy: 0.1983 - scaled_graph_loss: 0.0029 - val_loss: 1.9165 - val_accuracy: 0.2978\nEpoch 6/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.9745 - accuracy: 0.1918 - scaled_graph_loss: 0.0028 - val_loss: 1.9113 - val_accuracy: 0.3033\nEpoch 7/200\n5/5 [==============================] - 1s 170ms/step - loss: 1.9490 - accuracy: 0.1762 - scaled_graph_loss: 0.0023 - val_loss: 1.9068 - val_accuracy: 0.3075\nEpoch 8/200\n5/5 [==============================] - 1s 218ms/step - loss: 1.9262 - accuracy: 0.2016 - scaled_graph_loss: 0.0025 - val_loss: 1.9025 - val_accuracy: 0.3112\nEpoch 9/200\n5/5 [==============================] - 1s 208ms/step - loss: 1.9413 - accuracy: 0.2383 - scaled_graph_loss: 0.0023 - val_loss: 1.8983 - val_accuracy: 0.3066\nEpoch 10/200\n5/5 [==============================] - 1s 170ms/step - loss: 1.9136 - accuracy: 0.2278 - scaled_graph_loss: 0.0022 - val_loss: 1.8942 - val_accuracy: 0.3061\nEpoch 11/200\n5/5 [==============================] - 1s 154ms/step - loss: 1.9232 - accuracy: 0.2413 - scaled_graph_loss: 0.0022 - val_loss: 1.8900 - val_accuracy: 0.3066\nEpoch 12/200\n5/5 [==============================] - 1s 157ms/step - loss: 1.8885 - accuracy: 0.2682 - scaled_graph_loss: 0.0022 - val_loss: 1.8857 - val_accuracy: 0.3052\nEpoch 13/200\n5/5 [==============================] - 1s 192ms/step - loss: 1.8657 - accuracy: 0.2789 - scaled_graph_loss: 0.0021 - val_loss: 1.8813 - val_accuracy: 0.3052\nEpoch 14/200\n5/5 [==============================] - 1s 178ms/step - loss: 1.9115 - accuracy: 0.2158 - scaled_graph_loss: 0.0022 - val_loss: 1.8771 - val_accuracy: 0.3033\nEpoch 15/200\n5/5 [==============================] - 1s 174ms/step - loss: 1.8700 - accuracy: 0.2652 - scaled_graph_loss: 0.0020 - val_loss: 1.8725 - val_accuracy: 0.3029\nEpoch 16/200\n5/5 [==============================] - 1s 154ms/step - loss: 1.8883 - accuracy: 0.2607 - scaled_graph_loss: 0.0026 - val_loss: 1.8676 - val_accuracy: 0.3024\nEpoch 17/200\n5/5 [==============================] - 1s 141ms/step - loss: 1.8588 - accuracy: 0.2631 - scaled_graph_loss: 0.0025 - val_loss: 1.8625 - val_accuracy: 0.3019\nEpoch 18/200\n5/5 [==============================] - 1s 152ms/step - loss: 1.8774 - accuracy: 0.2593 - scaled_graph_loss: 0.0026 - val_loss: 1.8577 - val_accuracy: 0.3019\nEpoch 19/200\n5/5 [==============================] - 1s 187ms/step - loss: 1.8376 - accuracy: 0.2726 - scaled_graph_loss: 0.0028 - val_loss: 1.8530 - val_accuracy: 0.3019\nEpoch 20/200\n5/5 [==============================] - 1s 146ms/step - loss: 1.8590 - accuracy: 0.3045 - scaled_graph_loss: 0.0030 - val_loss: 1.8485 - val_accuracy: 0.3019\nEpoch 21/200\n5/5 [==============================] - 1s 204ms/step - loss: 1.8240 - accuracy: 0.2647 - scaled_graph_loss: 0.0028 - val_loss: 1.8439 - val_accuracy: 0.3019\nEpoch 22/200\n5/5 [==============================] - 1s 159ms/step - loss: 1.8541 - accuracy: 0.2940 - scaled_graph_loss: 0.0032 - val_loss: 1.8397 - val_accuracy: 0.3019\nEpoch 23/200\n5/5 [==============================] - 1s 208ms/step - loss: 1.8121 - accuracy: 0.3230 - scaled_graph_loss: 0.0034 - val_loss: 1.8350 - val_accuracy: 0.3019\nEpoch 24/200\n5/5 [==============================] - 1s 189ms/step - loss: 1.8325 - accuracy: 0.2986 - scaled_graph_loss: 0.0032 - val_loss: 1.8304 - val_accuracy: 0.3019\nEpoch 25/200\n5/5 [==============================] - 1s 170ms/step - loss: 1.8066 - accuracy: 0.2818 - scaled_graph_loss: 0.0032 - val_loss: 1.8254 - val_accuracy: 0.3019\nEpoch 26/200\n5/5 [==============================] - 1s 204ms/step - loss: 1.8389 - accuracy: 0.2880 - scaled_graph_loss: 0.0036 - val_loss: 1.8211 - val_accuracy: 0.3019\nEpoch 27/200\n5/5 [==============================] - 1s 208ms/step - loss: 1.8070 - accuracy: 0.3068 - scaled_graph_loss: 0.0034 - val_loss: 1.8168 - val_accuracy: 0.3024\nEpoch 28/200\n5/5 [==============================] - 1s 171ms/step - loss: 1.8070 - accuracy: 0.2909 - scaled_graph_loss: 0.0038 - val_loss: 1.8122 - val_accuracy: 0.3024\nEpoch 29/200\n5/5 [==============================] - 1s 167ms/step - loss: 1.7793 - accuracy: 0.2947 - scaled_graph_loss: 0.0046 - val_loss: 1.8069 - val_accuracy: 0.3024\nEpoch 30/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.7738 - accuracy: 0.2886 - scaled_graph_loss: 0.0048 - val_loss: 1.8016 - val_accuracy: 0.3024\nEpoch 31/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.7596 - accuracy: 0.3011 - scaled_graph_loss: 0.0043 - val_loss: 1.7957 - val_accuracy: 0.3024\nEpoch 32/200\n5/5 [==============================] - 1s 261ms/step - loss: 1.7768 - accuracy: 0.3165 - scaled_graph_loss: 0.0053 - val_loss: 1.7900 - val_accuracy: 0.3024\nEpoch 33/200\n5/5 [==============================] - 1s 196ms/step - loss: 1.7419 - accuracy: 0.3019 - scaled_graph_loss: 0.0052 - val_loss: 1.7840 - val_accuracy: 0.3024\nEpoch 34/200\n5/5 [==============================] - 1s 177ms/step - loss: 1.7615 - accuracy: 0.2834 - scaled_graph_loss: 0.0060 - val_loss: 1.7783 - val_accuracy: 0.3024\nEpoch 35/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.7419 - accuracy: 0.3210 - scaled_graph_loss: 0.0057 - val_loss: 1.7730 - val_accuracy: 0.3024\nEpoch 36/200\n5/5 [==============================] - 1s 179ms/step - loss: 1.7253 - accuracy: 0.3277 - scaled_graph_loss: 0.0061 - val_loss: 1.7670 - val_accuracy: 0.3024\nEpoch 37/200\n5/5 [==============================] - 1s 212ms/step - loss: 1.7170 - accuracy: 0.3362 - scaled_graph_loss: 0.0061 - val_loss: 1.7608 - val_accuracy: 0.3024\nEpoch 38/200\n5/5 [==============================] - 1s 212ms/step - loss: 1.7038 - accuracy: 0.3185 - scaled_graph_loss: 0.0064 - val_loss: 1.7548 - val_accuracy: 0.3024\nEpoch 39/200\n5/5 [==============================] - 1s 207ms/step - loss: 1.7044 - accuracy: 0.3301 - scaled_graph_loss: 0.0065 - val_loss: 1.7491 - val_accuracy: 0.3024\nEpoch 40/200\n5/5 [==============================] - 1s 152ms/step - loss: 1.7011 - accuracy: 0.3390 - scaled_graph_loss: 0.0069 - val_loss: 1.7428 - val_accuracy: 0.3029\nEpoch 41/200\n5/5 [==============================] - 1s 192ms/step - loss: 1.6931 - accuracy: 0.3415 - scaled_graph_loss: 0.0075 - val_loss: 1.7368 - val_accuracy: 0.3042\nEpoch 42/200\n5/5 [==============================] - 1s 159ms/step - loss: 1.7035 - accuracy: 0.3211 - scaled_graph_loss: 0.0074 - val_loss: 1.7310 - val_accuracy: 0.3038\nEpoch 43/200\n5/5 [==============================] - 1s 151ms/step - loss: 1.6884 - accuracy: 0.3293 - scaled_graph_loss: 0.0075 - val_loss: 1.7258 - val_accuracy: 0.3047\nEpoch 44/200\n5/5 [==============================] - 1s 148ms/step - loss: 1.7046 - accuracy: 0.3319 - scaled_graph_loss: 0.0079 - val_loss: 1.7206 - val_accuracy: 0.3047\nEpoch 45/200\n5/5 [==============================] - 1s 151ms/step - loss: 1.6325 - accuracy: 0.3369 - scaled_graph_loss: 0.0082 - val_loss: 1.7141 - val_accuracy: 0.3052\nEpoch 46/200\n5/5 [==============================] - 1s 167ms/step - loss: 1.6682 - accuracy: 0.3413 - scaled_graph_loss: 0.0082 - val_loss: 1.7070 - val_accuracy: 0.3061\nEpoch 47/200\n5/5 [==============================] - 1s 164ms/step - loss: 1.5976 - accuracy: 0.3434 - scaled_graph_loss: 0.0094 - val_loss: 1.6997 - val_accuracy: 0.3066\nEpoch 48/200\n5/5 [==============================] - 1s 183ms/step - loss: 1.6572 - accuracy: 0.3536 - scaled_graph_loss: 0.0093 - val_loss: 1.6933 - val_accuracy: 0.3084\nEpoch 49/200\n5/5 [==============================] - 1s 159ms/step - loss: 1.6508 - accuracy: 0.3540 - scaled_graph_loss: 0.0098 - val_loss: 1.6861 - val_accuracy: 0.3112\nEpoch 50/200\n5/5 [==============================] - 1s 148ms/step - loss: 1.6408 - accuracy: 0.3250 - scaled_graph_loss: 0.0097 - val_loss: 1.6793 - val_accuracy: 0.3149\nEpoch 51/200\n5/5 [==============================] - 1s 181ms/step - loss: 1.6336 - accuracy: 0.3462 - scaled_graph_loss: 0.0092 - val_loss: 1.6736 - val_accuracy: 0.3190\nEpoch 52/200\n5/5 [==============================] - 1s 227ms/step - loss: 1.6309 - accuracy: 0.3321 - scaled_graph_loss: 0.0103 - val_loss: 1.6677 - val_accuracy: 0.3236\nEpoch 53/200\n5/5 [==============================] - 1s 271ms/step - loss: 1.6388 - accuracy: 0.3596 - scaled_graph_loss: 0.0111 - val_loss: 1.6628 - val_accuracy: 0.3287\nEpoch 54/200\n5/5 [==============================] - 1s 178ms/step - loss: 1.5818 - accuracy: 0.3541 - scaled_graph_loss: 0.0103 - val_loss: 1.6570 - val_accuracy: 0.3338\nEpoch 55/200\n5/5 [==============================] - 1s 272ms/step - loss: 1.5723 - accuracy: 0.3685 - scaled_graph_loss: 0.0109 - val_loss: 1.6503 - val_accuracy: 0.3398\nEpoch 56/200\n5/5 [==============================] - 1s 220ms/step - loss: 1.5742 - accuracy: 0.3803 - scaled_graph_loss: 0.0103 - val_loss: 1.6434 - val_accuracy: 0.3467\nEpoch 57/200\n5/5 [==============================] - 1s 200ms/step - loss: 1.5509 - accuracy: 0.3862 - scaled_graph_loss: 0.0114 - val_loss: 1.6370 - val_accuracy: 0.3578\nEpoch 58/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.5821 - accuracy: 0.3674 - scaled_graph_loss: 0.0117 - val_loss: 1.6305 - val_accuracy: 0.3638\nEpoch 59/200\n5/5 [==============================] - 1s 151ms/step - loss: 1.5852 - accuracy: 0.3616 - scaled_graph_loss: 0.0118 - val_loss: 1.6237 - val_accuracy: 0.3698\nEpoch 60/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.5499 - accuracy: 0.3611 - scaled_graph_loss: 0.0115 - val_loss: 1.6169 - val_accuracy: 0.3726\nEpoch 61/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.5457 - accuracy: 0.3744 - scaled_graph_loss: 0.0142 - val_loss: 1.6107 - val_accuracy: 0.3795\nEpoch 62/200\n5/5 [==============================] - 1s 132ms/step - loss: 1.5397 - accuracy: 0.3819 - scaled_graph_loss: 0.0127 - val_loss: 1.6045 - val_accuracy: 0.3892\nEpoch 63/200\n5/5 [==============================] - 1s 153ms/step - loss: 1.5438 - accuracy: 0.4087 - scaled_graph_loss: 0.0136 - val_loss: 1.5984 - val_accuracy: 0.3947\nEpoch 64/200\n5/5 [==============================] - 1s 162ms/step - loss: 1.5161 - accuracy: 0.3815 - scaled_graph_loss: 0.0135 - val_loss: 1.5919 - val_accuracy: 0.4017\nEpoch 65/200\n5/5 [==============================] - 1s 143ms/step - loss: 1.5581 - accuracy: 0.3713 - scaled_graph_loss: 0.0134 - val_loss: 1.5854 - val_accuracy: 0.4086\nEpoch 66/200\n5/5 [==============================] - 1s 138ms/step - loss: 1.5347 - accuracy: 0.3858 - scaled_graph_loss: 0.0122 - val_loss: 1.5794 - val_accuracy: 0.4197\nEpoch 67/200\n5/5 [==============================] - 1s 204ms/step - loss: 1.4917 - accuracy: 0.4024 - scaled_graph_loss: 0.0141 - val_loss: 1.5733 - val_accuracy: 0.4326\nEpoch 68/200\n5/5 [==============================] - 1s 223ms/step - loss: 1.4804 - accuracy: 0.3927 - scaled_graph_loss: 0.0129 - val_loss: 1.5658 - val_accuracy: 0.4483\nEpoch 69/200\n5/5 [==============================] - 1s 252ms/step - loss: 1.4751 - accuracy: 0.4093 - scaled_graph_loss: 0.0138 - val_loss: 1.5586 - val_accuracy: 0.4575\nEpoch 70/200\n5/5 [==============================] - 1s 192ms/step - loss: 1.4870 - accuracy: 0.4230 - scaled_graph_loss: 0.0140 - val_loss: 1.5514 - val_accuracy: 0.4645\nEpoch 71/200\n5/5 [==============================] - 1s 213ms/step - loss: 1.4541 - accuracy: 0.4380 - scaled_graph_loss: 0.0146 - val_loss: 1.5447 - val_accuracy: 0.4686\nEpoch 72/200\n5/5 [==============================] - 1s 264ms/step - loss: 1.4528 - accuracy: 0.4193 - scaled_graph_loss: 0.0141 - val_loss: 1.5373 - val_accuracy: 0.4765\nEpoch 73/200\n5/5 [==============================] - 1s 199ms/step - loss: 1.4318 - accuracy: 0.4144 - scaled_graph_loss: 0.0177 - val_loss: 1.5302 - val_accuracy: 0.4820\nEpoch 74/200\n5/5 [==============================] - 1s 210ms/step - loss: 1.4495 - accuracy: 0.4282 - scaled_graph_loss: 0.0161 - val_loss: 1.5239 - val_accuracy: 0.4926\nEpoch 75/200\n5/5 [==============================] - 1s 164ms/step - loss: 1.4085 - accuracy: 0.4447 - scaled_graph_loss: 0.0161 - val_loss: 1.5170 - val_accuracy: 0.4958\nEpoch 76/200\n5/5 [==============================] - 1s 171ms/step - loss: 1.3982 - accuracy: 0.4639 - scaled_graph_loss: 0.0155 - val_loss: 1.5103 - val_accuracy: 0.5000\nEpoch 77/200\n5/5 [==============================] - 1s 186ms/step - loss: 1.3995 - accuracy: 0.4773 - scaled_graph_loss: 0.0168 - val_loss: 1.5037 - val_accuracy: 0.5037\nEpoch 78/200\n5/5 [==============================] - 1s 179ms/step - loss: 1.4244 - accuracy: 0.4426 - scaled_graph_loss: 0.0185 - val_loss: 1.4985 - val_accuracy: 0.5074\nEpoch 79/200\n5/5 [==============================] - 1s 194ms/step - loss: 1.4186 - accuracy: 0.4440 - scaled_graph_loss: 0.0163 - val_loss: 1.4933 - val_accuracy: 0.5102\nEpoch 80/200\n5/5 [==============================] - 1s 212ms/step - loss: 1.3805 - accuracy: 0.4560 - scaled_graph_loss: 0.0170 - val_loss: 1.4872 - val_accuracy: 0.5115\nEpoch 81/200\n5/5 [==============================] - 1s 204ms/step - loss: 1.3641 - accuracy: 0.4687 - scaled_graph_loss: 0.0173 - val_loss: 1.4800 - val_accuracy: 0.5166\nEpoch 82/200\n5/5 [==============================] - 1s 207ms/step - loss: 1.3796 - accuracy: 0.4717 - scaled_graph_loss: 0.0164 - val_loss: 1.4728 - val_accuracy: 0.5249\nEpoch 83/200\n5/5 [==============================] - 1s 203ms/step - loss: 1.4130 - accuracy: 0.4492 - scaled_graph_loss: 0.0177 - val_loss: 1.4667 - val_accuracy: 0.5314\nEpoch 84/200\n5/5 [==============================] - 1s 222ms/step - loss: 1.3239 - accuracy: 0.4916 - scaled_graph_loss: 0.0197 - val_loss: 1.4608 - val_accuracy: 0.5369\nEpoch 85/200\n5/5 [==============================] - 1s 220ms/step - loss: 1.3893 - accuracy: 0.4488 - scaled_graph_loss: 0.0186 - val_loss: 1.4551 - val_accuracy: 0.5434\nEpoch 86/200\n5/5 [==============================] - 1s 236ms/step - loss: 1.3161 - accuracy: 0.4909 - scaled_graph_loss: 0.0204 - val_loss: 1.4490 - val_accuracy: 0.5466\nEpoch 87/200\n5/5 [==============================] - 1s 179ms/step - loss: 1.3434 - accuracy: 0.4966 - scaled_graph_loss: 0.0200 - val_loss: 1.4419 - val_accuracy: 0.5476\nEpoch 88/200\n5/5 [==============================] - 1s 250ms/step - loss: 1.3027 - accuracy: 0.5098 - scaled_graph_loss: 0.0196 - val_loss: 1.4357 - val_accuracy: 0.5489\nEpoch 89/200\n5/5 [==============================] - 1s 230ms/step - loss: 1.3019 - accuracy: 0.4970 - scaled_graph_loss: 0.0201 - val_loss: 1.4293 - val_accuracy: 0.5526\nEpoch 90/200\n5/5 [==============================] - 1s 248ms/step - loss: 1.2992 - accuracy: 0.4944 - scaled_graph_loss: 0.0205 - val_loss: 1.4230 - val_accuracy: 0.5559\nEpoch 91/200\n5/5 [==============================] - 1s 231ms/step - loss: 1.3239 - accuracy: 0.4901 - scaled_graph_loss: 0.0179 - val_loss: 1.4171 - val_accuracy: 0.5619\nEpoch 92/200\n5/5 [==============================] - 1s 221ms/step - loss: 1.3136 - accuracy: 0.5136 - scaled_graph_loss: 0.0196 - val_loss: 1.4112 - val_accuracy: 0.5669\nEpoch 93/200\n5/5 [==============================] - 1s 183ms/step - loss: 1.2639 - accuracy: 0.5288 - scaled_graph_loss: 0.0209 - val_loss: 1.4053 - val_accuracy: 0.5665\nEpoch 94/200\n5/5 [==============================] - 1s 205ms/step - loss: 1.2763 - accuracy: 0.5047 - scaled_graph_loss: 0.0209 - val_loss: 1.3995 - val_accuracy: 0.5665\nEpoch 95/200\n5/5 [==============================] - 1s 238ms/step - loss: 1.2617 - accuracy: 0.5052 - scaled_graph_loss: 0.0207 - val_loss: 1.3948 - val_accuracy: 0.5656\nEpoch 96/200\n5/5 [==============================] - 1s 218ms/step - loss: 1.2874 - accuracy: 0.5022 - scaled_graph_loss: 0.0226 - val_loss: 1.3906 - val_accuracy: 0.5697\nEpoch 97/200\n5/5 [==============================] - 1s 256ms/step - loss: 1.2262 - accuracy: 0.5307 - scaled_graph_loss: 0.0216 - val_loss: 1.3858 - val_accuracy: 0.5702\nEpoch 98/200\n5/5 [==============================] - 1s 197ms/step - loss: 1.2362 - accuracy: 0.5532 - scaled_graph_loss: 0.0216 - val_loss: 1.3793 - val_accuracy: 0.5725\nEpoch 99/200\n5/5 [==============================] - 1s 262ms/step - loss: 1.2081 - accuracy: 0.5314 - scaled_graph_loss: 0.0236 - val_loss: 1.3731 - val_accuracy: 0.5748\nEpoch 100/200\n5/5 [==============================] - 1s 160ms/step - loss: 1.2115 - accuracy: 0.5213 - scaled_graph_loss: 0.0224 - val_loss: 1.3678 - val_accuracy: 0.5780\nEpoch 101/200\n5/5 [==============================] - 1s 153ms/step - loss: 1.1994 - accuracy: 0.5480 - scaled_graph_loss: 0.0230 - val_loss: 1.3620 - val_accuracy: 0.5785\nEpoch 102/200\n5/5 [==============================] - 1s 144ms/step - loss: 1.1956 - accuracy: 0.5351 - scaled_graph_loss: 0.0240 - val_loss: 1.3569 - val_accuracy: 0.5799\nEpoch 103/200\n5/5 [==============================] - 1s 154ms/step - loss: 1.2904 - accuracy: 0.4833 - scaled_graph_loss: 0.0225 - val_loss: 1.3529 - val_accuracy: 0.5803\nEpoch 104/200\n5/5 [==============================] - 1s 210ms/step - loss: 1.1866 - accuracy: 0.5519 - scaled_graph_loss: 0.0241 - val_loss: 1.3486 - val_accuracy: 0.5799\nEpoch 105/200\n5/5 [==============================] - 1s 184ms/step - loss: 1.2259 - accuracy: 0.5028 - scaled_graph_loss: 0.0225 - val_loss: 1.3449 - val_accuracy: 0.5822\nEpoch 106/200\n5/5 [==============================] - 1s 218ms/step - loss: 1.2135 - accuracy: 0.5383 - scaled_graph_loss: 0.0246 - val_loss: 1.3409 - val_accuracy: 0.5826\nEpoch 107/200\n5/5 [==============================] - 1s 215ms/step - loss: 1.1978 - accuracy: 0.5241 - scaled_graph_loss: 0.0244 - val_loss: 1.3366 - val_accuracy: 0.5831\nEpoch 108/200\n5/5 [==============================] - 1s 214ms/step - loss: 1.1885 - accuracy: 0.5566 - scaled_graph_loss: 0.0250 - val_loss: 1.3322 - val_accuracy: 0.5845\nEpoch 109/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.1876 - accuracy: 0.5501 - scaled_graph_loss: 0.0258 - val_loss: 1.3270 - val_accuracy: 0.5877\nEpoch 110/200\n5/5 [==============================] - 1s 172ms/step - loss: 1.1515 - accuracy: 0.5736 - scaled_graph_loss: 0.0251 - val_loss: 1.3238 - val_accuracy: 0.5896\nEpoch 111/200\n5/5 [==============================] - 1s 166ms/step - loss: 1.1388 - accuracy: 0.5336 - scaled_graph_loss: 0.0248 - val_loss: 1.3204 - val_accuracy: 0.5910\nEpoch 112/200\n5/5 [==============================] - 1s 176ms/step - loss: 1.1100 - accuracy: 0.5436 - scaled_graph_loss: 0.0266 - val_loss: 1.3165 - val_accuracy: 0.5928\nEpoch 113/200\n5/5 [==============================] - 1s 158ms/step - loss: 1.1504 - accuracy: 0.5861 - scaled_graph_loss: 0.0254 - val_loss: 1.3145 - val_accuracy: 0.5910\nEpoch 114/200\n5/5 [==============================] - 1s 214ms/step - loss: 1.1706 - accuracy: 0.5344 - scaled_graph_loss: 0.0262 - val_loss: 1.3123 - val_accuracy: 0.5905\nEpoch 115/200\n5/5 [==============================] - 1s 164ms/step - loss: 1.1645 - accuracy: 0.5694 - scaled_graph_loss: 0.0257 - val_loss: 1.3099 - val_accuracy: 0.5905\nEpoch 116/200\n5/5 [==============================] - 1s 227ms/step - loss: 1.1480 - accuracy: 0.5713 - scaled_graph_loss: 0.0249 - val_loss: 1.3066 - val_accuracy: 0.5919\nEpoch 117/200\n5/5 [==============================] - 1s 194ms/step - loss: 1.1302 - accuracy: 0.5679 - scaled_graph_loss: 0.0253 - val_loss: 1.3026 - val_accuracy: 0.5937\nEpoch 118/200\n5/5 [==============================] - 1s 137ms/step - loss: 1.1127 - accuracy: 0.5759 - scaled_graph_loss: 0.0240 - val_loss: 1.3002 - val_accuracy: 0.5942\nEpoch 119/200\n5/5 [==============================] - 1s 209ms/step - loss: 1.1154 - accuracy: 0.5697 - scaled_graph_loss: 0.0271 - val_loss: 1.2991 - val_accuracy: 0.5942\nEpoch 120/200\n5/5 [==============================] - 1s 187ms/step - loss: 1.0834 - accuracy: 0.5843 - scaled_graph_loss: 0.0245 - val_loss: 1.2963 - val_accuracy: 0.5951\nEpoch 121/200\n5/5 [==============================] - 1s 156ms/step - loss: 1.1061 - accuracy: 0.5903 - scaled_graph_loss: 0.0258 - val_loss: 1.2935 - val_accuracy: 0.5965\nEpoch 122/200\n5/5 [==============================] - 1s 167ms/step - loss: 1.0833 - accuracy: 0.5821 - scaled_graph_loss: 0.0254 - val_loss: 1.2900 - val_accuracy: 0.5970\nEpoch 123/200\n5/5 [==============================] - 1s 175ms/step - loss: 1.1348 - accuracy: 0.5637 - scaled_graph_loss: 0.0248 - val_loss: 1.2858 - val_accuracy: 0.5988\nEpoch 124/200\n5/5 [==============================] - 1s 170ms/step - loss: 1.0713 - accuracy: 0.5912 - scaled_graph_loss: 0.0252 - val_loss: 1.2819 - val_accuracy: 0.5997\nEpoch 125/200\n5/5 [==============================] - 1s 176ms/step - loss: 1.0583 - accuracy: 0.5960 - scaled_graph_loss: 0.0277 - val_loss: 1.2799 - val_accuracy: 0.6006\nEpoch 126/200\n5/5 [==============================] - 1s 179ms/step - loss: 1.0950 - accuracy: 0.6009 - scaled_graph_loss: 0.0253 - val_loss: 1.2770 - val_accuracy: 0.5993\nEpoch 127/200\n5/5 [==============================] - 1s 163ms/step - loss: 1.1018 - accuracy: 0.5771 - scaled_graph_loss: 0.0259 - val_loss: 1.2736 - val_accuracy: 0.6020\nEpoch 128/200\n5/5 [==============================] - 1s 147ms/step - loss: 1.1109 - accuracy: 0.5796 - scaled_graph_loss: 0.0273 - val_loss: 1.2704 - val_accuracy: 0.6016\nEpoch 129/200\n5/5 [==============================] - 1s 142ms/step - loss: 1.0983 - accuracy: 0.5808 - scaled_graph_loss: 0.0266 - val_loss: 1.2668 - val_accuracy: 0.6034\nEpoch 130/200\n5/5 [==============================] - 1s 135ms/step - loss: 1.1054 - accuracy: 0.5490 - scaled_graph_loss: 0.0296 - val_loss: 1.2626 - val_accuracy: 0.6066\nEpoch 131/200\n5/5 [==============================] - 1s 136ms/step - loss: 1.0896 - accuracy: 0.6092 - scaled_graph_loss: 0.0295 - val_loss: 1.2595 - val_accuracy: 0.6080\nEpoch 132/200\n5/5 [==============================] - 1s 148ms/step - loss: 1.0911 - accuracy: 0.5874 - scaled_graph_loss: 0.0292 - val_loss: 1.2571 - val_accuracy: 0.6076\nEpoch 133/200\n5/5 [==============================] - 1s 149ms/step - loss: 1.1144 - accuracy: 0.5697 - scaled_graph_loss: 0.0279 - val_loss: 1.2532 - val_accuracy: 0.6094\nEpoch 134/200\n5/5 [==============================] - 1s 140ms/step - loss: 1.0619 - accuracy: 0.5921 - scaled_graph_loss: 0.0314 - val_loss: 1.2494 - val_accuracy: 0.6103\nEpoch 135/200\n5/5 [==============================] - 1s 152ms/step - loss: 1.0882 - accuracy: 0.5957 - scaled_graph_loss: 0.0283 - val_loss: 1.2506 - val_accuracy: 0.6094\nEpoch 136/200\n5/5 [==============================] - 1s 181ms/step - loss: 1.0127 - accuracy: 0.6250 - scaled_graph_loss: 0.0296 - val_loss: 1.2510 - val_accuracy: 0.6090\nEpoch 137/200\n5/5 [==============================] - 1s 157ms/step - loss: 1.0254 - accuracy: 0.6049 - scaled_graph_loss: 0.0278 - val_loss: 1.2501 - val_accuracy: 0.6103\nEpoch 138/200\n5/5 [==============================] - 1s 145ms/step - loss: 1.0017 - accuracy: 0.6117 - scaled_graph_loss: 0.0298 - val_loss: 1.2472 - val_accuracy: 0.6108\nEpoch 139/200\n5/5 [==============================] - 1s 155ms/step - loss: 1.0102 - accuracy: 0.6226 - scaled_graph_loss: 0.0277 - val_loss: 1.2472 - val_accuracy: 0.6117\nEpoch 140/200\n5/5 [==============================] - 1s 187ms/step - loss: 1.0174 - accuracy: 0.6061 - scaled_graph_loss: 0.0314 - val_loss: 1.2470 - val_accuracy: 0.6127\nEpoch 141/200\n5/5 [==============================] - 1s 175ms/step - loss: 1.0487 - accuracy: 0.6027 - scaled_graph_loss: 0.0279 - val_loss: 1.2464 - val_accuracy: 0.6131\nEpoch 142/200\n5/5 [==============================] - 1s 164ms/step - loss: 1.0059 - accuracy: 0.5976 - scaled_graph_loss: 0.0290 - val_loss: 1.2446 - val_accuracy: 0.6131\nEpoch 143/200\n5/5 [==============================] - 1s 209ms/step - loss: 0.9457 - accuracy: 0.6522 - scaled_graph_loss: 0.0272 - val_loss: 1.2440 - val_accuracy: 0.6131\nEpoch 144/200\n5/5 [==============================] - 1s 176ms/step - loss: 1.0196 - accuracy: 0.6143 - scaled_graph_loss: 0.0281 - val_loss: 1.2449 - val_accuracy: 0.6136\nEpoch 145/200\n5/5 [==============================] - 1s 222ms/step - loss: 1.0264 - accuracy: 0.6045 - scaled_graph_loss: 0.0281 - val_loss: 1.2458 - val_accuracy: 0.6136\nEpoch 146/200\n5/5 [==============================] - 1s 181ms/step - loss: 0.9464 - accuracy: 0.6266 - scaled_graph_loss: 0.0315 - val_loss: 1.2463 - val_accuracy: 0.6136\nEpoch 147/200\n5/5 [==============================] - 1s 191ms/step - loss: 1.0403 - accuracy: 0.5913 - scaled_graph_loss: 0.0275 - val_loss: 1.2475 - val_accuracy: 0.6127\nEpoch 148/200\n5/5 [==============================] - 1s 231ms/step - loss: 1.0299 - accuracy: 0.6055 - scaled_graph_loss: 0.0302 - val_loss: 1.2493 - val_accuracy: 0.6127\nEpoch 149/200\n5/5 [==============================] - 1s 174ms/step - loss: 1.0777 - accuracy: 0.5722 - scaled_graph_loss: 0.0313 - val_loss: 1.2508 - val_accuracy: 0.6127\nEpoch 150/200\n5/5 [==============================] - 1s 265ms/step - loss: 1.0012 - accuracy: 0.6296 - scaled_graph_loss: 0.0288 - val_loss: 1.2516 - val_accuracy: 0.6131\nEpoch 151/200\n5/5 [==============================] - 1s 256ms/step - loss: 0.9506 - accuracy: 0.6113 - scaled_graph_loss: 0.0292 - val_loss: 1.2499 - val_accuracy: 0.6136\nEpoch 152/200\n5/5 [==============================] - 1s 250ms/step - loss: 1.0039 - accuracy: 0.6003 - scaled_graph_loss: 0.0286 - val_loss: 1.2448 - val_accuracy: 0.6145\nEpoch 153/200\n5/5 [==============================] - 1s 239ms/step - loss: 0.9514 - accuracy: 0.6343 - scaled_graph_loss: 0.0299 - val_loss: 1.2397 - val_accuracy: 0.6154\nEpoch 154/200\n5/5 [==============================] - 1s 199ms/step - loss: 0.9848 - accuracy: 0.6177 - scaled_graph_loss: 0.0309 - val_loss: 1.2356 - val_accuracy: 0.6177\nEpoch 155/200\n5/5 [==============================] - 1s 163ms/step - loss: 0.9157 - accuracy: 0.6665 - scaled_graph_loss: 0.0314 - val_loss: 1.2342 - val_accuracy: 0.6177\nEpoch 156/200\n5/5 [==============================] - 1s 160ms/step - loss: 0.9497 - accuracy: 0.6200 - scaled_graph_loss: 0.0301 - val_loss: 1.2320 - val_accuracy: 0.6177\nEpoch 157/200\n5/5 [==============================] - 1s 172ms/step - loss: 0.9808 - accuracy: 0.6151 - scaled_graph_loss: 0.0309 - val_loss: 1.2295 - val_accuracy: 0.6200\nEpoch 158/200\n5/5 [==============================] - 1s 181ms/step - loss: 0.9169 - accuracy: 0.6518 - scaled_graph_loss: 0.0283 - val_loss: 1.2264 - val_accuracy: 0.6219\nEpoch 159/200\n5/5 [==============================] - 1s 168ms/step - loss: 1.0104 - accuracy: 0.6188 - scaled_graph_loss: 0.0289 - val_loss: 1.2250 - val_accuracy: 0.6228\nEpoch 160/200\n5/5 [==============================] - 1s 180ms/step - loss: 0.9568 - accuracy: 0.5875 - scaled_graph_loss: 0.0311 - val_loss: 1.2251 - val_accuracy: 0.6219\nEpoch 161/200\n5/5 [==============================] - 1s 162ms/step - loss: 0.9131 - accuracy: 0.6352 - scaled_graph_loss: 0.0303 - val_loss: 1.2244 - val_accuracy: 0.6219\nEpoch 162/200\n5/5 [==============================] - 1s 168ms/step - loss: 0.9322 - accuracy: 0.6390 - scaled_graph_loss: 0.0308 - val_loss: 1.2250 - val_accuracy: 0.6223\nEpoch 163/200\n5/5 [==============================] - 1s 191ms/step - loss: 0.9138 - accuracy: 0.6420 - scaled_graph_loss: 0.0309 - val_loss: 1.2265 - val_accuracy: 0.6223\nEpoch 164/200\n5/5 [==============================] - 1s 161ms/step - loss: 0.9189 - accuracy: 0.6483 - scaled_graph_loss: 0.0310 - val_loss: 1.2288 - val_accuracy: 0.6214\nEpoch 165/200\n5/5 [==============================] - 1s 167ms/step - loss: 0.9210 - accuracy: 0.6330 - scaled_graph_loss: 0.0331 - val_loss: 1.2299 - val_accuracy: 0.6228\nEpoch 166/200\n5/5 [==============================] - 1s 201ms/step - loss: 0.9685 - accuracy: 0.6292 - scaled_graph_loss: 0.0329 - val_loss: 1.2324 - val_accuracy: 0.6251\nEpoch 167/200\n5/5 [==============================] - 1s 260ms/step - loss: 0.9593 - accuracy: 0.6231 - scaled_graph_loss: 0.0320 - val_loss: 1.2372 - val_accuracy: 0.6251\nEpoch 168/200\n5/5 [==============================] - 1s 186ms/step - loss: 0.9453 - accuracy: 0.6082 - scaled_graph_loss: 0.0301 - val_loss: 1.2409 - val_accuracy: 0.6260\nEpoch 169/200\n5/5 [==============================] - 1s 174ms/step - loss: 1.0013 - accuracy: 0.6015 - scaled_graph_loss: 0.0313 - val_loss: 1.2456 - val_accuracy: 0.6247\nEpoch 170/200\n5/5 [==============================] - 1s 225ms/step - loss: 0.9140 - accuracy: 0.6605 - scaled_graph_loss: 0.0311 - val_loss: 1.2488 - val_accuracy: 0.6228\nEpoch 171/200\n5/5 [==============================] - 1s 184ms/step - loss: 0.8999 - accuracy: 0.6485 - scaled_graph_loss: 0.0295 - val_loss: 1.2475 - val_accuracy: 0.6237\nEpoch 172/200\n5/5 [==============================] - 1s 163ms/step - loss: 0.9913 - accuracy: 0.6180 - scaled_graph_loss: 0.0299 - val_loss: 1.2500 - val_accuracy: 0.6242\nEpoch 173/200\n5/5 [==============================] - 1s 229ms/step - loss: 0.9542 - accuracy: 0.6138 - scaled_graph_loss: 0.0290 - val_loss: 1.2513 - val_accuracy: 0.6237\nEpoch 174/200\n5/5 [==============================] - 1s 283ms/step - loss: 0.9251 - accuracy: 0.6392 - scaled_graph_loss: 0.0309 - val_loss: 1.2524 - val_accuracy: 0.6247\nEpoch 175/200\n5/5 [==============================] - 1s 277ms/step - loss: 0.9016 - accuracy: 0.6572 - scaled_graph_loss: 0.0321 - val_loss: 1.2525 - val_accuracy: 0.6260\nEpoch 176/200\n5/5 [==============================] - 1s 271ms/step - loss: 0.9267 - accuracy: 0.6182 - scaled_graph_loss: 0.0311 - val_loss: 1.2514 - val_accuracy: 0.6260\nEpoch 177/200\n5/5 [==============================] - 1s 231ms/step - loss: 0.8702 - accuracy: 0.6715 - scaled_graph_loss: 0.0307 - val_loss: 1.2500 - val_accuracy: 0.6265\nEpoch 178/200\n5/5 [==============================] - 1s 179ms/step - loss: 0.8859 - accuracy: 0.6498 - scaled_graph_loss: 0.0300 - val_loss: 1.2444 - val_accuracy: 0.6260\nEpoch 179/200\n5/5 [==============================] - 1s 232ms/step - loss: 0.9165 - accuracy: 0.6484 - scaled_graph_loss: 0.0306 - val_loss: 1.2410 - val_accuracy: 0.6265\nEpoch 180/200\n5/5 [==============================] - 1s 269ms/step - loss: 0.8989 - accuracy: 0.6480 - scaled_graph_loss: 0.0308 - val_loss: 1.2395 - val_accuracy: 0.6260\nEpoch 181/200\n5/5 [==============================] - 1s 280ms/step - loss: 0.9084 - accuracy: 0.6570 - scaled_graph_loss: 0.0303 - val_loss: 1.2380 - val_accuracy: 0.6270\nEpoch 182/200\n5/5 [==============================] - 1s 184ms/step - loss: 0.8927 - accuracy: 0.6529 - scaled_graph_loss: 0.0321 - val_loss: 1.2398 - val_accuracy: 0.6293\nEpoch 183/200\n5/5 [==============================] - 1s 269ms/step - loss: 0.9331 - accuracy: 0.6413 - scaled_graph_loss: 0.0324 - val_loss: 1.2425 - val_accuracy: 0.6274\nEpoch 184/200\n5/5 [==============================] - 1s 232ms/step - loss: 0.8546 - accuracy: 0.6809 - scaled_graph_loss: 0.0297 - val_loss: 1.2477 - val_accuracy: 0.6283\nEpoch 185/200\n5/5 [==============================] - 1s 195ms/step - loss: 0.8826 - accuracy: 0.6345 - scaled_graph_loss: 0.0327 - val_loss: 1.2524 - val_accuracy: 0.6274\nEpoch 186/200\n5/5 [==============================] - 1s 234ms/step - loss: 0.8374 - accuracy: 0.6524 - scaled_graph_loss: 0.0321 - val_loss: 1.2602 - val_accuracy: 0.6288\nEpoch 187/200\n5/5 [==============================] - 1s 240ms/step - loss: 0.9199 - accuracy: 0.6291 - scaled_graph_loss: 0.0331 - val_loss: 1.2633 - val_accuracy: 0.6279\nEpoch 188/200\n5/5 [==============================] - 1s 198ms/step - loss: 0.8474 - accuracy: 0.6932 - scaled_graph_loss: 0.0315 - val_loss: 1.2667 - val_accuracy: 0.6270\nEpoch 189/200\n5/5 [==============================] - 1s 162ms/step - loss: 0.8857 - accuracy: 0.6527 - scaled_graph_loss: 0.0326 - val_loss: 1.2682 - val_accuracy: 0.6251\nEpoch 190/200\n5/5 [==============================] - 1s 167ms/step - loss: 0.8189 - accuracy: 0.6870 - scaled_graph_loss: 0.0335 - val_loss: 1.2717 - val_accuracy: 0.6251\nEpoch 191/200\n5/5 [==============================] - 1s 157ms/step - loss: 0.9053 - accuracy: 0.6332 - scaled_graph_loss: 0.0321 - val_loss: 1.2757 - val_accuracy: 0.6233\nEpoch 192/200\n5/5 [==============================] - 1s 156ms/step - loss: 0.9003 - accuracy: 0.6519 - scaled_graph_loss: 0.0333 - val_loss: 1.2747 - val_accuracy: 0.6256\nEpoch 193/200\n5/5 [==============================] - 1s 165ms/step - loss: 0.8634 - accuracy: 0.6420 - scaled_graph_loss: 0.0301 - val_loss: 1.2704 - val_accuracy: 0.6270\nEpoch 194/200\n5/5 [==============================] - 1s 164ms/step - loss: 0.8267 - accuracy: 0.6727 - scaled_graph_loss: 0.0314 - val_loss: 1.2641 - val_accuracy: 0.6274\nEpoch 195/200\n5/5 [==============================] - 1s 172ms/step - loss: 0.8430 - accuracy: 0.6941 - scaled_graph_loss: 0.0338 - val_loss: 1.2606 - val_accuracy: 0.6283\nEpoch 196/200\n5/5 [==============================] - 1s 177ms/step - loss: 0.8967 - accuracy: 0.6375 - scaled_graph_loss: 0.0320 - val_loss: 1.2575 - val_accuracy: 0.6316\nEpoch 197/200\n5/5 [==============================] - 1s 164ms/step - loss: 0.8748 - accuracy: 0.6446 - scaled_graph_loss: 0.0315 - val_loss: 1.2558 - val_accuracy: 0.6320\nEpoch 198/200\n5/5 [==============================] - 1s 190ms/step - loss: 0.9019 - accuracy: 0.6390 - scaled_graph_loss: 0.0323 - val_loss: 1.2531 - val_accuracy: 0.6316\nEpoch 199/200\n5/5 [==============================] - 1s 139ms/step - loss: 0.7997 - accuracy: 0.6777 - scaled_graph_loss: 0.0342 - val_loss: 1.2514 - val_accuracy: 0.6325\nEpoch 200/200\n5/5 [==============================] - 1s 247ms/step - loss: 0.9136 - accuracy: 0.6405 - scaled_graph_loss: 0.0328 - val_loss: 1.2526 - val_accuracy: 0.6320\n\n\n<tensorflow.python.keras.callbacks.History at 0x14fe35610>"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter06/01_Social_network_analysis.html",
    "href": "posts/2_Studies/GML/Chapter06/01_Social_network_analysis.html",
    "title": "[GML] Chap6: 소셜네트워크 그래프",
    "section": "",
    "text": "import os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\nIn this chapter we will focus on using the techniques outlined in previous chapters to analyze the most-common example of nowadays graphs: Social Networks. In particular we will apply the techniques outlined in previous chapters to investigate the topological properties of the networks, such as 1. identifying relevant communities as well as 2. identifying particularly important nodes in the network.\nWe will then use node embeddings to leverage on the power of topological information for different tasks, such as link prediction (as a potential recommendation engine for new friends)\n\n\nFirst, we need to download the dataset. We will be using the SNAP Facebook social graph. The dataset was created by collection Facebook user information from survey participants. More in detail, 10 ego-networks were created from ten users. Each user was asked to identify all the circles (list of friends) to which their friends belong. Then, all the “ego-network” were combined in a single graph.\n\n!wget http://snap.stanford.edu/data/facebook_combined.txt.gz\n!wget http://snap.stanford.edu/data/facebook.tar.gz\n!gzip -d facebook_combined.txt.gz\n!tar -xf facebook.tar.gz\n\n\n\n\nThe code above downloads two main files: * a text file containing the edge list of the graph. The graph is actually created * an archive containing a folder (“facebook”) with all the information related to each ego-network\n\n# check the downloaded content\n!ls\n\n\n# take a look at the first lines of the edge list\n!head facebook_combined.txt\n\nWe can now proceed loading the combined network using networkx. We will also load the nodeId of the 10 “ego-user”\n\nG = nx.read_edgelist(\"facebook_combined.txt\", create_using=nx.Graph(), nodetype=int)\n\n\nprint(nx.info(G))\n\n\n# Each file in the \"facebook\" directory is named as nodeId.format\n# where nodeId is the id of an ego-user and format is the format of the file\nego_nodes = set([int(name.split('.')[0]) for name in os.listdir(\"facebook/\")])\n\nLet’s visualize the network for a deeper understanding\n\n#Create network layout for visualizations\nspring_pos = nx.spring_layout(G)\n\n\nplt.axis(\"off\")\nnx.draw_networkx(G, pos=spring_pos, node_color=default_node_color, edge_color=default_edge_color, with_labels=False, node_size=35)\n\n\n\n\n\ndef draw_metric(G, dct, spring_pos):\n  \"\"\" draw the graph G using the layout spring_pos.\n      The top 10 nodes w.r.t. values in the dictionary dct\n      are enhanced in the visualization \"\"\"\n  top = 10\n  max_nodes =  sorted(dct.items(), key = lambda v: -v[1])[:top]\n  \n  max_keys = [key for key,_ in max_nodes]\n  max_vals = [val*300 for _, val in max_nodes]\n\n  plt.axis(\"off\")\n  \n  nx.draw_networkx(G, \n                   pos=spring_pos, \n                   cmap='Blues', \n                   edge_color=default_edge_color,\n                   node_color=default_node_color, \n                   node_size=3,\n                   alpha=0.4, \n                   with_labels=False)\n  \n  nx.draw_networkx_nodes(G, \n                         pos=spring_pos, \n                         nodelist=max_keys, \n                         node_color=enhanced_edge_color,\n                         node_size=max_vals)\n\n\n# betweenness centrality\nbC = nx.betweenness_centrality(G)\nnp.mean(list(bC.values()))\n\n\ndraw_metric(G,bC,spring_pos)\n\n\n# global efficiency\ngE = nx.global_efficiency(G)\nprint(gE)\n\n\n# average clustering\naC = nx.average_clustering(G)\nprint(aC)\n\n\n# degree centrality\ndeg_C = nx.degree_centrality(G)\nnp.mean(list(deg_C.values()))\n\n\ndraw_metric(G,deg_C,spring_pos)\n\n\n# closeness centrality\nclos_C = nx.closeness_centrality(G)\nnp.mean(list(clos_C.values()))\n\n\ndraw_metric(G,clos_C,spring_pos)\n\n\n# assortativity\nassortativity = nx.degree_pearson_correlation_coefficient(G)\nassortativity\n\n\nt = nx.transitivity(G)\nt\n\n\n#import networkx.algorithms.community as nx_comm\n#nx_comm.modularity(G, nx_comm.label_propagation_communities(G))\n\n\n\nIn the following cells we will automatically detect communities using infromation from the network topology\n\nimport community\n\nparts = community.best_partition(G)\nvalues = [parts.get(node) for node in G.nodes()]\n\nfor node in ego_nodes:\n  print(node, \"is in community number\", parts.get(node))\n  \nn_sizes = [5]*len(G.nodes())\nfor node in ego_nodes:\n  n_sizes[node] = 250\n\nplt.axis(\"off\")\nnx.draw_networkx(G, pos=spring_pos, cmap=plt.get_cmap(\"Blues\"), edge_color=default_edge_color, node_color=values, node_size=n_sizes, with_labels=False)\n\n# enhance color and size of the ego-nodes\nnodes = nx.draw_networkx_nodes(G,spring_pos,ego_nodes,node_color=[parts.get(node) for node in ego_nodes])\nnodes.set_edgecolor(enhanced_node_color)\n\n\n\n\n\nSince the combined network we are analyzing is actually composed by 10 sub-networks (ego-networks), it’s interesting to inspect all those subnetwork. In the following cells we will analyze the subnetwork of the ego-user “0”.\n\nG0 = nx.read_edgelist(\"facebook/0.edges\", create_using=nx.Graph(), nodetype=int)\nfor node in G0.copy():\n  G0.add_edge(0,node)\n\nplt.axis(\"off\")\npos_G0 = nx.spring_layout(G0)\nnx.draw_networkx(G0, pos=pos_G0, with_labels=False, node_size=35, edge_color=default_edge_color)\n\nNodes belonging to each subnetwork are stored in the “facebook” folder under the name nodeId.circles\n\nimport pandas as pd\ncircles = {}\n\nwith open(\"facebook/0.circles\") as f_in:\n  line = f_in.readline().rstrip().split(\"\\t\")\n  while line and not '' in line:\n    circles[line[0]] = [int(v) for v in line[1:]]\n    line = f_in.readline().rstrip().split(\"\\t\")\n\n\nnode_colors = [0] * G0.number_of_nodes()\ncount = 0\nfor key in circles:\n  circle = circles[key]\n  for node in circle:\n    if node < G0.number_of_nodes():\n      node_colors[node] = count\n  count += 1\n\nnx.draw_networkx(G0, pos=pos_G0, with_labels=False, node_size=35, node_color=node_colors, edge_color=default_edge_color)\n\n\nparts = community.best_partition(G0)\nvalues = [parts.get(node) for node in G0.nodes()]\n\nplt.axis(\"off\")\nnx.draw_networkx(G0, pos=pos_G0, cmap=plt.get_cmap(\"Blues\"), edge_color=default_edge_color, node_color=values, node_size=35, with_labels=False)\n\n\n# community found does not reflect the circles\nset(parts.values())\nlen(circles)\n\n\n# a node can be present in more than one list??\nfor i in circles:\n  for j in circles:\n    if i != j:\n      for n1 in circles[i]:\n        for n2 in circles[j]:\n          if n1 == n2:\n            print(n1, 'present in ',i,'found in', j)\n            assert(False)\n\n\n#@title  \nnx.average_shortest_path_length(G0)\nnx.global_efficiency(G0)\nnx.average_clustering(G0)\n\nnp.mean(list(nx.betweenness_centrality(G0).values()))\nnp.mean(list(nx.closeness_centrality(G0).values()))\nnp.mean(list(nx.degree_centrality(G0).values()))\nnx.degree_pearson_correlation_coefficient(G)\nnx.transitivity(G)\n\nimport networkx.algorithms.community as nx_comm\nnx_comm.modularity(G, nx_comm.label_propagation_communities(G))\n\n\n\n\nWe will now proceed with the actual machine learning task. In particular, we will perform an edge prediction task for the Facebook social graph.\n\n\nAs first, let’s load all the features describing each node. This is not a straightforward process and requires a bit of codes, since each subnetwork contains it’s own set of features, whose names and values are stored in different files.\n\n# Adapted from https://github.com/jcatw/snap-facebook\n\nfeat_file_name = \"feature_map.txt\"\nfeature_index = {}  #numeric index to name\ninverted_feature_index = {} #name to numeric index\nnetwork = nx.Graph()\n\ndef parse_featname_line(line):\n  \"\"\" used to parse each line of the files containing feature names \"\"\"\n  line = line[(line.find(' '))+1:]  # chop first field\n  split = line.split(';')\n  name = ';'.join(split[:-1]) # feature name\n  index = int(split[-1].split(\" \")[-1]) #feature index\n  return index, name\n\ndef load_features():\n  \"\"\" \n  parse each ego-network and creates two dictionaries:\n      - feature_index: maps numeric indices to names\n      - inverted_feature_index: maps names to numeric indices\n  \"\"\"\n  import glob\n  feat_file_name = 'tmp.txt'\n  # may need to build the index first\n  if not os.path.exists(feat_file_name):\n      feat_index = {}\n      # build the index from data/*.featnames files\n      featname_files = glob.iglob(\"facebook/*.featnames\")\n      for featname_file_name in featname_files:\n          featname_file = open(featname_file_name, 'r')\n          for line in featname_file:\n              # example line:\n              # 0 birthday;anonymized feature 376\n              index, name = parse_featname_line(line)\n              feat_index[index] = name\n          featname_file.close()\n      keys = feat_index.keys()\n      keys = sorted(keys)\n      out = open(feat_file_name,'w')\n      for key in keys:\n          out.write(\"%d %s\\n\" % (key, feat_index[key]))\n      out.close()\n\n  index_file = open(feat_file_name,'r')\n  for line in index_file:\n      split = line.strip().split(' ')\n      key = int(split[0])\n      val = split[1]\n      feature_index[key] = val\n  index_file.close()\n\n  for key in feature_index.keys():\n      val = feature_index[key]\n      inverted_feature_index[val] = key\n\ndef parse_nodes(network, ego_nodes):\n  \"\"\"\n  for each nodes in the network assign the corresponding features \n  previously loaded using the load_features function\n  \"\"\"\n  # parse each node\n  for node_id in ego_nodes:\n      featname_file = open(f'facebook/{node_id}.featnames','r')\n      feat_file     = open(f'facebook/{node_id}.feat','r')\n      egofeat_file  = open(f'facebook/{node_id}.egofeat','r')\n      edge_file     = open(f'facebook/{node_id}.edges','r')\n\n      ego_features = [int(x) for x in egofeat_file.readline().split(' ')]\n\n      # Add ego node features\n      network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n      \n      # parse ego node\n      i = 0\n      for line in featname_file:\n          key, val = parse_featname_line(line)\n          # Update feature value if necessary\n          if ego_features[i] + 1 > network.nodes[node_id]['features'][key]:\n              network.nodes[node_id]['features'][key] = ego_features[i] + 1\n          i += 1\n\n      # parse neighboring nodes\n      for line in feat_file:\n          featname_file.seek(0)\n          split = [int(x) for x in line.split(' ')]\n          node_id = split[0]\n          features = split[1:]\n\n          # Add node features\n          network.nodes[node_id]['features'] = np.zeros(len(feature_index))\n\n          i = 0\n          for line in featname_file:\n              key, val = parse_featname_line(line)\n              # Update feature value if necessary\n              if features[i] + 1 > network.nodes[node_id]['features'][key]:\n                  network.nodes[node_id]['features'][key] = features[i] + 1\n              i += 1\n          \n      featname_file.close()\n      feat_file.close()\n      egofeat_file.close()\n      edge_file.close()\n\n\n# parse edge features and add them to the networkx nodes\nload_features()\nparse_nodes(G, ego_nodes)\n\n\n# check features has been correctly assigned\nG.nodes[0]\n\n\n\n\n\nIt’s now time for machine learning. As first, we will be using stellargraph utility function to define a train and test set. More in detail, TODO\n\n!pip install stellargraph\n!pip install node2vec==0.3.3\n!pip install git+https://github.com/palash1992/GEM.git\n\n\nfrom sklearn.model_selection import train_test_split\nfrom stellargraph.data import EdgeSplitter\nfrom stellargraph import StellarGraph\n\nedgeSplitter = EdgeSplitter(G) \ngraph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)\n\nedgeSplitter = EdgeSplitter(graph_test, G) \ngraph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)\n\nWe will be comparing three different methods for predicting missing edges: - Method1: node2vec will be used to learn a node embedding. Such embeddings will be used to train a Random Forest classifier in a supervised manner - Method2: graphSAGE (with and without features) will be used for link prediction - Method3: hand-crafted features will be extracted and used to train a Random Forest classifier\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder \nfrom stellargraph.data import EdgeSplitter \n\nnode2vec = Node2Vec(graph_train) \nmodel = node2vec.fit() \nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) \ntrain_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv) \ntest_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nrf = RandomForestClassifier(n_estimators=10) \nrf.fit(train_embeddings, labels_train); \n \ny_pred = rf.predict(test_embeddings) \nprint('Precision:', metrics.precision_score(labels_test, y_pred)) \nprint('Recall:', metrics.recall_score(labels_test, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_test, y_pred)) \n\n\n\n\n\n# graphSAGE no feats\n\n\neye = np.eye(graph_train.number_of_nodes())\nfake_features = {n:eye[n] for n in G.nodes()}\nnx.set_node_attributes(graph_train, fake_features, \"fake\")\n\neye = np.eye(graph_test.number_of_nodes())\nfake_features = {n:eye[n] for n in G.nodes()}\nnx.set_node_attributes(graph_test, fake_features, \"fake\")\n\n\ngraph_train.nodes[0]\n\n\nfrom stellargraph.mapper import GraphSAGELinkGenerator\n\nbatch_size = 64\nnum_samples = [4, 4]\n\nsg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"fake\")\nsg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"fake\")\n\ntrain_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\ntrain_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\n\ntest_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\ntest_flow = test_gen.flow(samples_test, labels_test, seed=24)\n\n\nfrom stellargraph.layer import GraphSAGE, link_classification\nfrom tensorflow import keras\n\nlayer_sizes = [20, 20]\ngraphsage = GraphSAGE(\n    layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3\n)\n\nx_inp, x_out = graphsage.in_out_tensors()\n\nprediction = link_classification(\n    output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n)(x_out)\n\nmodel = keras.Model(inputs=x_inp, outputs=prediction)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=1e-3),\n    loss=keras.losses.mse,\n    metrics=[\"acc\"],\n)\n\n\nepochs = 10\nhistory = model.fit(train_flow, epochs=epochs, validation_data=test_flow)\n\n\nfrom sklearn import metrics \ny_pred = np.round(model.predict(train_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_train, y_pred)) \nprint('Recall:', metrics.recall_score(labels_train, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_train, y_pred)) \n\n\ny_pred = np.round(model.predict(test_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_test, y_pred)) \nprint('Recall:', metrics.recall_score(labels_test, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_test, y_pred)) \n\n\n# graphSAGE + feats\n\n\nsg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"features\")\nsg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"features\")\n\ntrain_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\ntrain_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\n\ntest_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\ntest_flow = test_gen.flow(samples_test, labels_test, seed=24)\n\n\nlayer_sizes = [20, 20]\ngraphsage = GraphSAGE(\n    layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3\n)\n\nx_inp, x_out = graphsage.in_out_tensors()\n\nprediction = link_classification(\n    output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\"\n)(x_out)\n\nmodel = keras.Model(inputs=x_inp, outputs=prediction)\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=1e-3),\n    loss=keras.losses.mse,\n    metrics=[\"acc\"],\n)\n\nepochs = 10\nhistory = model.fit(train_flow, epochs=epochs, validation_data=test_flow)\n\n\nfrom sklearn import metrics \ny_pred = np.round(model.predict(train_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_train, y_pred)) \nprint('Recall:', metrics.recall_score(labels_train, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_train, y_pred)) \n\n\ny_pred = np.round(model.predict(test_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_test, y_pred)) \nprint('Recall:', metrics.recall_score(labels_test, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_test, y_pred)) \n\n\n\n\n\nimport community\n\ndef get_shortest_path(G,u,v):\n  \"\"\" return the shortest path length between u,v \n      in the graph without the edge (u,v) \"\"\"\n  removed = False\n  if G.has_edge(u,v):\n    removed = True\n    G.remove_edge(u,v) # temporary remove edge\n  \n  try:\n    sp = len(nx.shortest_path(G, u, v))\n  except:\n    sp = 0\n\n  if removed:\n    G.add_edge(u,v) # add back the edge if it was removed\n\n  return sp\n\ndef get_hc_features(G, samples_edges, labels):\n  # precompute metrics\n  centralities = nx.degree_centrality(G)\n  parts = community.best_partition(G)\n  \n  feats = []\n  for (u,v),l in zip(samples_edges, labels):\n    shortest_path = get_shortest_path(G, u, v)\n    j_coefficient = next(nx.jaccard_coefficient(G, ebunch=[(u, v)]))[-1]\n    u_centrality = centralities[u]\n    v_centrality = centralities[v]\n    u_community = parts.get(u)\n    v_community = parts.get(v)\n    # add the feature vector\n    feats += [[shortest_path, j_coefficient, u_centrality, v_centrality]]\n  return feats\n\nfeat_train = get_hc_features(graph_train, samples_train, labels_train)\nfeat_test = get_hc_features(graph_test, samples_test, labels_test)\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nrf = RandomForestClassifier(n_estimators=10) \nrf.fit(feat_train, labels_train); \n \ny_pred = rf.predict(feat_test) \nprint('Precision:', metrics.precision_score(labels_test, y_pred)) \nprint('Recall:', metrics.recall_score(labels_test, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_test, y_pred))"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter05/01_link_prediction.html",
    "href": "posts/2_Studies/GML/Chapter05/01_link_prediction.html",
    "title": "[GML] Chap5: 응용문제 - 누락된 링크예측",
    "section": "",
    "text": "import networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.resource_allocation_index(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.5), (3, 4, 0.5)]\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\npreds = nx.jaccard_coefficient(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0.25), (3, 4, 0.3333333333333333)]\n\n\n\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\npreds = nx.cn_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 2), (2, 5, 1), (3, 4, 1)]\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nedges = [[1,3],[2,3],[2,4],[4,5],[5,6],[5,7]]\nG = nx.from_edgelist(edges)\n\nG.nodes[1][\"community\"] = 0\nG.nodes[2][\"community\"] = 0\nG.nodes[3][\"community\"] = 0\n\nG.nodes[4][\"community\"] = 1\nG.nodes[5][\"community\"] = 1\nG.nodes[6][\"community\"] = 1\nG.nodes[7][\"community\"] = 1\npreds = nx.ra_index_soundarajan_hopcroft(G,[(1,2),(2,5),(3,4)])\nprint(list(preds))\ndraw_graph(G)\n\n[(1, 2, 0.5), (2, 5, 0), (3, 4, 0)]\n\n\n\n\n\n\n\n\n\n\nimport networkx as nx\nimport pandas as pd\n\nedgelist = pd.read_csv(\"cora.cites\", sep='\\t', header=None, names=[\"target\", \"source\"])\nG = nx.from_pandas_edgelist(edgelist)\ndraw_graph(G)\n\n\n\n\n\nfrom stellargraph.data import EdgeSplitter\n\nedgeSplitter = EdgeSplitter(G)\ngraph_test, samples_test, labels_test = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n** Sampled 527 positive and 527 negative edges. **\n\n\n\nedgeSplitter = EdgeSplitter(graph_test, G)\ngraph_train, samples_train, labels_train = edgeSplitter.train_test_split(\n    p=0.1, method=\"global\"\n)\n\n** Sampled 475 positive and 475 negative edges. **\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder\n\nnode2vec = Node2Vec(graph_train)\nmodel = node2vec.fit()\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv)\ntrain_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n\nComputing transition probabilities: 100%|██████████| 2708/2708 [00:00<00:00, 4284.35it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [01:24<00:00,  8.43s/it]\n\n\n\ntest_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=1000)\nrf.fit(train_embeddings, labels_train);\n\n\nfrom sklearn import metrics\n\ny_pred = rf.predict(test_embeddings)\n\nprint('Precision:', metrics.precision_score(labels_test, y_pred))\nprint('Recall:', metrics.recall_score(labels_test, y_pred))\nprint('F1-Score:', metrics.f1_score(labels_test, y_pred))\n\nPrecision: 0.8557114228456913\nRecall: 0.8102466793168881\nF1-Score: 0.8323586744639375\n\n\n\nimport matplotlib.pyplot as plt\n\ndef draw_graph(G, node_names={}, node_size=500):\n    pos_nodes = nx.spring_layout(G)\n    nx.draw(G, pos_nodes, with_labels=True, node_size=node_size, edge_color='gray', arrowsize=30)\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    #nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    plt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter05/02_community_detection_algorithms.html",
    "href": "posts/2_Studies/GML/Chapter05/02_community_detection_algorithms.html",
    "title": "[GML] Chap5: 응용문제 - 커뮤니티 감지",
    "section": "",
    "text": "Network Communities Detection\nIn this notebook, we will explore some methods to perform a community detection using several algortihms. Before testing the algorithms, let us create a simple benchmark graph.\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\n\n\nimport numpy as np\nimport pandas as pd\n\n\nimport networkx as nx \nG = nx.barbell_graph(m1=10, m2=4) \n\n\nMatrix Factorization\nWe start by using some matrix factorization technique to extract the embeddings, which are visualized and then clustered traditional clustering algorithms.\n\nfrom gem.embedding.hope import HOPE \ngf = HOPE(d=4, beta=0.01) \ngf.learn_embedding(G) \nembeddings = gf.get_embedding() \n\nSVD error (low rank): 0.052092\n\n\n\nfrom sklearn.manifold import TSNE\n\n\ntsne = TSNE(n_components=2) \n\nemb2d = tsne.fit_transform(embeddings)\n\n\nplt.plot(embeddings[:, 0], embeddings[:, 1], 'o', linewidth=0)\n\n\n\n\nWe start by using a GaussianMixture model to perform the clustering\n\nfrom sklearn.mixture import GaussianMixture\n\n\ngm = GaussianMixture(n_components=3, random_state=0) #.(embeddings)\n\n\nlabels = gm.fit_predict(embeddings)\n\n\ncolors = [\"blue\", \"green\", \"red\"]\n\n\nnx.draw_spring(G, node_color=[colors[label] for label in labels])\n\n\n\n\n\n\nSpectral Clustering\nWe now perform a spectral clustering based on the adjacency matrix of the graph. It is worth noting that this clustering is not a mutually exclusive clustering and nodes may belong to more than one community\n\nadj=np.array(nx.adjacency_matrix(G).todense())\n\n\nfrom communities.algorithms import spectral_clustering\n\ncommunities = spectral_clustering(adj, k=3)\n\nIn the next plot we highlight the nodes that belong to a community using the red color. The blue nodes do not belong to the given community\n\nplt.figure(figsize=(20, 5))\n\nfor ith, community in enumerate(communities):\n    cols = [\"red\" if node in community else \"blue\" for node in G.nodes]\n    plt.subplot(1,3,ith+1)\n    plt.title(f\"Community {ith}\")\n    nx.draw_spring(G, node_color=cols)\n\n\n\n\nThe next command shows the node ids belonging to the different communities\n\ncommunities\n\n[{14, 15, 16, 17, 18, 19, 20, 21, 22, 23},\n {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11},\n {12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}]\n\n\n\n\nNon Negative Matrix Factorization\nHere, we again use matrix factorization, but now using the Non-Negative Matrix Factorization, and associating the clusters with the latent dimensions.\n\nfrom sklearn.decomposition import NMF\n\n\nnmf = NMF(n_components=2)\n\n\nemb = nmf.fit_transform(adj)\n\n/Users/deusebio/.pyenv/versions/3.8.6/envs/ml-book-5/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n  warnings.warn((\"The 'init' value, when 'init=None' and \"\n\n\n\nplt.plot(emb[:, 0], emb[:, 1], 'o', linewidth=0)\n\n\n\n\nBy setting a threshold value of 0.01, we determine which nodes belong to the given community.\n\ncommunities = [set(np.where(emb[:,ith]>0.01)[0]) for ith in range(2)]\n\n\nplt.figure(figsize=(20, 5))\n\nfor ith, community in enumerate(communities):\n    cols = [\"red\" if node in community else \"blue\" for node in G.nodes]\n    plt.subplot(1,3,ith+1)\n    plt.title(f\"Community {ith}\")\n    nx.draw_spring(G, node_color=cols)\n\n\n\n\nAlthough the example above does not show this, in general also this clustering method may be non-mutually exclusive, and nodes may belong to more than one community\n\n\nLouvain and Modularity Optimization\nHere, we use the Louvain method, which is one of the most popular methods for performing community detection, even on fairly large graphs. As described in the chapter, the Louvain method basically optimize the partitioning (it is a mutually exclusing community detection algorithm), identifying the one that maximize the modularity score, meaning that nodes belonging to the same community are very well connected among themself, and weakly connected to the other communities.\nLouvain, unlike other community detection algorithms, does not require to specity the number of communities in advance and find the best, optimal number of communities.\n\nfrom communities.algorithms import louvain_method\ncommunities = louvain_method(adj)\n\n\nc = pd.Series({node: colors[ith] for ith, nodes in enumerate(communities) for node in nodes}).values\nnx.draw_spring(G, node_color=c)\n\n\n\n\n\ncommunities\n\n\n\nGirvan Newman\nThe Girvan–Newman algorithm detects communities by progressively removing edges from the original graph. The algorithm removes the “most valuable” edge, traditionally the edge with the highest betweenness centrality, at each step. As the graph breaks down into pieces, the tightly knit community structure is exposed and the result can be depicted as a dendrogram.\nBE AWARE that because of the betweeness centrality computation, this method may not scale well on large graphs\n\nfrom communities.algorithms import girvan_newman\ncommunities = girvan_newman(adj, n=2)\n\n\nc = pd.Series({node: colors[ith] for ith, nodes in enumerate(communities) for node in nodes}).values\nnx.draw_spring(G, node_color=c)\n\n\ncommunities"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/04_Graph_Neural_Network.html",
    "href": "posts/2_Studies/GML/Chapter03/04_Graph_Neural_Network.html",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Graph Neural Network",
    "section": "",
    "text": "In this notebook we will be performing unsupervised graph representation learning using Graph ConvNet as encoder.\nThe model embeds a graph by using stacked Graph ConvNet layers\n\n#from networkx import karate_club_graph, to_numpy_matrix\nimport numpy as np\nimport networkx as nx\nfrom scipy.linalg import sqrtm\nimport matplotlib.pyplot as plt\n\nG = nx.barbell_graph(m1=10, m2=4)\n\norder = np.arange(G.number_of_nodes())\nA = nx.to_numpy_matrix(G, nodelist=order)\nI = np.eye(G.number_of_nodes())\n\n\nnp.random.seed(7)\n\nA_hat = A + np.eye(G.number_of_nodes()) # add self-connections\n\nD_hat = np.array(np.sum(A_hat, axis=0))[0]\nD_hat = np.array(np.diag(D_hat))\nD_hat = np.linalg.inv(sqrtm(D_hat))\n\nA_hat = D_hat @ A_hat @ D_hat\n\ndef glorot_init(nin, nout):\n  sd = np.sqrt(6.0 / (nin + nout))\n  return np.random.uniform(-sd, sd, size=(nin, nout))\n\nclass GCNLayer():\n  def __init__(self, n_inputs, n_outputs):\n      self.n_inputs = n_inputs\n      self.n_outputs = n_outputs\n      self.W = glorot_init(self.n_outputs, self.n_inputs)\n      self.activation = np.tanh\n      \n  def forward(self, A, X):\n      self._X = (A @ X).T # (N,N)*(N,n_outputs) ==> (n_outputs,N)\n      H = self.W @ self._X # (N, D)*(D, n_outputs) => (N, n_outputs)\n      H = self.activation(H)\n      return H.T # (n_outputs, N)\n\ngcn1 = GCNLayer(G.number_of_nodes(), 8)\ngcn2 = GCNLayer(8, 4)\ngcn3 = GCNLayer(4, 2)\n\nH1 = gcn1.forward(A_hat, I)\nH2 = gcn2.forward(A_hat, H1)\nH3 = gcn3.forward(A_hat, H2)\n\nembeddings = H3\n\n\ndef draw_graph(G, filename=None, node_size=50):\n  pos_nodes = nx.spring_layout(G)\n  nx.draw(G, pos_nodes, with_labels=False, node_size=node_size, edge_color='gray')\n  \n  pos_attrs = {}\n  for node, coords in pos_nodes.items():\n    pos_attrs[node] = (coords[0], coords[1] + 0.08)\n\n  plt.axis('off')\n  axis = plt.gca()\n  axis.set_xlim([1.2*x for x in axis.get_xlim()])\n  axis.set_ylim([1.2*y for y in axis.get_ylim()])\n\nembeddings = np.array(embeddings)\ndraw_graph(G)\n\n\n\n\n\nplt.scatter(embeddings[:, 0], embeddings[:, 1])\nplt.savefig('embedding_gcn.png',dpi=300)\n\n\n\n\n\n\nFor the next example, we need to install StellarGraph, the python library we will be using to build the model\n\n# install StellarGraph\n!pip install -q stellargraph[demos]==1.2.1\n\nzsh:1: no matches found: stellargraph[demos]==1.2.1\n\n\n\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport os\n\nimport stellargraph as sg\nfrom stellargraph.mapper import FullBatchNodeGenerator\nfrom stellargraph.layer import GCN\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers, losses, metrics, Model\nfrom sklearn import preprocessing, model_selection\nfrom IPython.display import display, HTML\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nIn this demo, we will be using the PROTEINS dataset, already integrated in StellarGraph\n\ndataset = sg.datasets.PROTEINS()\ndisplay(HTML(dataset.description))\ngraphs, graph_labels = dataset.load()\n\nEach graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes.\n\n\n\n# let's print some info to better understand the dataset\nprint(graphs[0].info())\ngraph_labels.value_counts().to_frame()\n\nStellarGraph: Undirected multigraph\n Nodes: 42, Edges: 162\n\n Node types:\n  default: [42]\n    Features: float32 vector, length 4\n    Edge types: default-default->default\n\n Edge types:\n    default-default->default: [162]\n        Weights: all 1 (default)\n        Features: none\n\n\n\n\n\n\n  \n    \n      \n      label\n    \n  \n  \n    \n      1\n      663\n    \n    \n      2\n      450\n    \n  \n\n\n\n\n\n\nIt’s now time to build-up the model. StellarGraph offers several utility function to load and process the dataset, as well as define the GNN model and train.\n\n# TODO\ngenerator = sg.mapper.PaddedGraphGenerator(graphs)\n\n\n# define a GCN model containing 2 layers of size 64 and 32, respectively. \n# ReLU activation function is used to add non-linearity between layers\ngc_model = sg.layer.GCNSupervisedGraphClassification(\n    [64, 32], [\"relu\", \"relu\"], generator, pool_all_layers=True\n)\n\n\ninp1, out1 = gc_model.in_out_tensors()\ninp2, out2 = gc_model.in_out_tensors()\n\nvec_distance = tf.norm(out1 - out2, axis=1)\n\n\npair_model = Model(inp1 + inp2, vec_distance)\nembedding_model = Model(inp1, out1)\n\n\ndef graph_distance(graph1, graph2):\n    spec1 = nx.laplacian_spectrum(graph1.to_networkx(feature_attr=None))\n    spec2 = nx.laplacian_spectrum(graph2.to_networkx(feature_attr=None))\n    k = min(len(spec1), len(spec2))\n    return np.linalg.norm(spec1[:k] - spec2[:k])\n\n\ngraph_idx = np.random.RandomState(0).randint(len(graphs), size=(100, 2))\ntargets = [graph_distance(graphs[left], graphs[right]) for left, right in graph_idx]\ntrain_gen = generator.flow(graph_idx, batch_size=10, targets=targets)\n\n\npair_model.compile(optimizers.Adam(1e-2), loss=\"mse\")\n\n\nhistory = pair_model.fit(train_gen, epochs=500, verbose=0)\nsg.utils.plot_history(history)\n\n\n\n\n\nembeddings = embedding_model.predict(generator.flow(graphs))\n\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(2)\ntwo_d = tsne.fit_transform(embeddings)\n\n\nplt.scatter(two_d[:, 0], two_d[:, 1], c=graph_labels.cat.codes, cmap=\"jet\", alpha=0.4)\nplt.savefig('embedding_TSNE.png',dpi=300)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/02_Autoencoders.html",
    "href": "posts/2_Studies/GML/Chapter03/02_Autoencoders.html",
    "title": "[GML] Chap3: 비지도 그래프 학습 - AutoEncoder",
    "section": "",
    "text": "AutoEncoder\nIn the following we will show you how to create, train and use a simple autoencoder. We will then show you how to make an auto-encoder more robust against noise.\n\nLoad Dataset\n\nimport tensorflow as tf\n\n\nfrom tensorflow.keras.datasets import fashion_mnist\n\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\nprint (x_train.shape)\nprint (x_test.shape)\n\n\nfrom matplotlib import pyplot as plt\n\n\nclasses = {\n    0:\"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\", \n}\n\n\nn = 6\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # display original\n    ax = plt.subplot(1, n, i + 1)\n    plt.imshow(x_test[i])\n    plt.title(classes[y_test[i]])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n# plt.savefig(\"TrainingSet.png\")\n\n\n\nCreate Autoencoder\n\nfrom tensorflow.keras.layers import Flatten, Conv2D, Dropout, MaxPooling2D, UpSampling2D, Input\n\n\nfrom tensorflow.keras import Model\n\n\ninput_img = Input(shape=(28, 28, 1))\n\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\n\n\nModel(input_img, encoded).summary()\n\n\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\nfrom tensorflow.keras.callbacks import TensorBoard\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\n\nautoencoder.save(\"./data/Batch50.p\")\n\n\nfrom tensorflow.keras.models import load_model\n\n\nautoencoder_first = load_model(\"./data/Batch50.p\")\n\n\ndecoded_imgs = autoencoder_first.predict(x_test)\n\nn = 6\nplt.figure(figsize=(20, 7))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n\n\nfrom tensorflow.keras.optimizers import Adam\n\n\nautoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy')\n\n\nautoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])\n\n\nautoencoder.save(\"./data/Batch100.p\")\n\n\ndecoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()\n\n\n\nEmbeddings\nWe use the trained layers in order to get the core representation in the middle layer of the autoencoder, and we represent them with the TSNE\n\nembeddings = Model(input_img, Flatten()(encoded)).predict(x_test)\n\n\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\n\ntsne = TSNE(n_components=2)\n\n\nemb2d = tsne.fit_transform(embeddings)\n\n\nx,y = np.squeeze(emb2d[:, 0]), np.squeeze(emb2d[:, 1])\n\n\nimport pandas as pd\n\n\nfrom matplotlib.cm import tab10\n\n\nsummary =  pd.DataFrame({\"x\": x, \"y\": y, \"target\": y_test, \"size\": 10})\n\nplt.figure(figsize=(10,8))\n\nfor key, sel in summary.groupby(\"target\"):\n    plt.scatter(sel[\"x\"], sel[\"y\"], s=10, color=tab10.colors[key], label=classes[key])\n    \nplt.legend()\nplt.axis(\"off\")\n\n\n\nDenoising\nIntroducing noise in order to train more robust auto-encoders\n\nfrom tensorflow.keras.layers import GaussianNoise\n\n\ninput_img = Input(shape=(28, 28, 1))\n\nnoisy_input = GaussianNoise(0.1)(input_img)\n\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(noisy_input)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nnoisy_autoencoder = Model(input_img, decoded)\n\n\nnoisy_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\n\nnoisy_autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=128,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                callbacks=[TensorBoard(log_dir='/tmp/noisy_autoencoder')])\n\n\nautoencoder.save(\"./data/DenoisingAutoencoder.p\")\n\n\nnoise_factor = 0.1\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\ndecoded_imgs = autoencoder.predict(x_test_noisy)\n\ndecoded_imgs_denoised = noisy_autoencoder.predict(x_test_noisy)\n\nn = 6\nplt.figure(figsize=(20, 10))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(3, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Original\")\n    else:\n        ax.get_yaxis().set_visible(False)\n        \n    # Display reconstruction\n    ax = plt.subplot(3, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Vanilla Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n     \n    ax = plt.subplot(3, n, i + 2*n)\n    plt.imshow(decoded_imgs_denoised[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    if i==0:\n        plt.ylabel(\"Denoising Autoencoder\")\n    else:\n        ax.get_yaxis().set_visible(False)\n    \n        \nplt.show()\n\n\ndecoded_imgs = noisy_autoencoder.predict(x_test_noisy)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\ndef draw_graph(G, node_names={}, node_size=500):\n    pos_nodes = nx.spring_layout(G)\n    nx.draw(G, pos_nodes, with_labels=True, node_size=node_size, edge_color='gray', arrowsize=30)\n    \n    pos_attrs = {}\n    for node, coords in pos_nodes.items():\n        pos_attrs[node] = (coords[0], coords[1] + 0.08)\n        \n    #nx.draw_networkx_labels(G, pos_attrs, font_family='serif', font_size=20)\n    \n    plt.axis('off')\n    axis = plt.gca()\n    axis.set_xlim([1.2*x for x in axis.get_xlim()])\n    axis.set_ylim([1.2*y for y in axis.get_ylim()])\n    plt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graph-factorization",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graph-factorization",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "Graph Factorization",
    "text": "Graph Factorization\n\nimport networkx as nx\n\nG = nx.barbell_graph(m1=3, m2=2)\ndraw_graph(G)\n\n\nfrom pathlib import Path\nPath(\"gem/intermediate\").mkdir(parents=True, exist_ok=True)\n\n\nfrom gem.embedding.gf import GraphFactorization\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngf = GraphFactorization(d=2,  data_set=None,max_iter=10000, eta=1*10**-4, regu=1.0)\ngf.learn_embedding(G)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = gf.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graphrep",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graphrep",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "GraphRep",
    "text": "GraphRep\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.grarep import GraRep\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ngr = GraRep(dimensions=2,order=3)\ngr.fit(G)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nida = 4\nidb = 5\nfor x in G.nodes():\n    \n    v = gr.get_embedding()[x]\n    ax.scatter(v[ida],v[idb], s=1000)\n    ax.annotate(str(x), (v[ida],v[idb]), fontsize=12)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#hope",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#hope",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "HOPE",
    "text": "HOPE\n\nimport networkx as nx\nfrom gem.embedding.hope import HOPE\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\nhp = HOPE(d=4, beta=0.01)\nhp.learn_embedding(G)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = hp.get_embedding()[x,2:]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=20)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#deepwalk",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#deepwalk",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "DeepWalk",
    "text": "DeepWalk\n\nimport networkx as nx\nfrom karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\ndw = DeepWalk(dimensions=2)\ndw.fit(G)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = dw.get_embedding()[x]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=12)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#node2vec",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#node2vec",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "Node2Vec",
    "text": "Node2Vec\n\nimport networkx as nx\nfrom node2vec import Node2Vec\n\nG = nx.barbell_graph(m1=10, m2=4)\ndraw_graph(G)\n\nnode2vec = Node2Vec(G, dimensions=2)\nmodel = node2vec.fit(window=10)\nembeddings = model.wv\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.nodes():\n    \n    v = model.wv[str(x)]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#edge2vec",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#edge2vec",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "Edge2Vec",
    "text": "Edge2Vec\n\nfrom node2vec.edges import HadamardEmbedder\nedges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor x in G.edges():\n    \n    v = edges_embs[(str(x[0]), str(x[1]))]\n    ax.scatter(v[0],v[1], s=1000)\n    ax.annotate(str(x), (v[0],v[1]), fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graph2vec",
    "href": "posts/2_Studies/GML/Chapter03/01_Shallow_Embeddings.html#graph2vec",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings",
    "section": "Graph2Vec",
    "text": "Graph2Vec\n\nimport random\nimport matplotlib.pyplot as plt\nfrom karateclub import Graph2Vec\n\nn_graphs = 20\n\ndef generate_radom():\n    n = random.randint(6, 20)\n    k = random.randint(5, n)\n    p = random.uniform(0, 1)\n    return nx.watts_strogatz_graph(n,k,p), [n,k,p]\n\nGs = [generate_radom() for x in range(n_graphs)]\n\nmodel = Graph2Vec(dimensions=2, wl_iterations=10)\nmodel.fit([x[0] for x in Gs])\nembeddings = model.get_embedding()\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfor i,vec in enumerate(embeddings):\n    \n    ax.scatter(vec[0],vec[1], s=1000)\n    ax.annotate(str(i), (vec[0],vec[1]), fontsize=40)"
  },
  {
    "objectID": "posts/2_Studies/GML/Chapter03/03_Structural_deep_neural_embeddings.html",
    "href": "posts/2_Studies/GML/Chapter03/03_Structural_deep_neural_embeddings.html",
    "title": "[GML] Chap3: 비지도 그래프 학습 - Structural Deep Network Embedding",
    "section": "",
    "text": "Structural Deep Network Embedding\nimport tensorflow as tf\n\nfrom gem.embedding.sdne import SDNE\n\n\nimport networkx as nx\n\n\ngraph = nx.karate_club_graph()\n\n\nm1 = SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3,n_units=[50, 15,], rho=0.3, n_iter=10, \n          xeta=0.01,n_batch=50,\n          modelfile=['enc_model.json', 'dec_model.json'],\n          weightfile=['enc_weights.hdf5', 'dec_weights.hdf5'])\n\n\nm1 = SDNE(d=2, beta=5, alpha=1e-5, nu1=1e-6, nu2=1e-6, K=3,n_units=[50, 15,], rho=0.3, n_iter=50, \n          xeta=0.01,n_batch=100,\n          modelfile=['enc_model.json', 'dec_model.json'],\n          weightfile=['enc_weights.hdf5', 'dec_weights.hdf5'])\n\n\nm1.learn_embedding(graph)\n\n\nx, y = list(zip(*m1.get_embedding()))\n\n\nfrom matplotlib import pyplot as plt\n\n\nplt.plot(x, y, 'o',linewidth=None)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "href": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅",
    "section": "",
    "text": "About this doc\n- 우분투에서 여러가지 개발환경을 설정하는 방법을 포스팅 하겠다.\n- 이 포스트는 우분투를 메인OS(사무용+연구용)로 사용하고 싶은 사람, 우분투를 활용하여 개발용 서버를 구축하고 싶은 사람에게 모두 유용한다.\n- 이 포스트는 2080 이상의 GPU를 활용한 학습을 원하는 사람에게 유용하다.\n- 이 포스트는 R과 파이썬을 동시에 쓰는 사람에게 유용하다.\n- 이 포스트는 Rstudio, Jupyter Lab을 동시에 쓰는 사람에게 유용하다.\n- 매년 조금씩 셋팅방법이 다른것 같다.\n- 가장 최근에는 2023년 3월8일에 이 블로그 내용으로 셋팅해보았음.\n\n\n우분투설치\n- 22.04부터는 파티션 나누지 않고 그냥 설치해도 잘 되는것 같다.\n\n\n네트워크 설정\n- ?표시있는 아이콘 \\(\\to\\) Wired Connected \\(\\to\\) Wired Settings \\(\\to\\) Connection의 설정 \\(\\to\\) IPv4 \\(\\to\\) Manual \\(\\to\\) Address, Netmask, Gateway, DNS 설정 \\(\\to\\) 네트워크 토글\n\n\n한글설정 (개발용 서버일 경우 생략 가능)\n- 아래와 같이 커맨드에 친다.\nibus-setup\n이걸 치면 IBus Preferences 라는 창이 나오는데 여기에서 (1) Input Method 탭 클릭 (2) Add 버튼 클릭 (3) Korean 선택 (4) Hangul 선택을 한다.\n- 위의 단계에서 Korean이 안보이면 Language Support로 가서 한국어팩을 설치하고 리부팅 하면 된다. (보통 실행하자마자 알아서 설치되더라.. 설치가 안되면 Install / Remove Languages... 이라는 탭을 클릭해서 설치하자) 리부팅을 꼭 해야한다는 것에 주의하자.\n- 이제 Region & Language로 가서 설정하면 된다.\n\n\n그래픽카드 드라이버설치\n- 전체적인 내용은 여기를 참고하자.\n- 준비작업\nsudo apt update \nsudo apt install gcc\nsudo apt install build-essential\n- 우선 gedit를 열고 아래를 복사해서 붙여넣는다.\nblacklist nouveau\noptions nouveau modeset=0\n파일이름을 blacklist-nouveau.conf로 home에 저장\n- 루트권한획득\nsudo -i\n아이디와 비밀번호를 입력하고 루트권한을 얻는다.\n- 아래를 입력한다.\nsudo cp /home/cgb2/blacklist-nouveau.conf /etc/modprobe.d\nsudo update-initramfs -u\nsudo reboot \n- 그래픽카드 다운로드: 드라이버 설치파일을 다운받는다. 앤비디아공식홈페이지에서 다운받자. OS를 리눅스 64-bit으로 선택하고 검색을 누르면 다운받아진다.\n- 그래픽키다 설치: 다운받은뒤에는 파일이 있는 폴더로 이동하여\nchmod +x NVIDIA-Linux-x86_64-410.78.run\n를 실행하자. 보통 NVI까지치고 적당히 탭을 누르면 알아서 뒷부분이 완성된다. 이 과정은 추후에 드라이버를 실행할수 있도록 권한을 풀어두는 것이다. 그리고 아래를 실행한다.\nsudo ./NVIDIA-Linux-x86_64-410.78.run\n그 다음 드라이버가 잘 설치되었는지 확인한다.\nnvidia-smi\n\n\n아나콘다\n- (아나콘다 설치) 아나콘다를 다운받은 폴더로 가서 아래와 같이 실행한다.\nbash Anaconda3-2019.03-Linux-x86_64.sh\n대충 bash Ana 정도까지만 치고 tab을 누르면 알아서 완성된다.\n- (환경만들기) 커맨드를 키고 아래를 실행한다.\n(base) conda create -n py38r40 python=3.8\n(base) conda create --name py38r40 python=3.8\n둘 중 아무거나 실행해도 된다. 파이썬 환경이 너무 높으면 나중에 conda tensorflow-gpu가 먹히지 않으니 환경을 만들때 파이썬버전을 3.8.x로 하자. (현시점 2021년 2월25일기준 3.9.x이면 conda tensorflow-gpu 가 동작하지 않음.)\n\n\nssh연결\n- 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다.\nsudo apt install openssh-server\n22번포트 우회하기\n- step1: /etc/ssh/sshd_config 파일을 연다.\nsudo vi /etc/ssh/sshd_config \n- step2: Port 22 라고 된 부분의 주석을 풀고 원하는 포트번호 설정\n...\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n...\n- step3: 수정내용을 적용\nsudo systemctl restart ssh.service\n- step4: 수정한 포트로 ssh접속\n\n\n주피터랩 원격제어\n- 1단계: 주피터랩설치\n(py38) conda install -c conda-forge jupyterlab\n\nNote: 사실 위에서 주피터랩을 따로 설치안해도 주피터랩이 잘만 실행된다. 하지만 이렇게하니까 나중에 R커널을 만들기위해 IRkernel::installspec()을 실행할때 에러가 난다.\n\n- 2단계: 패스워드 설정\n(py38) jupyter lab --generate-config\n(py38) jupyter lab password\n- 3단계: jupyter lab 환경설정\nnano /home/cgb/.jupyter/jupyter_lab_config.py \n아래를 변경\nc.ServerApp.ip = '192.168.0.4'\nc.ServerApp.port = 1306\nc.ServerApp.open_browser = False\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\n\n주피터노트북 원격제어\n- 1단계: 주피터노트북 설치 (보통 lab을 설치하면 이미 설치되어있음)\n(py38) conda install -c conda-forge notebook \n- 2단계: 패스워드 설정\nfrom notebook.auth import passwd\npasswd()\nEnter password: \nVerify password: \n생성된값 (argon 어쩌고..)을 복사\n- 3단계: 환경설정\njupyter notebook --generate-config\nnano /home/cgb/.jupyter/jupyter_notebook_config.py\n아이피주소와 패스워드를 바꾼다. (port는 선택, browser도 선택 )\nc.NotebookApp.open_browser = False\nc.NotebookApp.ip = '192.168.0.4'\nc.NotebookApp.port = 1307\nc.NotebookApp.password = ''\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\nTip: 주피터노트북과 랩을 양쪽으로 셋팅후 주피터 노트북으로 실행하면 2개를 모두 쓸 수 있음\n\n\n\nR설치ver1: (base)에 설치\n- 설치전: 기존의 R 삭제\nconda remove r-base -y \nsudo apt-get remove r-base-core \nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\n- R설치전 준비작업: 나노에디터를 키고 /etc/apt/sources.list를 연다.\nsudo nano /etc/apt/sources.list\n화살표로 이동하여 맨아래로 간뒤에 아래중 하나를 추가한다. (나는 focal-cran40으로 추가함)\ndeb https://cloud.r-project.org/bin/linux/ubuntu impish-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu hirsute-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran40/\n저장후 나노에디터 종료. 그리고 아래를 실행.\nsudo apt-get update\n경우에 따라서 아래와 같은 에러메시지가 뜰 수 있다.\n...\nW: GPG error: https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9 \n...\n공개키가 없어서 생기는 에러이므로 아래와 같이 가져온다.\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n#sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 51716619E084DAB9\n그리고 다시 아래를 실행\nsudo apt-get update\n에러가 없이 뭔가 마무리 되어야한다.\n(base) cgb3@cgb3:~$ sudo apt-get update\nIgn:1 http://linux.dropbox.com/ubuntu disco InRelease\nHit:2 http://security.ubuntu.com/ubuntu focal-security InRelease  \nHit:3 http://kr.archive.ubuntu.com/ubuntu focal InRelease                                 \nHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease                \nHit:5 http://linux.dropbox.com/ubuntu disco Release                 \nGet:6 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\nHit:8 http://kr.archive.ubuntu.com/ubuntu focal-updates InRelease\nGet:9 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [46.4 kB]\nHit:10 http://kr.archive.ubuntu.com/ubuntu focal-backports InRelease\nFetched 50.0 kB in 1s (36.5 kB/s)                   \nReading package lists... Done\n- R설치\nwget http://security.ubuntu.com/ubuntu/pool/main/i/icu/libicu66_66.1-2ubuntu2_amd64.deb\nsudo dpkg -i libicu66_66.1-2ubuntu2_amd64.deb\nsudo apt-get install r-base\n- tidyverse 설치 (R studio 설치전에 tidyverse 설치해야함)\n- Rstudio 설치: https://www.rstudio.com/products/rstudio/download-server/debian-ubuntu/\n\n우분투22로 설정할것!!\n\nsudo apt remove rstudio-server\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.12.0-353-amd64.deb\nsudo gdebi rstudio-server-2022.12.0-353-amd64.deb\n- Rstudio를 설치하면 ~/R/x86_64-conda-linux-gnu-library/4.1이 새로 생성된다.\n\nRstudio에서 설치한 패키지는 이 폴더에 저장된다.\n\n- 주피터와 R커널 연결\nR # sudo R \ninstall.packages(\"IRkernel\")\nIRkernel::installspec()\n\n\nR설치ver2: (py38r40)에 설치\n- R설치\n(py38r40) conda install -c conda-forge r-essentials=4.0\n이러면 콘다환경에는 R이 깔리고 base에는 R이 깔리지 않는다.\n- 커널연결\n콘다환경에서 R을 실행한다. Rstudio가 아니라 커맨드에서 R을 실행해야한다. 그리고 아래를 실행하면 주피터랩과 R환경이 연결된다.\nIRkernel::installspec()\n이제 주피터랩에서 R kernel을 사용할 수 있다.\n\n\n가상환경에서 Rstudio server 설치 (어려움)\n- 이제 Rstudio server를 설치하는 방법을 다룬다.\n- 먼저 Rstudio를 설치한다. 참고로 Rstudio server 설치하는법은 여기를 참고하라. 요약하면 터미널에서 아래3줄을 입력하기만 하면된다.\n(py38r40) sudo apt-get install gdebi-core\n(py38r40) wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb\n(py38r40) sudo gdebi rstudio-server-1.2.5033-amd64.deb\n\nWarning: Rstudio 1.3x 이상을 설치하지말고 1.2x를 설치해야 한다. 이상하게 1.3x이상은 후에 서술할 Gregor Strurm가 그의 깃허브에서 제안하는 방식이 잘 동작하지 않았다. 이는 알려진 문제였고 이를 해결하는 해결책을 서술한 스레드가 있어보이긴 했지만 나는 그냥 Rstudio 1.2x를 설치하고 쓰는 것을 선택했다.\n\n\nNote: 이미 rstudio server 가 다른버전으로 깔려있다면 sudo apt remove rstudio-server 를 통하여 삭제하고 설치하자.\n\n- 이제 Rstudio 설치가 끝났다. 설치된 Rstudio를 아나콘다 가상환경에 설치된 R과 연결해보자. 우선 아래를 실행한다.\n(py38r40) sudo apt install uuid\n(py38r40) sudo apt install git\n(py38r40) git clone https://github.com/grst/rstudio-server-conda.git\n위에 두줄은 Gregor Sturm이 만든 어떤 프로그램을 쓰기 위한 사전준비작업이다. 마지막줄을 실행하면 Gregor Sturm이 만든 프로그램이 다운받아진다. 이게 프로그램 설치가 완료된것이다. 이제 컴퓨터 껐다 킬때마다 아래를 실행한다.\n(py38r40) ./rstudio-server-conda/local/start_rstudio_server.sh 8787 # use any free port number here. \n이제 192.168.0.4:8787 따위의 주소로 접속하면 Rstudio를 쓸 수 있다. 참고로 system-wide Rstudio server를 죽여야 할 때가 있다. 그럴땐 아래 명령을 치면 된다.\n(py38r40) sudo systemctl disable rstudio-server.service\n(py38r40) sudo systemctl stop rstudio-server.service\n\n\n자주 설치하는 패키지 리스트\n- 아래를 미리 깔아두자..\n# conda install -c conda-forge jupyterlab \nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\npip install fastai\npip install plotly \npip install ipywidgets\npip install jupyter-dash\npip install dash \npip install plotnine\npip install seaborn\npip install opencv-python\npip install folium\npip install pandas_datareader\nconda install -c conda-forge r-essentials=4 \npip install rpy2\nconda install -c conda-forge python-graphviz\n- tensorflow-gpu 는 현재(2022-03-06) python=3.10 에서 동작함\nconda create -n py310 python=3.10 \nconda activate py310 \nconda install -c conda-forge tensorflow-gpu \n- 아래를 설치하면 좋음\nsudo apt install mc \n\n\n터미널 예쁘게 만들기\n- zsh 설치 + oh my zsh 설치\nsudo install zsh \nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n- 테마변경\n\n.zshrc 파일 열기\n\nnano ~/.zshrc \n\n아래의 내용 수정\n\n...\nZSH_THEME=\"agnoster\"\n...\n- 색상변경\n\n아래의 파일 열기\n\ncd ~/.oh-my-zsh/themes/\nnano agnoster.zsh-theme  \n\n내용수정\n\n...\nprompt_dir() {\n  prompt_segment 39d $CURRENT_FG '%~'\n}\n...\n\n\nsublime text and TeX (개발용 서버일 경우 생략 가능)\n- ‘Ubuntu Software’에 가서 ’sublime Text’를 치면 다운받을 수 있다. 다운받은뒤에 ’file’ -> ’open folder’를 활용하여 깃허브의 로칼저장소를 열어두면 편리하다.\n- 아래를 실행하여 TeX을 깐다.\nsudo apt install texlive-full\n- 이제 sublime과 latex을 연결하여보자. 여기를 참고하자. (1) sublime을 키고 ‘컨트롤+쉬프트+p’를 눌러 ’Install Package Control’ 선택 (2) 다시 ‘컨트롤+쉬프트+p’ 를 눌러 ‘Package Control: Install Package’를 실행 (3) 그러면 바로 검색창이 나오는데 거기서 ’LaTeXTools’를 입력해서 실행 (4) 다시 ’컨트롤+쉬프트+p’를 누르고 ’LaTeXTools: Check system’ 선택. 모두 ’available’이 나오면 잘 설치된 것이다.\n- *.tex파일을 열고 ’컨트롤+b’를 누르자. 처음이면 어떤 메뉴들이 보일텐데 그냥 ’Latex’을 선택하자. 그러면 코딩결과가 pdf로 나온다.\n- (수식미리보기) ‘Perferences’ > ‘Packages Setting’ > ‘LaTeXTools’ > ‘Settings-User’를 선택한다. ’93번째라인’에 ’preview_math_mode’를 “all”로 바꾼다. 그러면 수식들이 미리 출력된다. 그외에도 자유롭게 셋팅을 조정할 수 있다. 원래셋팅은 ’Perferences’ > ‘Packages Setting’ > ‘LaTeXTools’ > ‘Settings-Defaults’ 에 있다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-06-주피터랩- 설정 및 몇가지 팁.html",
    "href": "posts/4_Notes/2000-01-06-주피터랩- 설정 및 몇가지 팁.html",
    "title": "[Note] 주피터랩: 설정 및 몇가지 팁",
    "section": "",
    "text": "주피터에 R커널을 연결할 경우 그림크기 조정\noptions(repr.plot.width=10, repr.plot.height=3,repr.plot.res=300)\n\n\n깃허브에서 *.py파일 불러오기\nimport requests\nexec(requests.get('http://miruetoto.github.io/my_code/datahandling.py').text)\n\n\nrpy2 magic\nimport rpy2\n%load_ext rpy2.ipython\n\n\n깃허브에서 *.R파일 불러오기\nimport rpy2\n%load_ext rpy2.ipython\n%R library(devtools)\n%R source_url(\"http://miruetoto.github.io/my_code/datahandling.r\")\n\n\nmatplotlib 그림크기조정\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \nIpython_default=plt.rcParams.copy() # save initial value \nfrom matplotlib import cycler\nplt.rc('figure',dpi=150) # default value 4 figure.dpi is 72.0 \n# plt.rcParams.update(Ipython_default) # load initial value \n\n\nGPU 사용여부 체크\nfrom keras import backend as K\nprint('GPU check 4 Keras: '+ str(K.tensorflow_backend._get_available_gpus()))\nimport torch\nprint('GPU check 4 Pytorch: '+ str(torch.cuda.get_device_name(0)))\n\n\n깃랩관련 (회사아니면 필요없음)\n- load *.py from gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\n\n# (1) load RF.py, RF_withGIT.py, RF_withR.py\nRF_py = project.files.get(file_path='modeling/RF.py', ref='fridge').decode()\nRF_GIT_py = project.files.get(file_path='utils/RF_withGIT.py', ref='fridge').decode()\nRF_R_py = project.files.get(file_path='utils/RF_withR.py', ref='fridge').decode()\nexec(str(RF_py, 'utf-8'))\nexec(str(RF_GIT_py, 'utf-8'))\nexec(str(RF_R_py, 'utf-8'))\n- load *.R in gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\nRF_R_rcode = project.files.get(file_path='utils/RF_Rfunctions.r', ref='fridge').decode()\n# tricks for source('Rfunctions.r')\nfile1 = open(\"RF_Rfunctions.r\",\"w\") \nfile1.write(str(RF_R_rcode, 'utf-8'))\nfile1.close() \nro.r(\"source('RF_Rfunctions.r')\")\nimport os\nos.remove('RF_Rfunctions.r')\n\n\n& 옵션으로 주피터 실행\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n아래와 같이 끝에 &을 붙이면 된다.\nconda activate py20190129\njupyter lab &\n실행하고 난뒤에는 엔터를 쳐서 빠져나온다. 이렇게 하면 서버자체에 모니터를 연결하고 커널창을 띄운것과 같은 효과를 준다. 즉 서버에 접속한 컴퓨터를 끄는것과 상관없이 서버에서는 항상 주피터가 열려 있게 된다.\n\n\n& 옵션으로 실행한 주피터프로세스 죽이기\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n실행된 프로세스를 찾기위해 아래를 실행한다.\nps aux | grep jupyter-lab\n결과는 아래와 같이 나온다.\nlgcgb    26888  0.2  0.1 326760 86724 ?        Sl   10:14   0:12 /home/lgcgb/anaconda3/envs/py20190129/bin/python3.7 /home/lgcgb/anaconda3/envs/py20190129/bin/jupyter-lab\nlgcgb    27146  0.0  0.0  15720  1008 pts/3    S+   11:56   0:00 grep --color=auto jupyter-lab\n26888에 해당하는 것이 주피터를 띄운 커널이다. 이 번호를 기억했다가 프로세스를 아래와 같은 명령으로 죽인다.\nkill 26888\n\n\n패스워드 없이 주피터 실행\n- 아래와 같이 하면 외부에서 접속할때 패스워드를 입력하지 않음.\njupyter lab --LabApp.token='' --LabApp.password=''\njupyter notebook --NotebookApp.token='' --NotebookApp.password=''"
  },
  {
    "objectID": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "href": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "title": "[Note] 줄리아 설치 및 실행",
    "section": "",
    "text": "설치\n- 여기에 접속한다. 스크롤링하여 ’Generic Linux Binaries for x86 / 64-bit(GPG)’를 찾는다. 그리고 ’64-bit’를 클릭해서 다운받는다. (참고로 왼쪽에 ’help’를 누르면 설치페이지 설명서가 나온다.) 그러면 아래와 같은 파일이 나온다.\njulia-1.3.1-linux-x86_64.tar.gz\n이 파일을 더블클릭해서 압축을 풀어준다. 압축을 풀면 julia-1.3.1라는 폴더가 생긴다. 이 폴더를 원하는 위치로 (줄리아가 설치되기를 원하는 위치) 이동시킨다. 나는 home에 이동시켰다.\n- 아래를 실행하면 줄리아가 실행된다. (둘중 아무거나)\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n\n\n주피터와 연결\n- 아래중 하나를 실행하여 줄리아를 킨다.\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n- 줄리아를 실행한뒤에 아래를 입력하면 주피터노트북에 연결된다.\nusing Pkg\nPkg.add(\"IJulia\")\n- 한 가지 의문점이 있다. 나같은 경우는 ’(base)’에서 줄리아를 실행하고 연결하였다. 그런데 혹시 몰라서 (py38r40)에서도 줄리아를 실행해봤는데 잘 실행되었다. 줄리아를 실행시키고 위의 명령 Pkg.add(\"IJulia\")를 다시쳤는데, 이미 연결되어서 더이상 변화시킨게 없다는 메시지가 떴다. 이러면 (base)에 설치된 줄리아가 (py38r40)에서도 실행된 줄리아와 동일하다는 의미일까? \\(\\Longrightarrow\\) 그렇다. 왜냐하면 줄리아는 anaconda내의 폴더에 설치한 것이 아니기 때문에. home에 보통 설치하니깐.\n\n\n환경변수 조정\n- 참고로 어디서든 줄리아를 실행시키고 싶다면 환경변수를 조작하면 된다. 아래를 실행해서 나노에디터를 킨다.\nsudo nano /etc/environment\n맨끝에 다음과 같이 되어있을 것이다.\n~~ usr/local/games\"\n마지막에 /home/cgb/julia-1.3.1/bin/julia를 추가한다. 즉 아래와 같이 만든다.\n~~ usr/local/games:/home/cgb/julia-1.3.1/bin/julia\"\n세이브하고 나온다. (그런데 이 과정을 안거쳐도 되는것 같음.) 이제 커맨드에서 아래를 실행한다.\nexport PATH=$PATH:/home/cgb/julia-1.3.1/bin\n이렇게하면 이제 단순히 julia라고만 쳐도 julia가 실행된다.\n\n\n플루토에서 강의영상 넣는 방법\n- 아래를 삽입\nhtml\"\"\"\n<div style=\"display: flex; justify-content: center;\">\n<div  notthestyle=\"position: relative; right: 0; top: 0; z-index: 300;\">\n<iframe src=\n\"\nhttps://www.youtube.com/embed/\n\"\nwidth=600 height=375  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\"\"\"\n\n\n플루토를 이용한 홈페이지 만드는 방법\n- 단계1: https://github.com/JuliaPluto/static-export-template 에 가서 Clone\n- 단계2: Setting -> GitHub Pages -> Source -> gh-pages / root\n\n\n플루토 키는 방법\nimport Pluto\nPluto.run(host=\"0.0.0.0\",port=1234,launch_browser=false,require_secret_for_open_links=false,require_secret_for_access=false,threads=\"8\")"
  },
  {
    "objectID": "posts/4_Notes/2000-01-08-깃(Git).html",
    "href": "posts/4_Notes/2000-01-08-깃(Git).html",
    "title": "[Note] 깃(Git)",
    "section": "",
    "text": "clone\ngithub repository \\(\\to\\) code \\(\\to\\) clone tab, ssh를 복사 (git@github.com:miruetoto/yechan.git처럼 생김)\n터미널에서 아래를 입력\ngit clone git@github.com:miruetoto/yechan.git 01_yechan\n\n\npull\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력한다.\ngit pull\n\n\nbranch\n- 서버에 이미 guebin이라는 브랜치가 있다면 아래와 같이 동기화 시킨다.\ngit chechout guebin\ngit push -u origin guebin\n여기에서 git push -u origin guebin을 안해도 동기화가 잘될때도 있는데 아닐때도 있다.\n\n\nremote\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력하면 깃허브의 url 주소를 확인할 수 있다.\n(base) lgcgb2@lgcgb2:~/Documents/GitHub/miruetoto.github.io$ git remote -v\norigin https://github.com/miruetoto/miruetoto.github.io.git (fetch)\norigin https://github.com/miruetoto/miruetoto.github.io.git (push)\nupstream https://github.com/daattali/beautiful-jekyll.git (fetch)\nupstream https://github.com/daattali/beautiful-jekyll.git (push)\n\n\nconfig\n- 설정보기\ngit config —list \n- 설정삭제\ngit config --unset user.name\ngit config --unser user.email\n- 전역설정삭제\ngit config --unset --global user.name\ngit config --unset --global user.email\n- 중복값 설정삭제\ngit config --unset-all user.name\ngit config --unset-all user.email\n- 중복값 전역으로 설정삭제\ngit config --unset-all --global user.name\ngit config --unset-all --global user.email\n- 비번안치고 푸쉬하는법?\ngit config credential.helper store\n입력이후에 git push\n\n\nGit token\nhttps://github.com/settings/tokens 에서 확인가능\n\nAppendix\n\n리눅스에서 github desktop 설치\n\n여기로 간다.\n한 챕터의 (2.3.1 Linux RC1 와 같이 되어있음) 아래쪽에 보면 ▶ Assets 라고 되어있는데 이걸 클릭하면 다운받을 수 있는 파일들이 나온다. 확장자가 .deb로 끝나는걸 골라서 다운받은뒤에 실행한다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "title": "[Note] 우분투 익히기",
    "section": "",
    "text": "날짜, 달력\ndate \ncal \n\n\n파일시스템\n- 작업경로, 디렉토리 이동, 파일확인\npwd \ncd cgb2 \ncd ~cgb2 ## 어디서든 실행가능\ncd ## cgb2로 로그인 되어있을 경우 cd ~cgb2와 동일 \ncd ~cgb2/Dropbox\nls \nls -a \nls -l # 자세히\nls -lt # 자세히+파일수정시간에 따른 정렬\nls -lt --reverse # 자세히+파일수정시간의 역순으로 정렬 \n- 엄밀하게는 cd cgb2 는 cd ./cgb2로 써야한다. 하지만 ./는 생략가능하므로 그냥 cd cgb2라고 쓰는것\n- 뭐하는 파일인지 알고싶다면?\nfile picture.jpg\n- 카피, 이동, 새폴더, 삭제, 바로가기\ncp file1 file2 # file1을 복사하여 file2를 새로 만듬. file2가 이미 있다면 file1의 내용을 덮어씀 \ncp file1 file2 -i # 위와 동일한데 덮어쓰기 여부에 대한 확인메시지 생성\ncp file1 file2 --interactive # 위와 동일\ncp -r dir1 dir2 # dir1의 모든파일을 복사하여 dir2로 이동한뒤 붙어넣음. dir2가 없다면 새로 만듬. 기존의 dir2에 있던 파일이 삭제되는건 아님 \ncp --recursive dir1 dir2 # 위와 동일 \nmv # 카피사용방법과 동일 \nmkdir temp\nmkdir temp1, temp2, temp3 \nrm file1 # file1삭제 \nrm -r file1 dir1 # file1삭제 dir1폴더삭제 \nrm -rf file1 dir1 # 위와 동일한데 file1이나 dir1이 존재하지 않더라고 rm이 실행\nln ## 바로 가기 만드는건데 쓸줄모름 배우기 싫어\n- -v(verbose)를 쓰면 친절한 느낌이 든다.\n(base) cgb2@cgb2-desktop:~/Dropbox$ cp *.txt temppp -v\n'colab.txt' -> 'temppp/colab.txt'\n\n\necho\n- echo 기본기능\ncgb2@cgb2-desktop:~/Dropbox/temppp$ echo test\ntest\n- echo + * : *가 현재 디렉토리에 있는 모든 파일이름으로 확장된이후에 echo가 동작함.\n(base) cgb2@cgb2-desktop:~/Dropbox/temppp$ ls\ncolab.txt  sample.txt\n(base) cgb2@cgb2-desktop:~/Dropbox/temppp$ echo *\ncolab.txt sample.txt\n- 응용\n(base) cgb2@cgb2-desktop:~$ echo D*\nDesktop Documents Downloads Dropbox\n- 응용2\n(base) cgb2@cgb2-desktop:~$ echo /usr/*\n/usr/bin /usr/games /usr/include /usr/lib /usr/lib32 /usr/lib64 /usr/libexec /usr/libx32 /usr/local /usr/sbin /usr/share /usr/src\n- 응용3\n(base) cgb2@cgb2-desktop:~$ echo ~\n/home/cgb2\n- 응용4\n(base) cgb2@cgb2-desktop:~/Dropbox$ echo $((2+2))\n4\n\n\n기본디렉토리 설명\n/bin 시스템 부팅과 실행에 필요한 바이너리(프로그램)들을 포함\n/etc 시스템 전반의 환경설정. 이 디렉터리의 모든 파일은 텍스트형식임.\n\n/etc/password 사용자 계정정보\n\n/lib 시스템 프로그램에서 사용하는 공유 라이브러리가 저장. 윈도우즈의 DLL과 비슷한 것.\n/usr 사용자가 사용하는 모든 프로그램과 지원파일들 (Program files + 프로그램들의 설정값)\n\n/usr/bin 리눅스 배포판이 설치한 실행 프로그램들이 있다. (여기에 R이 깔린다!!)\n\nhp-align, hp-check, hp-config_usb-printer …\nX11\nvi\ngcc\nsu, sudo\nsar\nssh, ssh-agent, ssh-keygen, ….\nnvidia-smi\n\n/usr/lib 여기에는 /usr/bin에 있는 프로그램들을 위한 공유라이브러리가 저장된다. (여기에 R folder가 있다)\n\n여기에 R폴더가 있다. 그런데 R패키지가 여기에 깔리진 않음\n\n/usr/local/bin 소스코드로 컴파일된 파일, 보통 비어있음\n/usr/local/lib/R/site-library R패키지가 설치되어있음, 예를들면 tidyverse\n\n\n\nvim\n- 파일여는법/만드는법 (sudo는 읽기전용 파일을 만들수있음)\nsudo vim /etc/apt/sources.list\n- 에디터에서 사용법\ni \nESC \n:w \n:q\n:wq\n/keyword  \ndd # 현재행삭제 \n\n\nwget\nwget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb\n\n\ngdebi\nsudo gdebi rstudio-server-1.2.5033-amd64.deb\n\n\napt\nsudo apt-get remove r-base-core\nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\nsudo apt list \nsudo apt update\nsudo apt install openssh-server \nsudo apt-get install gdebi-core\n- sudo apt-get과 sudo apt 차이? 별 차이 없는듯 - https://askubuntu.com/questions/445384/what-is-the-difference-between-apt-and-apt-get\n\nThey are very similar command line tools available in Trusty (14.04) and later. apt-get and apt-cache’s most commonly used commands are available in apt. apt-get may be considered as lower-level and “back-end”, and support other APT-based tools. apt is designed for end-users (human) and its output may be changed between versions.\n\n\n\nconda\nconda \nconda env -h \nconda install -h \nconda remove -h  \nconda update -h \nconda env list\nconda create -n py38r40 python=3.8\nconda env remove -n py38r40 \nconda install -c conda-forge jupyterlab \nconda remove jupyterlab \nconda remove r-base -y \nconda remove -n py38r40 jupyterlab \nconda update scipy\nconda update -n py38r40 scipy\nconda list \n\n\npip\npip\npip list\npip list > list.txt\npip freeze # 좀 더 자세히 나온다 \npip freeze > list.txt \npip show matplotlib # 설치된패키지 정보가 나옴. 좋음.\npip install rpy2\npip install -r list.txt \npip install dash==1.13.3\npip install jupyterlab \"ipywidgets>=7.5\"\npip install -U numpy\npip install --upgrade pip\npip install --upgrade tensorflow\npip uninstall matplotlib\n\n\n리소스 모니터링\ndf # disk \nfree # memory \nnvidia-smi \nwatch -n 5 nvidia-smi -a --display=utilization\ntop\nsar -1 r \n\n# 나노에디터\n\n`-` 파일열기\n\n```default\nsudo nano /etc/apt/sources.list\n- 읽기전용 파일만들기: 파일을 만드는것도 파일 여는방법과 동일함. 즉 아래와 같이 하면 exam.txt가 현재 폴더에 있다면 열고아니면 (읽기전용으로) 만든다.\nsudo nano /home/cgb/Desktop/exam.txt\n- 읽기전용이 아닌 일반파일 만들기\nnano /home/cgb/Desktop/exam.txt\n- 나노에디터종료: 컨트롤+X\n- 파일저장: 컨트롤+O\n- 찾기: 컨트롤+W\n- 되돌리기: 알트+U\n\n\nssh\n- 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다.\nsudo apt install openssh-server\n\n22번포트 우회하기\n- step1: /etc/ssh/sshd_config 파일을 연다.\nsudo vi /etc/ssh/sshd_config \n- step2: Port 22 라고 된 부분의 주석을 풀고 원하는 포트번호 설정\n...\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n...\n- step3: 수정내용을 적용\nsudo systemctl restart ssh.service\n- step4: 수정한 포트로 ssh접속\n\n\n\nsublime\n- 찾아바꾸기: 컨트롤+h\n- 화면분할: 옵션+커맨드+2,3,4 …\n\n\n파일생성, 파일삭제\ncat > sample.txt \nrm -r folderName \nrm -rf .local/share/Trash/files/* # 휴지통삭제 \n\n\ndeb\ndpkg -i quarto-1.2.335-linux-amd64.deb # 설치\ndpkg -r quarto # 삭제"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "href": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "",
    "text": "import itstgcn\nimport torch\nimport itstgcn.planner"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "href": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 2, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.0, 0.2],\n    'lags': [2, 4], \n    'nof_filters': [4, 8], \n    'inter_method': ['nearest'],\n    'epoch': [3]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/2 is done\n2/2 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-03.csv"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "href": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex= [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_stgcn_block = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='five_nodes')\n\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n1/3 is done\n2/3 is done\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-55.csv"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "href": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n#    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-56.csv"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "href": "posts/3_Researches/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'inter_method': ['nearest','linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader,dataset_name='five_nodes')\nplnr.simulate(mindex,mtype='block')\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-57.csv"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "href": "posts/3_Researches/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "title": "[IT-STGCN] ITSTGCN-Tutorial",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\n\n\n\n예제1: vanilla STGCN\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n\n\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset,dataset_name='five_nodes')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) \nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제2: padding missing values\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 임의로 결측치 발생\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex=mindex,mtype='rand')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n- 적절한 method로 결측치를 채움 (default 는 linear)\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n다른 method로 결측치를 채울수도 있음. 사용할 수 있는 방법들은 아래에 정리되어 있음\n\nref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='nearest')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='quadratic')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='cubic')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.utils.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n- 블락으로 결측치 발생\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n예제3: vanilla STGCN with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.learners.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제4: vanilla STGCN with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제5: threshold example (random)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.utils.plot(f_miss,'o')\nitstgcn.utils.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제6: threshold example (block)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.plot(f_miss,'o')\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제7: iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제8: iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제9: GNAR (random missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제10: GNAR (block missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n<torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7faad2716a60>\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:16<00:00,  1.52s/it]\n\n\n\ndataset.features[0].shape\n\n(20, 4)"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/3_Researches/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "href": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "",
    "text": "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "href": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "data",
    "text": "data\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "href": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "href": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "Learn",
    "text": "Learn\n\n# from tqdm import tqdm\n\n# model = RecurrentGCN(node_features=14, filters=32)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# model.train()\n\n# for epoch in tqdm(range(50)):\n#     for time, snapshot in enumerate(train_dataset):\n#         y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n#         cost = torch.mean((y_hat-snapshot.y)**2)\n#         cost.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n\n\nmodel = RecurrentGCN(node_features=14, filters=32)\nmodel.train()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nfor s in train_dataset:\n    print((s.y-model(s.x,s.edge_index,s.edge_attr)).shape)\n\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "href": "posts/3_Researches/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "예제의 차원 조사",
    "text": "예제의 차원 조사\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\n1068: number of nodes // 1068개의 노드가 있음\n14: number of features // 하나의 노드에 맵핑된 차원의수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n1068: number of nodes\n\n\n_x.shape\n\ntorch.Size([1068, 14])"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "href": "posts/3_Researches/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "title": "[IT-STGCN] Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nT = 100\nt = np.arange(T)/T * 10 \n\nx = 0.3*np.sin(2*t)+0.1*np.sin(4*t)+0.1*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=4)\n\n\nmindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\ndataset_miss05031.edge_index\n\narray([[0],\n       [1]])\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n\ndataset_padded_cubic05031.edge_index\n\narray([[0],\n       [1]])\n\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nlags4/0.3/filter16\nlags4/0.38/filter8\nlags8/0.38/filter8\n\n\nlrnr05031.learn(filters=16,epoch=50)\n\n50/50\n\n\n- 모형 평가 및 시각화\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nevtor05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2,figsize=(30,20))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'.',color='C0')\nax2.plot(df1['y'][:],'.',color='C0')\nax3.plot(df1['xer'][:],'.',color='C1')\nax4.plot(df1['yer'][:],'.',color='C1')\nax5.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax6.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax7.plot(df1['x'][:],'.',color='C0')\nax7.plot(evtor05031.fhat_tr[:,0],color='C3')\nax8.plot(df1['y'][:],'.',color='C0')\nax8.plot(evtor05031.fhat_tr[:,1],color='C3')\nax9.plot(df1['x'][:],'.',color='C0')\nax9.plot(df1['xer'][:],'.',color='C1')\nax9.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax9.plot(evtor05031.fhat_tr[:,0],color='C3')\nax10.plot(df1['y'][:],'.',color='C0')\nax10.plot(df1['yer'][:],'.',color='C1')\nax10.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax10.plot(evtor05031.fhat_tr[:,1],color='C3')\n\nfor ax in fig.get_axes():\n    ax.label_outer()"
  },
  {
    "objectID": "posts/3_Researches/ITSTGCN/2023-02-14-Tables.html",
    "href": "posts/3_Researches/ITSTGCN/2023-02-14-Tables.html",
    "title": "[IT-STGCN] Tables",
    "section": "",
    "text": "import pandas as pd\n\n- Dataset 5nodes\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test',]\n\n\ndf = pd.DataFrame(columns=col)\ndf\n\n\n\n\n\n  \n    \n      \n      Dataset\n      iteration\n      method\n      missingrate\n      missingtype\n      lag\n      number_of_filters\n      interpolation\n      MSE_train\n      MSE_test\n    \n  \n  \n  \n\n\n\n\n- 실험마다 아래와 같은식으로 추가\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.0, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = None,  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = None, # None, 'mean', 'linear'\ndf_row['MSE_train'] = 0.96 \ndf_row['MSE_test'] = 0.55\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n  \n    \n      \n      Dataset\n      iteration\n      method\n      missingrate\n      missingtype\n      lag\n      number_of_filters\n      interpolation\n      MSE_train\n      MSE_test\n    \n  \n  \n    \n      0\n      fivenodes\n      1\n      stgcn\n      0.0\n      None\n      1\n      16\n      None\n      0.96\n      0.55\n    \n  \n\n\n\n\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.2, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = 'randomly',  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = 'mean', # None, 'mean', 'linear'\ndf_row['MSE_train'] = 1.23\ndf_row['MSE_test'] = 0.88\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n  \n    \n      \n      Dataset\n      iteration\n      method\n      missingrate\n      missingtype\n      lag\n      number_of_filters\n      interpolation\n      MSE_train\n      MSE_test\n    \n  \n  \n    \n      0\n      fivenodes\n      1\n      stgcn\n      0.0\n      None\n      1\n      16\n      None\n      0.96\n      0.55\n    \n    \n      0\n      fivenodes\n      1\n      stgcn\n      0.2\n      randomly\n      1\n      16\n      mean\n      1.23\n      0.88"
  },
  {
    "objectID": "posts/3_Researches/HST/2023-01-23-CommunityDetection.html",
    "href": "posts/3_Researches/HST/2023-01-23-CommunityDetection.html",
    "title": "[HST] CommunityDetection",
    "section": "",
    "text": "ref\n- Roddenberry et al. (2020)\n\nhttps://arxiv.org/pdf/2001.10944.pdf\nintro에 그래프는 single correct한 structure를 알 수 없다는 이야기가 있는데 이를 이용해야함.\n\n\n\n\n\n\n\n\n\n방법론\n\nRoddenberry, T Mitchell, Michael T Schaub, Hoi-To Wai, and Santiago Segarra. 2020. “Exact Blind Community Detection from Signals on Multiple Graphs.” IEEE Transactions on Signal Processing 68: 5016–30."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "title": "[PINKOCTO] 퓨리에변환4jy",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n회귀모형 (1)\n\nx = np.linspace(-10,10,1000)\nx0 = x*0+1\nx1 = x \nbeta0 = 3 \nbeta1 = 2\ny = x0*beta0+x1*beta1+np.random.randn(1000)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n회귀모형 (2)\n- 관측한자료\n\nN=1000\nx=np.linspace(0,1,N)\neps = np.random.randn(N)\nX0 = np.sin(x*0*np.pi)\nX1 = np.sin(x*2*np.pi)\nX2 = np.sin(x*4*np.pi)\nX3 = np.sin(x*6*np.pi)\n\ny=2*X1+1*X2+3*X3+eps\n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n\nobserved signal\n\n- 위의 자료를 해석하는 방법\n\ndef spec(y):\n    N= len(y)\n    return abs(np.fft.fft(y)/N)*2 \n\n\ny=2*X1+1*X2+3*X3+eps\nyfft =spec(y) \ny1=2*X1\ny2=1*X2\ny3=3*X3\nyfft1=spec(y1)\nyfft2=spec(y2)\nyfft3=spec(y3)\nepsfft=spec(eps)\n\n\nplt.plot(yfft[:20],'o',alpha=0.5)\nplt.plot(yfft1[:20],'x',alpha=1,)\nplt.plot(yfft2[:20],'x',alpha=1)\nplt.plot(yfft3[:20],'x',alpha=1)\nplt.plot(epsfft[:20],'x',alpha=1)\n\n\n\n\n- 퓨리에변환 -> threshold -> 역퓨리에변환을 이용한 스킬\n\nyfft=np.fft.fft(y)\n\n\nplt.plot(abs(yfft[1:50]),'o')\n\n\n\n\n\nyfft[abs(yfft)<100] = 0\n\n\nplt.plot(y,'o',alpha=0.1)\nyhat=np.fft.ifft(yfft)\nplt.plot(yhat,'--')\nplt.plot(y-eps,'-')\n\n\n\n\n\nplt.plot(spec2(y)[:50],'o')\nplt.plot(spec2(yhat)[:50],'x')\n\n\n\n\n\n\n삼성전자 주가자료를 스무딩해보기\n- 삼성전자 자료\n\nimport yfinance as yf\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ny = yf.download(\"005930.KS\", start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nplt.plot(y)\n\n\n\n\n- 스펙트럼\n\nyfft = np.fft.fft(y)\n\n\nplt.plot(abs(yfft))\n\n\n\n\n- 처음 50개정도만 관찰\n\nplt.plot(abs(yfft[:50]),'o')\n\n\n\n\n\n첫값이 너무커서 나머지는 잘안보임\n\n- 2번째부터 50번째까지만 관찰\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=22500, color='r', linestyle='--')\n\n<matplotlib.lines.Line2D at 0x7f6250d5bfd0>\n\n\n\n\n\n\n대충 이정도 짜르면 될것같음\n\n- thresholded value\n\ntresh_value = 22500\n\n\nyfft[abs(yfft)<tresh_value] =0 \n\n- 퓨리에역변환\n\nyhat = np.fft.ifft(yfft)\nyhat[:5]\n\narray([59664.72193044+8.87311904e-14j, 58572.98839934+8.87311904e-14j,\n       58066.07369126+3.39894326e-14j, 58169.18671667-6.87747670e-14j,\n       58706.41986821-1.14383435e-13j])\n\n\n실수화\n\nyhat = np.real(yhat)\nyhat[:5]\n\narray([59664.72193044, 58572.98839934, 58066.07369126, 58169.18671667,\n       58706.41986821])\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat,'--')\n\n\n\n\n- 숙제: treshold value를 관찰하며 시각화해볼것\n\n\nminor topics\n- y의 FFT 결과는 항상 y와 같은길이임\n\nlen(y)\n\n82\n\n\n\nlen(np.fft.fft(y))\n\n82\n\n\n- 에일리어싱: number of observation은 얼마나 세밀한 주파수까지 측정가능하냐를 결정함\n예시1: 에일리어싱\n\nx = np.linspace(-3.14,3.14,10)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.        , -0.99975131],\n       [-0.99975131,  1.        ]])\n\n\n\nplt.plot(x1,label='x1')\nplt.plot(x2,label='x2')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f6252112ee0>\n\n\n\n\n\n\n실제로는 x2가 더 고주파인데, 같은 주파수처럼 보임\n\n예시2: 에일리어싱이 없는 경우\n\nx = np.linspace(-3.14,3.14,100000)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.00000000e+00, -6.45767105e-08],\n       [-6.45767105e-08,  1.00000000e+00]])\n\n\n\nplt.plot(x1)\nplt.plot(x2)\n\n\n\n\n\n주파수 왜곡떄문에 실제로는 corr ceof = 0 일지라도 관측되는건 corr coef >0 일 수 있음"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "이론 및 예시",
    "text": "이론 및 예시\n- 이론: real-valued signal은 무조건 \\(|X[k]|^2\\)의 그래프가 대칭으로 나옴 (단, \\(X[0]\\)은 제외)\n- 예시1:\n\nx = np.array([1,2,3,4,5])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시2:\n\nx = np.array([1,2,3,-3,-2,-1])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시3: \\({\\bf x}\\)가 복소수일 경우는 첫항을 제외하고 대칭이 되지 않음\n\nx = np.array([1+1j,2+2j,3+3j,-3-3j,-2-2j,1-1j]) \nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?",
    "text": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?\n- 예비학습1\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\cos(2\\pi \\alpha) =\\cos(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.cos(2*np.pi*alpha),np.cos(2*np.pi*(1-alpha))\n\n(0.30901699437494745, 0.30901699437494723)\n\n\n\n그래프를 잘 그려보세여\n\n- 예비학습2\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\sin(2\\pi \\alpha) = -\\sin(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.sin(2*np.pi*alpha),np.sin(2*np.pi*(1-alpha))\n\n(0.9510565162951535, -0.9510565162951536)\n\n\n\n그래프를 잘 그려보세여\n\n- 왜 실수일경우만 대칭인지? (어디 정리된걸 아무리 찾아도 못찾겠어서 그냥 직접 수식을 썼는데요, 이걸 기억할 필요는 없어요.. 아마 제가 쓴것보다 쉽게 설명하는 방법이 있을겁니다)\n(해설) \\(k=0,1,2,\\dots,N-1\\)에 대하여 \\(X[k]\\)는 아래와 같이 표현가능하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]e^{-\\frac{j2\\pi kn}{N}}\\]\n오일러공식을 사용하면 아래와 같이 정리할 수 있다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(-\\frac{2\\pi kn}{N}\\right) + i \\sum_{n=0}^{N-1}x[n]\\sin\\left(-\\frac{2\\pi kn}{N}\\right)\\]\ncos은 짝함수, sin은 홀함수임을 이용하여 다시정리하면\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\ncase1 \\(k=1\\) 인 경우와 \\(k=N-1\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음을 보이자.\n\\[X[1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi n}{N}\\right)\\]\n\\[X[N-1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\]\n여기에서 예비학습1,2를 떠올리면 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 임을 알 수 있다. 따라서 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수이다. 즉\n\\[X[1] = X[N-1]^\\ast, \\quad X[1]^\\ast = X[N-1]\\]\n이다. 그런데 임의의 복소수 \\(x=a+bi\\)에 대하여 \\(|x|^2 = a^2+b^2 = (a+bi)(a-bi)= x x^\\ast\\) 가 성립하므로\n\\[|X[1]|^2 = |X[N-1]|^2\\]\n이 성립한다.\n\n만약에 \\(x[n]\\)이 실수가 아닌경우는 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 이라고 하여도 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수라고 주장할수 없다.\n\ncase2 \\(k=2\\) 인 경우와 \\(k=N-2\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음도 비슷한 논리로 보일 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "허수파트 해석",
    "text": "허수파트 해석\n관찰1: 모든 \\(k\\)에 대하여 \\(X[k]\\)의 허수파트는 항상 0이다.\nk=0\n\nk=0\nsin_part_0 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_0\n\narray([ 0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n        0.,  0.])\n\n\n\nsum(x*sin_part_0)\n\n0.0\n\n\nk=1\n\nk=1\nsin_part_1 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_1\n\narray([ 0.        ,  0.37157241,  0.49726095,  0.29389263, -0.10395585,\n       -0.4330127 , -0.47552826, -0.20336832,  0.20336832,  0.47552826,\n        0.4330127 ,  0.10395585, -0.29389263, -0.49726095, -0.37157241])\n\n\n\nsum(x*sin_part_1)\n\n1.0547118733938987e-15\n\n\n약간을 직관을 위해서 그림을 그려보자.\n\nplt.plot(x,'--o')\nplt.plot(sin_part_1,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 \\(\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\)에 대응하는 식은 \\(\\int_0^{2\\pi}\\cos(t)\\sin(t)dt\\)라고 볼 수 있어서 sum(x*sin_part_1)=0임을 더 쉽게 이해할 수 있다.\nk=2\n\nk=2\nsin_part_2 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(sin_part_2,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\sin(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nsum(x*sin_part_2)\n\n-1.2212453270876722e-15\n\n\n\\(\\cos(t)\\)는 임의의 \\(\\sin(kt)\\)와 항상 직교하므로, 임의의 \\(k\\)에 대하여 허수파트는 항상 0이다.\n따라서 이 경우 \\(X[k]\\)는 아래와 같이 써도 무방하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right)\\]"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "실수파트 해석",
    "text": "실수파트 해석\n관찰2: \\(X[k]\\)의 실수파트는 \\(k=1\\)혹은 \\(k=N-1\\)일때 아래와 같이 정리된다.\n\\[\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)^2\\]\n그외의 경우에는 아래와 같이 된다.\nk=0\n\nk=0\ncos_part_0 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_0,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(1\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nk=2\n\nk=2\ncos_part_2 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_2,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\cos(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\n임의의 \\(k\\)에 대하여 \\(\\cos(t)\\)와 \\(\\cos(kt)\\)는 항상 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n- 요약: 만약에 \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\]\n이때 퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=1,N-1\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)\n\n\n\n\n- 응용: \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{6\\pi n}{N} \\right)\\]\n퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=3,N-3\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nN = 15 \nx = np.array([np.cos(6*np.pi*n/N) for n in range(N)]) \nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 비편향추정량(UB)란 \\(\\theta\\)의 추정량 중\n\\[\\forall \\theta\\in \\Theta:~ E(\\hat{\\theta})=\\theta\\]\n를 만족하는 추정량 \\(\\hat{\\theta}\\)을 의미한다.\n- (예시) 아래와 같은 상황을 가정하자.\n\\[X_n \\overset{iid}{\\sim} N(\\theta,1)\\]\n여기에서\n\n\\(\\hat{\\theta}_1=0\\) 은 \\(\\theta=0\\) 일 경우에는 \\(E(\\hat{\\theta})=\\theta\\) 를 만족하지만 그 외의 경우에는 \\(E(\\hat{\\theta})\\neq\\theta\\) 이므로 UB가 아니다.\n\\(\\hat{\\theta}_2=X_1\\) 은 UB이다.\n\\(\\hat{\\theta}_3=\\frac{X_1+X_2}{2}\\) 역시 UB이다.\n\\(\\hat{\\theta}_4=X_1+X_2-X_3\\) 역시 UB이다.\n\\(\\hat{\\theta}_5=-99X_1+100X_2\\) 역시 UB이다.\n\\(\\hat{\\theta}_6=\\frac{X_1+0}{2}\\) 은 1과 동일한 이유로 UB가 아니다.\n\\(\\hat{\\theta}_7=\\bar{X}\\)는 UB이다.\n\\(\\hat{\\theta}_8=w_1X_1+\\dots+w_nX_n\\) ,where \\(\\sum_{i=1}^{n}w_i=1\\) 형태의 estimator는 모두 UB이다.\n\n- 최소분산비편향추정량(MVUE)란 \\(\\theta\\)에 대한 비편향추정량을 모아놓은 집합 \\(\\hat{\\Theta}_{UB}\\) 에서 최소분산을 가지는 추정량을 의미한다. MVUE를 구하는 방법은 아래와 같다.\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n예를들어 위의 예제에서 \\(V(\\hat{\\theta}_2)=1\\) 이고 \\(V(\\hat{\\theta}_3)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}\\) 이므로 \\(\\hat{\\theta}_3\\) 이 더 좋은 추정량이라 볼 수 있다.\n- (의문) 왜 비편향추정량만 모아서 그중에서 최소분산을 구할까?\n\n\\(\\hat{\\theta}_1\\)와 같은 추정량은 \\(V(\\hat{\\theta}_1)=0\\) 이므로 그냥 최소분산을 만족한다. 따라서 이러한 추정량은 제외해야지 게임이 성립함.\n\n- 불만: 아래의 방법으로 구하는건 거의 불가능하지 않나?\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n- 이론: 크래머라오 하한값(편의상 \\(L^\\star\\)이라고 하자)이라고 있는데, 이는 \\({\\Theta}_{UB}\\)에 존재하는 모든 추정량에 대한 분산의 하한값을 제공한다.1 즉 아래가 성립한다.\n\n\\(L^\\star\\) is Cramer-Rao lower bound \\(\\Rightarrow\\) \\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L^\\star\\)\n\n역은 성립하지 않음을 주의하자. 즉 아래를 만족하는 \\(L\\)이 존재할 수 있다.\n\n\\(V(\\hat{\\theta}) \\geq L > L^\\star\\) for some \\(\\hat{\\theta} \\in \\Theta_{UB}\\)\n\n- 위의 이론을 이용하면 아래의 논리전개를 펼 수 있다.\n\n\\(L^\\star\\)를 구한다.\n왠지 MVUE가 될 것 같은 \\(\\hat{\\theta}\\)을 하나 찍고 그것의 분산 \\(V(\\hat{\\theta})\\)를 구한다.\n만약에 \\(V(\\hat{\\theta})=L^\\star\\)를 만족하면 그 \\(\\hat{\\theta}\\)이 MVUE라고 주장할 수 있다.\n\n- 위의 논리전개에 대한 불만 [@ p.212]\n\n\\(V(\\hat{\\theta})=L^\\star\\) 이길 기도해야함.\n\\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L > L^\\star\\) 와 같은 \\(L\\)이 존재하는 경우는 쓸 수 없음.\n\n- 또 다른 방법: 완비충분통계량을 이용함\n\n\n아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -> MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! <– 외우세여\n분해정리를 쓰는 경우. <– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)\n\n\n\n\n- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, @Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "title": "[PINKOCTO] 커널리그레션",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nx = np.linspace(0,1,5)\nx\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\n\ny = x*2 + np.random.randn(5)*0.3\ny\n\narray([0.01044984, 0.93755458, 0.43942577, 1.0639859 , 2.1133726 ])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\n\n\n\n\n\\[y_i= \\sum_{i=1}^{5}\\theta_i\\exp\\left(-\\frac{|x-x_i|^2}{2h^2}\\right)\\]\n\nh= 0.15\nx0 = lambda xstar: np.exp(-(xstar-x[0])**2 / 2 / (h**2))\nx1 = lambda xstar: np.exp(-(xstar-x[1])**2 / 2 / (h**2))\nx2 = lambda xstar: np.exp(-(xstar-x[2])**2 / 2 / (h**2))\nx3 = lambda xstar: np.exp(-(xstar-x[3])**2 / 2 / (h**2))\nx4 = lambda xstar: np.exp(-(xstar-x[4])**2 / 2 / (h**2))\n\n\nθ0 = 0\nθ1 = 0.2\nθ2 = 0.5\nθ3 = 1.0\nθ4 = 1.5\n\n\nxstar = 0.6 \n\n\nθ0*x0(0.6)+θ1*x1(0.6)+θ2*x2(0.6)+θ3*x3(0.6)+θ4*x4(0.6)\n\n1.062893318071169\n\n\n\n_yhat = lambda xstar : θ0*x0(xstar)+\\\nθ1*x1(xstar)+\\\nθ2*x2(xstar)+\\\nθ3*x3(xstar)+\\\nθ4*x4(xstar)\n\n\n(_yhat(0)-y[0])**2\n\n0.0017104251892816106\n\n\n\nloss = (_yhat(0)-y[0])**2 + (_yhat(0.25)-y[1])**2+ \\\n(_yhat(0.5)-y[2])**2+(_yhat(0.75)-y[3])**2+\\\n(_yhat(1)-y[4])**2\n\n\\(loss(\\theta_0,\\theta_1,\\theta_2,\\theta_3,\\theta_4)\\) 를 최소화하는 \\({\\boldsymbol \\theta}\\)를 구한다.\n\n_x = np.linspace(0,1,1000)\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\nplt.plot(_x,_yhat(_x))"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-24-try2변형.html",
    "href": "posts/3_Researches/BORAM/2023-05-24-try2변형.html",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try2 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -> X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -> y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-24-try2변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/2023-05-24-try2변형.html#데이터-종류",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try2 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n  \n    \n      \n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      city\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      2449\n      2019-01-02 1:06\n      4.613310e+12\n      fraud_Rutherford-Mertz\n      grocery_pos\n      281.06\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      e8a81877ae9a0a7f883e15cb39dc4022\n      1325466397\n      36.430124\n      -81.179483\n      1\n    \n    \n      2472\n      2019-01-02 1:47\n      3.401870e+14\n      fraud_Jenkins, Hauck and Friesen\n      gas_transport\n      11.52\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      bc7d41c41103877b03232f03f1f8d3f5\n      1325468849\n      29.819364\n      -99.142791\n      1\n    \n    \n      2523\n      2019-01-02 3:05\n      3.401870e+14\n      fraud_Goodwin-Nitzsche\n      grocery_pos\n      276.31\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      b98f12f4168391b2203238813df5aa8c\n      1325473523\n      29.273085\n      -98.836360\n      1\n    \n    \n      2546\n      2019-01-02 3:38\n      4.613310e+12\n      fraud_Erdman-Kertzmann\n      gas_transport\n      7.03\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      397894a5c4c02e3c61c784001f0f14e4\n      1325475483\n      35.909292\n      -82.091010\n      1\n    \n    \n      2553\n      2019-01-02 3:55\n      3.401870e+14\n      fraud_Koepp-Parker\n      grocery_pos\n      275.73\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      7863235a750d73a244c07f1fb7f0185a\n      1325476547\n      29.786426\n      -98.683410\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      363827\n      2019-06-17 19:30\n      2.475090e+15\n      fraud_Frami Group\n      entertainment\n      81.13\n      John\n      Miller\n      M\n      153 Mccullough Springs Apt. 857\n      Lamberton\n      ...\n      44.2378\n      -95.2739\n      1507\n      Land/geomatics surveyor\n      1993-10-12\n      c66cb411019c7dfd4d89f42a1ba4765f\n      1339961448\n      44.212695\n      -95.661879\n      0\n    \n    \n      140154\n      2019-03-17 14:33\n      2.131550e+14\n      fraud_Bahringer-Streich\n      food_dining\n      55.00\n      Christopher\n      Sheppard\n      M\n      39218 Baker Shoals\n      Bristow\n      ...\n      38.1981\n      -86.6821\n      965\n      Horticultural therapist\n      1982-02-10\n      316b9d25b9fa7d08a6831b7dab6634cd\n      1331994839\n      38.394240\n      -86.413557\n      0\n    \n    \n      860597\n      2019-12-17 12:31\n      2.280870e+15\n      fraud_Lubowitz-Walter\n      kids_pets\n      8.12\n      Katherine\n      Cooper\n      F\n      3854 Lauren Springs Suite 648\n      Oakford\n      ...\n      40.0994\n      -89.9601\n      530\n      Transport planner\n      1967-09-23\n      d92e9e63d9b24c3ccb92d05cba4cac54\n      1355747517\n      39.695248\n      -89.853063\n      0\n    \n    \n      29341\n      2019-01-18 9:20\n      4.878360e+15\n      fraud_Denesik and Sons\n      shopping_pos\n      3.52\n      Tina\n      Alvarez\n      F\n      1976 Tyler Underpass\n      Early\n      ...\n      42.4483\n      -95.1726\n      885\n      Pilot, airline\n      1949-08-14\n      8390ce51cfb8482b618ebc4ac370bcf7\n      1326878457\n      42.633204\n      -95.598143\n      0\n    \n    \n      529797\n      2019-08-16 13:17\n      4.450830e+15\n      fraud_Beier and Sons\n      home\n      84.15\n      Donna\n      Davis\n      F\n      6760 Donovan Lakes\n      Clayton\n      ...\n      34.5906\n      -95.3800\n      1760\n      Occupational psychologist\n      1972-01-20\n      04e1be9bcb18ea8b96048659bd02177b\n      1345123058\n      33.885236\n      -95.885110\n      0\n    \n  \n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-12-try1.html",
    "href": "posts/3_Researches/BORAM/2023-05-12-try1.html",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -> X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -> y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-12-try1.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/2023-05-12-try1.html#데이터-종류",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try1",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n  \n    \n      \n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      city\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      2449\n      2019-01-02 1:06\n      4.613310e+12\n      fraud_Rutherford-Mertz\n      grocery_pos\n      281.06\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      e8a81877ae9a0a7f883e15cb39dc4022\n      1325466397\n      36.430124\n      -81.179483\n      1\n    \n    \n      2472\n      2019-01-02 1:47\n      3.401870e+14\n      fraud_Jenkins, Hauck and Friesen\n      gas_transport\n      11.52\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      bc7d41c41103877b03232f03f1f8d3f5\n      1325468849\n      29.819364\n      -99.142791\n      1\n    \n    \n      2523\n      2019-01-02 3:05\n      3.401870e+14\n      fraud_Goodwin-Nitzsche\n      grocery_pos\n      276.31\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      b98f12f4168391b2203238813df5aa8c\n      1325473523\n      29.273085\n      -98.836360\n      1\n    \n    \n      2546\n      2019-01-02 3:38\n      4.613310e+12\n      fraud_Erdman-Kertzmann\n      gas_transport\n      7.03\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      397894a5c4c02e3c61c784001f0f14e4\n      1325475483\n      35.909292\n      -82.091010\n      1\n    \n    \n      2553\n      2019-01-02 3:55\n      3.401870e+14\n      fraud_Koepp-Parker\n      grocery_pos\n      275.73\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      7863235a750d73a244c07f1fb7f0185a\n      1325476547\n      29.786426\n      -98.683410\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      363827\n      2019-06-17 19:30\n      2.475090e+15\n      fraud_Frami Group\n      entertainment\n      81.13\n      John\n      Miller\n      M\n      153 Mccullough Springs Apt. 857\n      Lamberton\n      ...\n      44.2378\n      -95.2739\n      1507\n      Land/geomatics surveyor\n      1993-10-12\n      c66cb411019c7dfd4d89f42a1ba4765f\n      1339961448\n      44.212695\n      -95.661879\n      0\n    \n    \n      140154\n      2019-03-17 14:33\n      2.131550e+14\n      fraud_Bahringer-Streich\n      food_dining\n      55.00\n      Christopher\n      Sheppard\n      M\n      39218 Baker Shoals\n      Bristow\n      ...\n      38.1981\n      -86.6821\n      965\n      Horticultural therapist\n      1982-02-10\n      316b9d25b9fa7d08a6831b7dab6634cd\n      1331994839\n      38.394240\n      -86.413557\n      0\n    \n    \n      860597\n      2019-12-17 12:31\n      2.280870e+15\n      fraud_Lubowitz-Walter\n      kids_pets\n      8.12\n      Katherine\n      Cooper\n      F\n      3854 Lauren Springs Suite 648\n      Oakford\n      ...\n      40.0994\n      -89.9601\n      530\n      Transport planner\n      1967-09-23\n      d92e9e63d9b24c3ccb92d05cba4cac54\n      1355747517\n      39.695248\n      -89.853063\n      0\n    \n    \n      29341\n      2019-01-18 9:20\n      4.878360e+15\n      fraud_Denesik and Sons\n      shopping_pos\n      3.52\n      Tina\n      Alvarez\n      F\n      1976 Tyler Underpass\n      Early\n      ...\n      42.4483\n      -95.1726\n      885\n      Pilot, airline\n      1949-08-14\n      8390ce51cfb8482b618ebc4ac370bcf7\n      1326878457\n      42.633204\n      -95.598143\n      0\n    \n    \n      529797\n      2019-08-16 13:17\n      4.450830e+15\n      fraud_Beier and Sons\n      home\n      84.15\n      Donna\n      Davis\n      F\n      6760 Donovan Lakes\n      Clayton\n      ...\n      34.5906\n      -95.3800\n      1760\n      Occupational psychologist\n      1972-01-20\n      04e1be9bcb18ea8b96048659bd02177b\n      1345123058\n      33.885236\n      -95.885110\n      0\n    \n  \n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#read-and-define-data",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#read-and-define-data",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"fraudTrain.csv\")\n_df1=  df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42)\n_df2 = df[df[\"is_fraud\"] == 1]\ndf = pd.concat([_df1,_df2])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      669418\n      669418\n      2019-10-12 18:21\n      4.089100e+18\n      fraud_Haley, Jewess and Bechtelar\n      shopping_pos\n      7.53\n      Debra\n      Stark\n      F\n      686 Linda Rest\n      ...\n      32.3836\n      -94.8653\n      24536\n      Multimedia programmer\n      1983-10-14\n      d313353fa30233e5fab5468e852d22fc\n      1350066071\n      32.202008\n      -94.371865\n      0\n    \n    \n      32567\n      32567\n      2019-01-20 13:06\n      4.247920e+12\n      fraud_Turner LLC\n      travel\n      3.79\n      Judith\n      Moss\n      F\n      46297 Benjamin Plains Suite 703\n      ...\n      39.5370\n      -83.4550\n      22305\n      Television floor manager\n      1939-03-09\n      88c65b4e1585934d578511e627fe3589\n      1327064760\n      39.156673\n      -82.930503\n      0\n    \n    \n      156587\n      156587\n      2019-03-24 18:09\n      4.026220e+12\n      fraud_Klein Group\n      entertainment\n      59.07\n      Debbie\n      Payne\n      F\n      204 Ashley Neck Apt. 169\n      ...\n      41.5224\n      -71.9934\n      4720\n      Broadcast presenter\n      1977-05-18\n      3bd9ede04b5c093143d5e5292940b670\n      1332612553\n      41.657152\n      -72.595751\n      0\n    \n    \n      1020243\n      1020243\n      2020-02-25 15:12\n      4.957920e+12\n      fraud_Monahan-Morar\n      personal_care\n      25.58\n      Alan\n      Parsons\n      M\n      0547 Russell Ford Suite 574\n      ...\n      39.6171\n      -102.4776\n      207\n      Network engineer\n      1955-12-04\n      19e16ee7a01d229e750359098365e321\n      1361805120\n      39.080346\n      -103.213452\n      0\n    \n    \n      116272\n      116272\n      2019-03-06 23:19\n      4.178100e+15\n      fraud_Kozey-Kuhlman\n      personal_care\n      84.96\n      Jill\n      Flores\n      F\n      639 Cruz Islands\n      ...\n      41.9488\n      -86.4913\n      3104\n      Horticulturist, commercial\n      1981-03-29\n      a0c8641ca1f5d6e243ed5a2246e66176\n      1331075954\n      42.502065\n      -86.732664\n      0\n    \n  \n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#embedding",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#embedding",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#learn",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#learn",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#evaluate",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#evaluate",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#read-and-define-data-1",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#read-and-define-data-1",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\nlrnr2, _,_,_ = anal(our_sampling1(df),n_estimators=100)\n\nComputing transition probabilities: 100%|██████████| 1289/1289 [00:49<00:00, 26.00it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:10<00:00,  1.10s/it]"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#네트워크-토폴로지",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#네트워크-토폴로지",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n<Figure size 1000x1000 with 0 Axes>\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =>\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#커뮤니티-감지",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#커뮤니티-감지",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96>>113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/3_Researches/BORAM/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "[BORAM] 신용카드 거래 사기탐지 Start",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\ndf.shape\n\n(318777, 23)\n\n\n\ndf_minority.shape\n\n(6006, 23)\n\n\n\ndf_majority.shape\n\n(312771, 23)\n\n\n\n6006 / 312771 \n\n0.019202547550763976\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(yhat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7236842105263158\nRecall: 0.1407849829351536\nF1-Score: 0.2357142857142857\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 0])\n\n\n\nyhat = rf.predict_proba(test_embeddings)\n\n\nyhat\n\narray([[0.457, 0.543],\n       [0.634, 0.366],\n       [0.609, 0.391],\n       ...,\n       [0.577, 0.423],\n       [0.59 , 0.41 ],\n       [0.557, 0.443]])\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-24-try1변형.html",
    "href": "posts/3_Researches/BORAM/2023-05-24-try1변형.html",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try1 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -> X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -> y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-24-try1변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/2023-05-24-try1변형.html#데이터-종류",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try1 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n  \n    \n      \n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      city\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      2449\n      2019-01-02 1:06\n      4.613310e+12\n      fraud_Rutherford-Mertz\n      grocery_pos\n      281.06\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      e8a81877ae9a0a7f883e15cb39dc4022\n      1325466397\n      36.430124\n      -81.179483\n      1\n    \n    \n      2472\n      2019-01-02 1:47\n      3.401870e+14\n      fraud_Jenkins, Hauck and Friesen\n      gas_transport\n      11.52\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      bc7d41c41103877b03232f03f1f8d3f5\n      1325468849\n      29.819364\n      -99.142791\n      1\n    \n    \n      2523\n      2019-01-02 3:05\n      3.401870e+14\n      fraud_Goodwin-Nitzsche\n      grocery_pos\n      276.31\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      b98f12f4168391b2203238813df5aa8c\n      1325473523\n      29.273085\n      -98.836360\n      1\n    \n    \n      2546\n      2019-01-02 3:38\n      4.613310e+12\n      fraud_Erdman-Kertzmann\n      gas_transport\n      7.03\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      397894a5c4c02e3c61c784001f0f14e4\n      1325475483\n      35.909292\n      -82.091010\n      1\n    \n    \n      2553\n      2019-01-02 3:55\n      3.401870e+14\n      fraud_Koepp-Parker\n      grocery_pos\n      275.73\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      7863235a750d73a244c07f1fb7f0185a\n      1325476547\n      29.786426\n      -98.683410\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      363827\n      2019-06-17 19:30\n      2.475090e+15\n      fraud_Frami Group\n      entertainment\n      81.13\n      John\n      Miller\n      M\n      153 Mccullough Springs Apt. 857\n      Lamberton\n      ...\n      44.2378\n      -95.2739\n      1507\n      Land/geomatics surveyor\n      1993-10-12\n      c66cb411019c7dfd4d89f42a1ba4765f\n      1339961448\n      44.212695\n      -95.661879\n      0\n    \n    \n      140154\n      2019-03-17 14:33\n      2.131550e+14\n      fraud_Bahringer-Streich\n      food_dining\n      55.00\n      Christopher\n      Sheppard\n      M\n      39218 Baker Shoals\n      Bristow\n      ...\n      38.1981\n      -86.6821\n      965\n      Horticultural therapist\n      1982-02-10\n      316b9d25b9fa7d08a6831b7dab6634cd\n      1331994839\n      38.394240\n      -86.413557\n      0\n    \n    \n      860597\n      2019-12-17 12:31\n      2.280870e+15\n      fraud_Lubowitz-Walter\n      kids_pets\n      8.12\n      Katherine\n      Cooper\n      F\n      3854 Lauren Springs Suite 648\n      Oakford\n      ...\n      40.0994\n      -89.9601\n      530\n      Transport planner\n      1967-09-23\n      d92e9e63d9b24c3ccb92d05cba4cac54\n      1355747517\n      39.695248\n      -89.853063\n      0\n    \n    \n      29341\n      2019-01-18 9:20\n      4.878360e+15\n      fraud_Denesik and Sons\n      shopping_pos\n      3.52\n      Tina\n      Alvarez\n      F\n      1976 Tyler Underpass\n      Early\n      ...\n      42.4483\n      -95.1726\n      885\n      Pilot, airline\n      1949-08-14\n      8390ce51cfb8482b618ebc4ac370bcf7\n      1326878457\n      42.633204\n      -95.598143\n      0\n    \n    \n      529797\n      2019-08-16 13:17\n      4.450830e+15\n      fraud_Beier and Sons\n      home\n      84.15\n      Donna\n      Davis\n      F\n      6760 Donovan Lakes\n      Clayton\n      ...\n      34.5906\n      -95.3800\n      1760\n      Occupational psychologist\n      1972-01-20\n      04e1be9bcb18ea8b96048659bd02177b\n      1345123058\n      33.885236\n      -95.885110\n      0\n    \n  \n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-19-try2.html",
    "href": "posts/3_Researches/BORAM/2023-05-19-try2.html",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -> X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -> y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\ndef amtano1(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano=0)\n    normalize = lambda arr: (arr-np.median(arr))/np.std(arr) if np.std(arr)!=0 else arr*0 \n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano']] = normalize(sub_df.amt).cumsum()\n    return df"
  },
  {
    "objectID": "posts/3_Researches/BORAM/2023-05-19-try2.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/2023-05-19-try2.html#데이터-종류",
    "title": "[BORAM] 신용카드 거래 사기탐지 Try2",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n  \n    \n      \n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      city\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      2449\n      2019-01-02 1:06\n      4.613310e+12\n      fraud_Rutherford-Mertz\n      grocery_pos\n      281.06\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      e8a81877ae9a0a7f883e15cb39dc4022\n      1325466397\n      36.430124\n      -81.179483\n      1\n    \n    \n      2472\n      2019-01-02 1:47\n      3.401870e+14\n      fraud_Jenkins, Hauck and Friesen\n      gas_transport\n      11.52\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      bc7d41c41103877b03232f03f1f8d3f5\n      1325468849\n      29.819364\n      -99.142791\n      1\n    \n    \n      2523\n      2019-01-02 3:05\n      3.401870e+14\n      fraud_Goodwin-Nitzsche\n      grocery_pos\n      276.31\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      b98f12f4168391b2203238813df5aa8c\n      1325473523\n      29.273085\n      -98.836360\n      1\n    \n    \n      2546\n      2019-01-02 3:38\n      4.613310e+12\n      fraud_Erdman-Kertzmann\n      gas_transport\n      7.03\n      Jason\n      Murphy\n      M\n      542 Steve Curve Suite 011\n      Collettsville\n      ...\n      35.9946\n      -81.7266\n      885\n      Soil scientist\n      1988-09-15\n      397894a5c4c02e3c61c784001f0f14e4\n      1325475483\n      35.909292\n      -82.091010\n      1\n    \n    \n      2553\n      2019-01-02 3:55\n      3.401870e+14\n      fraud_Koepp-Parker\n      grocery_pos\n      275.73\n      Misty\n      Hart\n      F\n      27954 Hall Mill Suite 575\n      San Antonio\n      ...\n      29.4400\n      -98.4590\n      1595797\n      Horticultural consultant\n      1960-10-28\n      7863235a750d73a244c07f1fb7f0185a\n      1325476547\n      29.786426\n      -98.683410\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      363827\n      2019-06-17 19:30\n      2.475090e+15\n      fraud_Frami Group\n      entertainment\n      81.13\n      John\n      Miller\n      M\n      153 Mccullough Springs Apt. 857\n      Lamberton\n      ...\n      44.2378\n      -95.2739\n      1507\n      Land/geomatics surveyor\n      1993-10-12\n      c66cb411019c7dfd4d89f42a1ba4765f\n      1339961448\n      44.212695\n      -95.661879\n      0\n    \n    \n      140154\n      2019-03-17 14:33\n      2.131550e+14\n      fraud_Bahringer-Streich\n      food_dining\n      55.00\n      Christopher\n      Sheppard\n      M\n      39218 Baker Shoals\n      Bristow\n      ...\n      38.1981\n      -86.6821\n      965\n      Horticultural therapist\n      1982-02-10\n      316b9d25b9fa7d08a6831b7dab6634cd\n      1331994839\n      38.394240\n      -86.413557\n      0\n    \n    \n      860597\n      2019-12-17 12:31\n      2.280870e+15\n      fraud_Lubowitz-Walter\n      kids_pets\n      8.12\n      Katherine\n      Cooper\n      F\n      3854 Lauren Springs Suite 648\n      Oakford\n      ...\n      40.0994\n      -89.9601\n      530\n      Transport planner\n      1967-09-23\n      d92e9e63d9b24c3ccb92d05cba4cac54\n      1355747517\n      39.695248\n      -89.853063\n      0\n    \n    \n      29341\n      2019-01-18 9:20\n      4.878360e+15\n      fraud_Denesik and Sons\n      shopping_pos\n      3.52\n      Tina\n      Alvarez\n      F\n      1976 Tyler Underpass\n      Early\n      ...\n      42.4483\n      -95.1726\n      885\n      Pilot, airline\n      1949-08-14\n      8390ce51cfb8482b618ebc4ac370bcf7\n      1326878457\n      42.633204\n      -95.598143\n      0\n    \n    \n      529797\n      2019-08-16 13:17\n      4.450830e+15\n      fraud_Beier and Sons\n      home\n      84.15\n      Donna\n      Davis\n      F\n      6760 Donovan Lakes\n      Clayton\n      ...\n      34.5906\n      -95.3800\n      1760\n      Occupational psychologist\n      1972-01-20\n      04e1be9bcb18ea8b96048659bd02177b\n      1345123058\n      33.885236\n      -95.885110\n      0\n    \n  \n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/SOLAR/2023-04-03-일사량.html",
    "href": "posts/3_Researches/SOLAR/2023-04-03-일사량.html",
    "title": "[SOLAR] 일사량자료정리",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndf0 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data0.csv', encoding='cp949') # 2021-01-01 ~ 2021-12-31\ndf1 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data1.csv') # 2022-01-01 ~ 2023-12-31\ndf2 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/test_raw.csv', encoding='cp949') # 2023-01-01 ~ 2023-01-15\n\n- df_raw\n\ndf_raw = pd.concat([df0, df1])\ndf_raw\n\n\n\n\n\n  \n    \n      \n      지점\n      지점명\n      일시\n      일사(MJ/m2)\n    \n  \n  \n    \n      0\n      93\n      북춘천\n      2021-01-01 08:00\n      0.00\n    \n    \n      1\n      93\n      북춘천\n      2021-01-01 09:00\n      0.37\n    \n    \n      2\n      93\n      북춘천\n      2021-01-01 10:00\n      0.96\n    \n    \n      3\n      93\n      북춘천\n      2021-01-01 11:00\n      1.40\n    \n    \n      4\n      93\n      북춘천\n      2021-01-01 12:00\n      1.72\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      229672\n      283\n      경주시\n      2022-12-31 14:00:00\n      1.82\n    \n    \n      229673\n      283\n      경주시\n      2022-12-31 15:00:00\n      1.52\n    \n    \n      229674\n      283\n      경주시\n      2022-12-31 16:00:00\n      0.96\n    \n    \n      229675\n      283\n      경주시\n      2022-12-31 17:00:00\n      0.35\n    \n    \n      229676\n      283\n      경주시\n      2022-12-31 18:00:00\n      0.01\n    \n  \n\n444720 rows × 4 columns\n\n\n\n- 지점칼럼 삭제 // 일시 \\(\\to\\) 날짜,시간 으로 분리\n\ndf_temp = df_raw.assign(날짜= list(map(lambda x: x[:10],df_raw['일시'])))\\\n.assign(시간= list(map(lambda x: x[11:16],df_raw['일시'])))\\\n.drop(['일시','지점'],axis=1).rename({'일사(MJ/m2)':'일사'},axis=1).reset_index(drop=True)\ndf_temp\n\n\n\n\n\n  \n    \n      \n      지점명\n      일사\n      날짜\n      시간\n    \n  \n  \n    \n      0\n      북춘천\n      0.00\n      2021-01-01\n      08:00\n    \n    \n      1\n      북춘천\n      0.37\n      2021-01-01\n      09:00\n    \n    \n      2\n      북춘천\n      0.96\n      2021-01-01\n      10:00\n    \n    \n      3\n      북춘천\n      1.40\n      2021-01-01\n      11:00\n    \n    \n      4\n      북춘천\n      1.72\n      2021-01-01\n      12:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      444715\n      경주시\n      1.82\n      2022-12-31\n      14:00\n    \n    \n      444716\n      경주시\n      1.52\n      2022-12-31\n      15:00\n    \n    \n      444717\n      경주시\n      0.96\n      2022-12-31\n      16:00\n    \n    \n      444718\n      경주시\n      0.35\n      2022-12-31\n      17:00\n    \n    \n      444719\n      경주시\n      0.01\n      2022-12-31\n      18:00\n    \n  \n\n444720 rows × 4 columns\n\n\n\n- 파주,상주,동두천,충주,제천은 삭제\n\ndf_temp = df_temp.query(\"지점명 not in ['파주','상주','동두천','충주','제천']\").reset_index(drop=True)\ndf_temp\n\n\n\n\n\n  \n    \n      \n      지점명\n      일사\n      날짜\n      시간\n    \n  \n  \n    \n      0\n      북춘천\n      0.00\n      2021-01-01\n      08:00\n    \n    \n      1\n      북춘천\n      0.37\n      2021-01-01\n      09:00\n    \n    \n      2\n      북춘천\n      0.96\n      2021-01-01\n      10:00\n    \n    \n      3\n      북춘천\n      1.40\n      2021-01-01\n      11:00\n    \n    \n      4\n      북춘천\n      1.72\n      2021-01-01\n      12:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      420955\n      경주시\n      1.82\n      2022-12-31\n      14:00\n    \n    \n      420956\n      경주시\n      1.52\n      2022-12-31\n      15:00\n    \n    \n      420957\n      경주시\n      0.96\n      2022-12-31\n      16:00\n    \n    \n      420958\n      경주시\n      0.35\n      2022-12-31\n      17:00\n    \n    \n      420959\n      경주시\n      0.01\n      2022-12-31\n      18:00\n    \n  \n\n420960 rows × 4 columns\n\n\n\n- 시간이 비어있지 않도록..\n\nreg = df_temp['지점명'].unique().tolist() \nday = df_temp['날짜'].unique().tolist() \ntime = list(df_temp['시간'].unique())\ntime = ['0{}:00'.format(i) for i in range(0,8)] + time\n\n\ndf_temp2 = pd.DataFrame(itertools.product(reg,day,time)).rename({0:'지점명',1:'날짜',2:'시간'},axis=1).merge(df_temp,how='left').fillna(0)\ndf_temp2\n\n\n\n\n\n  \n    \n      \n      지점명\n      날짜\n      시간\n      일사\n    \n  \n  \n    \n      0\n      북춘천\n      2021-01-01\n      00:00\n      0.0\n    \n    \n      1\n      북춘천\n      2021-01-01\n      01:00\n      0.0\n    \n    \n      2\n      북춘천\n      2021-01-01\n      02:00\n      0.0\n    \n    \n      3\n      북춘천\n      2021-01-01\n      03:00\n      0.0\n    \n    \n      4\n      북춘천\n      2021-01-01\n      04:00\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      802995\n      경주시\n      2022-12-31\n      07:00\n      0.0\n    \n    \n      802996\n      경주시\n      2022-12-31\n      20:00\n      0.0\n    \n    \n      802997\n      경주시\n      2022-12-31\n      06:00\n      0.0\n    \n    \n      802998\n      경주시\n      2022-12-31\n      21:00\n      0.0\n    \n    \n      802999\n      경주시\n      2022-12-31\n      05:00\n      0.0\n    \n  \n\n803000 rows × 4 columns\n\n\n\n\ndf_temp2[:20]\n\n\n\n\n\n  \n    \n      \n      지점명\n      날짜\n      시간\n      일사\n    \n  \n  \n    \n      0\n      북춘천\n      2021-01-01\n      00:00\n      0.00\n    \n    \n      1\n      북춘천\n      2021-01-01\n      01:00\n      0.00\n    \n    \n      2\n      북춘천\n      2021-01-01\n      02:00\n      0.00\n    \n    \n      3\n      북춘천\n      2021-01-01\n      03:00\n      0.00\n    \n    \n      4\n      북춘천\n      2021-01-01\n      04:00\n      0.00\n    \n    \n      5\n      북춘천\n      2021-01-01\n      05:00\n      0.00\n    \n    \n      6\n      북춘천\n      2021-01-01\n      06:00\n      0.00\n    \n    \n      7\n      북춘천\n      2021-01-01\n      07:00\n      0.00\n    \n    \n      8\n      북춘천\n      2021-01-01\n      08:00\n      0.00\n    \n    \n      9\n      북춘천\n      2021-01-01\n      09:00\n      0.37\n    \n    \n      10\n      북춘천\n      2021-01-01\n      10:00\n      0.96\n    \n    \n      11\n      북춘천\n      2021-01-01\n      11:00\n      1.40\n    \n    \n      12\n      북춘천\n      2021-01-01\n      12:00\n      1.72\n    \n    \n      13\n      북춘천\n      2021-01-01\n      13:00\n      1.84\n    \n    \n      14\n      북춘천\n      2021-01-01\n      14:00\n      1.74\n    \n    \n      15\n      북춘천\n      2021-01-01\n      15:00\n      1.30\n    \n    \n      16\n      북춘천\n      2021-01-01\n      16:00\n      0.93\n    \n    \n      17\n      북춘천\n      2021-01-01\n      17:00\n      0.29\n    \n    \n      18\n      북춘천\n      2021-01-01\n      18:00\n      0.01\n    \n    \n      19\n      북춘천\n      2021-01-01\n      19:00\n      0.00\n    \n  \n\n\n\n\n\ndf_temp2[-20:]\n\n\n\n\n\n  \n    \n      \n      지점명\n      날짜\n      시간\n      일사\n    \n  \n  \n    \n      802980\n      경주시\n      2022-12-31\n      05:00\n      0.00\n    \n    \n      802981\n      경주시\n      2022-12-31\n      06:00\n      0.00\n    \n    \n      802982\n      경주시\n      2022-12-31\n      07:00\n      0.00\n    \n    \n      802983\n      경주시\n      2022-12-31\n      08:00\n      0.02\n    \n    \n      802984\n      경주시\n      2022-12-31\n      09:00\n      0.41\n    \n    \n      802985\n      경주시\n      2022-12-31\n      10:00\n      1.05\n    \n    \n      802986\n      경주시\n      2022-12-31\n      11:00\n      1.52\n    \n    \n      802987\n      경주시\n      2022-12-31\n      12:00\n      1.86\n    \n    \n      802988\n      경주시\n      2022-12-31\n      13:00\n      1.93\n    \n    \n      802989\n      경주시\n      2022-12-31\n      14:00\n      1.82\n    \n    \n      802990\n      경주시\n      2022-12-31\n      15:00\n      1.52\n    \n    \n      802991\n      경주시\n      2022-12-31\n      16:00\n      0.96\n    \n    \n      802992\n      경주시\n      2022-12-31\n      17:00\n      0.35\n    \n    \n      802993\n      경주시\n      2022-12-31\n      18:00\n      0.01\n    \n    \n      802994\n      경주시\n      2022-12-31\n      19:00\n      0.00\n    \n    \n      802995\n      경주시\n      2022-12-31\n      07:00\n      0.00\n    \n    \n      802996\n      경주시\n      2022-12-31\n      20:00\n      0.00\n    \n    \n      802997\n      경주시\n      2022-12-31\n      06:00\n      0.00\n    \n    \n      802998\n      경주시\n      2022-12-31\n      21:00\n      0.00\n    \n    \n      802999\n      경주시\n      2022-12-31\n      05:00\n      0.00\n    \n  \n\n\n\n\n\ndf_temp2\n\n\n\n\n\n  \n    \n      \n      지점명\n      날짜\n      시간\n      일사\n    \n  \n  \n    \n      0\n      북춘천\n      2021-01-01\n      00:00\n      0.0\n    \n    \n      1\n      북춘천\n      2021-01-01\n      01:00\n      0.0\n    \n    \n      2\n      북춘천\n      2021-01-01\n      02:00\n      0.0\n    \n    \n      3\n      북춘천\n      2021-01-01\n      03:00\n      0.0\n    \n    \n      4\n      북춘천\n      2021-01-01\n      04:00\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      802995\n      경주시\n      2022-12-31\n      07:00\n      0.0\n    \n    \n      802996\n      경주시\n      2022-12-31\n      20:00\n      0.0\n    \n    \n      802997\n      경주시\n      2022-12-31\n      06:00\n      0.0\n    \n    \n      802998\n      경주시\n      2022-12-31\n      21:00\n      0.0\n    \n    \n      802999\n      경주시\n      2022-12-31\n      05:00\n      0.0\n    \n  \n\n803000 rows × 4 columns\n\n\n\n- 시간,날짜 \\(\\to\\) 일시\n\ndf_temp3=df_temp2.assign(일시 = list(map(lambda x,y: x+'-'+y,df_temp2['날짜'],df_temp2['시간'])))\\\n.drop(['날짜','시간'],axis=1)\ndf_temp3\n\n\n\n\n\n  \n    \n      \n      지점명\n      일사\n      일시\n    \n  \n  \n    \n      0\n      북춘천\n      0.0\n      2021-01-01-00:00\n    \n    \n      1\n      북춘천\n      0.0\n      2021-01-01-01:00\n    \n    \n      2\n      북춘천\n      0.0\n      2021-01-01-02:00\n    \n    \n      3\n      북춘천\n      0.0\n      2021-01-01-03:00\n    \n    \n      4\n      북춘천\n      0.0\n      2021-01-01-04:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      802995\n      경주시\n      0.0\n      2022-12-31-07:00\n    \n    \n      802996\n      경주시\n      0.0\n      2022-12-31-20:00\n    \n    \n      802997\n      경주시\n      0.0\n      2022-12-31-06:00\n    \n    \n      802998\n      경주시\n      0.0\n      2022-12-31-21:00\n    \n    \n      802999\n      경주시\n      0.0\n      2022-12-31-05:00\n    \n  \n\n803000 rows × 3 columns\n\n\n\n- 저장\n\ndf_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\n\n\n\n\n\n  \n    \n      \n      region\n      solar_radiation\n      date\n    \n  \n  \n    \n      0\n      북춘천\n      0.0\n      2021-01-01-00:00\n    \n    \n      1\n      북춘천\n      0.0\n      2021-01-01-01:00\n    \n    \n      2\n      북춘천\n      0.0\n      2021-01-01-02:00\n    \n    \n      3\n      북춘천\n      0.0\n      2021-01-01-03:00\n    \n    \n      4\n      북춘천\n      0.0\n      2021-01-01-04:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      802995\n      경주시\n      0.0\n      2022-12-31-07:00\n    \n    \n      802996\n      경주시\n      0.0\n      2022-12-31-20:00\n    \n    \n      802997\n      경주시\n      0.0\n      2022-12-31-06:00\n    \n    \n      802998\n      경주시\n      0.0\n      2022-12-31-21:00\n    \n    \n      802999\n      경주시\n      0.0\n      2022-12-31-05:00\n    \n  \n\n803000 rows × 3 columns\n\n\n\n\ndf = df_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\ndf.to_csv(\"solar_radiation.csv\",index=False)\n!git add .\n!git commit -m .\n!git push \n\n[main 299d058] .\n 3 files changed, 806273 insertions(+)\n create mode 100644 \"posts/3_Researches/SOLAR/.ipynb_checkpoints/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211-checkpoint.ipynb\"\n create mode 100644 \"posts/3_Researches/SOLAR/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211.ipynb\"\n create mode 100644 posts/3_Researches/SOLAR/solar_radiation.csv\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (7/7), done.\nWriting objects: 100% (7/7), 8.74 KiB | 8.74 MiB/s, done.\nTotal 7 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/miruetoto/yechan3.git\n   495d9ce..299d058  main -> main\n\n\n- 불러오기\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      region\n      solar_radiation\n      date\n    \n  \n  \n    \n      0\n      북춘천\n      0.0\n      2021-01-01-00:00\n    \n    \n      1\n      북춘천\n      0.0\n      2021-01-01-01:00\n    \n    \n      2\n      북춘천\n      0.0\n      2021-01-01-02:00\n    \n    \n      3\n      북춘천\n      0.0\n      2021-01-01-03:00\n    \n    \n      4\n      북춘천\n      0.0\n      2021-01-01-04:00\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      802995\n      경주시\n      0.0\n      2022-12-31-07:00\n    \n    \n      802996\n      경주시\n      0.0\n      2022-12-31-20:00\n    \n    \n      802997\n      경주시\n      0.0\n      2022-12-31-06:00\n    \n    \n      802998\n      경주시\n      0.0\n      2022-12-31-21:00\n    \n    \n      802999\n      경주시\n      0.0\n      2022-12-31-05:00\n    \n  \n\n803000 rows × 3 columns\n\n\n\n- 다운로드\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv"
  },
  {
    "objectID": "posts/3_Researches/SOLAR/2023-04-04-EPT.html",
    "href": "posts/3_Researches/SOLAR/2023-04-04-EPT.html",
    "title": "[SOLAR] EPT",
    "section": "",
    "text": "ref: https://www.sciencedirect.com/science/article/pii/S2352711021000492\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(EPT)\n\n\nurl = 'https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv'\ndf = read_csv(url)\n\nRows: 803000 Columns: 3\n── Column specification ────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): region, date\ndbl (1): solar_radiation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf = df |> filter(region == '북춘천') |> mutate(date=ymd_hm(date))\ndf\n\n\n\nA tibble: 18250 × 3\n\n    regionsolar_radiationdate\n    <chr><dbl><dttm>\n\n\n    북춘천0.002021-01-01 00:00:00\n    북춘천0.002021-01-01 01:00:00\n    북춘천0.002021-01-01 02:00:00\n    북춘천0.002021-01-01 03:00:00\n    북춘천0.002021-01-01 04:00:00\n    북춘천0.002021-01-01 05:00:00\n    북춘천0.002021-01-01 06:00:00\n    북춘천0.002021-01-01 07:00:00\n    북춘천0.002021-01-01 08:00:00\n    북춘천0.372021-01-01 09:00:00\n    북춘천0.962021-01-01 10:00:00\n    북춘천1.402021-01-01 11:00:00\n    북춘천1.722021-01-01 12:00:00\n    북춘천1.842021-01-01 13:00:00\n    북춘천1.742021-01-01 14:00:00\n    북춘천1.302021-01-01 15:00:00\n    북춘천0.932021-01-01 16:00:00\n    북춘천0.292021-01-01 17:00:00\n    북춘천0.012021-01-01 18:00:00\n    북춘천0.002021-01-01 19:00:00\n    북춘천0.002021-01-01 07:00:00\n    북춘천0.002021-01-01 20:00:00\n    북춘천0.002021-01-01 06:00:00\n    북춘천0.002021-01-01 21:00:00\n    북춘천0.002021-01-01 05:00:00\n    북춘천0.002021-01-02 00:00:00\n    북춘천0.002021-01-02 01:00:00\n    북춘천0.002021-01-02 02:00:00\n    북춘천0.002021-01-02 03:00:00\n    북춘천0.002021-01-02 04:00:00\n    ⋮⋮⋮\n    북춘천0.002022-12-30 07:00:00\n    북춘천0.002022-12-30 20:00:00\n    북춘천0.002022-12-30 06:00:00\n    북춘천0.002022-12-30 21:00:00\n    북춘천0.002022-12-30 05:00:00\n    북춘천0.002022-12-31 00:00:00\n    북춘천0.002022-12-31 01:00:00\n    북춘천0.002022-12-31 02:00:00\n    북춘천0.002022-12-31 03:00:00\n    북춘천0.002022-12-31 04:00:00\n    북춘천0.002022-12-31 05:00:00\n    북춘천0.002022-12-31 06:00:00\n    북춘천0.002022-12-31 07:00:00\n    북춘천0.002022-12-31 08:00:00\n    북춘천0.222022-12-31 09:00:00\n    북춘천0.502022-12-31 10:00:00\n    북춘천0.802022-12-31 11:00:00\n    북춘천2.132022-12-31 12:00:00\n    북춘천1.782022-12-31 13:00:00\n    북춘천1.452022-12-31 14:00:00\n    북춘천0.782022-12-31 15:00:00\n    북춘천0.382022-12-31 16:00:00\n    북춘천0.152022-12-31 17:00:00\n    북춘천0.002022-12-31 18:00:00\n    북춘천0.002022-12-31 19:00:00\n    북춘천0.002022-12-31 07:00:00\n    북춘천0.002022-12-31 20:00:00\n    북춘천0.002022-12-31 06:00:00\n    북춘천0.002022-12-31 21:00:00\n    북춘천0.002022-12-31 05:00:00\n\n\n\n\n- 지역을 북춘천으로 고정\n\ndf2 = df |> filter(region =='북춘천') \ndf2 = df2[order(df2$date),]\n\n\ny = df2$solar_radiation\ny\n\n\n0000000000000.370.961.41.721.841.741.30.930.290.010000000000000000.320.951.461.791.911.821.50.970.370.010000000000000000.290.891.41.751.761.350.820.520.30.010000000000000000.330.931.321.531.671.5110.790.1900000000000000000.190.791.411.761.931.851.561.050.450.020000000000000000.511.381.811.881.931.851.5210.410.020000000000000000.110.741.441.811.981.911.611.090.450.020000000000000000.360.971.491.8521.911.611.10.480.03000⋯000000000000.010.471.31.711.871.881.721.380.880.3200000000000000000.190.681.351.712.191.891.380.820.2600000000000000000.150.420.931.071.181.051.160.830.300000000000000000.080.280.430.741.41.841.530.850.2700000000000000000.411.021.841.921.851.721.420.930.340.010000000000000000.471.281.681.81.851.741.40.890.30.010000000000000000.150.641.11.732.121.741.290.860.310.010000000000000000.220.50.82.131.781.450.780.380.150000\n\n\n\nplot(y[1:500])\nlines(y[1:500],lty=2)\n\n\n\n\n- EPT 수행\n\nept = function(y){\n    EpM = eptransf(signal=y,tau=24,process=c(\"envelope\",\"average\"))$EpM\n    EpM*2\n}\n\n\nyU = ept(y)\n\n\nplot(y[1:500])\nlines(yU[1:500],col=2,lty=2)\n\n\n\n\n- todo: 모든 지역에대하여 yU를 구하여 저장"
  },
  {
    "objectID": "posts/1_Essays/2019-04-26-퓨리에 변환.html",
    "href": "posts/1_Essays/2019-04-26-퓨리에 변환.html",
    "title": "[Essays] 퓨리에 변환",
    "section": "",
    "text": "About this doc\n- 이번에는 퓨리에 표현들을 정리하도록 하겠다. 내생각엔 퓨리에 표현들도 벡터의 미분만큼 복잡한 것 같다. 정의가 너무 많고 그게 그거 같아서 그렇다. 이번기회에 깔끔하게 정리하도록 하자. 참고한 문헌은 아래와 같다.\n\nHaykin, S., & Van Veen, B. (2007). Signals and systems. John Wiley & Sons.\n\n\n\n들어가며\n- 우선 신호와 하나의 신호값을 구분하는 notation을 생각하자. 우리가 다루는 신호 즉 데이터는 값들의 집합이다. 우리가 시계열자료를 다룬다면 데이터는 아래와 같이 표현한다.\n\n\\(\\{x_i: i \\in \\mathbb{Z}\\}\\)\n\n이와 유시하게 우리가 다루는 자료가 \\(t \\in \\mathbb{R}\\)인 연속신호라면 아래와 같이 표현한다.\n\n\\(\\{x(t): t \\in \\mathbb{R}\\}\\)\n\n우리가 모든 \\(i \\in \\mathbb{Z}\\) 혹은 모든 \\(t \\in \\mathbb{R}\\)에서 신호를 다룰 생각이 없다면 아래와 같은 표현도 얼마든지 가능하다.\n\n\\(\\{x_i: i=0,1,\\dots, \\xi-1 \\}\\)\n\\(\\{x(t):t \\in (0,\\zeta) \\}\\)\n\n- 위와 같이 집합의 표현 없이 단독으로 \\(x_i\\), \\(x(t)\\)와 같이 쓰면 하나의 고정된 값 \\(i,t\\)에 대한 \\(x_i\\), \\(x(t)\\)로 이해하자. 솔직히 이렇게 꼭 신호를 엄밀하게 집합으로 정의하는게 유별나 보일수도 있다. 일반적으로 사람들은 \\(\\{x(t): ~t \\in \\mathbb{R} \\}\\) 대신에 보통 \\(x(t)\\)로 간단하게 줄여서 쓰곤한다.1 하지만 이 포스팅에 한정하여 위와 같이 집합의 형태로 엄밀하게 구분해 쓰도록 하자. 처음에는 익숙하지 않지만 나중에는 편리하다.\n\n\n퓨리에표현들\n- 지금부터 우리가 고려하는 모든 신호들은 기본적으로 (1) infinity range에서 정의된 신호라고 가정한다. 즉 연속신호이면 \\(\\mathbb{R}\\)에서 정의된다고 가정하고 이산신호면 \\(\\mathbb{Z}\\)에서 정의된다고 가정한다. 또한 우리가 분석하고자 하는 신호는 (2) integrable 하다고 가정한다. 이건 퓨리에표현들이 적분 혹은 무한합의 형태로 표현된다는 것을 상기하면 타당하여 보인다.\n- 즉 우리가 고려하는 신호는 인피니티-레인지에서 정의되며 인피니티-레인지에서 적분값이 유한한 연속신호 혹은 이산신호 임을 알 수 있다. 이러한 신호는 구체적으로는 아래와 같이 쓸 수 있다.\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt <\\infty,~ t \\in \\mathbb{R} \\right\\}\\)\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| <\\infty,~ i \\in \\mathbb{Z} \\right\\}\\)\n\n- 그런데 integrable 한 함수들만을 고려하다 보면 우리가 다룰 수 있는 신호의 범위가 확 줄어들게 된다. 가령 예를 들어서 아래와 같은 신호는 적분을 하면 무한대가 나오기 때문에 intergrable 하지 않다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in \\mathbb{R} \\right\\}\\)\n\n이것은 좀 불합리해 보이는데 위의 신호는 주기신호라서 한 주기의 패턴만 분석하면 될것 같이 보이기 때문이다. 위의 신호는 intergrable 하지않지만 아래의 신호는 intergrable 하다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in (0,2\\pi) \\right\\}\\)\n\n우리는 이런신호까지 분석하기로 한다. 이런신호를 분석할 수 있는 이유는 해석학 교재를 참고하면 된다.2\n- 아무튼 우리는 (1) 인피니티-레인지에서 정의되는 가지는 신호 (2) 인피니티-레인지에서 적분값이 잘 정의되는 신호, 혹은 한 주기만 적분해 보았을때 그 값이 잘 정의되는 주기신호 를 타겟팅해 분석한다. 즉 분석하는 신호는 구체적으로 아래의 4가지이다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt <\\infty,~ t \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt <\\infty, ~ , x(t)=x(t+\\zeta), ~ t \\in \\mathbb{R}, \\right\\}.\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| <\\infty,~ i \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| <\\infty,~ i, x_i=x_{i+\\xi},~ i \\in \\mathbb{Z} \\right\\}.\\)\n\n- 표현들을 정리하기에 앞서서 몇 가지 알아두어야 할 사항이 있다. (1) 시간축에서 연속인 신호는 주파수측에서는 비주기신호가 나온다. (2) 시간축에서 디스크릿한 신호는 주파수측에서는 주기신호이다. (3) 시간축에서 주기인 신호는 주파수에서는 디스크릿하다. (4) 시간축에서 비주기신호는 주파수에서 연속이다. 이 사실들을 종합하면 각각의 경우에 해당하는 퓨리에 표현들은 아래와 같은 특징을 가지고 있음을 알 수 있다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt <\\infty,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{-\\infty}^{\\infty} |\\hat x(\\omega)| d\\omega <\\infty,~ \\omega \\in \\mathbb{R} \\right\\}\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt <\\infty,~ x(t)=x(t+\\zeta),~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=-\\infty}^{\\infty} |\\hat x_k| <\\infty,~ k \\in \\mathbb{Z} \\right\\}\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| <\\infty,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{0}^{2\\pi} |\\hat x(\\omega)| d\\omega <\\infty,~ \\hat x(\\omega)=\\hat x(\\omega+2\\pi),~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| <\\infty,~ x_i=x_{i+\\xi},~ i \\in \\mathbb{R} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=0}^{\\xi-1} |\\hat x_k| <\\infty,~\\hat x_k = \\hat x_{k+\\xi} ,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 여기에서 \\(\\zeta\\)는 (시간축에서) 연속신호의 주기라고 정의하고 \\(\\xi\\)는 (시간축에서) 이산신호의 주기라고 약속하자. 주파수영역이 디스크릿하게 나오면 FS라고 부르고 주파수영역이 컨티뉴어스하게 나오면 FT라고 부른다. 특이한점은 비주기-이산신호에 대한 FS \\(\\hat x(\\omega)\\)는 주파수 영역에서 주기가 \\(2\\pi\\)임을 파악할 수 있다. 이유는 궁금해하지말자. (내생각에 그냥 \\(\\omega\\)를 적당히 스케일링하여 주기를 \\(2\\pi\\)로 맞췄을 거다.)\n- 이제 짜증나는 적분가능조건따위는 버리도록 하자. 대신에 각 경우에 퓨리에변환(혹은series)과 그 역이 어떻게 정의되는지 알아보자. 그리고 외우자. 각 신호가 어떠한 도메인에서 정의되는지만 잘 파악하면 의외로 외우기 쉽다.\ncase 1. 연속-비주기\n\n\\(\\left\\{x(t): x(t)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\int_{-\\infty}^{\\infty} x(t)e^{-j\\omega t} dt,~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\sum_{k=-\\infty}^{\\infty} \\hat x_k e^{j \\frac{2\\pi}{\\zeta} t} ,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 3. 이산-비주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega, ~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\sum_{i=-\\infty}^{\\infty}x_ie^{-j\\omega i}, ~\\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\sum_{k=0}^{\\xi-1} \\hat x_k e^{-j\\frac{2\\pi k}{\\xi}i},~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i},~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주기함수는 (그것이 이산이든 연속이든) 주파수영역에서의 값이 디스크릿하다. 즉 위에서 case2와 case4인 경우는 주파수영역에서 값이 디스크릿하다. 이것을 연속함수인것처럼 바꿔보면 아래와 같이 쓸 수 있다.\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\zeta}\\right), ~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega,,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i}\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\xi}\\right),~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주목할것은 주기가 \\(\\zeta\\) 혹은 \\(\\xi\\) 인 함수의 주파수 응답은 오로지\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\zeta}, k \\in \\mathbb{Z}\\right\\}\\)\n\n혹은\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\xi}, k \\in \\mathbb{Z}\\right\\}\\)\n\n에서만 존재한다는 점이다. 또한 이산신호의 경우 \\(x_i\\)의 주기가 \\(\\xi\\) 이면 \\(\\hat{x}(\\omega)\\)의 주기역시 \\(\\xi\\) 라는점 역시 주목할만한 부분이다.\n- 주파수영역에서 디스크릿한 함수를 연속인것처럼 표현했듯이 시간영역에서 디스크릿한 함수 역시 연속인것처럼 표현할 수 있다. 예를들면 \\(\\{x_i: x_i=x(iT),~i \\in \\mathbb{Z}\\}\\) 와 같은 관계가 있는 경우 아래와 같이 표현 가능하다.\n\\[x_{\\delta}(t)=\\sum_{i=-\\infty}^{\\infty}x_i\\delta(t-iT).\\]\n이거 엄청 중요하다.\n\n\n\n\n\nFootnotes\n\n\n나도 그렇다.↩︎\n사실 나도 잘 모름 (뭐 quotient group이런거 알아야 하는데 공부하려면 꽤 걸릴듯)↩︎"
  },
  {
    "objectID": "posts/1_Essays/2023-02-14-수통문제-지수분포평균검정.html",
    "href": "posts/1_Essays/2023-02-14-수통문제-지수분포평균검정.html",
    "title": "[Essays] 수리문제: 지수분포 평균검정",
    "section": "",
    "text": "using Distributions, Plots\n\n다음의 분포를 따르는\n\n\\(f(x; \\theta) = \\theta \\exp (-x\\theta )I(x>0)\\)\n\n모집단으로부터 랜덤표본 \\(X_1,\\dots,X_n\\)을 이용하여 \\(\\theta\\)에 대한 신뢰구간을 구하고, 다음 가설\n\n\\(H_0: \\theta = \\theta_0\\) vs \\(H_1:\\theta \\neq \\theta_0\\)\n\n을 검정하고자 한다. 다음에 답하라.\n(a) \\(\\theta\\)에 대한 적절한 추축변량을 구하고, 해당 추축변량의 분포를 명시하라.\n(풀이)\n추축변량은 \\(2n\\bar{X}\\theta\\) 이며 분포는 자유도가 \\(2n\\)인 카이제곱분포를 따름. 왜냐하면\n\n\\(X_1 \\sim \\text{Exp}\\) with mean \\(1/\\theta\\)\n\\(2X_1\\theta \\sim \\text{Exp}\\) with mean \\(2\\)\n\\(2n\\bar{X}\\theta \\sim \\chi^2(2n)\\) (왜? 평균이2인 지수분포를 \\(n\\)번 더하면 자유도가 \\(2n\\)인 카이제곱분포가 되므로. –> 참고)\n\n이기 떄문에.\n(시뮬레이션)\n\nn = 10\nθ = 2\npivotal_variable = [mean(rand(Exponential(1/2),n))*2*n*θ for i in 1:140000]\n# pivotal_variable = 추축변량\n\n140000-element Vector{Float64}:\n 19.866862184254465\n 17.492982610528998\n 13.463211578260314\n 19.472994478042697\n 13.424495255444352\n 21.744281289967418\n 22.929701779207697\n 13.561641745647645\n 25.277641996704048\n 16.566359730027052\n 19.550767074240074\n  9.914621718787568\n 11.857966801675106\n  ⋮\n 38.05226144210052\n 17.120691971399836\n 21.66831336505733\n 26.476356317147957\n 21.930886178263727\n 18.33570158249295\n 22.39822061516168\n 27.42899896779973\n 14.770009603430982\n 13.776401174299512\n 21.511825086436673\n 21.867878699644073\n\n\n\nhistogram(pivotal_variable)\nhistogram!(rand(Chisq(2*n),140000))\n\n\n\n\n\n추축변량은 자유도 \\(2n\\)인 카이제곱분포를 따름\n\n(b) \\(\\theta\\)에 대한 95% 신뢰구간을 구하라.\n(풀이)\n\\(P\\left(\\chi^2_{0.025}(2n) \\leq 2n\\bar{X}\\theta \\leq \\chi^2_{0.975}(2n) \\right) =0.95\\)\nThus the CI of \\(\\theta\\): \\(\\left(\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}},\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}} \\right)\\)\n(시뮬레이션)\n\nl = quantile(Chisq(2*n),0.025) ./ (pivotal_variable./θ)\nu = quantile(Chisq(2*n),0.975) ./ (pivotal_variable./θ)\n[l[i]<θ<u[i] for i in 1:140000] |> mean\n\n0.95035\n\n\n(c) \\(P(X>1)\\)에 대한 95% 신뢰구간\n(풀이)\n\\(P(X>1) = \\int_{1}^{\\infty}\\theta \\exp(-x\\theta)dx =\\left[e^{-x\\theta} \\right]_1^{\\infty}=e^{-\\theta}\\)\nThus the CI of \\(P(X>1)\\): \\(\\left(\\exp\\big(-\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}}\\big),\\exp\\big(-\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}}\\big) \\right)\\)\n(시뮬레이션)\n검토1: \\(P(X>1)=E[I(X>1)]=e^{-\\theta}\\) 임을 검토\n\nexp(-θ)\n\n0.1353352832366127\n\n\n\nmean(rand(Exponential(1/2),1000000) .> 1)\n\n0.135678\n\n\n검토2: 신뢰구간\n\nu2 = exp.(-l)\nl2 = exp.(-u)\n[l2[i]<exp(-θ)<u2[i] for i in 1:140000] |> mean\n\n0.95035\n\n\n(d)\n\\(\\Omega = \\{\\theta: \\theta>0\\}\\), \\(\\Omega_0 =\\{2\\}\\)\n(e) \\(\\theta\\)의 가능도 함수를 기술하시오\n(풀이)\n\\(L(\\theta)=\\theta^n\\exp(-\\theta n\\bar{x})\\)\n(f) \\(\\theta\\)의 \\(\\Omega\\)에서의 최대가능도 추정량과 \\(\\Omega_0\\)에서의 최대가능도 추정량을 구하시오\n(풀이)\n\\(\\hat{\\theta}^{\\Omega}=1/\\bar{x}\\), \\(\\hat{\\theta}^{\\Omega_0}=\\theta_0\\)\n(g) 일반화 가능도 비1 \\(\\Lambda\\)를 구하시오.\n(풀이)\n\\(\\frac{L\\big(\\hat{\\theta}^{\\Omega}\\big)}{L\\big(\\hat{\\theta}^{\\Omega_0}\\big)}=\\frac{1/\\bar{x}^n\\exp(-n)}{\\theta_0^n\\exp(-\\theta_0n\\bar{x})}\\)\n(h) 유의수준 \\(\\alpha\\)의 가능도비 검정법의 기각역을 \\(\\chi^2\\)의 분위수를 사용하여 표현하시오.\n(풀이)\n\n\n\n\nFootnotes\n\n\n이게 뭐지..?↩︎"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html",
    "title": "[Essays] 해석학(1)",
    "section": "",
    "text": "이걸 증명하려면 자연수를 정의하는 페아노공리를 알아야함.\n그리고 그 공리로부터 유도되는 여러가지 자연수 성질중 하나인 자연수 집합은 위로 유계가 아니라는 사실을 알아야함.\n아르키메데스성질: For all \\(x \\in \\mathbb{R}\\) there exists \\(N \\in \\mathbb{N}\\) such that \\(x<N\\).\n\n아무 실수나 잡으면 그 숫자보다 큰 자연수가 존재한다는 의미\n아르키메데스 성질은 완비성공리로부터 증명가능함.\n\n아르키메데스 성질을 이용하면 임의의 \\(\\epsilon\\)보다 작은 \\(\\frac{1}{N}\\)을 항상 잡을수 있다. 즉 모든 \\(\\epsilon>0\\) 에 대하여 \\(0\\leq \\frac{1}{N} < \\epsilon\\) 을 만족하는 \\(N\\)이 항상 존재함을 알 수 있다.\n(예제) 아래를 증명하라.\n\\[\\lim_{n\\to \\infty}\\frac{4n}{2n+1}=2\\]\n(풀이) 모든 \\(\\epsilon>0\\)에 대하여 아래가 항상 성립함을 보이면 된다.\n\\[\\exists N \\in \\mathbb{N} \\quad \\text{such that} \\quad n \\geq N \\Rightarrow 0\\leq \\left |\\frac{4n}{2n+1}-2 \\right|<\\epsilon\\]\n그런데 \\(|\\frac{4n}{2n+1}-2|\\leq \\frac{1}{n}\\) 이므로 우리는 모든 \\(\\epsilon >0\\)에 대하여 아래가 성립함을 보이면 된다.\n\\[\\exists N \\in \\mathbb{N} \\quad \\text{such that} \\quad n \\geq N \\Rightarrow 0\\leq \\frac{1}{n} <\\epsilon\\]\n그런데 \\(n\\geq N\\)이라는 조건하에서는 \\(\\frac{1}{n} \\leq \\frac{1}{N}\\) 이므로 우리는 다시 모든 \\(\\epsilon>0\\)에 대하여 아래가 성립함을 보이면 된다.\n\\[\\exists N \\in \\mathbb{N} \\quad \\text{such that} \\quad 0 \\leq \\frac{1}{N} <\\epsilon\\]\n이러한 \\(N\\)은 아르키메데스의 성질에 의하여 항상 존재한다."
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#체-공리",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#체-공리",
    "title": "[Essays] 해석학(1)",
    "section": "체 공리",
    "text": "체 공리\n- 실수는 더하기와 곱셈이라는 연산이 합리적으로 정리되는 집합이라는 의미"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#순서-공리",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#순서-공리",
    "title": "[Essays] 해석학(1)",
    "section": "순서 공리",
    "text": "순서 공리\n- 순서공리에 의하여 부등식이 정의됨\n- 3분성질"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#디리클레함수",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#디리클레함수",
    "title": "[Essays] 해석학(1)",
    "section": "디리클레함수",
    "text": "디리클레함수\n모든 점에서 불연속인 함수임"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#thomae-함수",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#thomae-함수",
    "title": "[Essays] 해석학(1)",
    "section": "Thomae 함수",
    "text": "Thomae 함수\n유리수에서는 불연속, 무리수에서는 연속"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#합성함수의-연속",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#합성함수의-연속",
    "title": "[Essays] 해석학(1)",
    "section": "합성함수의 연속",
    "text": "합성함수의 연속\n함수 \\(f(x)\\)와 \\(g(x)\\)가 모두 \\(x=c\\)에서 연속이면 함수 \\((f\\circ g)(x)\\) 혹은 \\((g \\circ f)(x)\\)도 \\(x=c\\)에서 연속이다."
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#중간값정리-leftrightarrow-완비성공리",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#중간값정리-leftrightarrow-완비성공리",
    "title": "[Essays] 해석학(1)",
    "section": "중간값정리 \\(\\Leftrightarrow\\) 완비성공리",
    "text": "중간값정리 \\(\\Leftrightarrow\\) 완비성공리"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속임을-판단하는-방법",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속임을-판단하는-방법",
    "title": "[Essays] 해석학(1)",
    "section": "균등연속임을 판단하는 방법",
    "text": "균등연속임을 판단하는 방법\n\n립쉬츠조건: 어떠한 함수 \\(f\\)가 \\(|f(x)-f(u)|\\leq K|x-u|\\)를 만족하면 이러한 함수는 균등연속이다. \\(\\delta=\\frac{\\epsilon}{K}\\) 로만 선택하면 되므로\n\\(f(x)\\)가 연속이라고하자. \\(f(x)\\)의 정의역이 유계폐구간이면 함수 \\(f\\)는 균등연속이라 주장할 수 있다. 만약에 \\(f(x)\\)의 정의역이 개구간이면 구간의 양 끝점에서의 극한이 존재할때 \\(f\\)를 균등연속이라고 주장할 수 있다.\n\\(f(x)\\)가 개구간에서 정의된 경우"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속이-아님을-판단하는-방법",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속이-아님을-판단하는-방법",
    "title": "[Essays] 해석학(1)",
    "section": "균등연속이 아님을 판단하는 방법",
    "text": "균등연속이 아님을 판단하는 방법\n\n어떠한 \\(\\epsilon_0>0\\)가 존재하여, 내가 \\(\\delta\\)를 어떻게 잡든 \\(|x-u|<\\delta\\) and \\(|f(x)-f(u)|\\geq\\epsilon_0\\) 를 만족하는 \\(x\\)와 \\(u\\)가 존재하면 균등연속이 아니다.\n\\(\\lim_{n\\to\\infty}(x_n-u_n)=0\\) 이지만 \\(|f(x_n)-f(u_n)| \\geq \\epsilon_0\\) 임을 확인하면 균등연속이 아니다. 즉 \\(x\\)축에서는 수렴하는 두개의 수열이 함수를 태우면 수렴하지 않는 경우"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속의-특징",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#균등연속의-특징",
    "title": "[Essays] 해석학(1)",
    "section": "균등연속의 특징",
    "text": "균등연속의 특징\n코시수열을 보존한다. 즉 \\(x_n\\)이 코시수열이고 함수 \\(f\\)가 균등연속이면 \\(f(x_n)\\)역시 코시수열이다.\n연속확장정리: 개구간에서의 균등연속을 사용하고 싶음. \\(f(x)\\)가 개구간 \\((a,b)\\)에서 연속이고 양 끝점, 즉 \\(a\\), \\(b\\)에서의 극한이 존재하면 \\(f(x)\\)는 균등연속이다.\n균등연속의 아이디어: x축에서 수렴하던 어떤애가 y축에서도 수렴했으면 좋겠음. (이게 원래 안되는 건데요, 균등연속일때는 가능합니다)"
  },
  {
    "objectID": "posts/1_Essays/2023-01-07-해석학(1).html#모티브",
    "href": "posts/1_Essays/2023-01-07-해석학(1).html#모티브",
    "title": "[Essays] 해석학(1)",
    "section": "모티브",
    "text": "모티브\n아래와 같은 합성함수의 미분을 생각하여 보자.4\n\\[(f\\circ g)'(c) = \\lim_{x\\to c}\\frac{f(g(x))-f(g(c))}{x-c}\\]\n(풀이1)\n고등학교 수준에서는 이것을 아래와 같이 쓸 수 있다.\n\\[\\lim_{x\\to c}\\frac{f(g(x))-f(g(c))}{x-c}=\\lim_{x\\to c}\\left(\\frac{f(g(x))-f(g(c))}{g(x)-g(c)}\\frac{g(x)-g(c)}{x-c}\\right)=f'\\big(g(c)\\big)\\cdot g'(c)\\]\n(아쉬움)\n이 증명의 단점은 \\(g(x)-g(c)\\neq 0\\)이라는 조건이 필요하다는 것이다. 이 조건을 제외할 수는 없을까? 그리스 수학자인 카라테오도리(Caratheodory)는 아래와 같은 명제를 발견하였다.\n(명제) \\(f(x)\\) 가 \\(x=c\\) 에서 미분가능하다 \\(\\Longleftrightarrow\\) 적당한 연속함수 \\(\\varphi(x)\\) 가 존재하여 (i) \\(\\varphi(x)\\) 는 \\(x=c\\) 에서 미분가능하고 (ii) \\(f(x)-f(c)=\\varphi(c)(x-c)\\) 이다.\n이것을 이용하면 합성함수의 미분을 다시 풀어보자.\n(풀이2)\n먼저 \\(g(x)\\) 는 \\(x=c\\) 에서 미분가능하다라는 조건은 아래와 같이 표현할 수 있다.\n\n\\(g(x)\\) 가 \\(x=c\\) 에서 미분가능하다 \\(\\Leftrightarrow\\) \\(g(x)-g(c)=\\varphi(c)(x-c)\\)\n\n또한 \\((f\\circ g)(x)=f(g(x))\\) 는 \\(x=g(c)\\) 에서 미분가능하다라는 조건은 아래와 같이 표현할 수 있다.\n\n\\((f\\circ g)(x)\\) 가 \\(x=g(c)\\) 에서 미분가능하다 \\(\\Leftrightarrow\\) \\(f(g(x))-f(g(c))=\\psi(g(c))(g(x)-g(c))\\)\n\n\n#"
  },
  {
    "objectID": "posts/1_Essays/2023-02-05-시계열의 주파수영역 분석.html",
    "href": "posts/1_Essays/2023-02-05-시계열의 주파수영역 분석.html",
    "title": "[Essays] 시계열의 주파수영역 분석",
    "section": "",
    "text": "이 포스트는 Wei (2006) 의 CHAPTER 12 Spectral Theory of Stationary Processes 를 요약한 것이다\n\n\\(Z_t\\)를 real-valued stationary process 라고 하자. 그리고 \\(\\gamma_k\\)를 \\(Z_t\\)의 autocovariance sequence 라고 하자. 만약에 \\(\\gamma_k\\) 가 absolutely summable 하다면1 아래와 같은 표현이 존재한다.\n\\[\nf(\\omega)=\\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_k e^{-i\\omega k}=\\frac{1}{2\\pi}\\gamma_0 + \\frac{1}{\\pi}\\sum_{k=1}^{\\infty}\\gamma_k\\cos(\\omega k)\n\\]\n첫 등호에서 두번째 등호로 넘어갈때 아래의 성질들이 이용되었다.\n\n\\(e^{i\\theta} = \\cos \\theta + i \\sin\\theta\\)\n\\(\\gamma_k = \\gamma_{-k}\\)\n\\(\\sin \\omega(-k) = - \\sin \\omega k\\)\n\\(\\cos \\omega(-k) = \\cos \\omega k\\)\n\n여기에서 \\(f(\\omega)\\)는 \\(\\gamma_k\\)를 퓨리에변환한 결과이다. \\(f(\\omega)\\)를 역 퓨리에변환하면 다시 \\(\\gamma_k\\)를 얻을 수 있는데 이를 수식으로 표현하면 아래와 같다.\n\\[\\gamma_k = \\int_{-\\pi}^{\\pi}f(\\omega) e^{i\\omega k}d\\omega\\]\n만약에 \\(\\gamma_k\\)가 absolutely summable 하지 않으면 \\(f(\\omega)\\)의 존재성을 장담할 수 없다. 따라서 위와 같은 표현은 불가능하다. 대신 아래와 같이 Fourier-Stieltjes integral 의 형으로는 표현가능하다.\n\\[\\gamma_k = \\int_{-\\pi}^{\\pi}e^{i\\omega k}dF(\\omega)\\]\n여기에서 \\(F(\\omega)\\)는 spectral distribution function 이라고 한다. \\(F(\\omega)\\)는 nondecreasing function 이다. \\(F(\\omega)\\)는 아래와 같은 3가지 컴포넌트의 합으로 표현할 수 있다.\n\n유한개의 jump만을 가지는 step function\nabsolutely continuous function\nsigular function\n\n여기에서 3은 대부분의 응용분야에서 중요하지 않으므로 결국 \\(F(\\omega)\\)는 아래와 같이 표현할 수 있다.\n\\[F(\\omega) \\approx F_s(\\omega) + F_c(\\omega)\\]\n여기에서 \\(F_s(\\omega)\\)는 유한개의 jump를 가지는 step function 이고 \\(F_c(\\omega)\\)는 absolutely continuous 한 함수이다.\n만약에 시계열이 absolutely summable 하다면 \\(F(\\omega) = F_c(\\omega)\\) 이며 이경우에는 \\(dF(\\omega) = f(\\omega)d\\omega\\) 로 표현할 수 있다.\n여기에서 step spectral distribution function에 대하여 설명하기 위하여 아래와 같은 모형2를 고려하자.\n\\[Z_t = \\sum_{i=1}^{M}A_i \\sin(\\omega_i t + \\Theta_i)\\]\n여기에서 \\(A_i\\)는 constant 이며 \\(\\Theta_i \\sim U(-\\pi,\\pi)\\) 이다.\n여기에서 \\(Z_t\\)는 stationary process 이다.\n일반적으로 stationary process는 아래와 같이 표현할 수 있다.\n\\[Z_t = A \\sin\\left(\\frac{2\\pi t}{12}+\\lambda \\right)+\\left[\\frac{\\theta(B)}{\\phi(B)}\\right]a_t\\]\n여기에서 \\(A\\)는 상수이며 \\(\\lambda\\)는 uniform \\([-\\pi,\\pi]\\)을 따르는 랜덤변수이다.\nWold (1938)는 임의의 covariance stationary process 는 아래와 같이 표현 할 수 있다는 것을 알아냈다.\n\\[Z_t = Z_t^{(d)} + Z_t^{(n)}\\]\n여기에서 \\(Z_t^{(d)}\\)는 purely deterministic component 이고 \\(Z_t^{(n)}\\)는 purely nondeterministic component 이다.\n이 분해는 아래의 수식에서 다룬 바 있던 spectral distribution function 의 decomposition 과 유사하다.\n\\[F(\\omega) \\approx F_s(\\omega) + F_c(\\omega)\\]\n왜냐하면 \\(Z_t^{(d)}\\)는 step spectral distribution 을 가지고 \\(Z_t^{(n)}\\)은 absolutely continuous spectral distribution 을 가진다. 따라서 \\(Z_t^{(d)}\\)를\n\n\n\n\n\nReferences\n\nWei, William WS. 2006. “Time Series Analysis: Univariate and Multivariate.” Methods. Boston, MA: Pearson Addison Wesley.\n\nFootnotes\n\n\n\\(\\sum_{k=-\\infty}^{\\infty}|\\gamma_k| <\\infty\\) 이라는 의미↩︎\ngeneral linear cyclical model 이라고 하는듯↩︎"
  },
  {
    "objectID": "posts/1_Essays/2023-01-11-여러가지 부등식.html",
    "href": "posts/1_Essays/2023-01-11-여러가지 부등식.html",
    "title": "[Essays] 여러가지 부등식",
    "section": "",
    "text": "여러가지 부등식 I\n\n(베르누이 부등식) \\(\\forall n \\in N, h>-1 \\Rightarrow (1+h)^n \\geq 1+nh\\)"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html",
    "href": "posts/1_Essays/2023-01-20-추정.html",
    "title": "[Essays] 추정",
    "section": "",
    "text": "using Distributions, Plots"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "href": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "title": "[Essays] 추정",
    "section": "MLE의 일치성에 대한 구체적인 논의",
    "text": "MLE의 일치성에 대한 구체적인 논의\n\\(X_1,\\dots,X_{10} \\overset{i.i.d.}{\\sim} Ber(\\theta)\\) 이라고 하자.\n\nx = rand(Bernoulli(0.3),10)\nx\n\n10-element Vector{Bool}:\n 0\n 0\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 0\n\n\n여기에서 \\(\\theta\\)는 추정해야할 미지의 모수이지만 우리는 시뮬레이션의 편의상 \\(\\theta\\)의 참값을 \\(\\theta_0=\\frac{1}{3}\\)로 알고 있다고 하자. MLE를 논의함에 있어 핵심적인 역할을 하는 것은 \\(Y_1=\\log f(X_1;\\theta)\\)이다. 아래는 \\(Y_1\\)에 대한 몇가지 코멘트이다.\n(1) \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n우선 \\(X_1\\)의 함수이므로 \\(Y_1\\)역시 확률변수이다. 따라서 \\(Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\\(Y_1\\)은 \\(\\theta\\)에 대한 함수이므로 \\(\\theta\\)에 대하여 미분할 수 있다.\n\n(베르누이 예제)\n우리의 베르누이 예제에서 \\(Y_1\\)은 아래와 같이 계산된다.\n\\[Y_1 = \\log f(X_1;\\theta)= X_1 \\log \\theta + (1-X_1)\\log(1-\\theta)\\]\n보는 것 처럼 \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수임\n(2) \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta\\) 만의 함수이다. 적당한 조건4이 만족된다면 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta_0\\) 에서 최대화 된다.\n(베르누이 예제)\n\\(\\mathbb{E}_{\\theta_0}(Y_1) = \\mathbb{E}_{\\theta_0}(X_1)\\log\\theta + (1-\\mathbb{E}_{\\theta_0}(X_1))\\log(1-\\theta) = \\frac{1}{3} \\log\\theta + (1-\\frac{1}{3})\\log(1-\\theta)\\)\n\n일반적인 상황에서는 참모수를 모르지만 우리는 시뮬레이션을 \\(\\theta=1/3\\)에서 하였으므로 참모수 \\(\\theta_0=\\mathbb{E}_{\\theta_0}(X_1)=\\frac{1}{3}\\)을 알고 있다고 가정한다.\n\n\nplot(θ -> (1/3)*log(θ) + (1-1/3)*log(1-θ)) \n\n\n\n\n보는것처럼 이 함수 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta=\\theta_0=\\frac{1}{3}\\) 에서 최대값을 가진다.\n(3) \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n따라서 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 확률변수이고 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\n(베르누이 예제)\n\\(\\frac{\\partial}{\\partial\\theta}Y_1 = X_1\\frac{1}{\\theta} + (1-X_1)\\frac{-1}{1-\\theta}\\)\n(4) \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial \\theta}Y_1]=0\\) 이다.\n(베르누이 예제)\n\\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1] = \\theta\\frac{1}{\\theta} + (1-\\theta)\\frac{-1}{1-\\theta}=0\\)\n(5) \\(\\mathbb{V}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=\\mathbb{E}_{\\theta}[-\\frac{\\partial^2}{\\partial \\theta^2}Y_1]=I(\\theta)\\)\n(베르누이 예제)\n\\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]=\\mathbb{E}_{\\theta}\\big[(\\frac{\\partial}{\\partial\\theta}Y_1)^2\\big]=\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]=\\frac{1}{\\theta(1-\\theta)}\\)\n\n두번째 등호는 \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=0\\)을 이용하여 증명가능하다.\n언뜻 보면 \\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]\\)를 계산하는 것이 \\(\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]\\)를 계산하는것보다 훨씬 쉬워보인다. 그런데 \\(X_1\\)와 \\(1-X_1\\)이 독립이 아니라서 \\(\\mathbb{V}(X+Y)=\\mathbb{V}(X)+\\mathbb{V}(V)+2\\text{Cov}(X,Y)\\)와 같이 공분산 term을 계산해야 하므로 계산이 까다롭다.\n\n\n베르누이에 대한 피셔정보량은 https://en.wikipedia.org/wiki/Fisher_information 에서 확인할 수 있음"
  },
  {
    "objectID": "posts/1_Essays/2022-01-12-토폴로지(1).html",
    "href": "posts/1_Essays/2022-01-12-토폴로지(1).html",
    "title": "[Essays] 토폴로지(1)",
    "section": "",
    "text": "About this doc\n- 수학공부\n- 학부수준\n- 이 문서는 논문을 읽을때 등장하는 topology 용어들을 좀더 명확하게 이해하고 싶어서 작성하였다. 가볍게 정의만 훑어보는 것이라 깊게 들어가지는 않을 예정이다. 교재는 Schaum’s General Topology 를 참고하였다. - Lipschutz, S. (1965). Schaum’s outline of theory and problems of general topology. Schaum’s Outline Series.\n- 여기에서는 토폴로지의 정의와 메트릭스페이스의 정의 그리고 컴플리션의 정의에 대하여 다룬다.\n\n\n토폴로지\n- \\({\\cal T}\\) 가 \\(X\\) 의 subset 으로 이루어진 collection 이라고 하자. \\({\\cal T}\\) 가 \\(X\\) 를 포함하며 uncountable union 에 닫혀있고 finite intersection 에 닫혀있다면 \\({\\cal T}\\) 를 \\(X\\) 의 topology 라고 한다. 그리고 \\((X,{\\cal T})\\) 를 topological space 라고 한다.\n- \\({\\cal T}\\) 가 \\(X\\)의 토플로지일때 \\({\\cal T}\\) 의 원소를 \\({\\cal T}\\)-open set 이라고 한다. 따라서 원래 오픈셋은 마치 확률변수처럼 단독으로 정의할 수 없고 어떠한 토폴로지 \\({\\cal T}\\)와 같이 정의된다.\n- 아래와 같은 collection 을 생각하자.\n\\[{\\cal O}:=\\{O: O=\\cup_i(a_i,b_i), a_i,b_i \\in \\mathbb{R} \\} \\]\n컬렉션 \\({\\cal O}\\) 는 \\(\\mathbb{R}\\) 의 토폴로지가 된다. (증명은 알아서..) 이러한 토폴로지(=오픈인터벌의 카운터블-유니온으로 표현가능한 집합들의 모임)을 특별히 usual topology 라고 한다. 그리고 이 토폴로지의 원소를 \\({\\cal O}\\)-오픈셋이라고 부른다. 따라서 어떤 집합 \\(O\\) 가 \\({\\cal O}\\)-오픈셋 이라는 말은 그 집합이 오픈인터벌의 카운터블-유니온으로 표현가능한 집합임을 의미한다.\n- 참고로 \\({\\cal O}\\) 가 우리가 일반적으로 생각하는 ‘오픈셋들의 모임’ 이고 \\({\\cal O}\\)-오픈셋이 보통 우리가 일반적으로 유클리드 공간에서 상상하는 오픈셋이다. 그래서 앞으로 특별한 언급없이 그냥 ‘오픈셋’ 이라고 부르면 토폴로지 \\((\\mathbb{R},{\\cal O})\\) 에서 정의가능한 ‘\\({\\cal O}\\)-오픈셋’ 을 의미하는 것이라고 생각하면 된다.\n- 즉 우리가 일반적으로 생각하는 오픈셋1은 오픈인터벌 \\((a,b)\\) 의 countable-many union 으로 표현가능한 집합이라고 이해해도 된다.\n- 오픈셋 \\(O\\)의 원소를 interior point of \\(O\\) 라고 한다. \\({\\cal O}\\)의 정의에 의해서 인테리어포인트는 모두 아래의 성질을 만족한다.\n\\[\\forall o \\in O ~ \\exists a,b \\in \\mathbb{R}~ st.~  o \\in (a,b)\\]\n증명은 귀류법을 쓰면 쉽게 된다.\n- 저 정리가 생각보다 중요하다. 그리고 이 정리를 나이테정리 라고 기억하자. 이 정리는 \\({\\cal O}\\)-오픈셋이 아닌 일반적인 \\({\\cal T}\\)-오픈셋에 대하여서도 성립한다. 즉 \\((X,{\\cal T})\\)가 위상공간이고 \\(T\\)가 \\({\\cal T}\\)의 임의의 집합이라 하자. \\(T\\)의 임의의 원소 \\(p\\)에 대하여 (1) \\(p\\) 를 포함하지만 (2) \\(T\\) 보다 작은 다른 \\({\\cal T}\\)-오픈셋이 항상 존재한다.\n- 그리고 교재에 따라서는 위와 같은 성질을 만족하는 것을 오픈셋이라고 정의하기도 한다. 이와 같은 논리흐름으로는 오픈인터벌 \\((a,b)\\)를 정의하고 그로부터 인테리어포인트 \\(o\\)와 오픈셋 \\(O\\)를 정의하고 그로부터 토폴로지 \\({\\cal O}\\)를 정의할 수 있다. 하지만 이러한 방식의 contruction 으로는 \\((\\mathbb{R},{\\cal O})\\) 만 만들수있다. 일반적으로는 적당한 \\({\\cal T}\\)가 \\(X\\)의 토폴로지임을 밝히고 그로부터 오픈셋을 정의하고 그 다음 인테리어포인트를 정의하는 식으로 각 요소들을 contruction 한다.\n- 오픈셋의 여집합을 클로즈드셋이라고 한다. 여기서 사람들이 “모든 집합은 오픈셋이거나 클로즈드셋 이어야 한다” 라고 착각하기 쉬운데 사실 그런것은 아니다.\n- 어떠한 construction을 사용하든지 아래의 사실들이 성립한다. 따로 설명을 쓰지 않은 것은 아주 약간의 머리를 쓰면 쉽게 증명할 수 있는 것들이다. (하지만 그냥 받아들이거나 외우는 것이 편하다.) 참고로 아래의 모든 사실들은 보통위상공간 즉 \\((\\mathbb{R},{\\cal O})\\) 를 전제하고 서술한 것이다.\n(1) \\((a,b)\\) 는 오픈셋이다.\n(2) \\(\\mathbb{R}\\) 은 오픈셋이다. 동시에 클로즈드셋이다2.\n(3) \\(\\emptyset\\) 은 오픈셋이다. 동시에 클로즈드셋이다3.\n(4) 오픈셋은 uncountable union 에 닫혀있다. 즉 \\(O_t\\)가 각각 오픈셋일때 \\(\\cup_t O_t\\) 역시 오픈셋이다.\n(5) 오픈셋은 finite intersection 에 닫혀있다. 즉 \\(O_i\\)가 각각 오픈셋이면 \\(\\cap_{i=1}^{n} O_i\\) 역시 오픈셋이다\n(6) 한점 \\(p\\)로 이루어진 집합 \\(\\{p\\}\\)는 오픈셋이 아니다. 이것이 오픈셋이 되려면 \\(\\{p\\}\\)의 모든원소(라고 해봤자 \\(p\\) 밖에 없음)가 내점이어야 하고 \\(p\\)가 \\(\\{p\\}\\)의 내점이려면 \\(p\\)를 포함하는 오픈인터벌 \\((a,b)\\)가 \\(\\{p\\}\\)의 부분집합으로 존재해야하는데 이것이 불가능하기 때문이다. 4\n(7) 오픈셋의 countable-many intersection 은 오픈셋이 아니다. 왜냐하면 \\(\\cap_{n=1}^{\\infty}(-1/n,1/n)=\\{0\\}\\) 인데 \\(\\{0\\}\\)은 오픈셋이 아니기 때문이다.\n\n\n기저\n- 오픈인터벌 \\((a,b)\\)를 적당히 countable-many union 하면 \\(\\mathbb{R}\\) 에 존재하는 어떠한 오픈셋 \\(O\\)도 표현할 수 있다. 이럴때 \\((a,b)\\) 모아놓은 collection \\({\\cal B}:=\\{(a,b): a<b \\in \\mathbb{R}\\}\\) 를 토폴로지 \\({\\cal O}\\)의 base 라고 한다. 이처럼 어떠한 위상공간 \\((X,{\\cal T})\\) 가 있을때 토폴로지 \\({\\cal T}\\) 의 임의의 집합을 \\({\\cal B}\\)의 원소들의 uncountable union 으로 표현가능때 \\({\\cal B}\\)를 \\({\\cal T}\\)의 base 라고 한다. 그리고 추가적으로 base의 모든 원소는 \\({\\cal T}\\)-오픈셋이어야 한다는 조건도 포함된다.\n- 토폴로지 \\({\\cal T}\\)의 base는 유일하지 않다.\n- 아래와 같은 collection을 상상하여 보자.\n\\[\\tilde{\\cal B}:=\\{all~ ray~ in~\\mathbb{R} \\} := \\{(-\\infty,b): b\\in \\mathbb{R} \\} \\cup \\{(a,\\infty): a \\in \\mathbb{R}\\}\\]\n보는 것처럼 \\(\\tilde{\\cal B}\\)는 위상의 정의를 만족한다. 그리고 \\(\\tilde{\\cal B}\\)는 \\({\\cal O}\\)의 base가 아니다. 하지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\((a,b)\\)를 포함하고 있기에 \\({\\cal O}\\) 의 base가 된다. 여기에서 \\(\\pi(\\tilde{\\cal B})\\) 는 \\(\\tilde{\\cal B}\\) 에 의해서 생성된 가장 작은 \\(\\pi\\)-system 이다.\n- 참고로 \\(\\pi\\)-시스템은 모든 원소가 finite intersection 에 닫혀있는 collection 을 의미한다. 전체집합은 empty intersection 으로 해석할 수 있으므로 모든 파이시스템은 전체집합을 포함한다. 따라서 파이시스템을 정의하면 전체집합을 같이 정의하는것과 마찬가지이다. 따라서 파이시스템 역시 시그마필드와 토폴로지처럼 전체집합과 동시에 정의된다. 그리고 정의에 따라서 임의의 집합에 대한 토폴로지와 시그마필드 모두 파이시스템이 된다.\n- 위에서 예를 든 \\(\\tilde{\\cal B}\\) 와 같이 그것 자체가 어떤 위상 \\({\\cal T}\\) 의 base는 아니지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\({\\cal T}\\) 의 base가 될때 \\(\\tilde{\\cal B}\\)를 \\({\\cal T}\\)의 subbase 라고 한다.\n- \\(\\tilde{\\cal B}\\) 가 토폴로지 \\({\\cal T}\\)의 subbase이면 \\(\\tilde{\\cal B}\\)로 \\({\\cal T}\\)를 generate 할 수 있다.\n\n\nmetric space\n- \\(d:X \\times X \\to \\mathbb{R}\\) 가 (1) 음이 아니고 (2) 대칭이며 (3) 삼각부등식을 만족하면 집합 \\(X\\) 에서의 metric 이라고 한다. 이때 음이 아닐 조건은\n\\[\\begin{cases}\nd(a,b) > 0 & a \\neq b \\\\\nd(a,b) = 0 & a=b\n\\end{cases}\\]\n이다. 만약에 메트릭의 모든 조건을 만족하는데 \\(d(a,b)=0\\) 인 서로 다른 \\(a,b \\in X\\) 가 존재하는 경우 \\(d\\) 를 pseudometric 이라고 한다.\n- \\(d\\) 을 집합 \\(X\\) 에서의 메트릭이라고 하자. 메트릭이 존재한다는 것은 집합 \\(X\\)의 어떠한 두 원소라도 그 사이의 거리를 잴 수 있다는 말이고 그것은 집합 \\(X\\)의 임의의 점 \\(p\\)에서 아래와 같은 ball 을 정의할 수 있는 말이다.\n\\[S(p,\\delta) := \\{x:d(p,x)<\\delta,x \\in X \\}\\]\n참고로 위와 같은 ball 들을 모은 collection 을 \\({\\cal B}\\)라고 하자. 그리고 \\({\\cal B}\\)의 임의의 원소를 언카운터블-유니온하여 얻을 수 있는 집합들의 모임을 \\({\\cal T}\\)라고 하자. 그러면 (1) \\({\\cal T}\\) 가 \\(X\\) 의 토폴로지임을 보이고 (2) \\({\\cal B}\\)의 모든 원소가 \\({\\cal T}\\)-오픈셋임을 보인다면 \\({\\cal B}\\)는 \\({\\cal T}\\)의 base가 된다고 주장할 수 있다(Thm 8.4). 그런데 (2)는 (1)이 성립하면 자동으로 성립하므로 (1)만 보이면 된다. 그러기 위해서는 아래의 (i)-(iii)을 보이면 된다.\n(i) 우선 \\({\\cal T}\\)가 언카운터블-유니온에 닫혀있음은 associative laws 에 의해서 쉽게 증명된다.\n(ii) 이제 \\({\\cal T}\\)가 파이나이트-인터섹션에 닫혀있음을 보이자. \\({\\cal T}\\)의 임의의 두 원소는 각각 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다. 가령 예를들어 임의의 \\(T,S \\in {\\cal T}\\) 가 아래와 같이 표현되었다고 치자.\n\\[T=\\bigcup_{t\\in [0,1]}B_{t}, \\quad S=\\bigcup_{s\\in [2,3]}B_{s}\\]\n따라서 \\(T\\cap S\\) 는 distributive laws 에 의해서 아래와 같이 표현가능하다.\n\\[T \\cap S = \\bigcup_{(t,s) \\in [0,1]\\times[2,3]} B_t \\cap B_s \\]\n(i)에 의해서 \\(B_t \\cap B_s\\)가 \\({\\cal T}\\)의 원소이기만 하면 \\(T \\cap S\\) 역시 \\({\\cal T}\\)의 원소가 되는 구조라 (ii)가 증명된다. 따라서 이제 우리가 할일은 \\(B_t\\cap B_s\\)가 \\({\\cal T}\\)의 원소임을 보이는 것이고 이것은 \\(B_t \\cap B_s\\)가 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다는 조건과 동치이다. 우선 \\(B_t \\cap B_s\\)에 속하는 임의의 원소를 $b^* $ 라고 하자. 이 점에 대하여 나이테정리를 만족시키는 ball이 존재한다. 즉\n\\[\\exists S(b^* ,\\delta)~ st. ~ S(b^* ,\\delta) \\subset B_t \\cap B_s\\]\n이다(Lemma 8.3). 그런데 \\(B_t \\cap B_s\\)의 모든점에서 이런식으로 나이테정리를 만족하는 ball을 잡을 수 있다. 이러한 ball들의 합집합을\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)\\]\n이라고 하자. 자명하게 이 집합은 \\(B_t\\cap B_s\\) 보다 작다(부분집합들의 합이므로). 하지만 \\(B_t\\cap B_s\\)의 모든 원소는 이 집합에 포함되므로 이 집합은 \\(B_t\\cap B_s\\)보다 크다. 따라서\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)=B_t \\cap B_s\\]\n이 성립한다.\n(iii) \\({\\cal T}\\)가 \\(X\\)를 포함한다는 것을 보이는것은 볼의 반지름을 크게 만들면 쉽게 증명할 수 있다.\n- 참고로 위의 (i)-(iii)을 요약하면 (1) \\(X\\)가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하고 (2) \\({\\cal B}\\)의 임의의 두 원소가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하기만 하면 볼들이 집합이 아니라 어떠한 \\({\\cal B}\\)라도 특정 토폴로지의 base라고 주장할 수 있다. 이것이 교재의 Thm 6.1 이다.\n- 아무튼 위의 과정을 거치면 \\(X\\)위에서 거리를 정의할 수 있을때 그 거리에 의해서 ball을 정의할 수 있고 ball들의 콜렉션을 base \\({\\cal B}\\)로 정의하고 \\({\\cal B}\\) 원소들의 언카운터블-유니온으로 표현가능한 집합모임을 토폴로지 \\({\\cal T}\\)로 정의해도 논리적모순점이 없다. 즉 \\(X\\)에서 메트릭이 정의되기만 하면 그것에 의해서 순차적으로 토폴로지 \\({\\cal T}\\)를 자연스럽게 유도할 수 있는데 이러한 토폴로지를 특별히 \\(X\\)와 \\(d\\)에 의해서 유도된 metric topology 라고 한다. 그리고 \\((X,d)\\)를 metric-space 라고 한다.\n- \\(\\mathbb{R}\\)에서 \\({\\cal O}\\)를 유도하는 메트릭은 우리가 보통 생각하는 유클리드거리이다. 이러한 메트릭을 usual metric 이라고 한다.\n- \\(\\mathbb{R}\\)에서 아래와 같은 거리를 정의할 수 있다.\n\\[d(a,b)=\\begin{cases}\n0 & a=b \\\\\n1 & a\\neq b\n\\end{cases}\\]\n이러한 거리를 trivial metric 이라고 한다. 그리고 이 거리가 유도하는 토폴로지는 \\(2^{\\mathbb{R}}\\) 이다. (아 몰라.. 따지기 싫어.. 그냥 외워..)\n- 만약에 집합 \\(X\\)에서 정의된 2개의 메트릭 \\(d_1\\), \\(d_2\\)가 같은 토폴로지를 유도한다면 두 메트릭 \\(d_1\\)과 \\(d_2\\)는 equivalent 하다고 말한다.\n- 토폴로지컬-스페이스 \\((X,{\\cal T})\\) 가 있다고 하자. 그런데 \\(X\\) 에서 어떠한 메트릭 \\(d\\)가 존재해 그것이 \\({\\cal T}\\)를 유도하였다고 하자. 그럼 \\({\\cal T}\\)는 메트릭-토폴로지가 된다. 이와 같이 (1) \\(X\\)에서 정의되고 (2) 메트릭-토폴로지 \\({\\cal T}\\)를 유도하는 적당한 메트릭 \\(d\\)가 명시된것은 아니지만 그런 메트릭의 존재를 하나 이상 우리가 알고 있을때 위상공간 \\((X,{\\cal T})\\)를 metrizable 하다고 한다.\n- 두 메트릭스페이스 \\((X,d_1)\\) 와 \\((Y,d_2)\\) 가 isometric 하다는 것은 아래가 만족하는 one-one, onto 인 \\(f:X \\to Y\\) 가 존재한다는 것이다.\n\\[d_1(p,q) = d_2(f(p),f(q))\\]\n- 이때 isometric 이라는 relation 은 보는것 처럼 모든 메트릭공간들의 집합 \\({\\cal M}\\)에서 equivalence relation 이다. 즉 아래가 성립한다.\n(i) \\((X,d_1) \\overset{ism}{\\sim} (X,d_1)\\),\n(ii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) implies \\((Y,d_2) \\overset{ism}{\\sim} (X,d_1)\\),\n(iii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) and \\((Y,d_2) \\overset{ism}{\\sim} (Z,d_3)\\) imply \\((X,d_1) \\overset{ism}{\\sim} (Z,d_3)\\).\n\n\ncomplete metric space\n- convergent sequence 은 단독으로 정의될 수 없으며 위상공간 \\((X,{\\cal T})\\) 와 묶어서 정의된다. 그리고 Cauchy sequence 역시 단독으로 정의될 수 없으며 메트릭스페이스 \\((X,d)\\) 와 묶어서 정의된다.\n- convergent sequence 와 Cauchy sequence 는 비슷해보이지만 미묘하게 다른점이 있다.\n(1) 컨버전트-시컨트는 위상공간 \\((X,{\\cal T})\\) 만 있으면 정의할 수 있지만 코시수열은 그 위상공간이 메트릭스페이스 이어야 한다는 제약이 있다. 왜냐하면 컨버전트-시컨스의 정의에는 오픈셋만 필요하지만 코시수열은 볼이 필요하고 볼은 메트릭에 의해서만 정의되기 때문이다.\n(2) 컨버전트-시컨스와 코시수열 모두 열의 각 항이 \\(X\\)의 원소이어야 한다는 조건이 있다. 하지만 컨버전트-시컨스는 그 limit 까지 \\(X\\)의 원소이어야 하는데 코시수열은 그렇지 않다는 차이점이 있다.\n- \\(X=(0,1)\\) 위의 usual metric 에 의해서 유도되는 메트릭스페이스 \\((X,d)\\) 를 생각하자. 수열\n\\[\\big\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{4},\\dots,\\big\\}\\]\n\\(X\\)에서 정의된 코시수열이지만 \\(X\\)에서 정의되는 컨버전트-시컨스는 아니다.\n- 내가 이해한 바는 아래와 같다.\n(1) 토폴로지 \\((X,{\\cal T})\\) 는 항상 컨버전트-시컨스를 준비가 되어있는 공간이다.\n(2) 위에서 정의가능한 컨버전트-시컨스는 코시수열과 아무런 관련이 없다. 그리고 우리가 통상적으로 고등학교때부터 다루어왔던 수열의 수렴의 개념과도 거리가 멀다.\n(3) 토폴로지 \\((X,{\\cal T})\\) 가 메트릭스페이스라면 컨버전트-시컨스는 코시수열과 어떤관계가 있으며 고등학교때부터 내가 다루어 왔던 상식적인 수렴하는 수열의 개념과도 관련이 있다.\n(4) \\((X,{\\cal T})\\) 가 메트릭스페이스 라고 가정하자. 그럼 아래가 만족한다고 생각할 수 있다.\n\n\\(\\{a_n\\}\\) converges on \\(X\\) \\(\\Longleftrightarrow\\) \\(\\{a_b\\}\\) is Cauchy sequence on \\(X\\) and \\(\\lim_{n\\to\\infty} a_n \\in X\\)\n\n즉 러프하게 말해서 \\(X\\)에서의 컨버전트-시컨스는 (i) \\(X\\)에서의 코시수열이면서 (ii) limit 이 \\(X\\)에 포함되는 수열이라고 말할 수 있다. 이런 정의로 치면 우리가 고등학교때부터 생각해왔던 소박한 정의의 수렴하는 수열은 사실 코시수열에 가깝고 컨버전트-시컨스는 고등학교때부터 배운 소박한 수렴을 하며 동시에 수렴값이 \\(X\\)이 잘 정의되는 수열을 의미한다고 볼 수 있다. 앞으로는 소박한 수렴과 컨버전트-시컨스를 엄밀하게 구분하여 말하도록 하자. 즉 \\(\\{a_n\\}\\)이 코시수열이라는 말은 \\(\\{a_n\\}\\)이 소박한 수렴을 한다는 의미이고 \\(\\{a_n\\}\\)이 컨버전트-시컨스라는 의미는 \\(\\{a_n\\}\\)이 소박한수렴을 하며 동시에 그 극한값이 well-define 된다는 의미(=\\(\\{a_n\\}\\)의 수렴값이 \\(X\\)의 원소라는 의미)이다.\n- (proposition 14.1) 메트릭스페이스 한정으로, 컨버전트-시컨스는 모두 코시수열이다. (당연한 소리를.. 이런걸 proposition 이라고..)\n- 당연히 위 정리의 역은 성립하지 않는다. 즉 메트릭스페이스 \\((X,{\\cal T})\\) 에서 정의된 코시수열이 반드시 컨버전트-시컨스라는 보장은 없다. (이것도 당연한 소리.. 왜냐하면 수렴값이 \\(X\\)에 포함된다는 보장이 없기 때문) 하지만 그 메트릭스페이스가 complete 하다면 위 정리의 역도 성립한다.\n- 컴플리트하지 않은 메트릭스페이스 \\((X,d)\\)를 컴플리트한 메트릭스페이스 \\((X^* , d)\\) 로 바꿀 수 없을까? 유주얼메트릭(usual metric) \\(d\\) 와 \\(X=(0,1)\\) 로 만들어지는 메트릭스페이스는 컴플리트하지 않지만 \\(d\\) 와 \\(X^* =[0,1]\\) 로 만들어지는 메트릭스페이스는 컴플리트하다. 이런 경우 $(X^* ,d) $ 는 \\((X,d)\\) 의 completion 이라고 한다.\n- 즉 아래의 조건들을 만족하면 $(X^* ,d) $ 는 \\((X,d)\\) 의 completion 이라고 부른다.\n(1) \\(X\\subset X^*\\)\n(2) \\((X^* ,d)\\) is complete metric space\n(3) \\((X,d) \\overset{ism}{\\sim} (X^* ,d)\\).\n- 메트릭스페이스 \\((X,d)\\)에서 아래의 식을 만족하는 두 코시수열 \\(\\{a_n\\}\\), \\(\\{\\tilde a_n\\}\\) 을 생각하여보자.\n\\[\\lim_{n\\to\\infty} d(a_n,\\tilde a_n)=0 \\]\n이러한 코시수열들을\n\\[\\{a_n\\} \\overset{slim}{\\sim} \\{\\tilde a_n\\}\\]\n이라고 표현하자. 이때 관계 \\(\\overset{slim}{\\sim}\\) 은 \\(X\\)에서 정의가능한 모든 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 equivalence relation 이 된다고 한다. (증명은 알아서) 따라서 이걸 이용하면 거리공간에서 \\(slim\\) 의 관계를 가지는 임의의 두 수열은 같은 극한을 가진다는 결론이 나온다. (이것도 잘 따져보자.)\n- 잠시 (1) 바이너리-릴레이션(binary relation), (2) 이퀴배런스-릴레이션(equivalence relation), (3) 이퀴배런스-클래스(equivalence class) 그리고 (4) 코션트셋(quotient set)에 대하여 설명하고 넘어가겠다.\n(1) 집합 \\({\\cal C}_ X\\) 의 두 원소 \\(\\{a_n\\}\\), \\(\\{b_n\\}\\) 간 바이너리-릴레이션 \\(R\\)이 존재한다는 문장은 집합론적인 언어로 표현가능하다. 구체적으로는 \\(R\\)을 곱집합 \\({\\cal C}_ X \\times {\\cal C}_ X\\) 의 적당한 부분집합으로 설정하고 순서쌍 \\(\\big(\\{a_n\\},\\{b_n\\}\\big)\\) 이 \\(R\\) 의 원소라는 식으로 표현한다. 예를 들면 아래와 같은 식으로 말이다.\n$ {a_n} and {b_n} has arelation with~ R \\ ({a_n},{b_n}) R _ X _ X \\ {a_n} {b_n}$\n(2) 그리고 \\({\\cal C}_ X\\) 위에서의 바이너리-릴레이션 \\(R\\)이 (i) reflexivity (ii) symmetricity (iii) transitivity 를 만족하면 이 릴레이션을 특별히 이퀴배런스-릴레이션 이라고 말한다.\n(3) 그리고 아래와 같이 \\({\\cal C}_ X\\) 에서 \\(\\{a_n\\}\\) 과 이퀴배런스-릴레이션을 가지는 원소들을 모아놓은 집합을 생각할 수 있다. \\[\\big[\\{a_n\\}\\big]_ R:=\\big\\{ \\{x_n\\} : \\{x_n\\} \\overset{R}{\\sim} \\{a_n\\} ~and~ \\{x_n\\} \\in {\\cal C}_ X \\big\\}\\]\n이 집합을 \\(\\{a_n\\}\\)의 equivalence class on \\({\\cal C}_ X\\) by \\(R\\) 이라고 부른다. 보통은 \\(R\\)을 생략하여 \\(\\big[\\{a_n\\}\\big]\\)와 같이만 표현하지만 나는 기호의 명확성을 위해서 관계까지 명시하였다.\n(4) 이퀴배런스-클래스는 본질적으로 파티션과 밀접한 연관이 있다. 여기에서 클래스 \\({\\cal P}_ A\\) 가 집합 \\(A\\)의 파티션이란 의미는 클래스 \\({\\cal P}_ A\\) 에 속한 모든 원소의 합이 \\(A\\) 이며 클래스 \\({\\cal P}_ A\\) 의 각 원소는 서로 배타적이라는 의미이다. 이퀴배런스-클래스가 그럼 왜 파티션과 관련이 있을까? 그것은 어떠한 집합에서 이퀴배런스-릴레이션이 존재하면 그 집합을 배타적인 이퀴배런스-클래스의 합집합으로 표현가능하기 때문이다. 즉 이퀴배런스-릴레이션 혹은 이퀴배런-클래스의 존재는 파티션의 존재를 임플라이 한다. 그리고 이러한 파티션을 이퀴배런스-릴레이션 \\(R\\)에 의해 생성된 quotient set 혹은 quotient space 라고 한다. 관계 \\(R\\)에 의한 \\(A\\)의 코션트 셋은 기호로 \\(A ~\\overset{R}{\\sim}\\) 와 같이 쓴다. 예를들어 \\[\\begin{align}\n{\\cal C}_ X ~ /\\overset{slim}{\\sim}\n\\end{align}\\] 은 집합 \\(X\\) 상에서 존재하는 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 이퀴배런스-릴레이션 \\(slim\\) 에 의해서 생성된 코션트셋을 의미한다.\n\n\n\n\n\nFootnotes\n\n\n정확하게는 \\(\\cal O\\)-오픈셋↩︎\n공집합이 오픈셋이므로↩︎\n\\(\\mathbb{R}\\)이 오픈셋이므로↩︎\n다만 이것은 위상공간을 \\((\\mathbb{R},{\\cal O})\\)로 생각하였을때 이야기이고 위상공간을 \\((\\mathbb{R},2^{\\mathbb{R}})\\)로 생각한다면 \\(\\{p\\}\\) 도 오픈셋이 된다.↩︎"
  },
  {
    "objectID": "posts/1_Essays/2023-04-13-마코프체인 인트로.html",
    "href": "posts/1_Essays/2023-04-13-마코프체인 인트로.html",
    "title": "[Essays] 마코프체인 인트로",
    "section": "",
    "text": "import numpy as np\n\n\n확률과정\n- 동전을 무한히 던지는 시행을 생각하자. 동전을 10번 던져서 결과를 관찰했다고 하자. 동전을 30번째 던져서 앞면이 나올지 뒷면이 나올지 알고 싶다면?\n- 현재 삼성전자 주가는 66000이다. 20일뒤의 삼성전자 주가가 얼마일지 알고 싶다면?\n- 원래 미래를 예측하기 위해서 해야하는 과정\n\n\n\n그림1: 1400만개의 미래를 탐색중인 Doctor Strange\n\n\n- 하지만 현실적으로는 이게 너무 힘들지 않을까?\n\n\n날씨예측\n- 아래와 같이 세상의 법칙이 있다고 하자.\n\n어제 맑음 \\(\\to\\) 오늘도 맑음: 40% // 오늘은 비: 60%\n어제 비 \\(\\to\\) 오늘은 맑음: 70% // 오늘도 비 30%\n\n- 모든 \\(t\\)에 대하여 확률변수 \\(X_t\\)를 아래와 같이 정의하자.\n\n\\(X_t=\\begin{cases} 0 & \\text{맑음} \\\\ 1 & \\text{비} \\end{cases}\\)\n\n- 오늘 (2023년4월13일) 비가 왔다고 치자. 10000일 뒤에도 비가 올 확률은 얼마일까?\n\n\n풀이1\n- \\(X_t=0\\) 이면? (어제 비가 안왔으면?)\n\nnp.random.rand() < 0.6\n\nTrue\n\n\n- \\(X_t=1\\) 이면? (어제 비가 왔으면?)\n\nnp.random.rand() < 0.3\n\nFalse\n\n\n- 두 코드를 합치면?\n\ndef rain(before):\n    if before == True: # 비가왔으면 \n        after = (np.random.rand() < 0.3)\n    else: # 비가 안왔으면 \n        after = (np.random.rand() < 0.6) \n    return after \n\n- 테스트\n\nsum([rain(0) for i in range(1000)])\n\n620\n\n\n\nsum([rain(1) for i in range(1000)])\n\n294\n\n\n- 하나의 \\(\\omega\\)에 대응하는 확률과정\n\ndef doctor_strange(today):\n    lst = [today] \n    for i in range(9999): \n        lst.append(rain(lst[i]))\n    return lst \n\n- 4305개의 \\(\\omega\\)에 대응하는 확률과정\n\ntoday = True # 오늘은 비가 왔음 \narr = np.stack([doctor_strange(today) for i in range(4305)])\n\n\narr.shape\n\n(4305, 10000)\n\n\n\narr[:,-1].mean()\n\n0.4429732868757259\n\n\n\narr\n\narray([[ True, False,  True, ..., False, False,  True],\n       [ True, False,  True, ...,  True, False,  True],\n       [ True, False,  True, ..., False, False, False],\n       ...,\n       [ True, False, False, ..., False, False, False],\n       [ True,  True, False, ...,  True, False,  True],\n       [ True,  True, False, ..., False,  True, False]])\n\n\n\n\n풀이2\n- 세상의 법칙\n\n\\(X_{t-1}=0 \\Rightarrow X_{t}\\overset{d}{=} Ber(0.6)\\)\n\\(X_{t-1}=1 \\Rightarrow X_{t}\\overset{d}{=} Ber(0.3)\\)\n\n- 정리하면\n\n\\(P(X_t=0) = P(X_{t-1}=0) \\times 0.4 + P(X_{t-1}=1) \\times 0.7\\)\n\n\\(P(X_t=1) = P(X_{t-1}=0) \\times 0.6 + P(X_{t-1}=1) \\times 0.3\\)\n\n- 매트릭스형태로 바꾸면\n\n\\(\\begin{bmatrix} P(X_t=0)\\\\ P(X_t=1) \\end{bmatrix}= \\begin{bmatrix} 0.4 & 0.7 \\\\ 0.6 & 0.3 \\end{bmatrix} \\begin{bmatrix} P(X_{t-1}=0)\\\\ P(X_{t-1}=1) \\end{bmatrix}\\)\n\\({\\boldsymbol \\mu}_t = {\\bf P} {\\boldsymbol \\mu}_{t-1}\\)\n\n- 이렇게 놓고 보니까 아래를 관찰할 수 있다.\n\n\\({\\boldsymbol \\mu}_2 = {\\bf P}{\\boldsymbol \\mu}_1\\)\n\\({\\boldsymbol \\mu}_3 = {\\bf P}{\\boldsymbol \\mu}_2 = {\\bf P}^2{\\boldsymbol \\mu}_1\\)\n\\(\\dots\\)\n\\({\\boldsymbol \\mu}_n = {\\bf P}^{n-1}{\\boldsymbol \\mu}_1\\)\n\n- \\({\\bf P}^{999}\\)만 계산하면 끝나겠군?\n\nP = np.array([[0.4,0.7],[0.6,0.3]])\nP\n\narray([[0.4, 0.7],\n       [0.6, 0.3]])\n\n\n\nP@P # P의 제곱\n\narray([[0.58, 0.49],\n       [0.42, 0.51]])\n\n\n\nP@P@P@P # P의 4제곱\n\narray([[0.5422, 0.5341],\n       [0.4578, 0.4659]])\n\n\n\nP@P@P@P@P@P@P@P # P의 8제곱\n\narray([[0.53849182, 0.53842621],\n       [0.46150818, 0.46157379]])\n\n\n\nP@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P # P의 16제곱\n\narray([[0.53846154, 0.53846154],\n       [0.46153846, 0.46153846]])\n\n\n근데 \\({\\bf P}\\)가 수렴하는거 같은데?\n\nP@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P # P의 32제곱\n\narray([[0.53846154, 0.53846154],\n       [0.46153846, 0.46153846]])\n\n\n대충 \\({\\bf P}^{9999} \\approx {\\bf P}^{32}\\) 로 두어도 무방할 듯\n\nPlim = P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P@P\n\n\nμ1 = np.array([[0],[1]])\nμ1\n\narray([[0],\n       [1]])\n\n\n\nPlim @ μ1\n\narray([[0.53846154],\n       [0.46153846]])\n\n\n\n\n풀이3\n- 세상의 법칙\n\n\\(X_{t-1}=0 \\Rightarrow X_{t}\\overset{d}{=} Ber(0.6)\\)\n\\(X_{t-1}=1 \\Rightarrow X_{t}\\overset{d}{=} Ber(0.3)\\)\n\n- 추측: 10000일 뒤에 비가 올 확률이 \\(p\\)라면 9999일 뒤에 비가 올 확률도 \\(p\\)일 것 같다.\n이걸 가정하고 계산해보면\n1 9999일 뒤에 비가 안 올 확률 = \\(1-p\\)\n\n9999일 뒤에 비가 오고 10000일 뒤에 비가 올 확률 = \\(0.6 (1-p)\\)\n9999일 뒤에 비가 오고 10000일 뒤에 비가 안올 올 확률 = \\(0.4 (1-p)\\)\n\n2 9999일 뒤에 비가 올 확률 = \\(p\\)\n\n9999일 뒤에 비가 오고 10000일 뒤에 비가 올 확률 = \\(0.3 p\\)\n9999일 뒤에 비가 오고 10000일 뒤에 비가 안올 올 확률 = \\(0.7 p\\)\n\n따라서 \\(0.6(1-p) + 0.3p = p\\) 이므로,\n\n\\(0.6-0.3p = p\\)\n\\(p=6/13\\)\n\n\n6/13\n\n0.46153846153846156\n\n\n\n\n풀이4\n\nnp.mean(doctor_strange(True))\n\n0.4629"
  },
  {
    "objectID": "posts/1_Essays/2020-03-12-확률,확률변수,시계열,정상성.html",
    "href": "posts/1_Essays/2020-03-12-확률,확률변수,시계열,정상성.html",
    "title": "[Essays] 확률, 확률변수, 시계열, 정상성",
    "section": "",
    "text": "About this doc\n\n확률의 개념을 정의하고 정상성의 의미를 이해한다.\n2020년 1학기 숭실대학교 시계열분석 강의노트\n\n\n\n확률과 르벡측도\n- 이번 강의에서는 확률을 정의하는 방법이 왜 어려운지 설명하겠다.\n(예제1) 동전을 던지면 앞면과 뒷면 중 하나가 나오게 될것이다. 앞면이 나오는 경우를 \\(H\\)라고 하고 뒷면이 나오는 경우를 \\(T\\)라고 하자. 그리고 모든 경우를 모은 집합을 \\(\\Omega\\)와 같이 정의하고, 이것을 sample space라고 부르자. 즉 sample space \\(\\Omega\\)는 아래와 같이 정의한다.\n\\[\\Omega=\\{H,T\\}\\]\n공평한 동전이라면 동전이 앞면이 나올 확률, 뒷면이 나올 확률을 아래와 같이 정의할 수 있다.\n\\[P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\]\n(예제2) sample space의 원소수가 유한이 아니라 무한개인 경우도 생각할 수 있다. 예를들어 시계바늘을 돌려서 시계바늘이 가르키는 시간을 sample space로 정의할 수 있다. 다만 시계바늘이 가리키는 시간은 시계바늘이 12시와 이루는 각도와 1:1 관계가 있으므로 \\(\\Omega\\)를 아래와 같이 편리하게 정의할 수 있다.\n\\[\\Omega=[0,2\\pi)\\]\n여기에서 \\(0\\)은 바늘이 12시를 가리키는 사건 \\(\\pi\\)는 6시를 가리키는 사건을 의미한다. 이 경우 sample space의 각 부분집합에 \\(\\Omega^{* }\\) 대한 확률은 아래와 같이 기하학적 확률 (즉 길이의 비율) 로 정의할 수 있다.\n\\[P(\\Omega^* )=\\frac{m(\\Omega^* )}{m(\\Omega)}\\]\n여기에서 \\(m\\)은 구간의 길이를 반환하는 함수이다. (사실 이 함수는 measure이다. 하지만 측도론(혹은 실변수함수론이라고도 불림)을 배우지 않았다면 단순히 길이를 반환하는 함수라고만 생각하자.) 예를들어\n\n\\(m([0,2\\pi))=2\\pi\\)\n\\(m([0,\\pi))=\\pi\\)\n\n와 같이 계산할 수 있다. 그럼 연습삼아 \\(\\Omega^* = [0,\\pi)\\)와 경우의 확률을 계산하여 보자. 사건 \\(\\Omega^* = [0,\\pi)\\)는 ‘’시계바늘이 12시와 6시 사이를 가리키는 사건들의 집합’’으로 해석할 수 있는데 이 확률은 아래와 같이 계산할 수 있다.\n\\[P(\\Omega^* )=\\frac{m(\\Omega^* )}{m(\\Omega)}=\\frac{m([0,\\pi))}{m([0,2\\pi))}=\\frac{1}{2}\\]\n이러한 확률은 잘 정의되는것 처럼 보인다.\n- 하지만 확률을 잘 정의하는 것은 생각보다 쉽지가 않다. 왜냐하면 확률을 잘 정의하기 위해서는\n\nSample space \\(\\Omega\\)의 모든(=임의의) 부분집합 \\(\\Omega^*\\)에 대하여 \\(P(\\Omega^*)\\)를 명확하고 모순없이 정의해야\n\n하는데 이것은 생각보다 쉬운일이 아니다. 또한 정의한 확률은 확률의 공리를 적용하였을때 모순이 없어야 한다. 확률의 공리는 간단하게 말하면\n\n모든 확률은 0보다 크고 1보다 작으며\n전체사건이 일어날 확률은 1이고\n서로소인 사건에 대하여 \\(\\sigma\\)-additive1가 성립해야 한다.\n\n확률의 공리에 대한 좀더 정확한 state는 김우철 수리통계교재 혹은 위키피디아를 참고하여 스스로 확인해 보길 바란다.\n- 확률의 공리는 너무 당연해서 확률을 정의하면 저절로 성립할것 같다. 하지만 예제2의 경우 그렇게 당연하지 않다. 이에 대하여 좀 더 자세히 살펴보자. 예제2에서는 \\(\\Omega=[0,2\\pi)\\)라고 생각하고 확률은 길이의 비율로 정의하였다. 예를들어서 \\(\\Omega^* \\in [0,\\pi)\\)일 확률은\n\\[\\frac{\\pi}{2\\pi}=\\frac{1}{2}\\]\n와 같은 식으로 정의한바 있다. 이런식의 정의는 여러가지 도전적인 질문에 직면한다.\n(질문1) 첫번째 도전적인 질문은 아래와 같다.\n\n\\(\\Omega^* =\\{0\\}\\)일 확률이 얼마인가?\n\n즉 바늘침이 정확하게 12시를 가르킬 확률이 얼마냐는 것이다. 한 점으로 이루어진 집합 \\(\\{0\\}\\)은 분명히 \\(\\Omega=[0,2\\pi)\\)의 부분집합 이므로 앞서 논의한대로라면 이러한 집합에 대한 확률을 명확하게, 모순없이 정의할 수 있어야 한다. 많은 사람들이 이 질문에 대한 답은 \\(0\\) 이라고 알고 있고 그 이유를 ‘’점의 길이는 0 이니까’’ 라고 이해하고 있다. 그럼 이제 아래의 질문을 생각하여 보자.\n(질문2) 두번째 질문은 아래와 같다.\n\n그렇다면 사건 \\(\\{0,\\pi\\}\\)가 일어날 확률은 얼마인가?\n\n질문을 다시 풀어쓰면 바늘침이 정확하게 12시를 가르키거나 혹은 정확하게 6시를 가르킬 확률이 얼마냐는 것이다. 이 질문에 대한 대답은 \\(0+0=0\\)이므로 \\(0\\)이라고 주장할 수 있다. 그렇다면 이제 아래의 질문을 생각해보자.\n(질문3) 세번째 질문은 아래와 같다.\n\n구간 \\([0,2\\pi)\\)는 무수히 많은 점들이 모여서 만들어지는 집합이다. 그런데 점 하나의 길이는 0이다. 0을 무수히 더해도 0이다. 그러므로 구간 \\([0,2\\pi)\\)의 길이도 0이 되어야 한다. 이것은 모순아닌가?\n\n이 질문에 대한 답변은 쉽지 않다. 왜냐하면 \\(m([0,2\\pi))=0\\) 임을 인정하면 지적한대로 전체확률은 1이어야 한다는 기본상식에 어긋나 모순이 생긴다. 확률의 공리2가 깨져버리는 것이다. 하지만 이 질문에 대한 논리는 그럴듯해 보인다. 따라서 이 질문에 대한 대답을 하려면 약간의 직관적인 가정을 덧붙여야 할 것 같다. 예를들면 “점들을 유한번 합치면 그냥 많은 점들이지만 무한히 합치면 이것은 선분이 된다. 따라서 길이가 생긴다.” 라는 식으로 설명할 수 있다. 우리는 이러한 현상을 “무한번 더해서 일어나는 기적”이라고 칭하자.\n(질문4) 그렇다면 아래의 질문은 어떻게 대답할 수 있을까?\n\n\\([0,\\pi)\\) 에서 유리수만 뽑아낸 집합이 있다고 생각하자. 편의상 이 집합을 \\(\\mathbb{Q}\\) 라고 하자. 이 집합은 분명히 무한개의 점을 포함하고 있다. 그렇다면 이 집합도 길이가 있는가? 있다면 얼마인가?\n\n- 이미 점들의 길이를 무한번 더하면 길이가 생긴다고 주장한 상태이므로 길이가 0이라고 주장할 수 없다. 따라서 길이가 있다고 주장해야 한다.\n- 단순히 길이가 \\(\\pi\\)라고 주장한다면 바로 모순에 빠짐을 알 수 있다. (길이가 \\(\\pi\\)라고 주장한다면 \\([0,\\pi)\\) 에서 무리수만 뽑아낸 집합의 길이가 뭐냐고 따질수가 있음.)\n- 길이는 일단 0보다 커야하고 \\(\\pi\\)보다 작아야함은 자명하므로 그 사이에 있는 어떤 값이 길이라고 주장하자. (구체적으로 어떤값인지는 모른다고 하자.) 따라서 (질문4)에 대한 답은 ‘’구체적으로 얼마인지는 모르겠지만 길이가 분명 존재하고 그 길이는 0 보다 크고 \\(\\pi\\) 보다는 작은 어떠한 값 \\(a\\)이다.’’ 정도로 정리할 수 있다.\n(질문5) 질문4로부터 만들어지는 논리는 아래의 질문을 적절하게 대답하지 못한다. (질문이 좀 길어서 나누어서 설명합니다)\n(빌드업1)\n\\(\\mathbb{Q}\\)의 모든점에 \\(\\sqrt{2}\\)를 더한다. 이 점들로 집합을 만들어 \\(\\mathbb{Q}_{\\sqrt{2}}\\)를 만든다. 여기에서 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 \\(\\Omega\\)의 부분집합이다.2 따라서 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 길이를 명확하고 모순없이 정의할 수 있어야 한다. 사실 이건 별로 어려운 일이 아니다. 평행이동은 길이를 변화시키지 않는다는 점을 상기하면 별로 어렵지 않게\n\\[m(\\mathbb{Q}_{\\sqrt{2}})=\\alpha\\]\n라고 정의할 수 있다.3\n(빌드업2)\n이제 좀 더 일반적으로 구간 \\([0,\\pi)\\)에 있는 모든 무리수의 집합 \\(\\mathbb{Q}^c:=[0,\\pi) -\\mathbb{Q}\\) 를 생각하자. 이 집합의 서로 다른 임의의 원소 \\(x,y,z\\)에 대하여 \\(\\mathbb{Q}_{x},\\mathbb{Q}_{y},\\mathbb{Q}_{z}\\)를 생각하자. 아래의 성질을 관찰할 수 있다.\n\n\\(\\mathbb{Q}_{x},\\mathbb{Q}_{y},\\mathbb{Q}_{z}\\)는 모두 \\(\\Omega\\)의 부분집합이다. 따라서 길이를 명확하고 모순없이 정의할 수 있어야 한다.\n\\(\\mathbb{Q}_{x},\\mathbb{Q}_{y},\\mathbb{Q}_{z}\\)의 길이는 각각 \\(\\alpha\\)로 정의할 수 있다.\n\\(\\mathbb{Q}_{x},\\mathbb{Q}_{y},\\mathbb{Q}_{z}\\)는 모두 서로소인 집합이다.\n따라서 \\(P(\\mathbb{Q}_{x} \\cup \\mathbb{Q}_{y} \\cup \\mathbb{Q}_{z})=P(\\mathbb{Q}_{x}) + P(\\mathbb{Q}_{y})+ P(\\mathbb{Q}_{z})\\) 이다. (확률의 공리3)\n\n굳이 \\(P(\\mathbb{Q}_{x}) + P(\\mathbb{Q}_{y})+ P(\\mathbb{Q}_{z})\\)를 계산하면 아래와 같이 계산할 수 있겠다.\n\\[P(\\mathbb{Q}_{x}) + P(\\mathbb{Q}_{y})+ P(\\mathbb{Q}_{z})=\\frac{\\alpha}{2\\pi}+\\frac{\\alpha}{2\\pi}+\\frac{\\alpha}{2\\pi}=3 \\times \\frac{\\alpha}{2\\pi}\\]\n(빌드업3)\n눈 여겨볼 점은 아래 식이 성립해야 한다는 것이다.\n\\[P(\\mathbb{Q}_{x}) + P(\\mathbb{Q}_{y})+ P(\\mathbb{Q}_{z}) = 3 \\times \\frac{\\alpha}{2\\pi} \\leq 1\\]\n따라서 \\(\\alpha\\)를 \\((0,\\pi)\\) 사이의 아무값이나 가지도록 하면 안되고 적당히 작은 값을 설정해야 한다. 그런데 임의의 경우에 대하여 이 식은 성립하도록 \\(\\alpha\\)를 작게 설정해야 하는데 그건 불가능할 것 같다. 왜냐하면 좌변의 값은 편의에 따라서 값을 임의로 키울 수 있기 때문이다. 지금은 \\(\\mathbb{Q}^c\\)에서 서로 다른 3개의 무리수를 뽑는다고 가정하였는데 예를 들어 서로 다른 100개의 무리수를 뽑았다면 아래 식이 성립하도록 \\(\\alpha\\)를 설정해야 한다.\n\\[ 100 \\times \\frac{\\alpha}{2\\pi} \\leq 1\\]\n무리수의 개수는 무한개만큼 많으니까 100이라는 숫자는 얼마든지 1000, 10000 과 같이 키울 수 있다. 좀 더 일반적으로 쓰면 아래과 같다.\n\\[ N \\times \\frac{\\alpha}{2\\pi} \\leq 1\\]\n그런데 \\(\\alpha\\)를 아무리 작게 잡아도 위의 부등식을 만족하는 아주 큰 \\(N\\)이 항상 존재하므로 결국 위의 부등식을 만족하려면 \\(\\alpha=0\\) 일 수 밖에 없다. 그런데 \\(\\alpha=0\\)이 된다면 “무한번 더해서 일어나는 기적”은 허구가 되므로 질문3 의 대답에 모순이 된다.\n- 따라서 이 질문은 지금까지 제시한 논리로 방어가 불가능하다. 이처럼 논리적인 모순없는 체계를 만드는 것은 매우 어려운 일이다.\n결론 결론적으로 말하면 길이를 재는 함수 \\(m\\)을 아래와 가정하면 위의 모든 질문에 대한 대답을 논리적 모순없이 설계할 수 있다.\n\n한 점에 대한 길이는 \\(0\\) 이다.\n\\([0,2\\pi)\\) 사이의 모든 유리수를 더한 집합은 그 길이가 \\(0\\)이다.\n\\([0,2\\pi)\\) 사이의 모든 무리수를 더한 집합은 그 길이가 \\(2\\pi\\)이다.\n\n참고로 르벡측도(Lebesgue measure)를 사용하면 위의 성질을 만족한다. (물론 르벡측도의 정의가 위와 같진 않다) 따라서 르벡측도를 활용하여 확률을 정의하는 것이 모순을 최대한 피할 수 있다.\n\n\n비탈리집합\n- 그렇다면 아래와 같이 주장할 수 있을까?\n\n위의 3가지 원리 1-3을 사용하면 \\([0,2\\pi)\\)의 어떤 부분집합에 대하여서도 그 집합의 길이를 모순없이 정의할 수 있다.\n\n결론적으로 말하면 이 주장은 틀렸다\n- 이 주장이 틀린이유를 설명하겠다. 이 주장이 틀린 이유는\n\n위의 3가지 원리 (i)-(iii)을 사용하여도 길이를 모순없이 정의불가능한 어떠한 집합이 \\([0,2\\pi)\\)내에 존재하기 때문\n\n이다. 이제 이러한 집합이 무엇인지 살펴보겠다. 먼저 아래와 같은 관계를 정의하자.\n\n두 실수 \\(x,y\\)에 대하여 \\(x-y \\in \\mathbb{Q}\\) 이면 \\(x\\sim y\\) 라고 정의한다.\n\n아래가 성립함을 바로 알 수 있다.\n\n\\(x \\sim x\\). (Reflexivity)\n\\(x \\sim y\\) if and only if \\(y \\sim x\\). (Symmetry)\nif \\(x \\sim y\\) and \\(y ~ z\\) then \\(x \\sim z\\). (Transitivity)\n\n위의 성질을 만족하는 관계를 동치관계라고 한다.\n- 참고로 집합 \\(A\\)의 원소간 동치관계가 존재하면 그 동치관계를 이용하여 \\(A\\)의 파티션을 항상 만들 수 있다. 여기에서 \\(A=[0,2\\pi)\\)로 바꾸면 우리가 관심 있는 주장이 된다.\n(proof)\n임의의 실수 \\(x \\in [0,2\\pi)\\)에 대하여 동치관계가 성립하는 실수들을 모은 집합을 \\([x]\\)라고 정의하자. 즉\n\\[[x]=\\{y: x\\sim y, y \\in \\mathbb{R}\\} \\cap [0,2\\pi)\\]\n이다. 여기에서 \\([x]\\)를 equivalence class라고 한다. 그런데 \\([x]\\)는 최소한 \\(x\\)를 포함하므로 아래가 항상 성립한다.\n\\[[0,2\\pi) \\subset \\bigcup_{x\\in [0,2\\pi)}[x]\\]\n또한 \\([x]=\\{y: x\\sim y, y \\in \\mathbb{R}\\} \\cap [0,2\\pi)\\)임을 이용하면 아래가 성립함을 알 수 있다.\n\\[[0,2\\pi) \\supset \\bigcup_{x\\in [0,2\\pi)}[x]\\]\n따라서 아래가 성립한다.\n\\[[0,2\\pi) = \\bigcup_{x\\in [0,2\\pi)}[x]\\]\n그리고 아래의 관계도 성립한다.\n\\[[x]\\neq[y] \\Longrightarrow [x] \\cap [y] = \\emptyset \\]\n따라서 \\([0,2\\pi)\\)의 파티션이 존재한다.\n\\[[0,2\\pi)=\\bigcup_ {t\\in {\\cal T} }A_t\\]\n여기에서 \\(A_t=\\{s: t-s \\in \\mathbb{Q} \\} \\cap [0,2\\pi)\\) 이고 \\(t\\)는 각 파티션을 대표하는 원소라고 하자. 그리고 \\({\\cal T}\\)는 단순히 대표원소들의 집합이라고 하자.\n- 이제 이 파티션에서 원소를 하나씩만 선택하여 어떤 집합 \\(V\\)를 만들었다고 하자. 그럼 이 집합은 (i)-(iii)의 원리에 의해서도 길이를 잴 수 없다. 이제부터 왜 \\(V\\)가 잴수없는 집합인지 설명하겠다. 우선 \\(V\\)가 잴수있는 집합이라고 하자. 즉 집합 \\(V\\)의 길이를 잴 수 있고 그 길이를 \\(a\\)이라고 하자. 즉 \\(m(V)=a\\)이라고 가정하자. 여기에서 \\(m\\)은 르벡메져이다. 그리고 아래의 집합들을 정의한다.\n1. \\((-2\\pi,2\\pi)\\)에서 유리수만 모은 집합을 \\(Q^{\\star}\\)라고 정의하자. 즉 \\(Q^{\\star}=\\mathbb{Q} \\cap (-2\\pi,2\\pi)\\).\n2. \\(Q^{\\star}\\)에서 적당한 유리수 하나를 뽑아서 \\(V\\)를 평행이동 시킨 집합 \\(V_q\\)를 정의하자.\n이때 위에서 언급합 \\(V_q\\)는 좀 더 형식적으로 쓰면 \\(V_q:=\\{v+q: v\\in V, q\\in Q^{\\star} \\}\\)와 같이 쓸 수 있다. 이 집합 \\(V_q\\)는 매우 오묘한 특징이 있는데 우선 아래들이 성립한다.\n\n\\(V_q\\)의 길이는 \\(V\\)의 길이와 같다.\n서로 다른 \\(q_1,q_2 \\in Q^{\\star}\\)에 대하여 \\(V_{q_1} \\cap V_{q_2} = \\emptyset\\) 이 성립한다.\n\n이제 이러한 집합 \\(V_q\\)를 가지고 아래의 성질이 성립함을 보이자.\nFact 1: \\([0,2\\pi) \\subset \\bigcup_{q \\in Q^{\\star} } V_q\\).\n(proof)\n증명이 그렇게 어렵진 않다. \\(x \\in [0,2\\pi)\\) 라고 하자. 그런데 \\([0,2\\pi)=\\bigcup_{t \\in {\\cal T} }A_t\\) 이므로 \\(x\\in A_{t'}\\)가 성립하는 \\(t'\\)가 적어도 하나는 \\({\\cal T}\\)에 존재한다. 그런데 아래가 성립한다.\nclaim: \\(A_{t'} \\subset \\bigcup_{q \\in Q^{\\star}} V_q\\)\n따라서 \\(x \\in \\bigcup_{q \\in Q^{\\star}} V_q\\) 가 성립한다. 이제 마지막 식이 성립하는 이유를 좀 더 자세히 살펴보자.\n(proof of claim)\n부분집합의 정의에 의해서 모든 \\(z \\in A_{t'}\\)에 대하여 \\(z \\in \\bigcup_{q\\in Q^{\\star}} V_q\\)임을 보이면 증명이 끝난다. 우선 아래가 성립함을 관찰하자.\n\\[\\{y\\} = V \\cap A_{t'}\\]\n즉 \\(y\\)는 집합 \\(A_{t'}\\)에서 부터 대표로 \\(V\\)에 뽑혀 나간 어떠한 원소이다. 따라서 당연히 \\(y\\)는 \\(A_{t'}\\)에서 랜덤하게 뽑은 임의의 원소 (\\(z\\))와 동치관계에 있다. 즉 아래가 성립한다.\n\nfor all \\(z \\in A_{t'}, y \\sim z\\)\n\n이말은 아래와 같다.\n\nfor all \\(z \\in A_{t'}, y-z \\in \\mathbb{Q}\\)\n\n그런데 \\(y,z\\)모두 \\(A_{t'}\\)의 원소이고 \\(A_{t'} \\subset [0,2\\pi)\\)이므로 \\(y-z \\in (-2\\pi,2\\pi)\\)임을 알 수 있다. 따라서\n\nfor all \\(z \\in A_{t'}, y-z \\in \\mathbb{Q} \\cap (-2\\pi,2\\pi)\\)\n\n라고 쓸 수 있다. 이는 적당한 \\(q' \\in \\mathbb{Q} \\cap (-2\\pi,2\\pi)=Q^{\\star}\\) 가 존재하여 아래식을 성립한다는 말과 같다.\n\nfor all \\(z \\in A_{t'}, y-z = q'\\)\n\n이는 다시 아래식이 성립한다는 것과 같다.\n\nfor all \\(z \\in A_{t'}, z = y-q' , q'\\in Q^{\\star}\\)\n\n그런데 \\(y\\in V\\) 이므로, \\(\\bigcup_{q\\in Q^{\\star}} V_q\\) 중에는 \\(z\\)를 포함하는 집합이 하나는 존재한다. 따라서\n\nfor all \\(z \\in A_{t'}, z \\in \\bigcup_{q\\in Q^{\\star}} V_q\\)\n\n가 성립한다.\n- Fact 1과 별개로 아래의 사실이 성립한다.\nFact 2: \\(\\bigcup_{q \\in Q^{\\star} } V_q \\subset (-2\\pi,4\\pi)\\)\n이는 증명없이 바로 이해할 수 있다. Fact 1,2를 종합하면 아래와 같이 쓸 수 있다.\n\\[[0,2\\pi) \\subset \\bigcup_{q \\in Q^{\\star} } V_q \\subset (-2\\pi,4\\pi)\\]\n따라서 아래가 성립한다.\n\\[m([0,2\\pi)) \\leq \\sum_{q \\in Q^{\\star} } m(V_q) \\leq m((-2\\pi,4\\pi))\\]\n따라서 아래가 성립해야 한다.\n\\[2\\pi \\leq \\infty  \\leq 6\\pi\\]\n이는 모순이다.\n- 결국 아래와 같이 주장할 수 없다.\n\n위의 3가지 원리 (i)-(iii)을 사용하면 \\([0,2\\pi)\\)의 어떤 부분집합에 대하여서도 그 집합의 길이를 모순없이 정의할 수 있다.\n\n따라서 아래와 같이 합의한다.\n\n위의 3가지 원리 (i)-(iii)을 받아들이자. 그럼에도 불구하고 \\([0,2\\pi)\\)의 내에서 그 집합의 길이를 정의할 수 없는 집합이 존재함은 알고 있다. 길이를 정의할 수 없으므로 확률을 정의할 수 없음도 알고 있다. 그러므로 이제부터 이러한 집합을 제외하고 확률을 정의하자.\n\n즉 길이를 정의하기 힘든 집합은 제외하고 확률을 정의하겠다는 의미이다. 이 말은 \\(\\Omega\\)의 모든 부분집합들에 대해서 확률을 정의하지 않고 \\(\\Omega\\)의 특정 부분집합에 대해서만 확률을 정의하겠다는 것과 같은 말인데 이 특정 부분집합들의 모임을 기호로 \\({\\cal F}\\)로 표시하고 \\(\\sigma\\)-field라고 이름 붙인다. 이 강의의 범위에서는 \\({\\cal F}\\)를 sample space의 부분집합 중 확률을 잘 정의할 수 있는 집합들의 모임정도로 이해해도 무방하다. 확률을 잘 정의하는 것은 길이따위를 잘 정의하는 것과 맞닿아 있으므로 사실상 \\({\\cal F}\\)는 sample space의 부분집합 중 잴 수 있는 집합들의 모임정도로 이해할 수 있다. (따라서 당연히 \\({\\cal F}\\)의 원소는 잴 수 있는 집합으로 이해할 수 있다.)\n\n\n시그마필드\n- \\(\\sigma\\)-field에 대한 이해를 돕기 위해서 (예제1)을 다시 생각해보자. \\(\\Omega=\\{H,T\\}\\) 이고, 이것에 대응하는 \\({\\cal F}\\)는 아래와 같이 쓸 수 있다.\n\\[{\\cal F}=\\{\\emptyset, \\{H\\},\\{T\\},\\{H,T\\}\\}\\]\n보는 것처럼 \\({\\cal F}\\)의 원소는 집합이므로 \\({\\cal F}\\)는 집합들의 집합임을 기억하자. 다시 한번 말하지만 \\({\\cal F}\\)의 모든 원소에 확률을 모순없이 정의할 수 있다.\n- \\({\\cal F}\\)는 \\(\\Omega\\)없이 단독으로 정의될 수 없다. 즉 \\(\\Omega\\)와 \\({\\cal F}\\)은 커플같은 것이라고 생각할 수 있다. 그래서 좀 더 명확한 표현을 위해 이 둘을 묶어 \\((\\Omega,{\\cal F})\\)라고 표현하기도 한다. 그리고 이 둘을 묶은 쌍(pair라고 표현함)을 measurable space라고 부른다.\n- \\(\\Omega\\)의 시그마필드 \\({\\cal F}\\)가 반드시 아래와 같을 필요는 없다.\n\\[{\\cal F}= ~all~ subset~ of ~\\Omega\\]\n이것을 이해하기 위해서 아래의 주사위 예제를 생각해보자.\n(예제3) 어떤 사람이 주사위를 던졌다고 하자. 주사위는 1~6의 눈 사이로 나올것이다. sample space는\n\\[\\Omega=\\{1,2,3,4,5,6\\}\\]\n와 같이 설정할 수 있고 각 sample space의 각 이벤트가 발생할 확률은 모두 \\(\\frac{1}{6}\\)로 모순없이 정의할 수 있다. 이 경우 시그마필드 \\({\\cal F}\\)는 아래와 같이 설정할 수 있다.\n\\[{\\cal F}= ~ all ~ subset ~ of ~ \\Omega = \\{\\emptyset,\\{1\\},\\dots,\\{6\\},\\{1,2\\},\\dots,\\{5,6\\},\\dots,\\Omega \\}\\]\n분명히 \\({\\cal F}\\)의 모든 원소에 대하여 확률을 모순없이 정의할 수 있지만 이렇게 모든 \\(\\Omega\\)의 부분집합에 대하여 굳이 확률을 정의할 필요가 없는 경우도 있다. 가령 주사위를 던져서 짝이 나오면 1점을 얻고 홀이 나오면 1점을 잃는 게임을 생각하여 보자. 관심이 있는 것은 짝 혹은 홀이 나올 확률이므로 이 경우는 \\({\\cal F}\\)를 아래와 같이 설정해도 무방하다.\n\\[{\\cal F}= \\{ \\emptyset, \\{1,3,5\\}, \\{2,4,6\\},\\Omega \\}\\]\n- \\(\\sigma\\)-field의 엄밀한 정의는 (i) 공집합과 전체집합을 포함하며 (ii) 여집합에 대하여 닫혀있고 (iii) countable union에 대하여 닫혀있는 집합이다. 이때 (i)의 조건은 생략가능하다. 참고로 앞서 제시한 예시들은 모두 이러한 조건을 만족한다. 하지만 엄밀한 정의를 이해하는 것 보다 왜 시그마필드라는 개념을 생각해야만 하는가?를 이해하는 것이 더 중요하다.\n\n\n현재까지의 요약\n\n확률을 모순없이 정의하는 것은 매우 어려운 작업이다. 왜냐하면 길이를 모순없이 정의하는것이 어렵기 때문이다. 한 예로 구간 \\([0,2\\pi)\\)에서의 임의의 부분집합에 대하여 모순없이 길이를 재는것조차 매우 어렵다.\n르벡메져라는 것을 도입하면 길이를 비교적 모순없이 정의할 수 있다. 하지만 르벡메져로도 잴 수 없는 집합이 존재한다.\n그래서 이러한 집합을 제외하고 길이 혹은 확률을 정의하기로 한다. 즉 확률은 길이를 모순없이 정의할 수 있는 집합들의 모임에서만 정의하기로 한다. 이러한 집합들의 모임을 시그마필드라고 한다.\n\n\n\n확률변수\n- 확률변수는 \\(X\\)는 \\(X:\\Omega \\to \\mathbb{R}\\)인 함수이다. 그런데 조금 특별한 성질을 가진 함수이다. 우선 이 특별한 성질에 대해서는 제쳐두고 단순히 \\(\\Omega\\)의 각 원소에 실수값을 맵핑한 규칙이라고 이해해도 괜찮다.\n(예제1) 동전을 던지는 예제를 다시 생각하자.\n\\[\\Omega=\\{H,T\\}\\]\n공평한 동전이라면 동전이 앞면이 나올 확률, 뒷면이 나올 확률을 아래와 같이 정의할 수 있다.\n\\[P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\]\n여기에서 \\(\\Omega\\)의 원소 \\(H\\), \\(T\\)를 각각 \\(0\\)과 \\(1\\)로 정의한다면 이는 확률변수가 된다. 즉 함수 \\(X:\\Omega \\to \\mathbb{R}\\)를 아래와 같이 정의하면\n\n\\(X(H)=0\\)\n\\(X(T)=1\\)\n\n\\(X\\)는 확률변수가 된다. 집합 \\(A\\)의 원소를 \\(a\\)로 표시하는 것처럼 종종 \\(\\Omega\\)의 원소를 \\(\\omega\\)로 표시한다. 예를들면 \\(\\Omega=\\{H,T\\}=\\{\\omega_1,\\omega_2\\}\\)와 같이 쓸 수 있다. 이 경우 확률변수는\n\n\\(X(\\omega_1)=0\\)\n\\(X(\\omega_2)=1\\)\n\n와 같이 정의할 수 있다.\n- 확률변수 \\(X\\)가 \\(X: \\Omega \\to \\mathbb{R}\\)인 함수라면 확률 \\(P\\)는 \\(P:{\\cal F} \\to [0,1]\\)인 함수이다. 확률의 정의역이 \\({\\cal F}\\)라는 의미를 잘 생각해보면 확률은 집합을 실수로 맵핑하는 함수가 된다. 즉 set function이 된다. 반대로 확률변수는 집합의 원소를 실수로 맵핑하는 함수이다. 따라서 아래와 같은 기호를 쓴다.\n\n\\(X(H)=0\\)\n$P({H})= $\n\\(P(\\{\\omega_1\\})=\\frac{1}{2}\\)\n\n즉 아래와 같이 표현하면 옳지 않다.\n\n\\(P(H)=\\frac{1}{2}\\)\n\\(P(\\omega_1)=\\frac{1}{2}\\)\n\n첫 번째 표현은 \\(H\\)자체를 집합으로 착각하여 생기는 혼란이다. 보통 확률을 표현할때 “집합 \\(A\\)에 대하여 집합 \\(A\\)가 발생할 확률을 \\(P(A)\\)라고 하자.”는 식의 표현을 많이 접했을 것이다. \\(P(A)\\)에서 \\(A\\)는 엄연히 집합이지만 \\(H\\)는 집합이 아니다. 두 번째 표현은 첫번째 표현과 같은 이유로 틀린 표현이다.\n- 하지만 아래와 같은 표현은 옳다.\n\\[P(X=1)=\\frac{1}{2}\\]\n언뜻보기에는 이렇게 생각할 수 있다. (1) 원래 \\(P\\)는 set function이므로 입력으로 집합을 받아야 한다. (2) 그런데 \\(X=1\\)는 집합이 아니라 수식이다. (3) 따라서 옳지않다. 이런 따짐은 옳다. 하지만 \\(P(X=1)\\)은 관용적으로 많이 쓰는데 그 이유는 \\(P(X=1)\\)을 아래와 같은 표현의 생략이라고 보기 때문이다.\n\\[P(X=1)=P(\\{\\omega: X(\\omega)=1\\})=P(\\{H\\})=\\frac{1}{2}\\]\n확률을 \\(P(\\{\\omega: X(\\omega)=1\\})\\)와 같은 표현하는 것은 명확하지만 너무 복잡하고 직관적이지 않다. 그래서 간단히 \\(P(X=1)\\)와 같이 쓴다. 이는 매우 관용적인 약속이다.\n- 확률변수는 매우 편리한 도구이므로 앞으로는 확률을 표현할때\n\n\\(P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}\\)\n\\(P(\\{\\omega_1\\})=P(\\{\\omega_2\\})=\\frac{1}{2}\\)\n\n와 같이 표현하는 일은 거의 없을 것이다. 대신에 아래와 같이 표현하는 일이 많이 생길것이다.\n\\[P(X=0)=P(X=1)=\\frac{1}{2}\\]\n- 확률변수가 단지 함수라는 것은 쉽게 납득이 되지 않는다. 왜냐하면 대부분의 사람들이 “확률변수=값이 랜덤하게 바뀌는 변수”의 느낌을 가지기 때문이다. 즉 (1) 확률변수는 변수이고 (2) 확률변수는 결과가 랜덤하게 바뀌어야 한다고 대부분 생각한다.\n- (1)을 좀 더 자세히 설명하여 보자. 보통 우리가 함수라는 표현을 쓸때 \\(y=f(x)\\)라는 기호로 많이 쓴다. 여기에서 \\(f\\)는 함수 즉 function이라고 하고 \\(x\\)를 input 이라고 부르고 \\(y\\) 혹은 \\(f(x)\\)를 output이라 부르기로 약속하자. 이때 input과 output이 엄밀히 정의된 용어는 아니지만 편의상 이렇게 부르기로 하자. 랜덤변수 \\(X\\)가 함수라는 의미는 랜덤변수 \\(X\\)에 대응하는 요소가 \\(f\\)라는 것이다. 따라서 \\(y=f(x)\\)에 대응하는 표현은 \\(x=X(\\omega)\\)와 같이 된다. 여기에서 \\(\\omega\\)를 outcome이라고 부르고 \\(x\\)를 realization이라고 부른다.\n- (2)를 살펴보자. 확률변수의 결과 즉 output이 랜덤으로 바뀐다는 말은 동일한 input에 대한 맵핑의 결과 output이 랜덤으로 바뀐다는 것처럼 오해할 수 있다. 하지만 이는 사실이 아닌데 확률변수는 단지 - \\(X(\\omega_1)=0\\) - \\(X(\\omega_2)=1\\)\n와 같은 맵핑규칙일 뿐이기 때문이다. 보는 것처럼 입력이 \\(\\omega_1\\)이면 항상 \\(0\\)이 출력되고 입력이 \\(\\omega_2\\)면 항상 \\(1\\)이 출력된다. 확률변수가 가지는 “랜덤한”느낌은 입력이 랜덤으로 바뀔 수 있기 때문이지 입력과 출력사이의 맵핑을 정의하는 규칙이 랜덤으로 바뀌는 것은 아니다. 즉 outcome이 랜덤으로 나올수 있기 때문이지 realization이 랜덤으로 나올 수 있는 것은 아니다.\n- 많은 사람들이 아래와 같은 표현 때문에 혼돈을 느낀다. 예제1의 확률변수 \\(X\\)를 아래와 같이 표현할 수 있다.\n\\[X=\\begin{cases}\n0 & w.p.~ \\frac{1}{2} \\\\\n1 & w.p.~ \\frac{1}{2}\n\\end{cases}\\]\n이 표현을 보면 마치 \\(X\\)가 함수가 아닌 변수처럼 느껴지고 그 값이 랜덤으로 바뀌는 것처럼 느껴진다. 놀랍게도 이 표현은 맞는 표현인데 사실 아래와 같은 표현이 간소화 된 표현이기 때문이다.\n\\[X(\\omega)=\\begin{cases}\n0 & \\omega \\in \\{H\\}\\\\\n1 & \\omega \\in \\{T\\}\n\\end{cases}, \\quad where~P(\\{H\\})=P(\\{T\\})=\\frac{1}{2}.\\]\n- 확률변수 \\(X\\)는 확률과 직접적으로는 관련이 없지만 간접적으로는 관련이 있다. 왜냐하면 확률변수 \\(X\\)의 역상(inverse image)이 \\(\\Omega\\)의 부분집합이 되는데, \\(\\Omega\\)의 부분집합에서는 확률을 정의할 수 있기 때문이다. 그렇다면\n\n확률변수의 \\(X\\)의 모든 역상에 대하여 확률을 모순없이 정의할 수 있을까?\n\n와 같은 질문을 할 수 있다.\n- 예제2를 다시 생각하여보자. 이전에는 논의를 편하게 하기 위해서 sample space를 아래와 같이 정의하였다.\n\\[\\Omega=[0,2\\pi)\\]\n사실 동전예제와 맞춰서 생각해보면 sample space는\n\\[\\Omega=\\{\\dots,3:00,\\dots,6:00,\\dots,\\}\\]\n이고 확률변수는\n\n…\n$X({3:00})=/2 $\n…\n\\(X(\\{6:00\\})=\\pi\\)\n…\n\n와 같이 정의해야 한다. 따라서 사실 이전의 예제에서는 확률변수의 realization들의 집합자체를 \\(\\Omega\\)로 정의한 셈이 된다. 따라서 이 경우 확률변수는 항등함수로 정의한 셈이 된다. 즉 \\(X\\)는\n\\[X:[0,2\\pi) \\to [0,2\\pi)\\]\n인 항등함수이다. \\(X\\)는 항등함수이므로 비탈리집합 \\(V\\)에 대한 \\(X\\)의 역상은 비탈리 집합이 된다. 그런데 비탈리집합은 잴 수 없는 집합이므로 확률을 정의할 수 없다. 따라서 \\(X\\)의 모든 역상에 대해서 확률을 모순없이 정의할 수 있는 것은 아니다.\n- 확률변수인데 확률을 정의하지 못하는것은 말이 되지 않으므로 확률변수의 정의를 조금 수정해보자. 수정은 매우 간단한데 확률변수 \\(X\\)의 역상이 잴 수 있는 집합만 나오도록 수정하면 된다. 결국 \\(X\\)가 가측함수(measurable function)이라는 것이 되면 이것이 가능해지는데 가측함수에 대한 자세한 내용은 이 강의의 범위를 넘어서므로 생략한다. 하지만 핵심은 파악할 수 있는데 바로 가능한 \\(X\\)의 역상이 \\({\\cal F}\\)의 원소가 되어야 한다는 것이다. (앞서서 확률변수 \\(X\\)는 조금 특별한 성질이 있는 함수라고 소개하였는데, 이 특별한 성질이 바로 \\(X\\)가 가측함수라는 것이다.) 결국\n\n***확률변수의 \\(X\\)의 모든 역상에 대하여 확률을 모순없이 정의할 수 있다.\n\n는 맞는 주장인데 이는 위의 주장이 성립하도록 우리가 \\(X\\)를 일반적인 함수가 아니라 특별한 성질을 가지는 함수로 재정의 하였기 때문이다.\n\n\n확률밀도함수\n- 예제2를 다시 생각하여보자. 사실 엄밀하게 생각해보면 sample space는\n$$={,3:00,,6:00,,}\n이고 확률변수는\n\n…\n$X({3:00})=/2 $\n…\n\\(X(\\{6:00\\})=\\pi\\)\n…\n\n와 같이 정의해야 하며 확률변수의 realization의 집합을 \\([0,2\\pi)\\)로 설정하는 것이 맞는것 같다. 하지만\n\\[\\Omega=[0,2\\pi)\\]\n와 같이 설정하는 것이 매우 편리해보인다.\n- 예제1의 경우도 사실 아래처럼 정의해도 무방할것 같다.\n\\[\\Omega=\\{0,1\\}\\]\n이것은 sample space대신에 realization을 모은 집합만 고려하고 싶다는 의미인데 이는 결국 \\((\\Omega,{\\cal F})\\)와 같은 것을 생략하고 싶다는 것이다.\n- 지금까지의 논의를 잘 생각해보면 이것이 가능할 것 같다. 결국\n\n\\(X\\)를 확률변수라고 하자!!!\n\n라고 선언하기만 한다면 \\(X\\)의 역상이 모두 가측집합(measurable set)이 되어서 그 역상에 대한 확률을 모순없이 정의할 수 있다. 비탈리집합과 같은 경우는 \\(X\\)의 역상에서 저절로 빠진다. 왜냐하면 그렇게 되도록 \\(X\\)를 정의하였기 때문이다.\n- 결국 확룰변수 \\(X\\)가 가측함수임을 이해하는 것과 그것이 무엇을 의미하는지 이해하는것이 힘들지만 한번 이해하면 단순히 확률변수를 선언하는 것만으로도 많은 것들을 생략하여 간소화 시킬 수 있다.\n- 간소화불가능한 점은 바로 확률 \\(P\\)인데 이것은 sample space \\(\\Omega\\)의 부분집합을 input으로 받는 함수이므로 \\((\\Omega,{\\cal F})\\)가 통째로 생략된다면 확률 \\(P\\)의 정의도 모호해지게 된다. 따라서 확률 \\(P\\)의 개념을 그대로 이어받은 무엇인가를 새롭게 정의해야 하는데 그것이 바로 probability density function(pdf)와 cumulative density function(cdf)이다. 이 둘의 정의는 각자 교재를 참고하길 바란다.\n\n\n시계열\n- 확률변수는 \\(X\\)는 \\(X:\\Omega \\to \\mathbb{R}\\)를 만족하고 추가로 특별한 성질을 가진 함수라고 생각할 수 있다. 이를 확장하면 아래와 같이 확률벡터를 정의할 수 있다.\n- 확률벡터 \\({\\boldsymbol X}\\)는\n\\[{\\boldsymbol X}:\\Omega \\to \\mathbb{R}^m\\]\n를 만족하고 추가로 특별한 성질을 가진 함수이다.\n- 기호는 아래와 같이 쓸 수 있다.\n\n확률변수: \\(X(\\omega)=x\\)\n확률벡터: \\({\\boldsymbol X}(\\omega)=(x_1,x_2,\\dots,x_m)\\)\n\n여기에서 확률벡터 \\({\\boldsymbol X}\\)가 볼드체인 이유는 realization이 스칼라가 아니라 벡터임을 강조하기 위해서이다.\n- 확률벡터는 확률변수들의 모임으로 해석할 수 있다. 즉\n\\[{\\boldsymbol X}=(X_1,X_2,\\dots,X_m)\\]\n와 같이 해석할 수 있다. 앞서서 \\(X(\\omega)=x\\) 와 같은 표현을 소개하였는데 이에 대응하는 확률벡터버전은 아래와 같다.\n\\[{\\boldsymbol X}(\\omega)=(X_1,X_2,\\dots,X_m)(\\omega)=(X_1(\\omega),X_2(\\omega),\\dots,X_m(\\omega))=(x_1,x_2,\\dots,x_m)\\]\n- 이 표현을 이해하는 것은 생각보다 쉽지 않다. 위의 기호중에서 가장 이해하기 쉬운것은 \\((x_1,\\dots,x_m)\\)이다. 시계열수업임을 감안하여 이를 해석해보도록 하자. 예를 들면 아래와 같이 해석할 수 있다.\n\n\\(x_1=\\) 2019-03-20, 삼성전자의 주식값\n\\(x_2=\\) 2019-03-21, 삼성전자의 주식값\n\n따라서\n\\[x_1,\\dots,x_m\\]\n은 삼성전자 주식의 주가를 2019년 3월20일부터 순서대로 \\(m\\)개 나열한 수열이라 볼 수 있다. 또한 \\((x_1,\\dots,x_m)\\)는 위의 수열의 원소를 모두 모아 벡터로 만든것이라 생각할 수 있다.\n- 이제 \\(X_1(\\omega)=x_1, \\dots, X_m(\\omega)=x_m\\) 라고 표현할 수 있음을 주의하자. \\(\\omega\\)만 알면 맵핑규칙에 따라 \\(x_1,\\dots,x_m\\)는 자동으로 결정된다. 그렇다면 여기에서 \\(\\omega\\)란 무엇일까?\n- 가능한 설명모형은 아래와 같다.\n\n14,000,605개의 평행세계가 있다고 가정하자.\n우리는 이중에 하나의 평행세계에 \\(\\frac{1}{14,000,605}\\)의 “확률”로 선택되어져 살고 있다고 생각하자.\n\n확률이라는 용어가 나온김에 좀 더 엄밀하게 정의해보자. 각 평행세계를 \\(\\omega_1,\\dots,\\omega_{14,000,605}\\)라고 정의하면\n\\[\\Omega=\\{\\omega_1,\\dots,\\omega_{14,000,605}\\}\\]\n이고\n\\[P(\\{\\omega_1\\})=\\dots=P(\\{\\omega_{14,000,605}\\})=\\frac{1}{14,000,605}\\]\n가 된다. 이제 다시 아래를 가정하자. \n\n하나의 평행세계가 결정되면 그 평행세계에 해당하는 모든 사건들이 이미 결정되어 있다. 가령 우리가 777번째 해당하는 평행세계로 선택되어 살고 있다고 하자. 즉 \\(\\omega=\\omega_{777}\\)이다. 삼성전자의 주식은 맵핑규칙 \\(X\\)에 따라 자동으로\n\n\\[x_1=X_1(\\omega_{777})\\dots,x_m=X_m(\\omega_{777})\\]\n와 같이 결정된다는 의미이다.\n- 이를 받아 들일 수 없는 사람도 있을 텐데 이는 미래가 고정되지 않고 불확실하다고 생각하기 때문이다. 예를들어 \\(m\\)시점의 삼성전자 주가가 \\(x_m=45,400\\)원이라고 가정하자. 그렇다면 다음날 삼성전자 주가 \\(x_{m+1}\\)은 \\(38,590\\sim 52,210\\) 사이의 어떤 값이다. 주가의 최소단위는 1원 단위이므로 삼성전자 주가를 기준으로 보면 내일은 총 \\(52,210-38,590=13,620\\)개의 새로운 미래가 가능하다. 미래는 언제가 열려있기에 하나의 평행세계가 결정된다고 그 다음을 항상 예측할 수 있는 것은 아니라 주장할 수 있다.\n- 이는 관점의 차이인데 어차피 두 관점은 같다. 왜냐하면 위의 주장은 다시 처음부터\n\\[14,000,605 \\times 13,620\\]\n개의 평행우주를 고려했다고 생각하면 해결된다. 즉 \\(m\\)개의 삼성전자 주가를 모두 파악하기 위해 \\(14,000,605\\)의 평행우주를 고려했고 다시 \\(m+1\\)의 삼성전자 주가를 모두 \\(14,000,605\\times 13,620\\)개의 평행우주를 고려했다고 주장하면 된다. 평행우주의 수는 편의에 따라 무한히 늘릴 수 있다.\n- 다시 본론으로 돌아오자. 아래과 같이 표현할 수도 있다.\n\\[X_m(\\omega)=X(\\omega,m)\\]\n예를들어 777번째 평행우주에서 시점 \\(m=5\\)에 해당하는 삼성전자 주가는\n\\[X_5(\\omega_{777})=X(\\omega_{777},5)\\]\n와 같이 정의할 수 있다.\n- 확률변수는 \\(\\Omega\\)의 원소 \\(\\omega\\)에 하나의 실수값이 대응한다. 하지만 확률벡터는 원소 \\(\\omega\\)에 두 개 이상의 실수값이 대응한다. 상상력을 발휘하면 하나의 원소 \\(\\omega\\)에 무한개의 실수값을 대응시키는 어떠한 존재를 생각할 수 있다. 즉 하나의 원소 \\(\\omega\\)에 대응하는 realization이 함수인 경우를 상상할 수 있다. 이를 확률과정이라고 한다. 확률과정은 아래와 같이 표현할 수 있다.\n\\[X(\\omega,t)=x(t)\\]\n즉 \\(x(t)\\)는 고정된 \\(\\omega\\)에 대한 realization이다. 이를 특별히 sample path 혹은 sample function이라고 부른다. \\(X(\\omega,t)\\)는 상당히 오묘한데 \\(t\\)를 fix하면 확률변수가 되고 \\(\\omega\\)를 fix하면 sample path가 된다. 확률변수를 나타낼때 \\(X(\\omega)\\)를 줄여서 \\(X\\)로 표현하듯이 확률과정을 나타낼때에도 \\(X(\\omega,t)\\)를 줄여 간단히 \\(X(t)\\)라고 쓴다.\n- 교재의 표현을 정리하면 아래와 같다.\n\n확률과정(stochastic process): \\(X(t)\\)\n확률법칙(probability law): 확률. 즉 \\(P:{\\cal F}\\to [0,1]\\).\n확률공간(probability space): \\((\\Omega,{\\cal F},P)\\).\n확률변수(random variable): \\(X\\).\n확률변수들의 모임(random vector,stochastic process):** \\({\\boldsymbol X}\\), \\(X(t)\\)\n집합 \\(T\\)(index set): 확률변수들의 모임 즉 확률벡터 혹은 확률과정에서 확률변수의 index를 표시해주는 집합. 따라서 집합 \\(T\\)의 원소가 하나만 있으면 확률변수를 의미하고, 집합 \\(T\\)의 원소가 유한개이면 확률벡터, 집합 \\(T\\)의 원소가 무한개이면 확률과정이라고 말한다.\n연속형 확률과정: 집합 \\(T\\)가 \\((0,\\infty)\\)와 같이 구간으로 표현된 경우의 확률과정. 즉 집합 \\(T\\)의 원소가 셀 수 없는 무한 (uncountable many)일 경우의 확률과정.\n이산형 확률과정: 집합 \\(T\\)가 \\(\\{1,2,\\dots,\\}\\) 혹은 \\(\\{\\dots,-1,0,1,2,3,\\dots\\}\\) 와 같이 셀 수 있는 무한 (countable many)로 표현된 경우의 확률과정. 이는 무한차원을 가지는 확률벡터로 이해할 수도 있고, 연속형 확률과정의 샘플버전으로 이해할 수도 있다. 우리가 다루는 교재의 시계열은 이산형 확률과정을 의미한다.\n실현값(realization): 확률변수, 확률벡터, 확률과정 따위의 실현값. 즉 \\(\\omega\\)가 고정되었을 경우 맵핑되는 결과. 즉 \\(x\\), \\((x_1,\\dots,x_m)\\) 혹은 \\(x(t)\\).\n표본통로(sample path): 확률과정의 실현값을 특별히 부르는 말. 즉 \\(x(t)\\).\n\n교재의 표현중 “우리가 만일 과거로 돌아갈 수 있어 반복관측을 할 수 있다면 현재 관측된 시계열은 무한히 많이 관측 가능한 확률변수들의 모임 중에서 특별히 실현된 하나에 해당할 것이다.” 의 의미는 “우리가 임의의 평행세계 중 하나로 자유롭게 이동할 수 있어 각 평행세계에서 시계열을 반복관측 할 수 있다고 하자. 그렇다면 현재 우리가 세계에서 관측된 시계열은 무한히 많은 평행세계 중 특별히 실현된 하나에 해당할 뿐이다.” 라는 의미이다. 또한 교재의 표현 중 “지구과학에서 다루는 확률과정에서의 집합 \\(T\\)처럼 지구표현의 위치 또는 어느 지역의 위치 등을 나타내는 위치의 집합인 경우” 는 index set \\(T\\)가\n\\[T=\\{2019-03-20,2019-03-21,\\dots, \\}\\]\n와 같이 시간순서로 나열된것이 아니라\n\\[T=\\{(20,29),(41,30),\\dots, \\}\\]\n아래와 같이 (위도,경도)와 같은 공간인덱스로 나열된 경우를 의미한다. 시계열은 자료가 가까운 시간끼리 상관관계가 있는 것처럼, 지구과학에서 다루는 자료는 가까운 공간끼리 상관관계가 있다. 예를들어 2019-03-20의 삼성전자 주식값과 2019-03-21의 삼성전자 주식값은 서로 연관이 있는것 처럼 (위도 30, 경도 29) 에서의 기온과 (위도 30, 경도 28) 에서의 기온도 서로 연관이 있다.\n\n\n정상성을 왜 가정해야 하는가?\n- \\(X(t)\\), \\(X_t(\\omega)\\), \\(X(\\omega,t)\\), \\(X(\\omega; t)\\), \\(X(t;\\omega)\\), \\(\\{X(\\omega,t): t \\in T\\}\\), \\(\\{X(\\omega,t)\\}\\) 를 확률과정이라고 하자. 만약에 여러 \\(\\omega\\)를 고려하면 여러 뭉치의 sample path가 얻어진다. 그럼 모든 \\(\\omega\\)에 대하여 가능한 sample path를 모두 모은 집합을 생각할 수 있는데 이러한 집합을 앙상블(ensemble)이라고 한다. 즉 앙상블은 아래처럼 각 원소가 함수인 집합이다.\n\n\\(\\Big\\{ \\{X(\\omega_1,t): t \\in T\\},\\{X(\\omega_2,t): t \\in T\\}, \\dots \\Big\\}\\)\n\\(\\Big\\{ X(\\omega_1,t),X(\\omega_2,t), \\dots \\Big\\}\\)\n각 평행세계 \\(\\omega_1,\\omega_2,\\dots\\) 를 자유롭게 선택하여 관측할 수 있는 존재를 상상하자. 편의상 이러한 존재를 “신”이라고 부르자. 신이 \\(\\omega_1,\\dots,\\omega_{776}\\)개의 평행우주에 대하여 삼성전자 주식을 모두 관측하였다고 하자. 하지만 신은 우리가 살고 있는 평행우주 (예를들면 \\(\\omega_{777}\\)) 에 대한 결과는 아직 관측하지 않았다고 하자. 이 존재에게 \\(m\\)시점의 삼성전자 주식이 얼마냐고 묻는다면 신은 뭐라고 대답할까? 아마도 776개의 평행우주에서 관측한 삼성전자 주식의 값을 평균내서 대답할 것이다. 즉 아래와 같이 추론할 것이다.\n예측값 \\(= \\frac{1}{776}\\sum_{i=1}^{776} X(\\omega_i,m)\\)\n\n이 추론은 매우 합리적이다. 그리고 이 추론은 신이 더 많은 평행세계를 관측할수록 정확도가 올라간다. 만약에 신이 가능한 모든 앙상블을 관측하여 \\(m\\)시점의 삼성전자 주식값을 평균냈다고 하자. 이 값을 \\(m\\)시점의 ensemble average라 부른다. 시점 \\(m\\)의 앙상블평균은 아래와 같이 친숙한 기호로 표현할 수 있다.\n\\[E(X_m)=\\int_{\\omega \\in \\Omega}X_m(\\omega)dP=\\int_{-\\infty}^{\\infty}x_mf(x)dx\\]\n여기에서 \\(f(x)\\)는 확률변수 \\(X_m\\)의 pdf이다.\n- 우리의 목표는 결국 신만이 알고 있을 \\(E(X_m)\\)을 추론하는 것이다. 하지만 시계열 자료에서는 \\(E(X_m)\\)을 적절히 추론하는게 매우 어려운데 이는 시계열분석의 경우 분석의 대상이 되는 자료가 반복관측될 수 없기 때문이다. (교재 5장 intro 맨 마지막 부분 참고.) 즉 지금까지 수행하였던 분석 회귀분석등은 여러개의 realization이 존재했다. 하지만 시계열 분석의 경우 오직 하나의 realization만이 존재하기 때문에 어떠한 분석도 수행할 수 없다.\n- (해결책) 만약 관측된 하나의 realization \\(x(t)\\)를 여러개의 realization의 묶음으로 볼 수 있다면 어떨까? 예를들어 동전을 3번 던지는 경우를 생각하자. 가능한 평행세계는 모두 8개이며 각각 아래와 같다.\n\n\\(\\Omega=\\{\\omega_1,\\dots,\\omega_8\\}=\\{HHH,HHT,\\dots,TTT\\}\\)\n\n이제 아래와 같은 맵핑규칙을 고려하자.\n\n\\(\\boldsymbol{X}(HHH)=(X_1,X_2,X_3)(HHH)=(0,0,0)\\)\n\\(\\boldsymbol{X}(TTT)=(X_1,X_2,X_3)(TTT)=(1,1,1)\\)\n\n\\(\\omega\\)를 3차원 벡터로 연결하였으므로 \\({\\boldsymbol X}\\)는 확률벡터가 된다. 동전을 무한번 던졌다고 가정하면 가능한 평행세계는 무한개이다. 따라서 이 경우\n\\[\\{X_t: t=1,2,\\dots,\\}\\]\n는 이산형확률과정이 된다. (이러한 과정을 베르누이 확률과정이라고 부른다.) 우리가 이 중 $t^* $번째 시점에서 동전이 앞면이 나올지 뒷면이 나올지 알고 싶다고 하자. 직관적으로 앞면이 나올 확률과 뒷면이 나올 확률은 같으므로 \\(E(X_{t^* })=\\frac{1}{2}\\times 0 +\\frac{1}{2}\\times 1 =\\frac{1}{2}\\)임을 알 수 있다.\n- 어떤 사람이 정말 \\(E(X_{t^* })=\\frac{1}{2}\\)이냐고 그렇게 생각한 근거가 뭐냐고 따질때 방어할 수 있는 논리가 무엇일까? 만약에 우리가 \\(n\\)개의 평행세계를 각각의 평행세계에서 $t^* $시점에 해당하는 realization을 평균내어 추정했다고 하면 될 것이다. (그리고 관측한 평행세계가 많을 수록 즉 \\(n\\to\\infty\\) 일수록 그 추정치가 \\(\\frac{1}{2}\\)로 수렴함을 보이면 설득될 것이다.)\n- 그런데 우리는 각 평행세계를 이동할 수 없으므로 이는 불가능하다. 즉 항상 \\(n=1\\)이다. 하지만 이 예제의 경우 쉽게 대안을 찾을 수 있는데 대안은 바로 관측된 값들을 평균내는 것이다. 즉 순차적으로 실험을 하여 \\(t^* - 1\\)시점까지의 자료\n\\[x_1,x_2,x_3,\\dots,x_{t^* -1}\\]\n을 확보했다고 하자. 그러면 \\(E(X_{t^* })\\)의 값을 단순히 아래와 같이 추론할 수 있다.\n\\[\\frac{1}{t^* -1} \\sum_{i=1}^{t^* -1 } x_i\\]\n이러한 평균을 time average라고 한다. 직관적으로 이 추론은 매우 합리적으로 느껴지는데 그 이유는 우리가 \\(X_1,X_2,\\dots,\\)이 i.i.d.임을 이해하고 있기 때문이다.\n- 결국 어떠한 이산형확률과정을 구성하는 각 확률변수들이 i.i.d.임을 가정한다면 (이를 백색잡음과정이라고 한다, 교재 5.2.1절 참고) 단순히 time average로 ensemble average를 추론할 수 있다. 이는 매우 당연하지만 놀라울 정도로 쓸모없는 결과인데 사실 대부분의 시계열자료는 i.i.d.를 만족하지 않기 때문이다. (이것이 만족되면 회귀분석을 하면 된다.) 그래서 i.i.d 보다 좀 더 약화된 가정을 구하려는 노력이 필요한데 예를 들면 독립가정을 uncorrelatedness로 바꾸는 것 따위의 노력이다. 정상성 즉 stationary는 이러한 노력의 결정체라고 이해해도 무방하다. 추후에 엄밀한 정의를 다루겠지만 정상성은 i.i.d. 와 uncorrelatedness보다 훨씬 약한 가정이다. 즉 \\(X_t\\)와 \\(X_{t+1}\\)는 서로 독립일 필요도 없으며 서로 무상관일 필요도 없다.\n- 위의 논의를 잘 곱씹어보면 정상성이 가정되지 않으면 하나의 시계열을 여러 확률변수들의 묶음으로 파악하여 추론하는 형태의 접근법을 사용하는 것이 매우 어려움을 의미한다. 따라서 시계열 분석에서는 항상 관심의 대상이 되는 확률과정이 정상성을 가지는지 따져봐야 한다.\n\n\n\n\n\nFootnotes\n\n\n그 우리가 맨날 쓰는거..↩︎\n\\(\\mathbb{Q}_{\\sqrt{2}}\\)는 구간 \\(\\big[\\sqrt{2},~~ \\sqrt{2}+\\pi\\big)\\)에 속하므로↩︎\n당연한거 아냐? \\(m(\\mathbb{Q})=\\alpha\\)이고 \\(\\mathbb{Q}_{\\sqrt{2}}\\)는 단지 \\(\\mathbb{Q}\\)를 \\(\\sqrt{2}\\)만큼 평행이동한 집합일 뿐이니까 길이는 변화없음↩︎"
  },
  {
    "objectID": "posts/1_Essays/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "href": "posts/1_Essays/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "title": "[Essays] 수통문제: type1 err, type2 err",
    "section": "",
    "text": "using Distributions, Plots\n\n(문제) \\(X_1,X_2\\)가 평균이 \\(\\theta\\)인 지수분포에서 추출한 랜덤표본이라고 하자. 가설 \\(H_0: \\theta=2\\) vs \\(H_1:\\theta=1\\) 에 대하여, \\(H_0\\)에 대한 기각영역을\n\\[\\frac{f(x_1;\\theta=2)f(x_2;\\theta=2)}{f(x_1;\\theta=1)f(x_2;\\theta=1)}<\\frac{1}{2}\\]\n와 같이 설정하자. 이와 같은 검정법에 대한 \\(\\alpha\\)와 \\(\\beta\\)를 구하라.\n(풀이)\n문제요약\n\n\\(f(x) = \\frac{1}{\\theta} \\exp(-\\frac{x}{\\theta})\\) -> 평균이 \\(\\theta\\) 인 지수분포\n검정통계량: \\(T=\\frac{f(x_1;2)f(x_2;2)}{f(x_1;1)f(x_2;1)}\\)\n\\(\\alpha = P(\\text{Reject $H_0$|$H_0$ is true}) = P(T<\\frac{1}{2} | \\text{$H_0$ is true})\\)\n\\(\\beta = P(\\text{Accept $H_0$|$H_1$ is true}) = P(T>\\frac{1}{2} | \\text{$H_1$ is true})\\)\n\n풀이시작\n\nT= x -> 0.5*exp(-0.5*x[1]) * 0.5*exp(-0.5*x[2])  / (exp(-x[1])*exp(-x[2]))\n\n#1 (generic function with 1 method)\n\n\n\\(\\alpha\\)를 구해보자. (시뮬)\n\nθ=2 \nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.4932782791687658\n 3.904496314340747\n\n\n\nT(x)\n\n3.715796051759978\n\n\n\nTs = [rand(Exponential(θ),2) |> T for i in 1:1400000]\nmean(Ts .< 1/2)\n\n0.1535007142857143\n\n\n\\(\\beta\\)를 구해보자. (시뮬)\n\nθ=1\nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.0915718974295616\n 3.322182470278192\n\n\n\nTs = [rand(Exponential(θ),2) |> T for i in 1:1400000]\nmean(Ts .> 1/2)\n\n0.5967985714285714\n\n\n\\(\\alpha\\)를 구해보자. (이론)\n\\(T(X_1,X_2) = \\frac{0.25\\exp(-0.5X_1 -0.5X_2)}{\\exp(-X_1-X_2)}=0.25\\exp(0.5X_1+0.5X_2)\\)\n$T(X_1,X_2)< (0.5X_1+0.5X_2) < 2 X_1+X_2< 2 $\n그런데 \\(X_1+X_2 \\sim \\chi^2(4)\\) under \\(H_0\\)\n\\(P(X_1+X_2 < 2\\ln2) = \\int_0^{2\\ln2} \\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_0^{\\ln2} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_0^{\\ln2}\\)\n\nt = log(2) \nu = t*(-exp(-t)) - exp(-t)\nt = 0\nl = t*(-exp(-t)) - exp(-t)\n\n-1.0\n\n\n\nu-l\n\n0.1534264097200273\n\n\n\\(\\beta\\)를 구해보자. (이론)\n$T(X_1,X_2)> (0.5X_1+0.5X_2) > 2 (X_1+X_2)> 4 $\n그런데 \\(2(X_1+X_2) \\sim \\chi^2(4)\\) under \\(H_1\\)\n\\(P(2(X_1+X_2) > 4\\ln2) = \\int_{4\\ln2}^{\\infty}\\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_{2\\ln2}^{\\infty} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_{2\\ln2}^{\\infty}\\)\n\nu = 0\nt = 2*log(2)\nl = t*(-exp(-t)) - exp(-t)\nu-l\n\n0.5965735902799727"
  },
  {
    "objectID": "posts/1_Essays/2023-06-27-힐버트스페이스.html",
    "href": "posts/1_Essays/2023-06-27-힐버트스페이스.html",
    "title": "신록예찬's Blog",
    "section": "",
    "text": "image.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n세미놈..\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n왜..?\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "3_hst.html",
    "href": "3_hst.html",
    "title": "HST",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 24, 2023\n\n\n[HST] CommunityDetection\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_pinkocto.html",
    "href": "3_pinkocto.html",
    "title": "PINKOCTO",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬\n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_python.html",
    "href": "2_python.html",
    "title": "Python",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 5, 2000\n\n\n[Python] Numpy\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_reviews.html",
    "href": "2_reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_essays.html",
    "href": "1_essays.html",
    "title": "Essays",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\n[Essays] 마코프체인 인트로\n\n\n신록예찬\n\n\n\n\nMar 1, 2023\n\n\n[Essays] 수통문제: type1 err, type2 err\n\n\n신록예찬\n\n\n\n\nJan 20, 2023\n\n\n[Essays] 시계열의 주파수영역 분석\n\n\n신록예찬\n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬\n\n\n\n\nJan 12, 2023\n\n\n[Essays] 수리문제: 지수분포 평균검정\n\n\n신록예찬\n\n\n\n\nJan 12, 2023\n\n\n[Essays] 토폴로지(1)\n\n\n신록예찬\n\n\n\n\nJan 11, 2023\n\n\n[Essays] 여러가지 부등식\n\n\n신록예찬\n\n\n\n\nJan 7, 2023\n\n\n[Essays] 해석학(1)\n\n\n신록예찬\n\n\n\n\nMar 12, 2020\n\n\n[Essays] 확률, 확률변수, 시계열, 정상성\n\n\n신록예찬\n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "4_notes.html",
    "href": "4_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 8, 2000\n\n\n[Note] 깃(Git)\n\n\n신록예찬\n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬\n\n\n\n\nJan 6, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬\n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬\n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_cgsp.html",
    "href": "2_cgsp.html",
    "title": "CGSP",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬\n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬\n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬\n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "신록예찬's Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2099\n\n\n연습장1\n\n\n최규빈\n\n\n\n\nJan 1, 2099\n\n\n연습장2\n\n\n신록예찬\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬\n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬\n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬\n\n\n\n\nMay 19, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬\n\n\n\n\nMay 19, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬\n\n\n\n\nMay 12, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬\n\n\n\n\nMay 12, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬\n\n\n\n\nMay 6, 2023\n\n\n[BORAM] 신용카드 거래 사기탐지 Start\n\n\n신록예찬\n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연\n\n\n\n\nApr 13, 2023\n\n\n[Essays] 마코프체인 인트로\n\n\n신록예찬\n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬\n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 1, 2023\n\n\n[Essays] 수통문제: type1 err, type2 err\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - Graph metrics\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - networkx로 그래프 이해하기\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap1: 시작하기 - 밴치마크 및 저장소\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap2: 그래프 머신러닝 - Node2Vec, Edge2Vec, Graph2Vec\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - AutoEncoder\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Graph Neural Network\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Shallow_Embeddings\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap3: 비지도 그래프 학습 - Structural Deep Network Embedding\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - Graph CNN\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 그래프 정규화 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 얕은 임베딩 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap4: 지도 그래프 학습 - 특징 기반 방법\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap5: 응용문제 - 누락된 링크예측\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap5: 응용문제 - 커뮤니티 감지\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap6: 소셜네트워크 그래프\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Graph Neural Network Topic Classifier\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Shallow-Learning Topic Modelling\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap7: Text Analytics and Natural Language Processing using Graphs\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap8: 신용카드 거래에 대한 그래프 분석\n\n\n신록예찬\n\n\n\n\nFeb 10, 2023\n\n\n[GML] Chap9: Graph Database Connection\n\n\n신록예찬\n\n\n\n\nJan 24, 2023\n\n\n[HST] CommunityDetection\n\n\n신록예찬\n\n\n\n\nJan 20, 2023\n\n\n[Essays] 시계열의 주파수영역 분석\n\n\n신록예찬\n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬\n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬\n\n\n\n\nJan 12, 2023\n\n\n[Essays] 수리문제: 지수분포 평균검정\n\n\n신록예찬\n\n\n\n\nJan 12, 2023\n\n\n[Essays] 토폴로지(1)\n\n\n신록예찬\n\n\n\n\nJan 11, 2023\n\n\n[Essays] 여러가지 부등식\n\n\n신록예찬\n\n\n\n\nJan 7, 2023\n\n\n[Essays] 해석학(1)\n\n\n신록예찬\n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬\n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬\n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬\n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬\n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬\n\n\n\n\nMar 12, 2020\n\n\n[Essays] 확률, 확률변수, 시계열, 정상성\n\n\n신록예찬\n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬\n\n\n\n\nApr 5, 2000\n\n\n[Python] Numpy\n\n\n신록예찬\n\n\n\n\nJan 8, 2000\n\n\n[Note] 깃(Git)\n\n\n신록예찬\n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬\n\n\n\n\nJan 6, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬\n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬\n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_itstgcn.html",
    "href": "3_itstgcn.html",
    "title": "IT-STGCN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연\n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_solar.html",
    "href": "3_solar.html",
    "title": "SOLAR",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬\n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\n\n\nNo matching items"
  }
]