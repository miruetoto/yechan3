[
  {
    "objectID": "3_boram.html",
    "href": "3_boram.html",
    "title": "BORAM-COCO",
    "section": "",
    "text": "https://github.com/boram-coco\nhttps://boram-coco.github.io/coco/\nhttps://boram-coco.github.io/Scribbling/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n[FRAUD] 데이터정리 시도(8.25 df02 커널죽음)\n\n\n \n\n\n\n\nJul 30, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN ToyEX\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬 \n\n\n\n\nMay 6, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Start\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_cgsp.html",
    "href": "2_cgsp.html",
    "title": "CGSP",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬 \n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬 \n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬 \n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_pl.html",
    "href": "2_pl.html",
    "title": "PyTorch Lightning",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_reviews.html",
    "href": "2_reviews.html",
    "title": "Reviews",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 19, 2023\n\n\n[Review] 사기탐지 + 그래프 관련연구\n\n\n신록예찬 \n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "1_essays.html",
    "href": "1_essays.html",
    "title": "Essays",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nJan 1, 2099\n\n\n[Essays] ggplot3\n\n\n신록예찬 \n\n\n\n\nAug 30, 2023\n\n\n[Essays] 강화학습(3) – LunarLander\n\n\n신록예찬 \n\n\n\n\nAug 23, 2023\n\n\n[Essays] 강화학습(1) – bandit\n\n\n신록예찬 \n\n\n\n\nAug 23, 2023\n\n\n[Essays] 강화학습(2) – 4x4 grid\n\n\n신록예찬 \n\n\n\n\nAug 16, 2023\n\n\n[Essays] 루나랜더 v2 (간략화)\n\n\n신록예찬 \n\n\n\n\nAug 7, 2023\n\n\n[Essays] 루나랜더 v1\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[Essays] 다중척도방법\n\n\n신록예찬 \n\n\n\n\nJul 4, 2023\n\n\n[Essays] 토폴로지\n\n\n신록예찬 \n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬 \n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_mp.html",
    "href": "2_mp.html",
    "title": "MP",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 5, 2023\n\n\n01wk-006: 코랩에서 분석\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-007: 아나콘다 가상환경 설정\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-008: 개인컴퓨터에서 분석\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-009: 타이타닉 자료 분석\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, 첫 분석\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, 피처엔지니어링\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 오버피팅\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: Autogluon, Fit\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-009: 타이타닉, best_quality\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-01-CTGAN.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-01-CTGAN.html",
    "title": "[CTGAN] CTGAN",
    "section": "",
    "text": "!conda env list\n\n# conda environments:\n#\nbase                     /home/cgb2/anaconda3\nctgan                 *  /home/cgb2/anaconda3/envs/ctgan\npy38                     /home/cgb2/anaconda3/envs/py38\nstgcn                    /home/cgb2/anaconda3/envs/stgcn\n\n\n\n- Ref: https://github.com/sdv-dev/CTGAN\n\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\nreal_data = load_demo()\n\n# Names of the columns that are discrete\ndiscrete_columns = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n    'income'\n]\n\nctgan = CTGAN(epochs=10)\nctgan.fit(real_data, discrete_columns)\n\n# Create synthetic data\nsynthetic_data = ctgan.sample(1000)\n\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/torch/cuda/__init__.py:126: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1688109080043/work/c10/cuda/CUDAFunctions.cpp:108.)\n  return torch._C._cuda_getDeviceCount() &gt; 0\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n\n\n- 실제자료\n\nreal_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n3\n53\nPrivate\n234721\n11th\n7\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n4\n28\nPrivate\n338409\nBachelors\n13\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFemale\n0\n0\n40\nCuba\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n32556\n27\nPrivate\n257302\nAssoc-acdm\n12\nMarried-civ-spouse\nTech-support\nWife\nWhite\nFemale\n0\n0\n38\nUnited-States\n&lt;=50K\n\n\n32557\n40\nPrivate\n154374\nHS-grad\n9\nMarried-civ-spouse\nMachine-op-inspct\nHusband\nWhite\nMale\n0\n0\n40\nUnited-States\n&gt;50K\n\n\n32558\n58\nPrivate\n151910\nHS-grad\n9\nWidowed\nAdm-clerical\nUnmarried\nWhite\nFemale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n32559\n22\nPrivate\n201490\nHS-grad\n9\nNever-married\nAdm-clerical\nOwn-child\nWhite\nMale\n0\n0\n20\nUnited-States\n&lt;=50K\n\n\n32560\n52\nSelf-emp-inc\n287927\nHS-grad\n9\nMarried-civ-spouse\nExec-managerial\nWife\nWhite\nFemale\n15024\n0\n40\nUnited-States\n&gt;50K\n\n\n\n\n32561 rows × 15 columns\n\n\n\n- 합성된자료\n\nsynthetic_data\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\nsex\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n31\nPrivate\n317142\nAssoc-acdm\n10\nMarried-spouse-absent\nTransport-moving\nOwn-child\nWhite\nMale\n92\n0\n56\nChina\n&lt;=50K\n\n\n1\n40\nPrivate\n248817\nSome-college\n11\nMarried-civ-spouse\nProf-specialty\nOwn-child\nWhite\nMale\n-101\n4\n40\nUnited-States\n&lt;=50K\n\n\n2\n21\nPrivate\n201022\nBachelors\n2\nNever-married\nOther-service\nOwn-child\nWhite\nMale\n-20\n-4\n40\nUnited-States\n&lt;=50K\n\n\n3\n33\nFederal-gov\n407226\nMasters\n5\nNever-married\nOther-service\nNot-in-family\nWhite\nFemale\n-103\n1683\n61\nYugoslavia\n&lt;=50K\n\n\n4\n49\nFederal-gov\n261481\nSome-college\n14\nNever-married\nFarming-fishing\nOwn-child\nWhite\nFemale\n5\n-4\n40\nUnited-States\n&lt;=50K\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n39\nPrivate\n228408\nHS-grad\n13\nMarried-civ-spouse\nProf-specialty\nNot-in-family\nAsian-Pac-Islander\nFemale\n-73\n-2\n16\nPhilippines\n&lt;=50K\n\n\n996\n31\nPrivate\n125109\nHS-grad\n0\nNever-married\nCraft-repair\nNot-in-family\nWhite\nFemale\n-86\n-4\n59\nUnited-States\n&lt;=50K\n\n\n997\n31\nPrivate\n291146\n11th\n13\nNever-married\nAdm-clerical\nWife\nBlack\nFemale\n-52\n5\n40\nUnited-States\n&gt;50K\n\n\n998\n25\nPrivate\n170115\nSome-college\n13\nNever-married\nMachine-op-inspct\nOwn-child\nWhite\nFemale\n-124\n-4\n40\nUnited-States\n&lt;=50K\n\n\n999\n37\nWithout-pay\n363805\nProf-school\n10\nMarried-civ-spouse\nSales\nUnmarried\nWhite\nFemale\n51\n-5\n40\nIran\n&lt;=50K\n\n\n\n\n1000 rows × 15 columns"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try1변형.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-12-try1.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try1",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2 변형",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-24-try2변형.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2 변형",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\ndef amtano1(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano=0)\n    normalize = lambda arr: (arr-np.median(arr))/np.std(arr) if np.std(arr)!=0 else arr*0 \n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano']] = normalize(sub_df.amt).cumsum()\n    return df"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-19-try2.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Try2",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-04-EPT.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-04-EPT.html",
    "title": "[SOLAR] EPT",
    "section": "",
    "text": "ref: https://www.sciencedirect.com/science/article/pii/S2352711021000492\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(EPT)\n\n\nurl = 'https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv'\ndf = read_csv(url)\n\nRows: 803000 Columns: 3\n── Column specification ────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): region, date\ndbl (1): solar_radiation\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndf = df |&gt; filter(region == '북춘천') |&gt; mutate(date=ymd_hm(date))\ndf\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dttm&gt;\n\n\n\n\n북춘천\n0.00\n2021-01-01 00:00:00\n\n\n북춘천\n0.00\n2021-01-01 01:00:00\n\n\n북춘천\n0.00\n2021-01-01 02:00:00\n\n\n북춘천\n0.00\n2021-01-01 03:00:00\n\n\n북춘천\n0.00\n2021-01-01 04:00:00\n\n\n북춘천\n0.00\n2021-01-01 05:00:00\n\n\n북춘천\n0.00\n2021-01-01 06:00:00\n\n\n북춘천\n0.00\n2021-01-01 07:00:00\n\n\n북춘천\n0.00\n2021-01-01 08:00:00\n\n\n북춘천\n0.37\n2021-01-01 09:00:00\n\n\n북춘천\n0.96\n2021-01-01 10:00:00\n\n\n북춘천\n1.40\n2021-01-01 11:00:00\n\n\n북춘천\n1.72\n2021-01-01 12:00:00\n\n\n북춘천\n1.84\n2021-01-01 13:00:00\n\n\n북춘천\n1.74\n2021-01-01 14:00:00\n\n\n북춘천\n1.30\n2021-01-01 15:00:00\n\n\n북춘천\n0.93\n2021-01-01 16:00:00\n\n\n북춘천\n0.29\n2021-01-01 17:00:00\n\n\n북춘천\n0.01\n2021-01-01 18:00:00\n\n\n북춘천\n0.00\n2021-01-01 19:00:00\n\n\n북춘천\n0.00\n2021-01-01 07:00:00\n\n\n북춘천\n0.00\n2021-01-01 20:00:00\n\n\n북춘천\n0.00\n2021-01-01 06:00:00\n\n\n북춘천\n0.00\n2021-01-01 21:00:00\n\n\n북춘천\n0.00\n2021-01-01 05:00:00\n\n\n북춘천\n0.00\n2021-01-02 00:00:00\n\n\n북춘천\n0.00\n2021-01-02 01:00:00\n\n\n북춘천\n0.00\n2021-01-02 02:00:00\n\n\n북춘천\n0.00\n2021-01-02 03:00:00\n\n\n북춘천\n0.00\n2021-01-02 04:00:00\n\n\n⋮\n⋮\n⋮\n\n\n북춘천\n0.00\n2022-12-30 07:00:00\n\n\n북춘천\n0.00\n2022-12-30 20:00:00\n\n\n북춘천\n0.00\n2022-12-30 06:00:00\n\n\n북춘천\n0.00\n2022-12-30 21:00:00\n\n\n북춘천\n0.00\n2022-12-30 05:00:00\n\n\n북춘천\n0.00\n2022-12-31 00:00:00\n\n\n북춘천\n0.00\n2022-12-31 01:00:00\n\n\n북춘천\n0.00\n2022-12-31 02:00:00\n\n\n북춘천\n0.00\n2022-12-31 03:00:00\n\n\n북춘천\n0.00\n2022-12-31 04:00:00\n\n\n북춘천\n0.00\n2022-12-31 05:00:00\n\n\n북춘천\n0.00\n2022-12-31 06:00:00\n\n\n북춘천\n0.00\n2022-12-31 07:00:00\n\n\n북춘천\n0.00\n2022-12-31 08:00:00\n\n\n북춘천\n0.22\n2022-12-31 09:00:00\n\n\n북춘천\n0.50\n2022-12-31 10:00:00\n\n\n북춘천\n0.80\n2022-12-31 11:00:00\n\n\n북춘천\n2.13\n2022-12-31 12:00:00\n\n\n북춘천\n1.78\n2022-12-31 13:00:00\n\n\n북춘천\n1.45\n2022-12-31 14:00:00\n\n\n북춘천\n0.78\n2022-12-31 15:00:00\n\n\n북춘천\n0.38\n2022-12-31 16:00:00\n\n\n북춘천\n0.15\n2022-12-31 17:00:00\n\n\n북춘천\n0.00\n2022-12-31 18:00:00\n\n\n북춘천\n0.00\n2022-12-31 19:00:00\n\n\n북춘천\n0.00\n2022-12-31 07:00:00\n\n\n북춘천\n0.00\n2022-12-31 20:00:00\n\n\n북춘천\n0.00\n2022-12-31 06:00:00\n\n\n북춘천\n0.00\n2022-12-31 21:00:00\n\n\n북춘천\n0.00\n2022-12-31 05:00:00\n\n\n\nA tibble: 18250 × 3\n\n- 지역을 북춘천으로 고정\n\ndf2 = df |&gt; filter(region =='북춘천') \ndf2 = df2[order(df2$date),]\n\n\ny = df2$solar_radiation\ny\n\n\n0000000000000.370.961.41.721.841.741.30.930.290.010000000000000000.320.951.461.791.911.821.50.970.370.010000000000000000.290.891.41.751.761.350.820.520.30.010000000000000000.330.931.321.531.671.5110.790.1900000000000000000.190.791.411.761.931.851.561.050.450.020000000000000000.511.381.811.881.931.851.5210.410.020000000000000000.110.741.441.811.981.911.611.090.450.020000000000000000.360.971.491.8521.911.611.10.480.03000⋯000000000000.010.471.31.711.871.881.721.380.880.3200000000000000000.190.681.351.712.191.891.380.820.2600000000000000000.150.420.931.071.181.051.160.830.300000000000000000.080.280.430.741.41.841.530.850.2700000000000000000.411.021.841.921.851.721.420.930.340.010000000000000000.471.281.681.81.851.741.40.890.30.010000000000000000.150.641.11.732.121.741.290.860.310.010000000000000000.220.50.82.131.781.450.780.380.150000\n\n\n\nplot(y[1:500])\nlines(y[1:500],lty=2)\n\n\n\n\n- EPT 수행\n\nept = function(y){\n    EpM = eptransf(signal=y,tau=24,process=c(\"envelope\",\"average\"))$EpM\n    EpM*2\n}\n\n\nyU = ept(y)\n\n\nplot(y[1:500])\nlines(yU[1:500],col=2,lty=2)\n\n\n\n\n- todo: 모든 지역에대하여 yU를 구하여 저장"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-17-EPT-RGCN.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-17-EPT-RGCN.html",
    "title": "[SOLAR] EPT + RGCN",
    "section": "",
    "text": "Import\n- source\n\nhttps://github.com/miruetoto/yechan3/blob/main/posts/3_Researches/PINKOCTO/SOLAR/0720.py\n\n- download source\n\n# https://github.com/miruetoto/yechan3/blob/main/posts/3_Researches/PINKOCTO/SOLAR/0720.py\n\n- run\n\n%run 0720.py\n\n\n\nLoad data\n\n# read dataframe \ndf = pd.read_csv('https://media.githubusercontent.com/media/miruetoto/yechan3/main/posts/3_Researches/PINKOCTO/SOLAR/data_eng_230710.csv')\n\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)\nt = df.loc[:,'date']\nregions = list(df.loc[:,'Bukchoncheon':'Gyeongju-si'].columns)\n\n# plot rawdata \nstart = 50 \nend = 50+24*3\ncity = 19 # 광주 \nwith plt.style.context('cyberpunk'):\n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(t[start:end],y[start:end,city],'o',label='y')\n    plt.plot(t[start:end],yU[start:end,city],'--',label='yU')\n    plt.plot(t[start:end],yP[start:end,city],'o',label='y/yU')\n    tick_interval = 4\n    plt.xticks(range(1, len(t) + 1, tick_interval),rotation=25)        \n    plt.legend()\n    plt.title(\"The solar radiation data in {} from {} to {} \".format(regions[city],t.to_list()[0][:10],t.to_list()[-1][:10]))\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n\n\nLearn\n- y \\(\\to\\) yhat\n\n# hyper params\nLAGS = 4\nFILTERS = 8\nEPOCH = 5\n\n# global params\nTRAIN_RATIO = 0.8\nT,N = len(t),len(regions) \nLEN_TEST = int(np.floor(T*(1-TRAIN_RATIO)))\nLEN_TR = T - LEN_TEST\n\n# 방법1\nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyhat = rgcn(y, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\nyhat[yhat &lt; 0]= 0 \n\n# 방법2 \nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyUhat = rgcn(yU, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\n\nmodel = RecurrentGCN(node_features=LAGS, filters=FILTERS)\nyPhat = rgcn(yP, model = model, train_ratio = TRAIN_RATIO, lags=LAGS, epoch=EPOCH)\nyPhat[yPhat &lt; 0] =0 \n\n\nclassic = ((y - yhat)[LEN_TR:, :] ** 2)\nproposed = ((y - yUhat*yPhat)[LEN_TR:, :] ** 2)\n\n# print mse\nprint(\"\"\"\ntotal_mse(classic): {:.4f}\ntotal_mse(proposed): {:.4f}\n\"\"\".format(classic.mean(), proposed.mean()))\n\n5/5\ntotal_mse(classic): 0.0573\ntotal_mse(proposed): 0.0476\n\n\n\n\n# plot \nstart = LEN_TR \nend = LEN_TR + LEN_TEST \ncity = 20\ntick_interval = 24 \n\nclassic = (y - yhat)[start:end, :] ** 2\nproposed = (y - yUhat*yPhat)[start:end, :] ** 2\n\nprint(\"\"\"\ntotal_mse(classic): {:.4f}\ntotal_mse(proposed): {:.4f}\n\"\"\".format(classic.mean(), proposed.mean()))\n\nwith plt.style.context('cyberpunk'): \n    plt.rcParams['figure.figsize'] = [20, 4]  # [가로 크기, 세로 크기]\n    plt.plot(t[start:end],y[start:end,city],'-',label='real')\n    plt.plot(t[start:end],yhat[start:end,city],'--',label='predited(classic)')\n    plt.plot(t[start:end],(yUhat*yPhat)[start:end,city],'--',label='predited(proposed)')\n    plt.xticks(range(1, len(t[start:end]) + 1, tick_interval),rotation=25,ha='right')\n    plt.title(\"The predicted results in {} from {} to {} \".format(regions[city],t[start:end].to_list()[0][:10],t[start:end].to_list()[-1][:10]))    \n    plt.legend()\n    mplcyberpunk.add_glow_effects()\n\n\ntotal_mse(classic): 0.0573\ntotal_mse(proposed): 0.0476\n\n\n\n\n\n\n\n\nAnalyze\n- y vs yhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(y[50:150,0],'o-',label='real')\n    plt.plot(yhat[50:150,0],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- yU vs yUhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(yU[:,25],label='real')\n    plt.plot(yUhat[:,25],label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- yP vs yPhat\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(yP[50:150,city],'o-',label='real')\n    plt.plot(yPhat[50:150,city],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- y vs (yUhat*yPhat)\n\nwith plt.style.context('seaborn-bright'): \n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(y[50:150,0],'o-',label='real')\n    plt.plot((yUhat*yPhat)[50:150,0],'o--',label='predited')\n    plt.legend()\n    mplcyberpunk.add_glow_effects()    \n\n\n\n\n- 지역별분석\n\nwith plt.style.context('cyberpunk'):\n    plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n    plt.plot(regions,classic.mean(axis=0)-proposed.mean(axis=0),'--o');\n    plt.xticks(regions, rotation=55, ha='right');    \n    mplcyberpunk.add_glow_effects()"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html",
    "title": "[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++",
    "section": "",
    "text": "변경사항: RGCN_Learner 수정 – 일부기능추가\n\n\n포인트: 배치학습을 적용하여 update가 빨리이루어지도록 코드수정\n\n\n\n\n# modules \nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport eptstgcn\nimport eptstgcn.planner\nimport torch_geometric_temporal\nfrom torch_geometric_temporal.signal.static_graph_temporal_signal import StaticGraphTemporalSignal\nimport mplcyberpunk\n\n# torch\nimport torch\nimport torch.nn.functional as F\n#import torch_geometric_temporal\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n# from torch_geometric_temporal.nn.recurrent import GCLSTM\n# from torch_geometric_temporal.nn.recurrent import LRGCN\n# from torch_geometric_temporal.nn.recurrent import TGCN\n# from torch_geometric_temporal.nn.recurrent import DCRNN\n\n# read data\ndef makedict(FX,W=None,node_ids=None):\n    T,N = np.array(FX).shape\n    if W==None:\n        W = pd.DataFrame(FX).corr().to_numpy().reshape(-1).tolist() # corr 로 weights\n    if node_ids==None:\n        node_ids = ['node'+str(n) for n in range(N)]\n    _dict={\n        'edges':[[i,j] for i in range(N) for j in range(N)], \n        'node_ids': node_ids,\n        'weights':W,\n        'FX':FX\n    }\n    return _dict\n\nclass Loader(object):\n    def __init__(self, data_dict):\n        self._dataset = data_dict\n    \n    def _get_edges(self):\n        self._edges = np.array(self._dataset[\"edges\"]).T\n\n    def _get_edge_weights(self):\n        # self._edge_weights = np.array(self._dataset[\"weights\"]).T\n        edge_weights = np.array(self._dataset[\"weights\"]).T\n        #scaled_edge_weights = minmaxscaler(edge_weights)\n        self._edge_weights = edge_weights\n\n    def _get_targets_and_features(self):\n        stacked_target = np.stack(self._dataset[\"FX\"])\n        self.features = np.stack([\n            stacked_target[i : i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ])\n        self.targets = np.stack([\n            stacked_target[i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ])\n\n\n    def get_dataset(self, lags: int = 4) -&gt; StaticGraphTemporalSignal:\n        self.lags = lags\n        self._get_edges()\n        self._get_edge_weights()\n        self._get_targets_and_features()\n        dataset = StaticGraphTemporalSignal(\n            self._edges, self._edge_weights, self.features, self.targets\n        )\n        dataset.node_ids = self._dataset['node_ids']\n        return dataset\n    \nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(in_channels = node_features, out_channels = filters, K = 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n    \n    \n# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\nclass RGCN_Learner:\n    def __init__(self):\n        self.method = 'RecurrentGCN'\n        self.figs = []\n        self.epochs = 0\n        self.losses = []\n        self._node_idx = 0\n    def load(self,y): \n        if (self.lags is None) or (self.train_ratio is None):\n            self.lags = 4\n            self.train_ratio = 0.8 \n        self.t,self.n = y.shape\n        dct = makedict(FX=y.tolist())\n        self.loader = Loader(dct)\n        self.dataset = self.loader.get_dataset(lags=self.lags)\n        self.X = torch.tensor(self.dataset.features).float()\n        self.y = torch.tensor(self.dataset.targets).float()\n        self.train_dataset, self.test_dataset = eptstgcn.utils.temporal_signal_split(self.dataset, train_ratio = self.train_ratio)\n        self.len_test = self.test_dataset.snapshot_count\n        self.len_tr = self.train_dataset.snapshot_count\n        #self.dataset_name = str(self.train_dataset) if dataset_name is None else dataset_name\n    def get_batches(self, batch_size=256):\n        num_batches =  self.len_tr // batch_size + (1 if self.len_tr % batch_size != 0 else 0)\n        self.batches = []\n        for i in range(num_batches):\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n            self.batches.append(self.train_dataset[start_idx:end_idx])\n    \n    def learn(self,epoch=1):\n        self.model.train()\n        for e in range(epoch):\n            losses_batch = []\n            for b,batch in enumerate(self.batches):\n                loss = 0\n                self.h, self.c = None, None\n                for t, snapshot in enumerate(batch):\n                    snapshot = snapshot.to(device)\n                    yt_hat, self.h, self.c = self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, self.h, self.c)\n                    loss = loss + torch.mean((yt_hat.reshape(-1)-snapshot.y.reshape(-1))**2)\n                    print(f'\\rbatch={b}\\t t={t+1}\\t loss={loss/(t+1)}\\t', end='', flush=True)\n                loss = loss / (t+1)\n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                losses_batch.append(loss.item())\n            self.epochs = self.epochs + 1\n            print(f'\\repoch={self.epochs}\\t loss={np.mean(losses_batch)}\\n', end='', flush=True)\n        self.losses.append(np.mean(losses_batch))\n        self._savefigs() \n    def _savefigs(self): \n        self.__call__()\n        self._node_idx\n        with plt.style.context('default'): \n            plt.ioff()\n            plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n            fig,ax = plt.subplots()\n            ax.plot(self.y[:,self._node_idx],label='real',lw=0.5)\n            ax.plot(self.yhat[:,self._node_idx],'--',label='predited',alpha=0.5)\n            ax.set_title(f'Epoch={self.epochs}, node_idx={self._node_idx}',size=15)\n            ax.legend()\n            #mplcyberpunk.add_glow_effects()  \n        self.figs.append(fig)\n        plt.close()\n    def __call__(self,dataset=None):\n        if dataset == None: \n            dataset = self.dataset\n        self.yhat = torch.stack([self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, self.h, self.c)[0] for snapshot in dataset]).detach().squeeze().float()\n        return {'X':self.X, 'y':self.y, 'yhat':self.yhat} \n\n# learn \n# def rgcn(FX,train_ratio,lags,filters,epoch):\n#     dct = makedict(FX=FX.tolist())\n#     loader = Loader(dct)\n#     dataset = loader.get_dataset(lags=lags)\n#     dataset_tr, dataset_test = eptstgcn.utils.temporal_signal_split(dataset, train_ratio = train_ratio)\n#     lrnr = RGCN_Learner(dataset_tr, dataset_name = 'org & arbitrary')\n#     lrnr.learn(filters=filters, epoch=epoch)\n#     yhat = np.array(lrnr(dataset)['yhat'])\n#     yhat = np.concatenate([np.array([list(yhat[0])]*lags),yhat],axis=0)\n#     return yhat\n\n\n\n\n\n# read dataframe \ndf = pd.read_csv('data_eng_230710.csv')\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)\n\n\n\n\n- 예시1: y를 학습\n학습~\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(y) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.348616252371738191366577        \nepoch=2  loss=0.096857151493083602352905            \nepoch=3  loss=0.085899897092996645071335            \nepoch=4  loss=0.082436966038374011698761            \nepoch=5  loss=0.080951251696015514699783            \n\n\n\nlrnr.y.shape\n\ntorch.Size([2564, 44])\n\n\n\nlrnr.get_batches(batch_size=24)\n\n\nlrnr.batches[-1]\n\n&lt;torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7fa2cfbc7cd0&gt;\n\n\n\nplt.plot(y[:100,0])\n\n에폭별 적합결과(\\(\\hat{y}\\)) 시각화\n\nlrnr.figs[0]\n\n\n\n\n\nlrnr.figs[1]\n\n\n\n\n\nlrnr.figs[-1]\n\n\n\n\n- 예시2: yU 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yU) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=1.98579962707536175294647     \nepoch=2  loss=0.202098417204133351100159        \nepoch=3  loss=0.052548440736393594218872        \nepoch=4  loss=0.0272646296531135271772957   \nepoch=5  loss=0.0191931016526596582006645   \n\n\n\nlrnr.figs[-1]\n\n\n\n\n5번정도 더 돌려보자\n\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=6  loss=0.0169690931215882312923717       \nepoch=7  loss=0.0141667058247379891905975       \nepoch=8  loss=0.0124263072353896015870743       \nepoch=9  loss=0.0103476242200189951082373       \nepoch=10     loss=0.009508996026937005688799        \n\n\n\nlrnr.figs[-1]\n\n\n\n\n- 예시3: yP 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yP) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.030719049366970748393135            \nepoch=2  loss=0.0119630681123411254793358       \nepoch=3  loss=0.0109235838672882638420868       \nepoch=4  loss=0.0107088599754714682600975       \nepoch=5  loss=0.0105961727974719788335514       \n\n\n\nlrnr.figs[-1]"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#import",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#import",
    "title": "[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++",
    "section": "",
    "text": "# modules \nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport eptstgcn\nimport eptstgcn.planner\nimport torch_geometric_temporal\nfrom torch_geometric_temporal.signal.static_graph_temporal_signal import StaticGraphTemporalSignal\nimport mplcyberpunk\n\n# torch\nimport torch\nimport torch.nn.functional as F\n#import torch_geometric_temporal\nfrom torch_geometric_temporal.nn.recurrent import GConvLSTM\n# from torch_geometric_temporal.nn.recurrent import GCLSTM\n# from torch_geometric_temporal.nn.recurrent import LRGCN\n# from torch_geometric_temporal.nn.recurrent import TGCN\n# from torch_geometric_temporal.nn.recurrent import DCRNN\n\n# read data\ndef makedict(FX,W=None,node_ids=None):\n    T,N = np.array(FX).shape\n    if W==None:\n        W = pd.DataFrame(FX).corr().to_numpy().reshape(-1).tolist() # corr 로 weights\n    if node_ids==None:\n        node_ids = ['node'+str(n) for n in range(N)]\n    _dict={\n        'edges':[[i,j] for i in range(N) for j in range(N)], \n        'node_ids': node_ids,\n        'weights':W,\n        'FX':FX\n    }\n    return _dict\n\nclass Loader(object):\n    def __init__(self, data_dict):\n        self._dataset = data_dict\n    \n    def _get_edges(self):\n        self._edges = np.array(self._dataset[\"edges\"]).T\n\n    def _get_edge_weights(self):\n        # self._edge_weights = np.array(self._dataset[\"weights\"]).T\n        edge_weights = np.array(self._dataset[\"weights\"]).T\n        #scaled_edge_weights = minmaxscaler(edge_weights)\n        self._edge_weights = edge_weights\n\n    def _get_targets_and_features(self):\n        stacked_target = np.stack(self._dataset[\"FX\"])\n        self.features = np.stack([\n            stacked_target[i : i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ])\n        self.targets = np.stack([\n            stacked_target[i + self.lags, :].T\n            for i in range(stacked_target.shape[0] - self.lags)\n        ])\n\n\n    def get_dataset(self, lags: int = 4) -&gt; StaticGraphTemporalSignal:\n        self.lags = lags\n        self._get_edges()\n        self._get_edge_weights()\n        self._get_targets_and_features()\n        dataset = StaticGraphTemporalSignal(\n            self._edges, self._edge_weights, self.features, self.targets\n        )\n        dataset.node_ids = self._dataset['node_ids']\n        return dataset\n    \nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvLSTM(in_channels = node_features, out_channels = filters, K = 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight, h, c):\n        h_0, c_0 = self.recurrent(x, edge_index, edge_weight, h, c)\n        h = F.relu(h_0)\n        h = self.linear(h)\n        return h, h_0, c_0\n    \n    \n# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice = torch.device('cpu')\n\nclass RGCN_Learner:\n    def __init__(self):\n        self.method = 'RecurrentGCN'\n        self.figs = []\n        self.epochs = 0\n        self.losses = []\n        self._node_idx = 0\n    def load(self,y): \n        if (self.lags is None) or (self.train_ratio is None):\n            self.lags = 4\n            self.train_ratio = 0.8 \n        self.t,self.n = y.shape\n        dct = makedict(FX=y.tolist())\n        self.loader = Loader(dct)\n        self.dataset = self.loader.get_dataset(lags=self.lags)\n        self.X = torch.tensor(self.dataset.features).float()\n        self.y = torch.tensor(self.dataset.targets).float()\n        self.train_dataset, self.test_dataset = eptstgcn.utils.temporal_signal_split(self.dataset, train_ratio = self.train_ratio)\n        self.len_test = self.test_dataset.snapshot_count\n        self.len_tr = self.train_dataset.snapshot_count\n        #self.dataset_name = str(self.train_dataset) if dataset_name is None else dataset_name\n    def get_batches(self, batch_size=256):\n        num_batches =  self.len_tr // batch_size + (1 if self.len_tr % batch_size != 0 else 0)\n        self.batches = []\n        for i in range(num_batches):\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n            self.batches.append(self.train_dataset[start_idx:end_idx])\n    \n    def learn(self,epoch=1):\n        self.model.train()\n        for e in range(epoch):\n            losses_batch = []\n            for b,batch in enumerate(self.batches):\n                loss = 0\n                self.h, self.c = None, None\n                for t, snapshot in enumerate(batch):\n                    snapshot = snapshot.to(device)\n                    yt_hat, self.h, self.c = self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, self.h, self.c)\n                    loss = loss + torch.mean((yt_hat.reshape(-1)-snapshot.y.reshape(-1))**2)\n                    print(f'\\rbatch={b}\\t t={t+1}\\t loss={loss/(t+1)}\\t', end='', flush=True)\n                loss = loss / (t+1)\n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                losses_batch.append(loss.item())\n            self.epochs = self.epochs + 1\n            print(f'\\repoch={self.epochs}\\t loss={np.mean(losses_batch)}\\n', end='', flush=True)\n        self.losses.append(np.mean(losses_batch))\n        self._savefigs() \n    def _savefigs(self): \n        self.__call__()\n        self._node_idx\n        with plt.style.context('default'): \n            plt.ioff()\n            plt.rcParams['figure.figsize'] = [20, 3]  # [가로 크기, 세로 크기]\n            fig,ax = plt.subplots()\n            ax.plot(self.y[:,self._node_idx],label='real',lw=0.5)\n            ax.plot(self.yhat[:,self._node_idx],'--',label='predited',alpha=0.5)\n            ax.set_title(f'Epoch={self.epochs}, node_idx={self._node_idx}',size=15)\n            ax.legend()\n            #mplcyberpunk.add_glow_effects()  \n        self.figs.append(fig)\n        plt.close()\n    def __call__(self,dataset=None):\n        if dataset == None: \n            dataset = self.dataset\n        self.yhat = torch.stack([self.model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, self.h, self.c)[0] for snapshot in dataset]).detach().squeeze().float()\n        return {'X':self.X, 'y':self.y, 'yhat':self.yhat} \n\n# learn \n# def rgcn(FX,train_ratio,lags,filters,epoch):\n#     dct = makedict(FX=FX.tolist())\n#     loader = Loader(dct)\n#     dataset = loader.get_dataset(lags=lags)\n#     dataset_tr, dataset_test = eptstgcn.utils.temporal_signal_split(dataset, train_ratio = train_ratio)\n#     lrnr = RGCN_Learner(dataset_tr, dataset_name = 'org & arbitrary')\n#     lrnr.learn(filters=filters, epoch=epoch)\n#     yhat = np.array(lrnr(dataset)['yhat'])\n#     yhat = np.concatenate([np.array([list(yhat[0])]*lags),yhat],axis=0)\n#     return yhat"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#load-data",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#load-data",
    "title": "[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++",
    "section": "",
    "text": "# read dataframe \ndf = pd.read_csv('data_eng_230710.csv')\n# make y, y_upper, y_period, time, regions \ny = df.loc[:,'Bukchoncheon':'Gyeongju-si'].to_numpy()\nyU = df.loc[:,'Bukchoncheon_Upper':'Gyeongju-si_Upper'].to_numpy()\nyP = np.divide(y, yU+1e-10)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#sample_codes",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-08-14-EPT-GConvLSTM-setting-Copy1.out.html#sample_codes",
    "title": "[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++",
    "section": "",
    "text": "- 예시1: y를 학습\n학습~\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(y) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.348616252371738191366577        \nepoch=2  loss=0.096857151493083602352905            \nepoch=3  loss=0.085899897092996645071335            \nepoch=4  loss=0.082436966038374011698761            \nepoch=5  loss=0.080951251696015514699783            \n\n\n\nlrnr.y.shape\n\ntorch.Size([2564, 44])\n\n\n\nlrnr.get_batches(batch_size=24)\n\n\nlrnr.batches[-1]\n\n&lt;torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7fa2cfbc7cd0&gt;\n\n\n\nplt.plot(y[:100,0])\n\n에폭별 적합결과(\\(\\hat{y}\\)) 시각화\n\nlrnr.figs[0]\n\n\n\n\n\nlrnr.figs[1]\n\n\n\n\n\nlrnr.figs[-1]\n\n\n\n\n- 예시2: yU 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yU) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=1.98579962707536175294647     \nepoch=2  loss=0.202098417204133351100159        \nepoch=3  loss=0.052548440736393594218872        \nepoch=4  loss=0.0272646296531135271772957   \nepoch=5  loss=0.0191931016526596582006645   \n\n\n\nlrnr.figs[-1]\n\n\n\n\n5번정도 더 돌려보자\n\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=6  loss=0.0169690931215882312923717       \nepoch=7  loss=0.0141667058247379891905975       \nepoch=8  loss=0.0124263072353896015870743       \nepoch=9  loss=0.0103476242200189951082373       \nepoch=10     loss=0.009508996026937005688799        \n\n\n\nlrnr.figs[-1]\n\n\n\n\n- 예시3: yP 학습\n\n# step1: create lrnr object\nlrnr = RGCN_Learner()\n# step2: load data \nlrnr.lags = 4 \nlrnr.train_ratio = 0.8\nlrnr.load(yP) \n# step3: construct networks \nlrnr.nof_filters = 16\nlrnr.model = RecurrentGCN(node_features=lrnr.lags, filters=lrnr.nof_filters).to(device)\nlrnr.optimizer = torch.optim.Adam(lrnr.model.parameters(),lr=10/1000)\n# step4: learn \nlrnr.get_batches(batch_size=24)\nfor e in range(5):    \n    lrnr.learn()\n\nepoch=1  loss=0.030719049366970748393135            \nepoch=2  loss=0.0119630681123411254793358       \nepoch=3  loss=0.0109235838672882638420868       \nepoch=4  loss=0.0107088599754714682600975       \nepoch=5  loss=0.0105961727974719788335514       \n\n\n\nlrnr.figs[-1]"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#이론-및-예시",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "이론 및 예시",
    "text": "이론 및 예시\n- 이론: real-valued signal은 무조건 \\(|X[k]|^2\\)의 그래프가 대칭으로 나옴 (단, \\(X[0]\\)은 제외)\n- 예시1:\n\nx = np.array([1,2,3,4,5])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시2:\n\nx = np.array([1,2,3,-3,-2,-1])\nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')\n\n\n\n\n\n첫항을 제외하고 대칭임\n\n- 예시3: \\({\\bf x}\\)가 복소수일 경우는 첫항을 제외하고 대칭이 되지 않음\n\nx = np.array([1+1j,2+2j,3+3j,-3-3j,-2-2j,1-1j]) \nX = np.fft.fft(x) \n\n\nplt.plot(abs(X)**2,'o')"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#왜-xn이-실수일-경우만-xk2이-대칭으로-나올까",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?",
    "text": "왜 \\(x[n]\\)이 실수일 경우만 \\(|X[k]|^2\\)이 대칭으로 나올까?\n- 예비학습1\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\cos(2\\pi \\alpha) =\\cos(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.cos(2*np.pi*alpha),np.cos(2*np.pi*(1-alpha))\n\n(0.30901699437494745, 0.30901699437494723)\n\n\n\n그래프를 잘 그려보세여\n\n- 예비학습2\n임의의 \\(0 \\leq \\alpha \\leq 1\\)에 대하여 \\(\\sin(2\\pi \\alpha) = -\\sin(2\\pi (1-\\alpha))\\) 가 성립함\n\nalpha = 0.2\nnp.sin(2*np.pi*alpha),np.sin(2*np.pi*(1-alpha))\n\n(0.9510565162951535, -0.9510565162951536)\n\n\n\n그래프를 잘 그려보세여\n\n- 왜 실수일경우만 대칭인지? (어디 정리된걸 아무리 찾아도 못찾겠어서 그냥 직접 수식을 썼는데요, 이걸 기억할 필요는 없어요.. 아마 제가 쓴것보다 쉽게 설명하는 방법이 있을겁니다)\n(해설) \\(k=0,1,2,\\dots,N-1\\)에 대하여 \\(X[k]\\)는 아래와 같이 표현가능하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]e^{-\\frac{j2\\pi kn}{N}}\\]\n오일러공식을 사용하면 아래와 같이 정리할 수 있다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(-\\frac{2\\pi kn}{N}\\right) + i \\sum_{n=0}^{N-1}x[n]\\sin\\left(-\\frac{2\\pi kn}{N}\\right)\\]\ncos은 짝함수, sin은 홀함수임을 이용하여 다시정리하면\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\ncase1 \\(k=1\\) 인 경우와 \\(k=N-1\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음을 보이자.\n\\[X[1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi n}{N}\\right)\\]\n\\[X[N-1] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\]\n여기에서 예비학습1,2를 떠올리면 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 임을 알 수 있다. 따라서 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수이다. 즉\n\\[X[1] = X[N-1]^\\ast, \\quad X[1]^\\ast = X[N-1]\\]\n이다. 그런데 임의의 복소수 \\(x=a+bi\\)에 대하여 \\(|x|^2 = a^2+b^2 = (a+bi)(a-bi)= x x^\\ast\\) 가 성립하므로\n\\[|X[1]|^2 = |X[N-1]|^2\\]\n이 성립한다.\n\n만약에 \\(x[n]\\)이 실수가 아닌경우는 \\(\\cos\\left(\\frac{2\\pi n}{N}\\right)=\\cos\\left(\\frac{2\\pi (N-1)n}{N}\\right)\\) 이고 \\(\\sin\\left(\\frac{2\\pi n}{N}\\right)=-\\sin\\left(\\frac{2\\pi(N-1) n}{N}\\right)\\) 이라고 하여도 \\(X[1]\\)와 \\(X[N-1]\\)은 켤레복소수라고 주장할수 없다.\n\ncase2 \\(k=2\\) 인 경우와 \\(k=N-2\\)인 경우는 서로 \\(|X[k]|^2\\)이 같음도 비슷한 논리로 보일 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#허수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "허수파트 해석",
    "text": "허수파트 해석\n관찰1: 모든 \\(k\\)에 대하여 \\(X[k]\\)의 허수파트는 항상 0이다.\nk=0\n\nk=0\nsin_part_0 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_0\n\narray([ 0.,  0.,  0.,  0., -0., -0., -0., -0., -0., -0., -0., -0.,  0.,\n        0.,  0.])\n\n\n\nsum(x*sin_part_0)\n\n0.0\n\n\nk=1\n\nk=1\nsin_part_1 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nx*sin_part_1\n\narray([ 0.        ,  0.37157241,  0.49726095,  0.29389263, -0.10395585,\n       -0.4330127 , -0.47552826, -0.20336832,  0.20336832,  0.47552826,\n        0.4330127 ,  0.10395585, -0.29389263, -0.49726095, -0.37157241])\n\n\n\nsum(x*sin_part_1)\n\n1.0547118733938987e-15\n\n\n약간을 직관을 위해서 그림을 그려보자.\n\nplt.plot(x,'--o')\nplt.plot(sin_part_1,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 \\(\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\)에 대응하는 식은 \\(\\int_0^{2\\pi}\\cos(t)\\sin(t)dt\\)라고 볼 수 있어서 sum(x*sin_part_1)=0임을 더 쉽게 이해할 수 있다.\nk=2\n\nk=2\nsin_part_2 = np.array([np.sin(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(sin_part_2,'--o')\n\n\n\n\n\\(x\\)가 \\(\\mathbb{R}\\)에서 정의된 연속함수라고 상상하면 파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\sin(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nsum(x*sin_part_2)\n\n-1.2212453270876722e-15\n\n\n\\(\\cos(t)\\)는 임의의 \\(\\sin(kt)\\)와 항상 직교하므로, 임의의 \\(k\\)에 대하여 허수파트는 항상 0이다.\n따라서 이 경우 \\(X[k]\\)는 아래와 같이 써도 무방하다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right)\\]"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-퓨리에변환(detailed).html#실수파트-해석",
    "title": "[PINKOCTO] 퓨리에변환(detailed)",
    "section": "실수파트 해석",
    "text": "실수파트 해석\n관찰2: \\(X[k]\\)의 실수파트는 \\(k=1\\)혹은 \\(k=N-1\\)일때 아래와 같이 정리된다.\n\\[\\sum_{n=0}^{N-1}\\cos\\left(\\frac{2\\pi n}{N} \\right)^2\\]\n그외의 경우에는 아래와 같이 된다.\nk=0\n\nk=0\ncos_part_0 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_0,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(1\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\nk=2\n\nk=2\ncos_part_2 = np.array([np.cos(2*np.pi*k*n/N) for n in range(N)])\n\n\nplt.plot(x,'--o')\nplt.plot(cos_part_2,'--o')\n\n\n\n\n\n파란선에 대응하는것은 \\(\\cos(t)\\) 주황선에 대응하는 것은 \\(\\cos(2t)\\)로 볼 수 있는데 둘은 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n\n임의의 \\(k\\)에 대하여 \\(\\cos(t)\\)와 \\(\\cos(kt)\\)는 항상 직교하므로 둘을 곱한뒤 적분하면 (더하면) 0이 된다.\n- 요약: 만약에 \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{2\\pi n}{N} \\right)\\]\n이때 퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=1,N-1\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)\n\n\n\n\n- 응용: \\(x[n]\\)이 아래와 같은 꼴이라고 하자.\n\\[x[n] = \\cos\\left(\\frac{6\\pi n}{N} \\right)\\]\n퓨리에변환 \\(X[k]\\)는 아래와 같이 정리된다.\n\\[X[k] = \\sum_{n=0}^{N-1}x[n]\\cos\\left(\\frac{2\\pi kn}{N}\\right) - i \\sum_{n=0}^{N-1}x[n]\\sin\\left(\\frac{2\\pi kn}{N}\\right)\\]\n\\(X[k]\\)의 허수파트는 항상 0이 되고, 실수 파트는 \\(k=3,N-3\\)일 경우에만 값을 가지고 나머지는 0의 값을 가진다.\n\nN = 15 \nx = np.array([np.cos(6*np.pi*n/N) for n in range(N)]) \nX = np.fft.fft(x)\nfig, ax = plt.subplots(1,3)\nax[0].plot(x,'o--'); ax[0].set_title('x[n]')\nax[1].plot(np.real(X),'x'); ax[1].set_title('real(X[k])')\nax[2].plot(np.imag(X),'x'); ax[2].set_title('imag(X[k])')\nfig.set_figwidth(15)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-28-커널리그레션.html",
    "title": "[PINKOCTO] 커널리그레션",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nx = np.linspace(0,1,5)\nx\n\narray([0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\n\ny = x*2 + np.random.randn(5)*0.3\ny\n\narray([0.01044984, 0.93755458, 0.43942577, 1.0639859 , 2.1133726 ])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\n\n\n\n\n\\[y_i= \\sum_{i=1}^{5}\\theta_i\\exp\\left(-\\frac{|x-x_i|^2}{2h^2}\\right)\\]\n\nh= 0.15\nx0 = lambda xstar: np.exp(-(xstar-x[0])**2 / 2 / (h**2))\nx1 = lambda xstar: np.exp(-(xstar-x[1])**2 / 2 / (h**2))\nx2 = lambda xstar: np.exp(-(xstar-x[2])**2 / 2 / (h**2))\nx3 = lambda xstar: np.exp(-(xstar-x[3])**2 / 2 / (h**2))\nx4 = lambda xstar: np.exp(-(xstar-x[4])**2 / 2 / (h**2))\n\n\nθ0 = 0\nθ1 = 0.2\nθ2 = 0.5\nθ3 = 1.0\nθ4 = 1.5\n\n\nxstar = 0.6 \n\n\nθ0*x0(0.6)+θ1*x1(0.6)+θ2*x2(0.6)+θ3*x3(0.6)+θ4*x4(0.6)\n\n1.062893318071169\n\n\n\n_yhat = lambda xstar : θ0*x0(xstar)+\\\nθ1*x1(xstar)+\\\nθ2*x2(xstar)+\\\nθ3*x3(xstar)+\\\nθ4*x4(xstar)\n\n\n(_yhat(0)-y[0])**2\n\n0.0017104251892816106\n\n\n\nloss = (_yhat(0)-y[0])**2 + (_yhat(0.25)-y[1])**2+ \\\n(_yhat(0.5)-y[2])**2+(_yhat(0.75)-y[3])**2+\\\n(_yhat(1)-y[4])**2\n\n\\(loss(\\theta_0,\\theta_1,\\theta_2,\\theta_3,\\theta_4)\\) 를 최소화하는 \\({\\boldsymbol \\theta}\\)를 구한다.\n\n_x = np.linspace(0,1,1000)\nplt.plot(x,y,'o')\nplt.plot(x,2*x,'--')\nplt.plot(_x,_yhat(_x))"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "",
    "text": "import itstgcn\nimport torch\nimport itstgcn.planner"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_RAND",
    "text": "PLNR_STGCN_RAND\n\nplans_stgcn_rand = {\n    'max_iteration': 2, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mrate': [0.0, 0.2],\n    'lags': [2, 4], \n    'nof_filters': [4, 8], \n    'inter_method': ['nearest'],\n    'epoch': [3]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_RAND(plans_stgcn_rand,loader,dataset_name='five_nodes')\n\n\nplnr.simulate()\n\n1/2 is done\n2/2 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-03.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_stgcn_manual",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_STGCN_MANUAL",
    "text": "PLNR_STGCN_MANUAL\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex= [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_stgcn_block = {\n    'max_iteration': 3, \n    'method': ['STGCN', 'IT-STGCN'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_STGCN_MANUAL(plans_stgcn_block,loader,dataset_name='five_nodes')\n\n\nplnr.simulate(mindex=mindex,mtype='block')\n\n1/3 is done\n2/3 is done\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-55.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_rand",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_RAND",
    "text": "PLNR_GNAR_RAND\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nplans_gnar_rand = {\n    'max_iteration': 3, \n#    'method': ['GNAR'], \n    'mrate': [0.0, 0.2, 0.4],\n    'lags': [2, 4], \n#    'nof_filters': [8,16], \n    'inter_method': ['nearest','linear'],\n#    'epoch': [1]\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_RAND(plans_gnar_rand,loader,dataset_name='five_nodes')\nplnr.simulate()\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-56.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-18-SimulationPlanner-Tutorial.html#plnr_gnar_block",
    "title": "[IT-STGCN] SimualtionPlanner-Tutorial",
    "section": "PLNR_GNAR_BLOCK",
    "text": "PLNR_GNAR_BLOCK\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\nplans_gnar_block = {\n    'max_iteration': 3, \n    'method': ['GNAR'], \n    'mindex': [mindex],\n    'lags': [2, 4], \n    'inter_method': ['nearest','linear'],\n}\n\n\nplnr = itstgcn.planner.PLNR_GNAR_MANUAL(plans_gnar_block,loader,dataset_name='five_nodes')\nplnr.simulate(mindex,mtype='block')\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n1/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n2/3 is done\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n3/3 is done\nAll results are stored in ./simulation_results/2023-03-18_11-56-57.csv"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "",
    "text": "import networkx as nx\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#data",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "data",
    "text": "data\n\nfrom torch_geometric_temporal.dataset import WikiMathsDatasetLoader\nfrom torch_geometric_temporal.signal import temporal_signal_split\n\nloader = WikiMathsDatasetLoader()\n\ndataset = loader.get_dataset(lags=14)\n\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.5)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#recurrentgcn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "RecurrentGCN",
    "text": "RecurrentGCN\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric_temporal.nn.recurrent import GConvGRU\n\nclass RecurrentGCN(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(RecurrentGCN, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#learn",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "Learn",
    "text": "Learn\n\n# from tqdm import tqdm\n\n# model = RecurrentGCN(node_features=14, filters=32)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# model.train()\n\n# for epoch in tqdm(range(50)):\n#     for time, snapshot in enumerate(train_dataset):\n#         y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n#         cost = torch.mean((y_hat-snapshot.y)**2)\n#         cost.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n\n\nmodel = RecurrentGCN(node_features=14, filters=32)\nmodel.train()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(14, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nfor s in train_dataset:\n    print((s.y-model(s.x,s.edge_index,s.edge_attr)).shape)\n\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])\ntorch.Size([1068, 1068])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-30-STGCN-Toy Example.html#예제의-차원-조사",
    "title": "[IT-STGCN] STGCN: Toy Example",
    "section": "예제의 차원 조사",
    "text": "예제의 차원 조사\n\nfor time, snapshot in enumerate(train_dataset):\n    _x = snapshot.x \n    _edge_index = snapshot.edge_index \n    _edge_attr = snapshot.edge_attr\n    _y = snapshot.y\n    break\n\n\n_x.shape\n\ntorch.Size([1068, 14])\n\n\n\n1068: number of nodes // 1068개의 노드가 있음\n14: number of features // 하나의 노드에 맵핑된 차원의수\n\n\n_edge_index.shape\n\ntorch.Size([2, 27079])\n\n\n\n_edge_attr.shape\n\ntorch.Size([27079])\n\n\n\n_y.shape\n\ntorch.Size([1068])\n\n\n\n1068: number of nodes\n\n\n_x.shape\n\ntorch.Size([1068, 14])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-02-14-Tables.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-02-14-Tables.html",
    "title": "[IT-STGCN] Tables",
    "section": "",
    "text": "import pandas as pd\n\n- Dataset 5nodes\n\ncol = ['Dataset','iteration', 'method', 'missingrate', 'missingtype', 'lag', 'number_of_filters', 'interpolation','MSE_train', 'MSE_test',]\n\n\ndf = pd.DataFrame(columns=col)\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n\n\n\n\n\n- 실험마다 아래와 같은식으로 추가\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.0, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = None,  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = None, # None, 'mean', 'linear'\ndf_row['MSE_train'] = 0.96 \ndf_row['MSE_test'] = 0.55\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n0\nfivenodes\n1\nstgcn\n0.0\nNone\n1\n16\nNone\n0.96\n0.55\n\n\n\n\n\n\n\n\ndf_row = pd.DataFrame(columns=col)\ndf_row['Dataset'] = 'fivenodes', # ...\ndf_row['iteration'] = 1, # 1,2,3,...,10 \ndf_row['method'] = 'stgcn', # 'stgcn','estgcn','gnar' \ndf_row['missingrate'] = 0.2, # 0.0, 0.2, 0.4, 0.6, 0.8 \ndf_row['missingtype'] = 'randomly',  # None, 'randomly' and 'block' \ndf_row['lag'] = 1, # 1,2,3,4 ... \ndf_row['number_of_filters'] = 16, # 16,24,32, ... \ndf_row['interpolation'] = 'mean', # None, 'mean', 'linear'\ndf_row['MSE_train'] = 1.23\ndf_row['MSE_test'] = 0.88\n\n\ndf = pd.concat([df,df_row])\ndf\n\n\n\n\n\n\n\n\nDataset\niteration\nmethod\nmissingrate\nmissingtype\nlag\nnumber_of_filters\ninterpolation\nMSE_train\nMSE_test\n\n\n\n\n0\nfivenodes\n1\nstgcn\n0.0\nNone\n1\n16\nNone\n0.96\n0.55\n\n\n0\nfivenodes\n1\nstgcn\n0.2\nrandomly\n1\n16\nmean\n1.23\n0.88"
  },
  {
    "objectID": "posts/3_Researches/YEON/2023-02-14-수통문제-지수분포평균검정.html",
    "href": "posts/3_Researches/YEON/2023-02-14-수통문제-지수분포평균검정.html",
    "title": "[SEOYEONC] 지수분포 평균검정",
    "section": "",
    "text": "using Distributions, Plots\n\n다음의 분포를 따르는\n\n\\(f(x; \\theta) = \\theta \\exp (-x\\theta )I(x&gt;0)\\)\n\n모집단으로부터 랜덤표본 \\(X_1,\\dots,X_n\\)을 이용하여 \\(\\theta\\)에 대한 신뢰구간을 구하고, 다음 가설\n\n\\(H_0: \\theta = \\theta_0\\) vs \\(H_1:\\theta \\neq \\theta_0\\)\n\n을 검정하고자 한다. 다음에 답하라.\n(a) \\(\\theta\\)에 대한 적절한 추축변량을 구하고, 해당 추축변량의 분포를 명시하라.\n(풀이)\n추축변량은 \\(2n\\bar{X}\\theta\\) 이며 분포는 자유도가 \\(2n\\)인 카이제곱분포를 따름. 왜냐하면\n\n\\(X_1 \\sim \\text{Exp}\\) with mean \\(1/\\theta\\)\n\\(2X_1\\theta \\sim \\text{Exp}\\) with mean \\(2\\)\n\\(2n\\bar{X}\\theta \\sim \\chi^2(2n)\\) (왜? 평균이2인 지수분포를 \\(n\\)번 더하면 자유도가 \\(2n\\)인 카이제곱분포가 되므로. –&gt; 참고)\n\n이기 떄문에.\n(시뮬레이션)\n\nn = 10\nθ = 2\npivotal_variable = [mean(rand(Exponential(1/2),n))*2*n*θ for i in 1:140000]\n# pivotal_variable = 추축변량\n\n140000-element Vector{Float64}:\n 19.866862184254465\n 17.492982610528998\n 13.463211578260314\n 19.472994478042697\n 13.424495255444352\n 21.744281289967418\n 22.929701779207697\n 13.561641745647645\n 25.277641996704048\n 16.566359730027052\n 19.550767074240074\n  9.914621718787568\n 11.857966801675106\n  ⋮\n 38.05226144210052\n 17.120691971399836\n 21.66831336505733\n 26.476356317147957\n 21.930886178263727\n 18.33570158249295\n 22.39822061516168\n 27.42899896779973\n 14.770009603430982\n 13.776401174299512\n 21.511825086436673\n 21.867878699644073\n\n\n\nhistogram(pivotal_variable)\nhistogram!(rand(Chisq(2*n),140000))\n\n\n\n\n\n추축변량은 자유도 \\(2n\\)인 카이제곱분포를 따름\n\n(b) \\(\\theta\\)에 대한 95% 신뢰구간을 구하라.\n(풀이)\n\\(P\\left(\\chi^2_{0.025}(2n) \\leq 2n\\bar{X}\\theta \\leq \\chi^2_{0.975}(2n) \\right) =0.95\\)\nThus the CI of \\(\\theta\\): \\(\\left(\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}},\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}} \\right)\\)\n(시뮬레이션)\n\nl = quantile(Chisq(2*n),0.025) ./ (pivotal_variable./θ)\nu = quantile(Chisq(2*n),0.975) ./ (pivotal_variable./θ)\n[l[i]&lt;θ&lt;u[i] for i in 1:140000] |&gt; mean\n\n0.95035\n\n\n(c) \\(P(X&gt;1)\\)에 대한 95% 신뢰구간\n(풀이)\n\\(P(X&gt;1) = \\int_{1}^{\\infty}\\theta \\exp(-x\\theta)dx =\\left[e^{-x\\theta} \\right]_1^{\\infty}=e^{-\\theta}\\)\nThus the CI of \\(P(X&gt;1)\\): \\(\\left(\\exp\\big(-\\frac{\\chi^2_{0.975}(2n)}{2n\\bar{X}}\\big),\\exp\\big(-\\frac{\\chi^2_{0.025}(2n)}{2n\\bar{X}}\\big) \\right)\\)\n(시뮬레이션)\n검토1: \\(P(X&gt;1)=E[I(X&gt;1)]=e^{-\\theta}\\) 임을 검토\n\nexp(-θ)\n\n0.1353352832366127\n\n\n\nmean(rand(Exponential(1/2),1000000) .&gt; 1)\n\n0.135678\n\n\n검토2: 신뢰구간\n\nu2 = exp.(-l)\nl2 = exp.(-u)\n[l2[i]&lt;exp(-θ)&lt;u2[i] for i in 1:140000] |&gt; mean\n\n0.95035\n\n\n(d)\n\\(\\Omega = \\{\\theta: \\theta&gt;0\\}\\), \\(\\Omega_0 =\\{2\\}\\)\n(e) \\(\\theta\\)의 가능도 함수를 기술하시오\n(풀이)\n\\(L(\\theta)=\\theta^n\\exp(-\\theta n\\bar{x})\\)\n(f) \\(\\theta\\)의 \\(\\Omega\\)에서의 최대가능도 추정량과 \\(\\Omega_0\\)에서의 최대가능도 추정량을 구하시오\n(풀이)\n\\(\\hat{\\theta}^{\\Omega}=1/\\bar{x}\\), \\(\\hat{\\theta}^{\\Omega_0}=\\theta_0\\)\n(g) 일반화 가능도 비1 \\(\\Lambda\\)를 구하시오.\n1 이게 뭐지..?(풀이)\n\\(\\frac{L\\big(\\hat{\\theta}^{\\Omega}\\big)}{L\\big(\\hat{\\theta}^{\\Omega_0}\\big)}=\\frac{1/\\bar{x}^n\\exp(-n)}{\\theta_0^n\\exp(-\\theta_0n\\bar{x})}\\)\n(h) 유의수준 \\(\\alpha\\)의 가능도비 검정법의 기각역을 \\(\\chi^2\\)의 분위수를 사용하여 표현하시오.\n(풀이)"
  },
  {
    "objectID": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-18-데이터전처리.html",
    "href": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-18-데이터전처리.html",
    "title": "[EPT-DISSIM] PRCP_KOR2 데이터정리",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv('PRCP_KOR.csv')\ndf\n\n\n\n\n\n\n\n\nStation_ID\nStation_Name\nDate\nPrecipitation\n\n\n\n\n0\n90\n속초\n2013-08-09\n15.5\n\n\n1\n90\n속초\n2013-08-10\n9.5\n\n\n2\n90\n속초\n2013-08-16\n0.1\n\n\n3\n90\n속초\n2013-08-18\n0.0\n\n\n4\n90\n속초\n2013-08-19\n9.0\n\n\n...\n...\n...\n...\n...\n\n\n127200\n295\n남해\n2023-07-22\n4.3\n\n\n127201\n295\n남해\n2023-07-23\n13.4\n\n\n127202\n295\n남해\n2023-07-24\n17.0\n\n\n127203\n295\n남해\n2023-07-25\n4.7\n\n\n127204\n295\n남해\n2023-07-26\n42.5\n\n\n\n\n127205 rows × 4 columns\n\n\n\n\n# 'Date' 칼럼의 데이터 타입을 datetime으로 변경\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# 다시 각 기상 관측소별로 누락된 날짜를 찾아 0으로 채움\nfilled_dfs = []\nfor (station_id, station_name), group in df.groupby(['Station_ID', 'Station_Name']):\n    all_dates = pd.date_range(start=group['Date'].min(), end=group['Date'].max())\n    filled_data = all_dates.to_frame(name='Date').merge(group, on='Date', how='left')\n    filled_data['Station_ID'].fillna(station_id, inplace=True)\n    filled_data['Station_Name'].fillna(station_name, inplace=True)\n    filled_data['Precipitation'].fillna(0, inplace=True)\n    filled_dfs.append(filled_data)\n\n# 모든 기상 관측소의 보정된 데이터를 결합\ndf2 = pd.concat(filled_dfs)\n\ndf2.head()\n\n\n\n\n\n\n\n\nDate\nStation_ID\nStation_Name\nPrecipitation\n\n\n\n\n0\n2013-08-09\n90.0\n속초\n15.5\n\n\n1\n2013-08-10\n90.0\n속초\n9.5\n\n\n2\n2013-08-11\n90.0\n속초\n0.0\n\n\n3\n2013-08-12\n90.0\n속초\n0.0\n\n\n4\n2013-08-13\n90.0\n속초\n0.0\n\n\n\n\n\n\n\n\ndf2.to_csv(\"PRCP_KOR2.csv\")"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi를 이용하여 파일 만들고 asdf 입력 후 저장 및 종료를 수행하여 보자.\n\n\nstep1step2step3step4step5\n\n\n- 파일생성\nvim test230803.txt \n\n\n\n- 편집모드로 전환: i를 누른다.\n\n아래에 -- INSERT -- 라고 표현되어 있으면 편집모드라는 의미\n\n\n\n\n- asdf 입력\n\n\n\n- 편집모드 종료, 노말모드 진입:esc 입력\n\n아래에 -- INSERT -- 라는 표현이 사라져있음. 이 상황에서 명령어 입력가능.\n\n\n\n\n- 저장(w)+종료(q): : 입력하여 명령모드로 진입. w+ q + Enter 입력\n\n\n\n\n\n\n\n\n예제: vi 에서 좌우로 커서를 이동해보자.\n\n\nstep1step2step3\n\n\n- 생성된 파일로 들어가기\nvim test230803.txt \n\n\n\n- l을 세번눌러서 오른쪽으로 3칸 이동\n\nl이 아니라 실수로 ㅣ를 누르면 동작하지 않음.\n대충 화살표 눌러도 동작함\n\n\n\n\n- h을 세번눌러서 왼쪽으로 3칸 이동\n\nh가 아니라 실수로 ㅗ를 누르면 동작하지 않음.\n대충 화살표 눌러도 동작함\n\n\n\n\n\n\n\n\n\n예제: vi에서 새로운라인을 추가하고 asdf2입력하여 보자.\n\n\nstep1step2step3\n\n\n- 준비작업: 명령모드에서 다음라인을 만들고 싶은 라인에 커서를 위치\n\n\n\n- o를 누른다: 편집모드로 전환 + 새로운 라인이 추가의 효과\n\n\n\n- asdf2입력\n\n\n\n\n\n\n\n\n예제: vi에서 j,k를 이용하여 커서를 이동하여 보자. (풀이는 생략)\n\n\n\n\n\n예제: vi에서 home/end 와 같은 방식으로 커서를 이동해보자\n\n\nstep1step2step3\n\n\n- 준비작업: 명령모드에서 home/end를 사용할 line으로 커서를 이동시킴\n\n\n\n- 라인의 맨 마지막으로 가고싶다면? $를 입력한다.\n\n\n\n- 다시 라인의 맨 처음으로 가고싶다면? 숫자 0을 입력한다.\n\n\n\n\n\n\n\n\n예제: vi에서 하나의 줄을 복사하고 아래로 붙여넣기 해보자.\n\n\nstep1step2step3\n\n\n- 복사하고싶은 라인으로 이동 + yy 입력\n\n\n\n- 아래로 붙여넣을 라인으로 이동\n\n\n\n- p 입력\n\n\n\n\n\n\n\n\n예제: vi에서 w와 b를 이용하여 단어단위로 커서를 이동시켜보자. (풀이생략)\n\n\n\n\n\n예제: vi에서 원하는 단어는 찾아보자.\n\n\nstep1step2step3step4\n\n\n- /asdf2 입력\n\n\n\n- Enter 입력\n\n\n\n- n 입력하여 다음단어로 이동\n\n\n\n- shift+n 입력하여 이전단어로 이동 (?로 검색할 경우 n와 shift+n의 역할이 뒤바뀜)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#i-esc-w-q-편집-명령-저장-종료",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#i-esc-w-q-편집-명령-저장-종료",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi를 이용하여 파일 만들고 asdf 입력 후 저장 및 종료를 수행하여 보자.\n\n\nstep1step2step3step4step5\n\n\n- 파일생성\nvim test230803.txt \n\n\n\n- 편집모드로 전환: i를 누른다.\n\n아래에 -- INSERT -- 라고 표현되어 있으면 편집모드라는 의미\n\n\n\n\n- asdf 입력\n\n\n\n- 편집모드 종료, 노말모드 진입:esc 입력\n\n아래에 -- INSERT -- 라는 표현이 사라져있음. 이 상황에서 명령어 입력가능.\n\n\n\n\n- 저장(w)+종료(q): : 입력하여 명령모드로 진입. w+ q + Enter 입력"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#hl-좌우로-커서이동",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#hl-좌우로-커서이동",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi 에서 좌우로 커서를 이동해보자.\n\n\nstep1step2step3\n\n\n- 생성된 파일로 들어가기\nvim test230803.txt \n\n\n\n- l을 세번눌러서 오른쪽으로 3칸 이동\n\nl이 아니라 실수로 ㅣ를 누르면 동작하지 않음.\n대충 화살표 눌러도 동작함\n\n\n\n\n- h을 세번눌러서 왼쪽으로 3칸 이동\n\nh가 아니라 실수로 ㅗ를 누르면 동작하지 않음.\n대충 화살표 눌러도 동작함"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#o-새로운라인-추가",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#o-새로운라인-추가",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 새로운라인을 추가하고 asdf2입력하여 보자.\n\n\nstep1step2step3\n\n\n- 준비작업: 명령모드에서 다음라인을 만들고 싶은 라인에 커서를 위치\n\n\n\n- o를 누른다: 편집모드로 전환 + 새로운 라인이 추가의 효과\n\n\n\n- asdf2입력"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#jk-위-아래-이동",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#jk-위-아래-이동",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 j,k를 이용하여 커서를 이동하여 보자. (풀이는 생략)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#homeend와-같은-기능",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#homeend와-같은-기능",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 home/end 와 같은 방식으로 커서를 이동해보자\n\n\nstep1step2step3\n\n\n- 준비작업: 명령모드에서 home/end를 사용할 line으로 커서를 이동시킴\n\n\n\n- 라인의 맨 마지막으로 가고싶다면? $를 입력한다.\n\n\n\n- 다시 라인의 맨 처음으로 가고싶다면? 숫자 0을 입력한다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#yy-p-라인복사-붙여넣기",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#yy-p-라인복사-붙여넣기",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 하나의 줄을 복사하고 아래로 붙여넣기 해보자.\n\n\nstep1step2step3\n\n\n- 복사하고싶은 라인으로 이동 + yy 입력\n\n\n\n- 아래로 붙여넣을 라인으로 이동\n\n\n\n- p 입력"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#wb-다음단어로-점프-이전단어로-점프",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#wb-다음단어로-점프-이전단어로-점프",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 w와 b를 이용하여 단어단위로 커서를 이동시켜보자. (풀이생략)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#단어-찾기",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#단어-찾기",
    "title": "[Note] vi 익히기",
    "section": "",
    "text": "예제: vi에서 원하는 단어는 찾아보자.\n\n\nstep1step2step3step4\n\n\n- /asdf2 입력\n\n\n\n- Enter 입력\n\n\n\n- n 입력하여 다음단어로 이동\n\n\n\n- shift+n 입력하여 이전단어로 이동 (?로 검색할 경우 n와 shift+n의 역할이 뒤바뀜)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#esc-normal-모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#esc-normal-모드",
    "title": "[Note] vi 익히기",
    "section": "esc: normal 모드",
    "text": "esc: normal 모드\n- 기능1: 되돌리기, 되돌리기를 되돌리기\n- 기능2: 커서이동 (기본) – 방향키, 쉬프트+방향키, home/end, pageup/pagedown 으로 대체가능\n\nk,j,h,l: 상,하,좌,우\nw,b: 다음 단어의 시작으로 이동, 이전 단어의 시작으로 이동\n0 (숫자 0), $: 줄의 맨 앞, 맨 끝으로 이동\ncontrol + d, control + u: 반페이지씩 page down, page up\n{, }: 문단 시작, 끝으로 이동\n\n- 기눙3: 커서이동 (고급)\n\n%: 현재 괄호의 짝으로 이동\ne: 현재단어의 끝으로 이동\ngg: 파일의 첫 줄로 이동\nG: 파일의 마지막 줄로 이동\n\n- 기능4: 다른모드로 진입하기 위한 중간허브\n\nv,V: 선택모드, 줄단위선택모드로 변환\ni, I, a, A: 커서앞, 줄맨앞, 커서뒤, 줄맨뒤에서 삽입모드로 전환\nr, R: 한글자 수정후 복귀, 수정모드로 진입\n\n- 기능5: 들여쓰기, 내어쓰기\n\n&gt; + Enter, &lt; + Enter: 들여쓰기, 내어쓰기"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#i-삽입모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#i-삽입모드",
    "title": "[Note] vi 익히기",
    "section": "i: 삽입모드",
    "text": "i: 삽입모드"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#d-삭제모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#d-삭제모드",
    "title": "[Note] vi 익히기",
    "section": "d: 삭제모드",
    "text": "d: 삭제모드"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#명령모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#명령모드",
    "title": "[Note] vi 익히기",
    "section": ":: 명령모드",
    "text": ":: 명령모드"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#검색모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#검색모드",
    "title": "[Note] vi 익히기",
    "section": "/: 검색모드",
    "text": "/: 검색모드"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#v-비주얼모드선택모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#v-비주얼모드선택모드",
    "title": "[Note] vi 익히기",
    "section": "v: 비주얼모드(선택모드)",
    "text": "v: 비주얼모드(선택모드)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-05-vi 익히기.html#r-수정모드",
    "href": "posts/4_Notes/2000-01-05-vi 익히기.html#r-수정모드",
    "title": "[Note] vi 익히기",
    "section": "R: 수정모드",
    "text": "R: 수정모드"
  },
  {
    "objectID": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "href": "posts/4_Notes/2000-01-01-우분투 포맷 및 개발용 서버 셋팅.html",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅 (old)",
    "section": "",
    "text": "About this doc\n- 우분투에서 여러가지 개발환경을 설정하는 방법을 포스팅 하겠다.\n- 이 포스트는 우분투를 메인OS(사무용+연구용)로 사용하고 싶은 사람, 우분투를 활용하여 개발용 서버를 구축하고 싶은 사람에게 모두 유용한다.\n- 이 포스트는 2080 이상의 GPU를 활용한 학습을 원하는 사람에게 유용하다.\n- 이 포스트는 R과 파이썬을 동시에 쓰는 사람에게 유용하다.\n- 이 포스트는 Rstudio, Jupyter Lab을 동시에 쓰는 사람에게 유용하다.\n- 매년 조금씩 셋팅방법이 다른것 같다.\n- 가장 최근에는 2023년 3월8일에 이 블로그 내용으로 셋팅해보았음.\n\n\n우분투설치\n- 22.04부터는 파티션 나누지 않고 그냥 설치해도 잘 되는것 같다.\n\n\n네트워크 설정\n- ?표시있는 아이콘 \\(\\to\\) Wired Connected \\(\\to\\) Wired Settings \\(\\to\\) Connection의 설정 \\(\\to\\) IPv4 \\(\\to\\) Manual \\(\\to\\) Address, Netmask, Gateway, DNS 설정 \\(\\to\\) 네트워크 토글\n\n\n한글설정 (개발용 서버일 경우 생략 가능)\n- 아래와 같이 커맨드에 친다.\nibus-setup\n이걸 치면 IBus Preferences 라는 창이 나오는데 여기에서 (1) Input Method 탭 클릭 (2) Add 버튼 클릭 (3) Korean 선택 (4) Hangul 선택을 한다.\n- 위의 단계에서 Korean이 안보이면 Language Support로 가서 한국어팩을 설치하고 리부팅 하면 된다. (보통 실행하자마자 알아서 설치되더라.. 설치가 안되면 Install / Remove Languages... 이라는 탭을 클릭해서 설치하자) 리부팅을 꼭 해야한다는 것에 주의하자.\n- 이제 Region & Language로 가서 설정하면 된다.\n\n\n그래픽카드 드라이버설치\n- 전체적인 내용은 여기를 참고하자.\n- 준비작업\nsudo apt update \nsudo apt install gcc\nsudo apt install build-essential\n- 우선 gedit를 열고 아래를 복사해서 붙여넣는다.\nblacklist nouveau\noptions nouveau modeset=0\n파일이름을 blacklist-nouveau.conf로 home에 저장\n- 루트권한획득\nsudo -i\n아이디와 비밀번호를 입력하고 루트권한을 얻는다.\n- 아래를 입력한다.\nsudo cp /home/cgb2/blacklist-nouveau.conf /etc/modprobe.d\nsudo update-initramfs -u\nsudo reboot \n- 그래픽카드 다운로드: 드라이버 설치파일을 다운받는다. 앤비디아공식홈페이지에서 다운받자. OS를 리눅스 64-bit으로 선택하고 검색을 누르면 다운받아진다.\n- 그래픽키다 설치: 다운받은뒤에는 파일이 있는 폴더로 이동하여\nchmod +x NVIDIA-Linux-x86_64-410.78.run\n를 실행하자. 보통 NVI까지치고 적당히 탭을 누르면 알아서 뒷부분이 완성된다. 이 과정은 추후에 드라이버를 실행할수 있도록 권한을 풀어두는 것이다. 그리고 아래를 실행한다.\nsudo ./NVIDIA-Linux-x86_64-410.78.run\n그 다음 드라이버가 잘 설치되었는지 확인한다.\nnvidia-smi\n\n\n아나콘다\n- (아나콘다 설치) 아나콘다를 다운받은 폴더로 가서 아래와 같이 실행한다.\nbash Anaconda3-2019.03-Linux-x86_64.sh\n대충 bash Ana 정도까지만 치고 tab을 누르면 알아서 완성된다.\n- (환경만들기) 커맨드를 키고 아래를 실행한다.\n(base) conda create -n py38r40 python=3.8\n(base) conda create --name py38r40 python=3.8\n둘 중 아무거나 실행해도 된다. 파이썬 환경이 너무 높으면 나중에 conda tensorflow-gpu가 먹히지 않으니 환경을 만들때 파이썬버전을 3.8.x로 하자. (현시점 2021년 2월25일기준 3.9.x이면 conda tensorflow-gpu 가 동작하지 않음.)\n\n\nssh연결\n- 처음에 ssh를 연결하기위해서는 연결당하는 컴퓨터에 가서 아래를 실행해야 한다.\nsudo apt install openssh-server\n22번포트 우회하기\n- step1: /etc/ssh/sshd_config 파일을 연다.\nsudo vi /etc/ssh/sshd_config \n- step2: Port 22 라고 된 부분의 주석을 풀고 원하는 포트번호 설정\n...\n\n#Port 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n...\n- step3: 수정내용을 적용\nsudo systemctl restart ssh.service\n- step4: 수정한 포트로 ssh접속\n\n\n주피터 원격제어\n- 1단계: 주피랩설치\n(py38) conda install -c conda-forge jupyterlab\n\nNote: 사실 위에서 주피터랩을 따로 설치안해도 주피터랩이 잘만 실행된다. 하지만 이렇게하니까 나중에 R커널을 만들기위해 IRkernel::installspec()을 실행할때 에러가 난다.\n\n- 2단계: 패스워드 설정\n(py38) jupyter lab --generate-config\n(py38) jupyter lab password\n- 3단계: jupyter lab 환경설정\nnano /home/cgb/.jupyter/jupyter_lab_config.py \n아래를 변경\nc.ServerApp.ip = '192.168.0.4'\nc.ServerApp.port = 1306\nc.ServerApp.open_browser = False\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\n\n주피터노트북 원격제어\n- 1단계: 주피터노트북 설치 (보통 lab을 설치하면 이미 설치되어있음)\n(py38) conda install -c conda-forge notebook \n- 2단계: 패스워드 설정\nfrom notebook.auth import passwd\npasswd()\nEnter password: \nVerify password: \n생성된값 (argon 어쩌고..)을 복사\n- 3단계: 환경설정\njupyter notebook --generate-config\nnano /home/cgb/.jupyter/jupyter_notebook_config.py\n아이피주소와 패스워드를 바꾼다. (port는 선택, browser도 선택 )\nc.NotebookApp.open_browser = False\nc.NotebookApp.ip = '192.168.0.4'\nc.NotebookApp.port = 1307\nc.NotebookApp.password = ''\n여기에서 192.168.0.4 는 내부아이피다. 고정아피이가 있다면 고정아이피 주소를 쓰면 된다.\n\nTip: 주피터노트북과 랩을 양쪽으로 셋팅후 주피터 노트북으로 실행하면 2개를 모두 쓸 수 있음\n\n\n\nR설치ver1: (base)에 설치\n- 설치전: 기존의 R 삭제\nconda remove r-base -y \nsudo apt-get remove r-base-core \nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\n- R설치전 준비작업: 나노에디터를 키고 /etc/apt/sources.list를 연다.\nsudo nano /etc/apt/sources.list\n화살표로 이동하여 맨아래로 간뒤에 아래중 하나를 추가한다. (나는 focal-cran40으로 추가함)\ndeb https://cloud.r-project.org/bin/linux/ubuntu impish-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu hirsute-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/\ndeb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran40/\n저장후 나노에디터 종료. 그리고 아래를 실행.\nsudo apt-get update\n경우에 따라서 아래와 같은 에러메시지가 뜰 수 있다.\n...\nW: GPG error: https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9 \n...\n공개키가 없어서 생기는 에러이므로 아래와 같이 가져온다.\nwget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n#sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 51716619E084DAB9\n그리고 다시 아래를 실행\nsudo apt-get update\n에러가 없이 뭔가 마무리 되어야한다.\n(base) cgb3@cgb3:~$ sudo apt-get update\nIgn:1 http://linux.dropbox.com/ubuntu disco InRelease\nHit:2 http://security.ubuntu.com/ubuntu focal-security InRelease  \nHit:3 http://kr.archive.ubuntu.com/ubuntu focal InRelease                                 \nHit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease                \nHit:5 http://linux.dropbox.com/ubuntu disco Release                 \nGet:6 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\nHit:8 http://kr.archive.ubuntu.com/ubuntu focal-updates InRelease\nGet:9 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [46.4 kB]\nHit:10 http://kr.archive.ubuntu.com/ubuntu focal-backports InRelease\nFetched 50.0 kB in 1s (36.5 kB/s)                   \nReading package lists... Done\n- R설치\nwget http://security.ubuntu.com/ubuntu/pool/main/i/icu/libicu66_66.1-2ubuntu2_amd64.deb\nsudo dpkg -i libicu66_66.1-2ubuntu2_amd64.deb\nsudo apt-get install r-base\n- tidyverse 설치 (R studio 설치전에 tidyverse 설치해야함)\n- Rstudio 설치: https://www.rstudio.com/products/rstudio/download-server/debian-ubuntu/\n\n우분투22로 설정할것!!\n\nsudo apt remove rstudio-server\nsudo apt-get install gdebi-core\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.12.0-353-amd64.deb\nsudo gdebi rstudio-server-2022.12.0-353-amd64.deb\n- Rstudio를 설치하면 ~/R/x86_64-conda-linux-gnu-library/4.1이 새로 생성된다.\n\nRstudio에서 설치한 패키지는 이 폴더에 저장된다.\n\n- 주피터와 R커널 연결\nR # sudo R \ninstall.packages(\"IRkernel\")\nIRkernel::installspec()\n\n\nR설치ver2: (py38r40)에 설치\n- R설치\n(py38r40) conda install -c conda-forge r-essentials=4.0\n이러면 콘다환경에는 R이 깔리고 base에는 R이 깔리지 않는다.\n- 커널연결\n콘다환경에서 R을 실행한다. Rstudio가 아니라 커맨드에서 R을 실행해야한다. 그리고 아래를 실행하면 주피터랩과 R환경이 연결된다.\nIRkernel::installspec()\n이제 주피터랩에서 R kernel을 사용할 수 있다.\n\n\n가상환경에서 Rstudio server 설치 (어려움)\n- 이제 Rstudio server를 설치하는 방법을 다룬다.\n- 먼저 Rstudio를 설치한다. 참고로 Rstudio server 설치하는법은 여기를 참고하라. 요약하면 터미널에서 아래3줄을 입력하기만 하면된다.\n(py38r40) sudo apt-get install gdebi-core\n(py38r40) wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb\n(py38r40) sudo gdebi rstudio-server-1.2.5033-amd64.deb\n\nWarning: Rstudio 1.3x 이상을 설치하지말고 1.2x를 설치해야 한다. 이상하게 1.3x이상은 후에 서술할 Gregor Strurm가 그의 깃허브에서 제안하는 방식이 잘 동작하지 않았다. 이는 알려진 문제였고 이를 해결하는 해결책을 서술한 스레드가 있어보이긴 했지만 나는 그냥 Rstudio 1.2x를 설치하고 쓰는 것을 선택했다.\n\n\nNote: 이미 rstudio server 가 다른버전으로 깔려있다면 sudo apt remove rstudio-server 를 통하여 삭제하고 설치하자.\n\n- 이제 Rstudio 설치가 끝났다. 설치된 Rstudio를 아나콘다 가상환경에 설치된 R과 연결해보자. 우선 아래를 실행한다.\n(py38r40) sudo apt install uuid\n(py38r40) sudo apt install git\n(py38r40) git clone https://github.com/grst/rstudio-server-conda.git\n위에 두줄은 Gregor Sturm이 만든 어떤 프로그램을 쓰기 위한 사전준비작업이다. 마지막줄을 실행하면 Gregor Sturm이 만든 프로그램이 다운받아진다. 이게 프로그램 설치가 완료된것이다. 이제 컴퓨터 껐다 킬때마다 아래를 실행한다.\n(py38r40) ./rstudio-server-conda/local/start_rstudio_server.sh 8787 # use any free port number here. \n이제 192.168.0.4:8787 따위의 주소로 접속하면 Rstudio를 쓸 수 있다. 참고로 system-wide Rstudio server를 죽여야 할 때가 있다. 그럴땐 아래 명령을 치면 된다.\n(py38r40) sudo systemctl disable rstudio-server.service\n(py38r40) sudo systemctl stop rstudio-server.service\n\n\n자주 설치하는 패키지 리스트\n- 아래를 미리 깔아두자..\n# conda install -c conda-forge notebook\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\nconda install -c conda-forge plotly \npip install plotnine\npip install seaborn\npip install opencv-python\npip install folium\npip install pandas_datareader\nconda install -c conda-forge r-essentials=4 \npip install rpy2\nconda install -c conda-forge python-graphviz\n- tensorflow-gpu 는 현재(2022-03-06) python=3.10 에서 동작함\nconda create -n py310 python=3.10 \nconda activate py310 \nconda install -c conda-forge tensorflow-gpu \n- 아래를 설치하면 좋음\nsudo apt install mc \n\n\n터미널 예쁘게 만들기\n- zsh 설치 + oh my zsh 설치\nsudo install zsh \nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n- 테마변경\n\n.zshrc 파일 열기\n\nnano ~/.zshrc \n\n아래의 내용 수정\n\n...\nZSH_THEME=\"agnoster\"\n...\n- 색상변경\n\n아래의 파일 열기\n\ncd ~/.oh-my-zsh/themes/\nnano agnoster.zsh-theme  \n\n내용수정\n\n...\nprompt_dir() {\n  prompt_segment 39d $CURRENT_FG '%~'\n}\n...\n\n\nsublime text and TeX (개발용 서버일 경우 생략 가능)\n- ‘Ubuntu Software’에 가서 ’sublime Text’를 치면 다운받을 수 있다. 다운받은뒤에 ’file’ -&gt; ’open folder’를 활용하여 깃허브의 로칼저장소를 열어두면 편리하다.\n- 아래를 실행하여 TeX을 깐다.\nsudo apt install texlive-full\n- 이제 sublime과 latex을 연결하여보자. 여기를 참고하자. (1) sublime을 키고 ‘컨트롤+쉬프트+p’를 눌러 ’Install Package Control’ 선택 (2) 다시 ‘컨트롤+쉬프트+p’ 를 눌러 ‘Package Control: Install Package’를 실행 (3) 그러면 바로 검색창이 나오는데 거기서 ’LaTeXTools’를 입력해서 실행 (4) 다시 ’컨트롤+쉬프트+p’를 누르고 ’LaTeXTools: Check system’ 선택. 모두 ’available’이 나오면 잘 설치된 것이다.\n- *.tex파일을 열고 ’컨트롤+b’를 누르자. 처음이면 어떤 메뉴들이 보일텐데 그냥 ’Latex’을 선택하자. 그러면 코딩결과가 pdf로 나온다.\n- (수식미리보기) ‘Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-User’를 선택한다. ’93번째라인’에 ’preview_math_mode’를 “all”로 바꾼다. 그러면 수식들이 미리 출력된다. 그외에도 자유롭게 셋팅을 조정할 수 있다. 원래셋팅은 ’Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-Defaults’ 에 있다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "",
    "text": "- 우분투에서 여러가지 개발환경을 설정하는 방법을 포스팅 (시대가 너무 바뀌어서 새로정리)"
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#rpython-사용가능한-개발환경",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#rpython-사용가능한-개발환경",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "6-1. R+Python 사용가능한 개발환경",
    "text": "6-1. R+Python 사용가능한 개발환경\n- 아래를 실행\n(base) conda create -n rpy \n(base) conda activate rpy\n(rpy) conda install -c conda-forge r-essentials\n(rpy) conda install -c conda-forge plotly\n(rpy) pip install ryp2 \n- 여기에서 conda install -c conda-forge r-essentials로 인하여 R, Python, Jupyter 가 모두 최신버전으로 설치된다.\n- 또한 R에는 이미 tidyverse, IRkernel 등의 패키지가 기본으로 깔려있다.\n- R은 아래의 경로에 설치되어있다.\n\n- 실행파일은 bin 폴더에 있다.\n\n(base) 환경이지만 이곳의 실행파일에 직접 접근하여 실행할 수도 있다.\n\n- 설치한 패키지는 library 폴더에 있다. /// EPT도 보인다.."
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#pytorch-사용이-가능한-개발환경",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#pytorch-사용이-가능한-개발환경",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "6-2. PyTorch 사용이 가능한 개발환경",
    "text": "6-2. PyTorch 사용이 가능한 개발환경\n- 아래를 실행\n(base) conda create -n torch\n(base) conda activate torch\n(torch) conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n(torch) conda install -c conda-forge notebook \n(torch) conda install -c conda-forge plotly \n(torch) conda install pyg -c pyg \n(torch) pip install torch-geometric-temporal\n(torch) conda install -c pytorch -c conda-forge ctgan  \n(torch) pip install gymnasium \n(base) sudo apt-get install swig\n(torch) pip install gymnasium[box2d]\nrefs\n\nPyTorch: https://pytorch.org/get-started/locally/\nPyG: https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html\nPyTorch Geometric Temporal1: https://pytorch-geometric-temporal.readthedocs.io/en/latest/notes/installation.html\nCTGAN: https://github.com/sdv-dev/CTGAN\nGymnasium: https://gymnasium.farama.org/index.html2\n\n1 이 패키지는 꼭 PyG 이후에 설치할것2 설치방법은 따로 없음.."
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a1.-r-삭제하는-방법",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a1.-r-삭제하는-방법",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "A1. R 삭제하는 방법",
    "text": "A1. R 삭제하는 방법\n- R 삭제\nconda remove r-base -y \nsudo apt-get remove r-base-core \nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove"
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a2.-r-패키지-설치",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a2.-r-패키지-설치",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "A2. R 패키지 설치",
    "text": "A2. R 패키지 설치\n- 주피터등에서 직접설치\n- conda를 이용한 설치\nconda install -c r package-name\n\nhttps://docs.anaconda.com/free/anaconda/reference/packages/r-language-pkg-docs/"
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a3.-우분투를-주컴퓨터로-사용할-경우-한글설정",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a3.-우분투를-주컴퓨터로-사용할-경우-한글설정",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "A3. 우분투를 주컴퓨터로 사용할 경우 한글설정",
    "text": "A3. 우분투를 주컴퓨터로 사용할 경우 한글설정\n- 아래와 같이 커맨드에 친다.\nibus-setup\n이걸 치면 IBus Preferences 라는 창이 나오는데 여기에서 (1) Input Method 탭 클릭 (2) Add 버튼 클릭 (3) Korean 선택 (4) Hangul 선택을 한다.\n- 위의 단계에서 Korean이 안보이면 Language Support로 가서 한국어팩을 설치하고 리부팅 하면 된다. (보통 실행하자마자 알아서 설치되더라.. 설치가 안되면 Install / Remove Languages... 이라는 탭을 클릭해서 설치하자) 리부팅을 꼭 해야한다는 것에 주의하자.\n- 이제 Region & Language로 가서 설정하면 된다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a4.-sublime-text-and-tex",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a4.-sublime-text-and-tex",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "A4. sublime text and TeX",
    "text": "A4. sublime text and TeX\n- ‘Ubuntu Software’에 가서 ’sublime Text’를 치면 다운받을 수 있다. 다운받은뒤에 ’file’ -&gt; ’open folder’를 활용하여 깃허브의 로칼저장소를 열어두면 편리하다.\n- 아래를 실행하여 TeX을 깐다.\nsudo apt install texlive-full\n- 이제 sublime과 latex을 연결하여보자. 여기를 참고하자. (1) sublime을 키고 ‘컨트롤+쉬프트+p’를 눌러 ’Install Package Control’ 선택 (2) 다시 ‘컨트롤+쉬프트+p’ 를 눌러 ‘Package Control: Install Package’를 실행 (3) 그러면 바로 검색창이 나오는데 거기서 ’LaTeXTools’를 입력해서 실행 (4) 다시 ’컨트롤+쉬프트+p’를 누르고 ’LaTeXTools: Check system’ 선택. 모두 ’available’이 나오면 잘 설치된 것이다.\n- *.tex파일을 열고 ’컨트롤+b’를 누르자. 처음이면 어떤 메뉴들이 보일텐데 그냥 ’Latex’을 선택하자. 그러면 코딩결과가 pdf로 나온다.\n- (수식미리보기) ‘Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-User’를 선택한다. ’93번째라인’에 ’preview_math_mode’를 “all”로 바꾼다. 그러면 수식들이 미리 출력된다. 그외에도 자유롭게 셋팅을 조정할 수 있다. 원래셋팅은 ’Perferences’ &gt; ‘Packages Setting’ &gt; ‘LaTeXTools’ &gt; ‘Settings-Defaults’ 에 있다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a5.-터미널-예쁘게-만들기",
    "href": "posts/4_Notes/2000-01-02-우분투 포맷 및 개발용 서버 셋팅2.html#a5.-터미널-예쁘게-만들기",
    "title": "[Note] 우분투 포맷 및 개발용 서버 셋팅2",
    "section": "A5. 터미널 예쁘게 만들기",
    "text": "A5. 터미널 예쁘게 만들기\n- zsh 설치 + oh my zsh 설치\nsudo install zsh \nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n- 테마변경\n\n.zshrc 파일 열기\n\nnano ~/.zshrc \n\n아래의 내용 수정\n\n...\nZSH_THEME=\"agnoster\"\n...\n- 색상변경\n\n아래의 파일 열기\n\ncd ~/.oh-my-zsh/themes/\nnano agnoster.zsh-theme  \n\n내용수정\n\n...\nprompt_dir() {\n  prompt_segment 39d $CURRENT_FG '%~'\n}\n..."
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html",
    "title": "[Note] 우분투 익히기",
    "section": "",
    "text": "/root\n\n루트사용자의 홈 디렉토리\nsudo로도 들어갈 수 없음\n\n/etc\n\n리눅스 시스템 전반적인 환경설정 파일들을 모은 디렉터리임.\n이 디렉터리의 모든 파일은 텍스트형식임.\n/etc/password 사용자 계정정보\n\n/home\n\n사용자의 홈 디렉터리\n\n/mnt\n\n다른 파일 시스템이 파일 시스템에 연결되거나 마운트 되는 위치\n\n/media\n\nCD, USB 장치가 파일 시스템에 연결되거나 마운트되는 위치임\n\n/bin\n\n시스템 부팅과 실행에 필요한 바이너리(=윈도도의 실행파일=macOS의 애플리케이션)들을 포함\n\n/lib\n\n시스템 프로그램에서 사용하는 공유 라이브러리가 저장. 윈도우즈의 DLL과 비슷한 것.\n\n/usr\n\n사용자가 사용하는 모든 프로그램과 지원파일들 (Program files + 프로그램들의 설정값)\n/usr/bin 리눅스 배포판이 설치한 실행 프로그램들이 있다. (여기에 R이 깔린다!!)\n\nhp-align, hp-check, hp-config_usb-printer …\nX11\nvi\ngcc\nsu, sudo\nsar\nssh, ssh-agent, ssh-keygen, ….\nnvidia-smi\n\n/usr/lib 여기에는 /usr/bin에 있는 프로그램들을 위한 공유라이브러리가 저장된다. 여기에 R folder가 있다. (R패키지는 여기말고 다른데 깔림)\n/usr/local/bin 소스코드로 컴파일된 파일, 보통 비어있음\n/usr/local/lib/R/site-library R패키지가 설치되어있음, 예를들면 tidyverse"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#pwd-whoami",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#pwd-whoami",
    "title": "[Note] 우분투 익히기",
    "section": "pwd, whoami",
    "text": "pwd, whoami\n- 현재위치확인\n(base) cgb2@cgb2-desktop:~$ pwd\n/home/cgb2\n- 유저확인\n### 예시1: 유저확인 \n(base) cgb2@cgb2-desktop:~$ whoami \ncgb2 \n\n### 예시2: 루트권한 획득이후 유저확인 \n(base) cgb2@cgb2-desktop:~$ sudo -i # 루트권한 획득\nroot@cgb2-desktop:~# whoami \nroot \n\n### 예시3: 일반유저로 전환후 유저확인\nroot@cgb2-desktop:~# su - cgb2 \n(base) cgb2@cgb2-desktop:~$ whoami\ncgb2"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#cd-ls",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#cd-ls",
    "title": "[Note] 우분투 익히기",
    "section": "cd, ls",
    "text": "cd, ls\n- 홈디렉토리로 이동 + 목록\n(base) cgb2@cgb2-desktop:~/Dropbox$ cd ~\n(base) cgb2@cgb2-desktop:~$ ls\nDesktop                 julia-1.8.5\nDocuments               julia-1.8.5-linux-x86_64.tar.gz\nDownloads               lm.txt\nDropbox                 nbdev_tst\nMusic                   quarto-1.0.37-linux-amd64.deb\nPictures                rstudio-server-2021.09.2-382-amd64.deb\nPublic                  scikit_learn_data\nR                       snap\nTemplates               test.txt\nVideos                  v3net\nanaconda3               v3net-linux-3.6.10.11.805.tar.Z\nblacklist-nouveau.conf  vscode_cli.tar.gz\ncode\n- 루트로 이동 + 목록\n(base) cgb2@cgb2-desktop:~/Dropbox$ cd /\n(base) cgb2@cgb2-desktop:/$ ls\nbin    dev   lib    libx32      media  proc  sbin  swapfile  usr\nboot   etc   lib32  log         mnt    root  snap  sys       var\ncdrom  home  lib64  lost+found  opt    run   srv   tmp\n- ls 자세하게\n(base) cgb2@cgb2-desktop:/$ ls -l\ntotal 2097256\nlrwxrwxrwx   1 root  root           7  2월 11  2022 bin -&gt; usr/bin\ndrwxr-xr-x   3 root  root        4096  7월 27 12:19 boot\ndrwxrwxr-x   2 root  root        4096  2월 11  2022 cdrom\ndrwxr-xr-x  19 root  root        4480  7월 28 12:19 dev\ndrwxr-xr-x 143 root  root       12288  7월 27 21:08 etc\ndrwxr-xr-x   6 root  root        4096  7월  7 11:14 home\nlrwxrwxrwx   1 root  root           7  2월 11  2022 lib -&gt; usr/lib\nlrwxrwxrwx   1 root  root           9  2월 11  2022 lib32 -&gt; usr/lib32\nlrwxrwxrwx   1 root  root           9  2월 11  2022 lib64 -&gt; usr/lib64\nlrwxrwxrwx   1 root  root          10  2월 11  2022 libx32 -&gt; usr/libx32\ndrwxr-x---   2 root  root        4096  2월 11  2022 log\ndrwx------   2 root  root       16384  2월 11  2022 lost+found\ndrwxr-xr-x   2 root  root        4096  8월 19  2021 media\ndrwxr-xr-x   2 root  root        4096  8월 19  2021 mnt\ndrwxr-xr-x   4 jaein saned       4096  7월 16 11:21 opt\ndr-xr-xr-x 418 root  root           0  7월 28 12:16 proc\ndrwx------   6 root  root        4096  7월 27 12:21 root\ndrwxr-xr-x  40 root  root        1220  8월  3 13:31 run\nlrwxrwxrwx   1 root  root           8  2월 11  2022 sbin -&gt; usr/sbin\ndrwxr-xr-x  14 root  root        4096  7월 16 11:10 snap\ndrwxr-xr-x   2 root  root        4096  8월 19  2021 srv\n-rw-------   1 root  root  2147483648  2월 11  2022 swapfile\ndr-xr-xr-x  13 root  root           0  7월 28 12:16 sys\ndrwxrwxrwt  19 root  root       20480  8월  3 13:29 tmp\ndrwxr-xr-x  14 root  root        4096  8월 19  2021 usr\ndrwxr-xr-x  15 root  root        4096  7월 30  2022 var"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#h-man",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#h-man",
    "title": "[Note] 우분투 익히기",
    "section": "-h, man",
    "text": "-h, man\n- 도움말확인\n(base) cgb2@cgb2-desktop:~$ git --help\nusage: git [--version] [--help] [-C &lt;path&gt;] [-c &lt;name&gt;=&lt;value&gt;]\n           [--exec-path[=&lt;path&gt;]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=&lt;path&gt;] [--work-tree=&lt;path&gt;] [--namespace=&lt;name&gt;]\n           &lt;command&gt; [&lt;args&gt;]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone             Clone a repository into a new directory\n   init              Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add               Add file contents to the index\n   mv                Move or rename a file, a directory, or a symlink\n   restore           Restore working tree files\n   rm                Remove files from the working tree and from the index\n   sparse-checkout   Initialize and modify the sparse-checkout\n\nexamine the history and state (see also: git help revisions)\n   bisect            Use binary search to find the commit that introduced a bug\n   diff              Show changes between commits, commit and working tree, etc\n   grep              Print lines matching a pattern\n   log               Show commit logs\n   show              Show various types of objects\n   status            Show the working tree status\n\ngrow, mark and tweak your common history\n   branch            List, create, or delete branches\n   commit            Record changes to the repository\n   merge             Join two or more development histories together\n   rebase            Reapply commits on top of another base tip\n   reset             Reset current HEAD to the specified state\n   switch            Switch branches\n   tag               Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch             Download objects and refs from another repository\n   pull              Fetch from and integrate with another repository or a local branch\n   push              Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n- 메뉴얼 확인\n(base) cgb2@cgb2-desktop:~$ man git"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#cp-mv-mkdir-rm",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#cp-mv-mkdir-rm",
    "title": "[Note] 우분투 익히기",
    "section": "cp, mv, mkdir, rm",
    "text": "cp, mv, mkdir, rm\n- 카피\ncp file1 file2 # file1을 복사하여 file2를 새로 만듬. file2가 이미 있다면 file1의 내용을 덮어씀 \ncp -r dir1 dir2 # dir1의 모든파일을 복사하여 dir2로 이동한뒤 붙어넣음. dir2가 없다면 새로 만듬. 기존의 dir2에 있던 파일이 삭제되는건 아님 \n- 이동\nmv file1 file2 # file1을 이동하여 file2로 이름바꿈. file2가 이미 있다면 file1의 내용을 덮어씀 \nmv -r dir1 dir2 # dir1의 모든파일을 잘라내어 dir2로 이동. \n- 디렉토리 생성\nmkdir temp\nmkdir temp1, temp2, temp3 # 여러개를 만듬\n- 삭제\nrm file1 # file1삭제 \nrm -r file1 dir1 # file1삭제 dir1폴더삭제 \nrm -rf file1 dir1 # 위와 동일한데 file1이나 dir1이 존재하지 않더라고 rm이 실행\n- -v(verbose)를 쓰면 친절한 느낌이 든다.\n(base) cgb2@cgb2-desktop:~/Dropbox$ cp *.txt temppp -v\n'colab.txt' -&gt; 'temppp/colab.txt'"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#scp",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#scp",
    "title": "[Note] 우분투 익히기",
    "section": "scp",
    "text": "scp\n- cgb2에 있는 lm.txt를 cgb5로 옮김. 포트는 cgb5에 대응하는 포트\ncgb2@cgb2-desktop:~$ scp -P 4653 lm.txt cgb5@210.117.173.185:~\n- cgb2에 있는 lm.txt를 맥으로 가져옴. 포트는 cgb2에 대응하는 포트"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#find",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#find",
    "title": "[Note] 우분투 익히기",
    "section": "find",
    "text": "find\n- Dropbox라는 이름이 포함된 폴더 찾기\nfind ~ -type d -name \"Dropbox\"\n\n여기에서 ~는 검색을 시작할 최상위 디렉토리를 쓴다.\n여기에서 -type d는 디렉토리라는 의미이다.\n\n- sy.tex이라는 파일찾기\nfind ~ -type f -name \"sy.tex\"\n\n여기에서 ~는 검색을 시작할 최상위 디렉토리를 쓴다.\n여기에서 -type d는 디렉토리라는 의미이다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#wget",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#wget",
    "title": "[Note] 우분투 익히기",
    "section": "wget",
    "text": "wget\n- 인터넷에서 파일 다운로드\nwget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.2.5033-amd64.deb"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#gdebi-deb",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#gdebi-deb",
    "title": "[Note] 우분투 익히기",
    "section": "gdebi, deb",
    "text": "gdebi, deb\n- deb파일을 gdebi로 설치하는 방법\nsudo gdebi rstudio-server-1.2.5033-amd64.deb\n- deb파일을 dpkg를 이용하여 설치하는 방법\ndpkg -i quarto-1.2.335-linux-amd64.deb # 설치\ndpkg -r quarto # 삭제 \n- gdebi, deb의 차이\n\ngdebi는 관련패키지를 모두 찾아서 설치 dpkg는 하나의 패키지만 설치. 즉 gdebi는 conda 느낌이고 dpkg는 pip 느낌\ngdebi는 별도의 삭제옵션을 지원하지 않음. 따라서 gdebi로 설치한 deb을 삭제하려면 머리아플수도?\n\n- gdebi로 설치한 패키지를 삭제하는 방법\n\n\n\n\n\n\nChat-GPT 답변\n\n\n\ngdebi로 패키지를 삭제하는 것은 아쉽게도 직접적으로 지원하지 않습니다. gdebi는 주로 패키지를 설치할 때 사용되는 도구이며, 패키지의 삭제에는 사용되지 않습니다. 패키지를 삭제하려면 다른 도구인 dpkg나 패키지 관리자 명령을 사용해야 합니다.\n패키지를 삭제하는 방법은 아래와 같습니다. 패키지를 삭제하기 전에 해당 패키지가 다른 의존하는 패키지에 영향을 주지 않는지 주의깊게 확인하시기 바랍니다.\ndpkg를 사용하여 패키지 삭제하기:\nsudo dpkg -r 패키지이름\n여기서 패키지이름은 삭제하려는 패키지의 이름입니다. 이 명령을 실행하면 해당 패키지가 삭제됩니다.\napt를 사용하여 패키지 삭제하기 (의존성 관리 포함):\nsudo apt remove 패키지이름\n위 명령은 apt를 통해 패키지를 삭제하며, 의존성을 포함하여 관련된 패키지들도 자동으로 처리합니다. 이 방법은 일반적으로 패키지 삭제에 더 효과적입니다.\n위의 방법 중 하나를 선택하여 패키지를 삭제할 수 있습니다. 패키지 삭제 시에는 주의하여 의존성 문제가 발생하지 않도록 해야 합니다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#apt",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#apt",
    "title": "[Note] 우분투 익히기",
    "section": "apt",
    "text": "apt\nsudo apt-get remove r-base-core\nsudo apt purge r-base* r-recommended r-cran-*\nsudo apt autoremove\nsudo apt list \nsudo apt update\nsudo apt install openssh-server \nsudo apt-get install gdebi-core\n- sudo apt-get과 sudo apt 차이? 별 차이 없는듯\n\nhttps://askubuntu.com/questions/445384/what-is-the-difference-between-apt-and-apt-get\n\n\nThey are very similar command line tools available in Trusty (14.04) and later. apt-get and apt-cache’s most commonly used commands are available in apt. apt-get may be considered as lower-level and “back-end”, and support other APT-based tools. apt is designed for end-users (human) and its output may be changed between versions."
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#conda",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#conda",
    "title": "[Note] 우분투 익히기",
    "section": "conda",
    "text": "conda\nconda \nconda env -h \nconda install -h \nconda remove -h  \nconda update -h \nconda env list\nconda create -n py38r40 python=3.8\nconda env remove -n py38r40 \nconda install -c conda-forge jupyterlab \nconda remove jupyterlab \nconda remove r-base -y \nconda remove -n py38r40 jupyterlab \nconda update scipy\nconda update -n py38r40 scipy\nconda list"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#pip",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#pip",
    "title": "[Note] 우분투 익히기",
    "section": "pip",
    "text": "pip\npip\npip list\npip list &gt; list.txt\npip freeze # 좀 더 자세히 나온다 \npip freeze &gt; list.txt \npip show matplotlib # 설치된패키지 정보가 나옴. 좋음.\npip install rpy2\npip install -r list.txt \npip install dash==1.13.3\npip install jupyterlab \"ipywidgets&gt;=7.5\"\npip install -U numpy\npip install --upgrade pip\npip install --upgrade tensorflow\npip uninstall matplotlib"
  },
  {
    "objectID": "posts/4_Notes/2000-01-04-우분투 익히기.html#a1.-덜-중요한-명령어",
    "href": "posts/4_Notes/2000-01-04-우분투 익히기.html#a1.-덜-중요한-명령어",
    "title": "[Note] 우분투 익히기",
    "section": "A1. 덜 중요한 명령어",
    "text": "A1. 덜 중요한 명령어\n\nfile\n- 뭐하는 파일인지 알고싶다면?\n(base) cgb2@cgb2-desktop:~/Dropbox/03_yechan3$ file 1_essays.qmd \n1_essays.qmd: ASCII text\n\n\ndate, cal\n- 날짜\n(base) cgb2@cgb2-desktop:~$ date\n2023. 08. 03. (목) 16:10:38 KST\n- 달력\n(base) cgb2@cgb2-desktop:~$ cal\n      8월 2023         \n일 월 화 수 목 금 토  \n       1  2  3  4  5  \n 6  7  8  9 10 11 12  \n13 14 15 16 17 18 19  \n20 21 22 23 24 25 26  \n27 28 29 30 31"
  },
  {
    "objectID": "posts/1_Essays/2023-07-04-토폴로지(1).html",
    "href": "posts/1_Essays/2023-07-04-토폴로지(1).html",
    "title": "[Essays] 토폴로지",
    "section": "",
    "text": "About this doc\n- 수학공부\n- 학부수준\n- 이 문서는 논문을 읽을때 등장하는 topology 용어들을 좀더 명확하게 이해하고 싶어서 작성하였다. 가볍게 정의만 훑어보는 것이라 깊게 들어가지는 않을 예정이다. 교재는 Schaum’s General Topology 를 참고하였다. - Lipschutz, S. (1965). Schaum’s outline of theory and problems of general topology. Schaum’s Outline Series.\n- 여기에서는 토폴로지의 정의와 메트릭스페이스의 정의 그리고 컴플리션의 정의에 대하여 다룬다.\n\n\nChap 5: 토폴로지\n- (Lipschutz (1965), p.66) \\({\\cal T}\\) 가 \\(X\\) 의 subset 으로 이루어진 collection 이라고 하자. \\({\\cal T}\\) 가 \\(X\\) 를 포함하며 uncountable union 에 닫혀있고 finite intersection 에 닫혀있다면 \\({\\cal T}\\) 를 \\(X\\) 의 topology 라고 한다. 그리고 \\((X,{\\cal T})\\) 를 topological space 라고 한다. \\({\\cal T}\\) 가 \\(X\\)의 토플로지일때 \\({\\cal T}\\) 의 원소를 \\({\\cal T}\\)-open set 이라고 한다. 따라서 원래 오픈셋은 마치 확률변수처럼 단독으로 정의할 수 없고 어떠한 토폴로지 \\({\\cal T}\\)와 같이 정의된다.\n- (Lipschutz (1965), p.66) 아래와 같은 collection 을 생각하자.\n\\[{\\cal O}:=\\{O: O=\\cup_i(a_i,b_i), a_i,b_i \\in \\mathbb{R} \\} \\]\n컬렉션 \\({\\cal O}\\) 는 \\(\\mathbb{R}\\) 의 토폴로지가 된다. (증명은 알아서..) 이러한 토폴로지(=오픈인터벌의 카운터블-유니온으로 표현가능한 집합들의 모임)을 특별히 usual topology 라고 한다. 그리고 이 토폴로지의 원소를 \\({\\cal O}\\)-오픈셋이라고 부른다. 따라서 어떤 집합 \\(O\\) 가 \\({\\cal O}\\)-오픈셋 이라는 말은 그 집합이 오픈인터벌의 카운터블-유니온으로 표현가능한 집합임을 의미한다.\n- 참고로 \\({\\cal O}\\) 가 우리가 일반적으로 생각하는 ‘오픈셋들의 모임’ 이고 \\({\\cal O}\\)-오픈셋이 보통 우리가 일반적으로 유클리드 공간에서 상상하는 오픈셋이다. 그래서 앞으로 특별한 언급없이 그냥 ‘오픈셋’ 이라고 부르면 토폴로지 \\((\\mathbb{R},{\\cal O})\\) 에서 정의가능한 ‘\\({\\cal O}\\)-오픈셋’ 을 의미하는 것이라고 생각하면 된다.\n- 즉 우리가 일반적으로 생각하는 오픈셋1은 오픈인터벌 \\((a,b)\\) 의 countable-many union 으로 표현가능한 집합이라고 이해해도 된다.\n1 정확하게는 \\(\\cal O\\)-오픈셋- 오픈셋 \\(O\\)의 원소를 interior point of \\(O\\) 라고 한다. \\({\\cal O}\\)의 정의에 의해서 인테리어포인트는 모두 아래의 성질을 만족한다.\n\\[\\forall o \\in O ~ \\exists a,b \\in \\mathbb{R}~ st.~  o \\in (a,b)\\]\n증명은 귀류법을 쓰면 쉽게 된다.\n- 저 정리가 생각보다 중요하다. 그리고 이 정리를 나이테정리 라고 기억하자. 이 정리는 \\({\\cal O}\\)-오픈셋이 아닌 일반적인 \\({\\cal T}\\)-오픈셋에 대하여서도 성립한다. 즉 \\((X,{\\cal T})\\)가 위상공간이고 \\(T\\)가 \\({\\cal T}\\)의 임의의 집합이라 하자. \\(T\\)의 임의의 원소 \\(p\\)에 대하여 (1) \\(p\\) 를 포함하지만 (2) \\(T\\) 보다 작은 다른 \\({\\cal T}\\)-오픈셋이 항상 존재한다.\n- 그리고 교재에 따라서는 위와 같은 성질을 만족하는 것을 오픈셋이라고 정의하기도 한다. 이와 같은 논리흐름으로는 오픈인터벌 \\((a,b)\\)를 정의하고 그로부터 인테리어포인트 \\(o\\)와 오픈셋 \\(O\\)를 정의하고 그로부터 토폴로지 \\({\\cal O}\\)를 정의할 수 있다. 하지만 이러한 방식의 contruction 으로는 \\((\\mathbb{R},{\\cal O})\\) 만 만들수있다. 일반적으로는 적당한 \\({\\cal T}\\)가 \\(X\\)의 토폴로지임을 밝히고 그로부터 오픈셋을 정의하고 그 다음 인테리어포인트를 정의하는 식으로 각 요소들을 contruction 한다.\n- 오픈셋의 여집합을 클로즈드셋이라고 한다. 여기서 사람들이 “모든 집합은 오픈셋이거나 클로즈드셋 이어야 한다” 라고 착각하기 쉬운데 사실 그런것은 아니다.\n- 어떠한 construction을 사용하든지 아래의 사실들이 성립한다. 따로 설명을 쓰지 않은 것은 아주 약간의 머리를 쓰면 쉽게 증명할 수 있는 것들이다. (하지만 그냥 받아들이거나 외우는 것이 편하다.) 참고로 아래의 모든 사실들은 보통위상공간 즉 \\((\\mathbb{R},{\\cal O})\\) 를 전제하고 서술한 것이다.\n(1) \\((a,b)\\) 는 오픈셋이다.\n(2) \\(\\mathbb{R}\\) 은 오픈셋이다. 동시에 클로즈드셋이다2.\n2 공집합이 오픈셋이므로3 \\(\\mathbb{R}\\)이 오픈셋이므로(3) \\(\\emptyset\\) 은 오픈셋이다. 동시에 클로즈드셋이다3.\n(4) 오픈셋은 uncountable union 에 닫혀있다. 즉 \\(O_t\\)가 각각 오픈셋일때 \\(\\cup_t O_t\\) 역시 오픈셋이다.\n(5) 오픈셋은 finite intersection 에 닫혀있다. 즉 \\(O_i\\)가 각각 오픈셋이면 \\(\\cap_{i=1}^{n} O_i\\) 역시 오픈셋이다\n(6) 한점 \\(p\\)로 이루어진 집합 \\(\\{p\\}\\)는 오픈셋이 아니다. 이것이 오픈셋이 되려면 \\(\\{p\\}\\)의 모든원소(라고 해봤자 \\(p\\) 밖에 없음)가 내점이어야 하고 \\(p\\)가 \\(\\{p\\}\\)의 내점이려면 \\(p\\)를 포함하는 오픈인터벌 \\((a,b)\\)가 \\(\\{p\\}\\)의 부분집합으로 존재해야하는데 이것이 불가능하기 때문이다. 4\n4 다만 이것은 위상공간을 \\((\\mathbb{R},{\\cal O})\\)로 생각하였을때 이야기이고 위상공간을 \\((\\mathbb{R},2^{\\mathbb{R}})\\)로 생각한다면 \\(\\{p\\}\\) 도 오픈셋이 된다.(7) 오픈셋의 countable-many intersection 은 오픈셋이 아니다. 왜냐하면 \\(\\cap_{n=1}^{\\infty}(-1/n,1/n)=\\{0\\}\\) 인데 \\(\\{0\\}\\)은 오픈셋이 아니기 때문이다.\n- (Lipschutz (1965), p. 69) 위상공간 \\((X,{\\cal T})\\) 를 상상하자. \\(X\\)의 부분집합 \\(A\\)를 상상하자. “집합 \\(A\\)가 \\(X\\)에서 dense 하다” 라는 의미는 \\(\\bar{A}=X\\)라는 의미이다. 가장 빈번하게 사용하는 표현은 “유리수집합 \\(\\mathbb{Q}\\)는 실수 \\(\\mathbb{R}\\)에서 dense 하다” 인데 이것은\\(\\bar{\\mathbb{Q}}=\\mathbb{R}\\)이 성립하기 때문이다 (예제 4.3).\n\n\nChap 6: 기저\n- 오픈인터벌 \\((a,b)\\)를 적당히 countable-many union 하면 \\(\\mathbb{R}\\) 에 존재하는 어떠한 오픈셋 \\(O\\)도 표현할 수 있다. 이럴때 \\((a,b)\\) 모아놓은 collection \\({\\cal B}:=\\{(a,b): a&lt;b \\in \\mathbb{R}\\}\\) 를 토폴로지 \\({\\cal O}\\)의 base 라고 한다. 이처럼 어떠한 위상공간 \\((X,{\\cal T})\\) 가 있을때 토폴로지 \\({\\cal T}\\) 의 임의의 집합을 \\({\\cal B}\\)의 원소들의 uncountable union 으로 표현가능때 \\({\\cal B}\\)를 \\({\\cal T}\\)의 base 라고 한다. 그리고 추가적으로 base의 모든 원소는 \\({\\cal T}\\)-오픈셋이어야 한다는 조건도 포함된다.\n- 토폴로지 \\({\\cal T}\\)의 base는 유일하지 않다.\n- 아래와 같은 collection을 상상하여 보자.\n\\[\\tilde{\\cal B}:=\\{all~ ray~ in~\\mathbb{R} \\} := \\{(-\\infty,b): b\\in \\mathbb{R} \\} \\cup \\{(a,\\infty): a \\in \\mathbb{R}\\}\\]\n보는 것처럼 \\(\\tilde{\\cal B}\\)는 위상의 정의를 만족한다. 그리고 \\(\\tilde{\\cal B}\\)는 \\({\\cal O}\\)의 base가 아니다. 하지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\((a,b)\\)를 포함하고 있기에 \\({\\cal O}\\) 의 base가 된다. 여기에서 \\(\\pi(\\tilde{\\cal B})\\) 는 \\(\\tilde{\\cal B}\\) 에 의해서 생성된 가장 작은 \\(\\pi\\)-system 이다.\n- 참고로 \\(\\pi\\)-시스템은 모든 원소가 finite intersection 에 닫혀있는 collection 을 의미한다. 전체집합은 empty intersection 으로 해석할 수 있으므로 모든 파이시스템은 전체집합을 포함한다. 따라서 파이시스템을 정의하면 전체집합을 같이 정의하는것과 마찬가지이다. 따라서 파이시스템 역시 시그마필드와 토폴로지처럼 전체집합과 동시에 정의된다. 그리고 정의에 따라서 임의의 집합에 대한 토폴로지와 시그마필드 모두 파이시스템이 된다.\n- 위에서 예를 든 \\(\\tilde{\\cal B}\\) 와 같이 그것 자체가 어떤 위상 \\({\\cal T}\\) 의 base는 아니지만 \\(\\pi(\\tilde{\\cal B})\\) 는 \\({\\cal T}\\) 의 base가 될때 \\(\\tilde{\\cal B}\\)를 \\({\\cal T}\\)의 subbase 라고 한다.\n- \\(\\tilde{\\cal B}\\) 가 토폴로지 \\({\\cal T}\\)의 subbase이면 \\(\\tilde{\\cal B}\\)로 \\({\\cal T}\\)를 generate 할 수 있다.\n\n\nChap 8: Metric and Normed Spaces\n- \\(d:X \\times X \\to \\mathbb{R}\\) 가 (1) 음이 아니고 (2) 대칭이며 (3) 삼각부등식을 만족하면 집합 \\(X\\) 에서의 metric 이라고 한다. 이때 음이 아닐 조건은\n\\[\\begin{cases}\nd(a,b) &gt; 0 & a \\neq b \\\\\nd(a,b) = 0 & a=b\n\\end{cases}\\]\n이다. 만약에 메트릭의 모든 조건을 만족하는데 \\(d(a,b)=0\\) 인 서로 다른 \\(a,b \\in X\\) 가 존재하는 경우 \\(d\\) 를 pseudometric 이라고 한다.\n- \\(d\\) 을 집합 \\(X\\) 에서의 메트릭이라고 하자. 메트릭이 존재한다는 것은 집합 \\(X\\)의 어떠한 두 원소라도 그 사이의 거리를 잴 수 있다는 말이고 그것은 집합 \\(X\\)의 임의의 점 \\(p\\)에서 아래와 같은 ball 을 정의할 수 있는 말이다.\n\\[S(p,\\delta) := \\{x:d(p,x)&lt;\\delta,x \\in X \\}\\]\n참고로 위와 같은 ball 들을 모은 collection 을 \\({\\cal B}\\)라고 하자. 그리고 \\({\\cal B}\\)의 임의의 원소를 언카운터블-유니온하여 얻을 수 있는 집합들의 모임을 \\({\\cal T}\\)라고 하자. 그러면 (1) \\({\\cal T}\\) 가 \\(X\\) 의 토폴로지임을 보이고 (2) \\({\\cal B}\\)의 모든 원소가 \\({\\cal T}\\)-오픈셋임을 보인다면 \\({\\cal B}\\)는 \\({\\cal T}\\)의 base가 된다고 주장할 수 있다(Thm 8.4). 그런데 (2)는 (1)이 성립하면 자동으로 성립하므로 (1)만 보이면 된다. 그러기 위해서는 아래의 (i)-(iii)을 보이면 된다.\n(i) 우선 \\({\\cal T}\\)가 언카운터블-유니온에 닫혀있음은 associative laws 에 의해서 쉽게 증명된다.\n(ii) 이제 \\({\\cal T}\\)가 파이나이트-인터섹션에 닫혀있음을 보이자. \\({\\cal T}\\)의 임의의 두 원소는 각각 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다. 가령 예를들어 임의의 \\(T,S \\in {\\cal T}\\) 가 아래와 같이 표현되었다고 치자.\n\\[T=\\bigcup_{t\\in [0,1]}B_{t}, \\quad S=\\bigcup_{s\\in [2,3]}B_{s}\\]\n따라서 \\(T\\cap S\\) 는 distributive laws 에 의해서 아래와 같이 표현가능하다.\n\\[T \\cap S = \\bigcup_{(t,s) \\in [0,1]\\times[2,3]} B_t \\cap B_s \\]\n(i)에 의해서 \\(B_t \\cap B_s\\)가 \\({\\cal T}\\)의 원소이기만 하면 \\(T \\cap S\\) 역시 \\({\\cal T}\\)의 원소가 되는 구조라 (ii)가 증명된다. 따라서 이제 우리가 할일은 \\(B_t\\cap B_s\\)가 \\({\\cal T}\\)의 원소임을 보이는 것이고 이것은 \\(B_t \\cap B_s\\)가 \\({\\cal B}\\)의 언카운터블-유니온으로 표현가능하다는 조건과 동치이다. 우선 \\(B_t \\cap B_s\\)에 속하는 임의의 원소를 $b^* $ 라고 하자. 이 점에 대하여 나이테정리를 만족시키는 ball이 존재한다. 즉\n\\[\\exists S(b^* ,\\delta)~ st. ~ S(b^* ,\\delta) \\subset B_t \\cap B_s\\]\n이다(Lemma 8.3). 그런데 \\(B_t \\cap B_s\\)의 모든점에서 이런식으로 나이테정리를 만족하는 ball을 잡을 수 있다. 이러한 ball들의 합집합을\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)\\]\n이라고 하자. 자명하게 이 집합은 \\(B_t\\cap B_s\\) 보다 작다(부분집합들의 합이므로). 하지만 \\(B_t\\cap B_s\\)의 모든 원소는 이 집합에 포함되므로 이 집합은 \\(B_t\\cap B_s\\)보다 크다. 따라서\n\\[\\bigcup_{b^* \\in (B_t \\cap B_s)} S(b^* , \\delta)=B_t \\cap B_s\\]\n이 성립한다.\n(iii) \\({\\cal T}\\)가 \\(X\\)를 포함한다는 것을 보이는것은 볼의 반지름을 크게 만들면 쉽게 증명할 수 있다.\n- 참고로 위의 (i)-(iii)을 요약하면 (1) \\(X\\)가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하고 (2) \\({\\cal B}\\)의 임의의 두 원소가 \\({\\cal B}\\)의 언카운터블 유니온으로 표현가능하기만 하면 볼들이 집합이 아니라 어떠한 \\({\\cal B}\\)라도 특정 토폴로지의 base라고 주장할 수 있다. 이것이 교재의 Thm 6.1 이다.\n- 아무튼 위의 과정을 거치면 \\(X\\)위에서 거리를 정의할 수 있을때 그 거리에 의해서 ball을 정의할 수 있고 ball들의 콜렉션을 base \\({\\cal B}\\)로 정의하고 \\({\\cal B}\\) 원소들의 언카운터블-유니온으로 표현가능한 집합모임을 토폴로지 \\({\\cal T}\\)로 정의해도 논리적모순점이 없다. 즉 \\(X\\)에서 메트릭이 정의되기만 하면 그것에 의해서 순차적으로 토폴로지 \\({\\cal T}\\)를 자연스럽게 유도할 수 있는데 이러한 토폴로지를 특별히 \\(X\\)와 \\(d\\)에 의해서 유도된 metric topology 라고 한다. 그리고 \\((X,d)\\)를 metric-space 라고 한다.\n- \\(\\mathbb{R}\\)에서 \\({\\cal O}\\)를 유도하는 메트릭은 우리가 보통 생각하는 유클리드거리이다. 이러한 메트릭을 usual metric 이라고 한다.\n- \\(\\mathbb{R}\\)에서 아래와 같은 거리를 정의할 수 있다.\n\\[d(a,b)=\\begin{cases}\n0 & a=b \\\\\n1 & a\\neq b\n\\end{cases}\\]\n이러한 거리를 trivial metric 이라고 한다. 그리고 이 거리가 유도하는 토폴로지는 \\(2^{\\mathbb{R}}\\) 이다. (아 몰라.. 따지기 싫어.. 그냥 외워..)\n- 만약에 집합 \\(X\\)에서 정의된 2개의 메트릭 \\(d_1\\), \\(d_2\\)가 같은 토폴로지를 유도한다면 두 메트릭 \\(d_1\\)과 \\(d_2\\)는 equivalent 하다고 말한다.\n- 토폴로지컬-스페이스 \\((X,{\\cal T})\\) 가 있다고 하자. 그런데 \\(X\\) 에서 어떠한 메트릭 \\(d\\)가 존재해 그것이 \\({\\cal T}\\)를 유도하였다고 하자. 그럼 \\({\\cal T}\\)는 메트릭-토폴로지가 된다. 이와 같이 (1) \\(X\\)에서 정의되고 (2) 메트릭-토폴로지 \\({\\cal T}\\)를 유도하는 적당한 메트릭 \\(d\\)가 명시된것은 아니지만 그런 메트릭의 존재를 하나 이상 우리가 알고 있을때 위상공간 \\((X,{\\cal T})\\)를 metrizable 하다고 한다.\n- 두 메트릭스페이스 \\((X,d_1)\\) 와 \\((Y,d_2)\\) 가 isometric 하다는 것은 아래가 만족하는 one-one, onto 인 \\(f:X \\to Y\\) 가 존재한다는 것이다.\n\\[d_1(p,q) = d_2(f(p),f(q))\\]\n- 이때 isometric 이라는 relation 은 보는것 처럼 모든 메트릭공간들의 집합 \\({\\cal M}\\)에서 equivalence relation 이다. 즉 아래가 성립한다.\n(i) \\((X,d_1) \\overset{ism}{\\sim} (X,d_1)\\),\n(ii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) implies \\((Y,d_2) \\overset{ism}{\\sim} (X,d_1)\\),\n(iii) \\((X,d_1) \\overset{ism}{\\sim} (Y,d_2)\\) and \\((Y,d_2) \\overset{ism}{\\sim} (Z,d_3)\\) imply \\((X,d_1) \\overset{ism}{\\sim} (Z,d_3)\\).\n\n\nChap 9: Countability\n- (Lipschutz (1965), p. 132) 위상공간 \\((X,{\\cal T})\\)가 separable 하다는 의미는 \\((X,{\\cal T})\\) countable dense subset 을 가진다는 의미이다.\n- 위상공간 \\((\\mathbb{R}, {\\cal O})\\)를 상상하자.\n\n\\(A_n = [-n,n] \\cap \\mathbb{Q}\\)\n\n라고 한다면 (1) \\({\\cal A} = \\{A_n\\}\\) 은 countable 하고 (2) \\(A_n\\)은 모두 (\\([-n,n]\\)에서) dense 하다. 따라서 \\(\\mathbb{R}\\)은 countable한 dense subset을 가진다. 따라서 \\((\\mathbb{R},{\\cal O})\\)는 seperable 하다.\n\n\nChap 10: Separation Axioms\n- 예비학습1: 위상공간 \\((X,{\\cal T})\\)를 고려하자. \\(X\\)의 임의의 닫힌집합 \\(F\\)를 상상하자. 그리고 \\(F\\)에 소속되지 않은 한 점 \\(p \\in X\\)를 상상하자. 이제 닫힌집합 \\(F\\)를 포함하는 아주 작은 열린집합 \\(G\\)와 \\(p\\)를 포함하는 아주 작은 열린집합 \\(H\\)를 상상하자. 위상공간 \\((X,{\\cal T})\\)에서 임의의 \\(F\\)와 \\(p\\)에 대하여서도 두 열린집합 \\(G\\),\\(H\\)가 서로소가 되도록 선택할 수 있다면 그 위상공간은 regular 하다고 표현한다 (Lipschutz (1965), p. 140).\n이해를 위한 예시\n\n\\(F= [0,1]\\)\n\\(p = 1.01\\)\n\\(G= (-0.001, 1.001)\\)\n\\(H= (1.009,1.011)\\)\n\n- 예비학습2: 위상공간 \\((X,{\\cal T})\\)를 고려하자. \\(X\\)의 부분집합중 서로소인 닫힌집합 \\(F_1,F_2\\)를 상상하자.이제 닫힌집합 \\(F_1\\)를 포함하는 아주 작은 열린집합 \\(G\\)와 \\(F_2\\)를 포함하는 아주 작은 열린집합 \\(H\\)를 상상하자. 위상공간 \\((X,{\\cal T})\\)에서 임의의 \\(F_1\\)와 \\(F_2\\)에 대하여서도 두 열린집합 \\(G\\),\\(H\\)가 서로소가 되도록 선택할 수 있다면 그 위상공간은 normal 하다고 표현한다 (Lipschutz (1965), p. 141).\n5 \\({\\cal T}=\\{\\emptyset, X\\}\\)로 설정한다면 이 조건은 당연히 성립하지 않겠지?- 위상공간 \\((X,{\\cal T})\\) 에는 얼마나 많은 (혹은 다양한) 열린집합이 있을까? 위상공간 \\((X,{\\cal T}_1)\\)이 \\(T_1\\)-space라는 의미는 서로 다른 \\(a,b \\in X\\)에 대하여 \\(a\\)만 포함하는 열린집합 혹은 \\(b\\)만 포함하는 열린집합이 각각 존재한다는 의미이다.5 위상공간 \\((X,{\\cal T}_2)\\)이 \\(T_2\\)-space 혹은 Hausdorff space라는 의미는 \\(T_1\\)-space에서, \\(a\\)만 포함하는 열린집합과 \\(b\\)만 포함하는 열린집합이 서로 disjoint한 경우를 의미한다. 위상공간 \\((X,{\\cal T}_3)\\)이 \\(T_3\\)-space 라는 의미는 \\(T_1\\)이고 regular space 라는 의미이다. 위상공간 \\((X,{\\cal T}_4)\\)이 \\(T_4\\)-space 라는 의미는 \\(T_1\\)이고 normal space 라는 의미이다.\n- 포함관계: \\(T_1\\)-space는 \\(T_2\\)-space를 포함하고, \\(T_2\\)-space는 \\(T_3\\)-space를, \\(T_3\\)-space는 \\(T_4\\)-space를 포함한다 (Lipschutz (1965), p. 141 그림). 그리고 metric space는 \\(T_4\\) space에 포함된다.\n- \\(T_1\\) 위상공간에서는 “singleton = closed set” 이라고 주장할 수 있다. (Lipschutz (1965), Thm 10.1) \\(T_2\\) 위상공간에서는 “convergent sequence has a unique limit” 을 주장할 수 있다 (Lipschutz (1965), Thm 10.3). \\(T_3\\) 위상공간은 특별히 기억할만한 부분이 없어보인다. \\(T_4\\) 위상공간은 우리손레마 (urysohn lemma) 가 성립하는 공간이다 (Lipschutz (1965), Thm 10.7).\n- 우리손레마: \\(T_4\\) 위상공간 \\((X,{\\cal T}_4)\\)를 상상하자. 그리고 \\(X\\)의 부분집합 중 서로소인 닫힌 부분집합 \\(F_1,F_2\\)를 상상하자. 우리손레마에 의하면 어떠한 \\(F_1,F_2\\)에 대하여서도,\n\n\\(f(F_1)=\\{0\\}\\)\n\\(f(F_1)=\\{1\\}\\)\n\n를 만족하는 적당한 연속함수 \\(f:X \\to [0,1]\\)이 항상 존재함이 알려져 있다 (Lipschutz (1965), Thm 10.7). 이것은 \\(F_1,F_2\\)를 구분할 수 있는 어떠한 함수 \\(f\\)가 항상 존재함을 의미하는데 이는 \\((X,{\\cal T}_4)\\)를 거리공간화 하는데 이용할 수 있다. 구체적으로 위상공간 \\((X,{\\cal T}_4)\\)가 추가적으로 second coutable 조건을 만족한다면6 \\((X,{\\cal T}_4)\\) 는 거리공간으로 바꿀 수 있음이 알려져 있다 (Lipschutz (1965), Thm 10.8).\n\nLipschutz, Seymour. 1965. “Schaum’s Outline of Theory and Problems of General Topology.” (No Title).\n6 즉 \\((X,{\\cal T}_4)\\)가 countable한 base를 가진다면- 위상공간 \\((X,{\\cal T})\\)가 \\(T_4\\)-space이고 추가적으로 second coutable space 라면 \\((X,{\\cal T})\\)는 Hilbert cube 와 호모몰픽(homeomorphic) 하다는 것이 알려져 있다 (Lipschutz (1965), p. 142).\n\n\nChap 14: Complete Metric Spaces\n- Convergent sequence 은 단독으로 정의될 수 없으며 위상공간 \\((X,{\\cal T})\\) 와 묶어서 정의된다. 그리고 Cauchy sequence 역시 단독으로 정의될 수 없으며 메트릭스페이스 \\((X,d)\\) 와 묶어서 정의된다.\n- Convergent sequence 와 Cauchy sequence 는 비슷해보이지만 미묘하게 다른점이 있다.\n(1) 컨버전트-시컨트는 위상공간 \\((X,{\\cal T})\\) 만 있으면 정의할 수 있지만 코시수열은 그 위상공간이 메트릭스페이스 이어야 한다는 제약이 있다. 왜냐하면 컨버전트-시컨스의 정의에는 오픈셋만 필요하지만 코시수열은 볼이 필요하고 볼은 메트릭에 의해서만 정의되기 때문이다.\n(2) 컨버전트-시컨스와 코시수열 모두 열의 각 항이 \\(X\\)의 원소이어야 한다는 조건이 있다. 하지만 컨버전트-시컨스는 그 limit 까지 \\(X\\)의 원소이어야 하는데 코시수열은 그렇지 않다는 차이점이 있다.\n- \\(X=(0,1)\\) 위의 usual metric 에 의해서 유도되는 메트릭스페이스 \\((X,d)\\) 를 생각하자. 수열\n\\[\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{4},\\dots,\\right\\}\\]\n\\(X\\)에서 정의된 코시수열이지만 \\(X\\)에서 정의되는 컨버전트-시컨스는 아니다.\n- 내가 이해한 바는 아래와 같다.\n(1) 토폴로지 \\((X,{\\cal T})\\) 는 항상 컨버전트-시컨스를 정의할 준비가 되어있는 공간이다.\n(2) 위에서 정의가능한 컨버전트-시컨스는 코시수열과 아무런 관련이 없다. 그리고 우리가 통상적으로 고등학교때부터 다루어왔던 수열의 수렴의 개념과도 거리가 멀다.\n(3) 토폴로지 \\((X,{\\cal T})\\) 가 메트릭스페이스라면 컨버전트-시컨스는 코시수열과 어떤관계가 있으며 고등학교때부터 내가 다루어 왔던 상식적인 수렴하는 수열의 개념과도 관련이 있다.\n(4) \\((X,{\\cal T})\\) 가 메트릭스페이스 라고 가정하자. 그럼 아래가 만족한다고 생각할 수 있다.\n\n\\(\\{a_n\\}\\) converges on \\(X\\) \\(\\Longleftrightarrow\\) \\(\\{a_b\\}\\) is Cauchy sequence on \\(X\\) and \\(\\lim_{n\\to\\infty} a_n \\in X\\)\n\n즉 러프하게 말해서 \\(X\\)에서의 컨버전트-시컨스는 (i) \\(X\\)에서의 코시수열이면서 (ii) limit 이 \\(X\\)에 포함되는 수열이라고 말할 수 있다. 이런 정의로 치면 우리가 고등학교때부터 생각해왔던 소박한 정의의 수렴하는 수열은 사실 코시수열에 가깝고 컨버전트-시컨스는 고등학교때부터 배운 소박한 수렴을 하며 동시에 수렴값이 \\(X\\)에서 잘 정의되는 수열을 의미한다고 볼 수 있다. 앞으로는 소박한 수렴과 컨버전트-시컨스를 엄밀하게 구분하여 말하도록 하자. 즉 \\(\\{a_n\\}\\)이 코시수열이라는 말은 \\(\\{a_n\\}\\)이 소박한 수렴을 한다는 의미이고 \\(\\{a_n\\}\\)이 컨버전트-시컨스라는 의미는 \\(\\{a_n\\}\\)이 소박한수렴을 하며 동시에 그 극한값이 well-define 된다는 의미(=\\(\\{a_n\\}\\)의 수렴값이 \\(X\\)의 원소라는 의미)이다.\n- (proposition 14.1) 메트릭스페이스 한정으로, 컨버전트-시컨스는 모두 코시수열이다. (당연한 소리를.. 이런걸 proposition 이라고..)\n- 당연히 위 정리의 역은 성립하지 않는다. 즉 메트릭스페이스 \\((X,{\\cal T})\\) 에서 정의된 코시수열이 반드시 컨버전트-시컨스라는 보장은 없다. (이것도 당연한 소리.. 왜냐하면 수렴값이 \\(X\\)에 포함된다는 보장이 없기 때문) 하지만 그 메트릭스페이스가 complete 하다면 위 정리의 역도 성립한다.\n- 컴플리트하지 않은 메트릭스페이스 \\((X,d)\\)를 컴플리트한 메트릭스페이스 \\((X^* , d)\\) 로 바꿀 수 없을까? 유주얼메트릭(usual metric) \\(d\\) 와 \\(X=(0,1)\\) 로 만들어지는 메트릭스페이스는 컴플리트하지 않지만 \\(d\\) 와 \\(X^* =[0,1]\\) 로 만들어지는 메트릭스페이스는 컴플리트하다. 이런 경우 $(X^* ,d) $ 는 \\((X,d)\\) 의 completion 이라고 한다.\n- 즉 아래의 조건들을 만족하면 공간 $(X^* ,d) $ 는 공간 \\((X,d)\\) 의 completion 이라고 부른다.\n(1) \\(X\\subset X^*\\)\n(2) \\((X^* ,d)\\) is complete metric space\n(3) \\((X,d) \\overset{ism}{\\sim} (X^* ,d)\\).\n\n- 메트릭스페이스 \\((X,d)\\)에서 아래의 식을 만족하는 두 코시수열 \\(\\{a_n\\}\\), \\(\\{\\tilde a_n\\}\\) 을 생각하여보자.\n\\[\\lim_{n\\to\\infty} d(a_n,\\tilde a_n)=0 \\]\n이러한 코시수열들을\n\\[\\{a_n\\} \\overset{slim}{\\sim} \\{\\tilde a_n\\}\\]\n이라고 표현하자. 이때 관계 \\(\\overset{slim}{\\sim}\\) 은 \\(X\\)에서 정의가능한 모든 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 equivalence relation 이 된다고 한다. (증명은 알아서) 따라서 이걸 이용하면 거리공간에서 \\(slim\\) 의 관계를 가지는 임의의 두 수열은 같은 극한을 가진다는 결론이 나온다. (이것도 잘 따져보자.)\n- 잠시 (1) 바이너리-릴레이션(binary relation), (2) 이퀴배런스-릴레이션(equivalence relation), (3) 이퀴배런스-클래스(equivalence class) 그리고 (4) 코션트셋(quotient set)에 대하여 설명하고 넘어가겠다.\n(1) 집합 \\({\\cal C}_ X\\) 의 두 원소 \\(\\{a_n\\}\\), \\(\\{b_n\\}\\) 간 바이너리-릴레이션 \\(R\\)이 존재한다는 문장은 집합론적인 언어로 표현가능하다. 구체적으로는 \\(R\\)을 곱집합 \\({\\cal C}_ X \\times {\\cal C}_ X\\) 의 적당한 부분집합으로 설정하고 순서쌍 \\(\\big(\\{a_n\\},\\{b_n\\}\\big)\\) 이 \\(R\\) 의 원소라는 식으로 표현한다. 예를 들면 아래와 같은 식으로 말이다.\n$ {a_n} and {b_n} has arelation with~ R \\ ({a_n},{b_n}) R _ X _ X \\ {a_n} {b_n}$\n(2) 그리고 \\({\\cal C}_ X\\) 위에서의 바이너리-릴레이션 \\(R\\)이 (i) reflexivity (ii) symmetricity (iii) transitivity 를 만족하면 이 릴레이션을 특별히 이퀴배런스-릴레이션 이라고 말한다.\n(3) 그리고 아래와 같이 \\({\\cal C}_ X\\) 에서 \\(\\{a_n\\}\\) 과 이퀴배런스-릴레이션을 가지는 원소들을 모아놓은 집합을 생각할 수 있다. \\[\\big[\\{a_n\\}\\big]_ R:=\\big\\{ \\{x_n\\} : \\{x_n\\} \\overset{R}{\\sim} \\{a_n\\} ~and~ \\{x_n\\} \\in {\\cal C}_ X \\big\\}\\]\n이 집합을 \\(\\{a_n\\}\\)의 equivalence class on \\({\\cal C}_ X\\) by \\(R\\) 이라고 부른다. 보통은 \\(R\\)을 생략하여 \\(\\big[\\{a_n\\}\\big]\\)와 같이만 표현하지만 나는 기호의 명확성을 위해서 관계까지 명시하였다.\n(4) 이퀴배런스-클래스는 본질적으로 파티션과 밀접한 연관이 있다. 여기에서 클래스 \\({\\cal P}_ A\\) 가 집합 \\(A\\)의 파티션이란 의미는 클래스 \\({\\cal P}_ A\\) 에 속한 모든 원소의 합이 \\(A\\) 이며 클래스 \\({\\cal P}_ A\\) 의 각 원소는 서로 배타적이라는 의미이다. 이퀴배런스-클래스가 그럼 왜 파티션과 관련이 있을까? 그것은 어떠한 집합에서 이퀴배런스-릴레이션이 존재하면 그 집합을 배타적인 이퀴배런스-클래스의 합집합으로 표현가능하기 때문이다. 즉 이퀴배런스-릴레이션 혹은 이퀴배런-클래스의 존재는 파티션의 존재를 임플라이 한다. 그리고 이러한 파티션을 이퀴배런스-릴레이션 \\(R\\)에 의해 생성된 quotient set 혹은 quotient space 라고 한다. 관계 \\(R\\)에 의한 \\(A\\)의 코션트 셋은 기호로 \\(A ~\\overset{R}{\\sim}\\) 와 같이 쓴다. 예를들어 \\[\\begin{align}\n{\\cal C}_ X ~ /\\overset{slim}{\\sim}\n\\end{align}\\] 은 집합 \\(X\\) 상에서 존재하는 코시수열들의 집합 \\({\\cal C}_ X\\) 에서 이퀴배런스-릴레이션 \\(slim\\) 에 의해서 생성된 코션트셋을 의미한다."
  },
  {
    "objectID": "posts/1_Essays/2023-08-16-루나랜더 v2 (간략화).html",
    "href": "posts/1_Essays/2023-08-16-루나랜더 v2 (간략화).html",
    "title": "[Essays] 루나랜더 v2 (간략화)",
    "section": "",
    "text": "imports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch \nimport collections\nimport IPython\nimport random\n\n\n\n클래스선언\n\nclass Agent():\n    def __init__(self,env):\n        self.buffer_size = 10000 \n        self.batch_size = 128\n        self.state_size = env.observation_space.shape[0]\n        self.action_size = env.action_space.n\n        self.eps = 0\n        self.n_experiences = 0\n        self.n_epoch = 0\n        self.scores = [] \n        self.playtimes = []\n\n        # Q-Network\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(self.state_size,128), \n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,self.action_size)) \n        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n\n        # ReplayBuffer\n        self.memory = collections.deque(maxlen=self.buffer_size)\n        self.current_states = collections.deque(maxlen=self.buffer_size)\n        self.actions = collections.deque(maxlen=self.buffer_size)\n        self.rewards = collections.deque(maxlen=self.buffer_size)\n        self.next_states = collections.deque(maxlen=self.buffer_size)\n        self.terminations = collections.deque(maxlen=self.buffer_size)\n\n    # agent &gt;&gt; env \n    def __rshift__(self,env):\n        if np.random.rand() &gt; self.eps:\n            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n        else:\n            self.action = random.choice(np.arange(self.action_size))\n        env.received_action = self.action \n\n    # agent &lt;&lt; env \n    def __lshift__(self,env): \n        self.next_state, self.reward, self.terminated, _, _ = env.step(env.received_action)\n        \n    def save_experience(self):\n        \"\"\"Add a new experience to memory.\"\"\"\n        self.current_states.append(self.current_state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences = len(self.current_states)\n    \n    def get_batch(self):\n        idx = np.random.randint(0,self.n_experiences,size=self.batch_size) \n        self.current_states_batch = torch.tensor(np.array(self.current_states)[idx],dtype=torch.float32)\n        self.actions_batch = torch.tensor(np.array(self.actions)[idx],dtype=torch.int64).reshape(self.batch_size,1) \n        self.rewards_batch = torch.tensor(np.array(self.rewards)[idx],dtype=torch.float32).reshape(self.batch_size,-1) \n        self.next_states_batch = torch.tensor(np.array(self.next_states)[idx],dtype=torch.float32)\n        self.terminations_batch = torch.tensor(np.array(self.terminations)[idx],dtype=torch.int64).reshape(self.batch_size,-1) \n\n    def learn(self):\n        if self.n_experiences &lt; self.batch_size:\n            pass\n        else: \n            self.get_batch()\n            \n            q_targets_next = self.q_net(self.next_states_batch).detach().max(1)[0].unsqueeze(1)\n            q_targets = self.rewards_batch + 0.99 * q_targets_next * (1 - self.terminations_batch)\n            q_expected = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n            \n            loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n    \n    # agent**env: reset \n    def __pow__(self,env): \n        self.current_state, _ = env.reset()\n        self.terminated = False\n        self.score = 0 \n\n\n\n초기화\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\n\n\nagent = Agent(env)\n\n\n\nrun\n\nn_episodes = 2000\nmax_t = 500\nagent.eps = 1.0 \nfor e in range(2000):\n    ### 1. 본질적인 코드\n    agent ** env \n    for t in range(500):\n        # step1: \n        agent &gt;&gt; env \n        # step2: \n        agent &lt;&lt; env \n        # step3: 데이터저장 및 학습\n        agent.save_experience()\n        agent.learn()\n        # step4: 다음 iteration 준비 + 종료조건체크\n        agent.current_state = agent.next_state \n        agent.score += agent.reward\n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.eps = agent.eps * 0.995 \n    ### 2. 비본질적 코드\n    print('\\rEpisode {}\\tAverage Score: {:.2f}\\t'.format(e, np.mean(agent.scores[-100:])), end=\"\")\n    if e % 100 == 0:\n        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(agent.scores[-100:])))\n        torch.save(agent.q_net.state_dict(), 'checkpoint.pth')\n    if np.mean(agent.scores[-100:])&gt;=200.0:\n        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(agent.scores[-100:])))\n        torch.save(agent.q_net.state_dict(), 'checkpoint.pth')\n        break\n\nEpisode 0   Average Score: -111.70  \nEpisode 100 Average Score: -133.68  \nEpisode 200 Average Score: -30.75       \nEpisode 300 Average Score: 86.69        \nEpisode 400 Average Score: 184.71   \nEpisode 414 Average Score: 200.22   \nEnvironment solved in 414 episodes! Average Score: 200.22\n\n\n\nplt.plot(agent.scores)\nplt.ylabel('Score')\nplt.xlabel('Episode #')\n\nText(0.5, 0, 'Episode #')\n\n\n\n\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\nagent = Agent(env)\nagent ** env\nagent.q_net.load_state_dict(torch.load('checkpoint.pth'))\nagent.terminated = False\nfigs = [] \n\nwhile not agent.terminated:\n    figs.append(env.render())\n    agent &gt;&gt; env \n    agent &lt;&lt; env \n    agent.current_state = agent.next_state \nenv.close()\n\n\nfig, ax = plt.subplots()\nani = FuncAnimation(fig,lambda i: ax.imshow(figs[::10][i]),frames=len(figs[::10]))\n\n\n\n\n- 방법1\n\n#ani.save('LunarLander-v2.mp4', writer='ffmpeg', fps=15, extra_args=['-vcodec', 'mpeg4'])\n\n- 방법2\n\nIPython.display.HTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#imports",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#imports",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "imports",
    "text": "imports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport IPython"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#예비학습-시각화",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#예비학습-시각화",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "예비학습: 시각화",
    "text": "예비학습: 시각화\n\ndef show(states):\n    fig = plt.Figure()\n    ax = fig.subplots()\n    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n    sc = ax.scatter(0, 0, color='red', s=500)  \n    ax.text(0, 0, 'start', ha='center', va='center')\n    ax.text(3, 3, 'end', ha='center', va='center')\n    # Adding grid lines to the plot\n    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n    def update(t):\n        sc.set_offsets(states[t])\n    ani = FuncAnimation(fig,update,frames=len(states))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\nstates = [[0,0],[0,1],[1,1],[1,2],[1,3]]\n\n\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#env-클래스-구현",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#env-클래스-구현",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "Env 클래스 구현",
    "text": "Env 클래스 구현\n- GridWorld: 강화학습에서 많이 사용되는 기본적인 시뮬레이션 환경\n\nState: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중 하나에 있을 수 있음.\nAction: 에이전트는 상태에서 다른 상태로 이동하기 위해 상, 하, 좌, 우로 이동하는 행동을 할 수 있음.\nReward: 에이전트가 특정 행동을 취할 때 환경에서 보상이 주어짐.\nTerminal State: 일반적으로 하나 또는 그 이상의 종료 상태가 있으며, 에이전트가 이 상태에 도달하면 에피소드가 종료됨.\n\n\nclass GridWorld:\n    def __init__(self):\n        self.reset()\n        self._action_to_direction = {\n            0: np.array([1, 0]), # x+ \n            1: np.array([0, 1]), # y+\n            2: np.array([-1, 0]), # x-  \n            3: np.array([0, -1]), # y-\n        }\n        self.state_space = gym.spaces.MultiDiscrete([4, 4])\n        self.action_space = gym.spaces.Discrete(4)\n    def reset(self):\n        self.agent_action = None\n        self.agent_state = np.array([0, 0])\n        return self.agent_state\n    def step(self,action):\n        direction = self._action_to_direction[action]\n        self.agent_state = self.agent_state + direction\n        # 목표지점에 도달 \n        if np.array_equal(np.array([3,3]), self.agent_state):\n            reward = 100\n            terminated = True\n        else:\n            reward = -1\n            terminated = False\n        # 4*4밖에 있을 경우 \n        if self.agent_state not in self.state_space:\n            reward = -10\n            terminated = True\n            self.agent_state = self.agent_state - 1/2*direction\n        return self.agent_state, reward, terminated\n\n\nenv = GridWorld()\nenv.reset()\nstates = []\nrewards = [] \nterminations = [] \nfor t in range(500):\n    action = env.action_space.sample()\n    state, reward, terminated = env.step(action)\n    states.append(state)\n    rewards.append(reward)\n    terminations.append(terminated)\n    if terminated: \n        break \n\n\n[np.array([0,0])]+states\n\n[array([0, 0]),\n array([0, 1]),\n array([0, 2]),\n array([0, 1]),\n array([0, 0]),\n array([1, 0]),\n array([0, 0]),\n array([1, 0]),\n array([2, 0]),\n array([1, 0]),\n array([ 1. , -0.5])]\n\n\n\nshow([np.array([0,0])]+states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent1-클래스-구현-run",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent1-클래스-구현-run",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "Agent1 클래스 구현 + Run",
    "text": "Agent1 클래스 구현 + Run\n- 첫번째 시도\n\n# learn 추가\nclass Agent1:\n    def __init__(self,env):\n        self.action_space = env.action_space\n        self.state_space = env.state_space\n        self.n_experiences = 0\n        self.n_episode = 0  \n        \n        ## episode-wise info \n        self.scores = [] \n        self.playtimes = [] \n\n        ## time-wise info\n        self.current_state = None \n        self.action = None\n        self.reward = None        \n        self.next_state = None \n        self.socre = 0\n        \n        ## ReplayBuffer\n        self.actions = []\n        self.rewards = []\n        self.current_states = []\n        self.next_states = [] \n        self.terminations = []\n        \n    def act(self):\n        self.action = self.action_space.sample()\n        \n    def save_experience(self): \n        self.actions.append(self.action)\n        self.current_states.append(self.current_state)\n        self.next_states.append(self.next_state)\n        self.rewards.append(self.reward)\n        self.terminations.append(self.terminated)\n        self.n_experiences += 1\n        self.score += self.reward\n\n    def learn(self):\n        pass\n\n\nenv = GridWorld()\nagent = Agent1(env)\nfor _ in range(20):\n    ### 1. 본질적인 코드\n    agent.current_state = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act()\n        env.agent_action = agent.action\n        # step2: env &lt;&lt; agent \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience()\n        # step3: 학습\n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: 종료조건 체크\n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    ## 2. 비본질적 코드\n    print(\n        f'Episode {agent.n_episode}\\t'\n        f'Score: {agent.scores[-1]}\\t'\n        f'Playtime: {agent.playtimes[-1]}'\n    )\n\nEpisode 1   Score: -16  Playtime: 7\nEpisode 2   Score: -12  Playtime: 3\nEpisode 3   Score: -10  Playtime: 1\nEpisode 4   Score: -11  Playtime: 2\nEpisode 5   Score: -10  Playtime: 1\nEpisode 6   Score: -11  Playtime: 2\nEpisode 7   Score: -10  Playtime: 1\nEpisode 8   Score: -19  Playtime: 10\nEpisode 9   Score: -10  Playtime: 1\nEpisode 10  Score: -12  Playtime: 3\nEpisode 11  Score: -22  Playtime: 13\nEpisode 12  Score: -10  Playtime: 1\nEpisode 13  Score: -10  Playtime: 1\nEpisode 14  Score: -12  Playtime: 3\nEpisode 15  Score: -11  Playtime: 2\nEpisode 16  Score: -10  Playtime: 1\nEpisode 17  Score: -16  Playtime: 7\nEpisode 18  Score: -10  Playtime: 1\nEpisode 19  Score: -20  Playtime: 11\nEpisode 20  Score: -10  Playtime: 1\n\n\n- 어떻게 학습을 할까? 즉 어떻게 “환경의 이해 \\(\\to\\) 행동의 결정” 의 과정을 수행할까?\n\n어떠한 상태에서, 어떠한 행동을 했을때, 어떠한 보상과 어떠한 다음상태를 받았는지 기록하자.\n1을 바탕으로 다음행동을 어떻게 할지 판단하자.\n\n\nstates = [np.array([0,0])]+agent.next_states[-5:-1]\nstates\n\n[array([0, 0]),\n array([0, 2]),\n array([0, 1]),\n array([0, 2]),\n array([-0.5,  2. ])]\n\n\n\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#환경의-이해-1차원적-이해",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#환경의-이해-1차원적-이해",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "환경의 이해 (1차원적 이해)",
    "text": "환경의 이해 (1차원적 이해)\n- 무작위로 10000판을 진행하여 보자.\n\nenv = GridWorld()\nagent = Agent1(env)\nfor _ in range(10000):\n    ### 1. 본질적인 코드\n    agent.current_state = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act()\n        env.agent_action = agent.action\n        # step2: agent &lt;&lt; env\n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience()\n        # step3: 학습\n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: 종료조건 체크\n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n\n\nagent.n_experiences\n\n32226\n\n\n- 데이터관찰\n\nagent.current_states[0], agent.actions[0], agent.rewards[0]\n\n(array([0, 0]), 0, -1)\n\n\n\nagent.current_states[1], agent.actions[1], agent.rewards[1]\n\n(array([1, 0]), 1, -1)\n\n\n\nagent.current_states[2], agent.actions[2], agent.rewards[2]\n\n(array([1, 1]), 0, -1)\n\n\n\nagent.current_states[3], agent.actions[3], agent.rewards[3]\n\n(array([2, 1]), 2, -1)\n\n\n- 환경을 이해하기 위한 기록 (1)\n\nq = np.zeros([4,4,4])\ncount = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    a = agent.actions[i]\n    q[x,y,a] = q[x,y,a] + agent.rewards[i]\n    count[x,y,a] = count[x,y,a] + 1 \n\n\ncount[count==0] = 0.1\ncount\n\narray([[[4.311e+03, 3.950e+02, 2.840e+02, 2.690e+02],\n        [3.750e+02, 9.500e+01, 8.400e+01, 9.100e+01],\n        [1.890e+02, 4.200e+01, 4.300e+01, 3.500e+01],\n        [1.500e+01, 1.500e+01, 1.400e+01, 7.500e+01]],\n\n       [[1.540e+02, 3.997e+03, 1.680e+02, 1.920e+02],\n        [6.510e+02, 3.634e+03, 2.150e+02, 1.660e+02],\n        [2.599e+03, 1.213e+03, 1.390e+02, 1.420e+02],\n        [9.960e+02, 7.700e+01, 7.700e+01, 1.710e+02]],\n\n       [[1.990e+02, 3.800e+01, 3.400e+01, 4.600e+01],\n        [1.740e+02, 5.000e+02, 1.520e+02, 1.350e+02],\n        [3.130e+02, 2.847e+03, 9.900e+01, 1.420e+02],\n        [3.420e+03, 7.500e+01, 9.300e+01, 2.550e+02]],\n\n       [[4.000e+01, 1.590e+02, 2.800e+01, 2.900e+01],\n        [5.800e+01, 2.290e+02, 1.300e+02, 5.700e+01],\n        [5.900e+01, 2.930e+02, 4.700e+01, 1.430e+02],\n        [1.000e-01, 1.000e-01, 1.000e-01, 1.000e-01]]])\n\n\n\nq = (q/count).round(2)\nq\n\narray([[[ -1.,  -1., -10., -10.],\n        [ -1.,  -1., -10.,  -1.],\n        [ -1.,  -1., -10.,  -1.],\n        [ -1., -10., -10.,  -1.]],\n\n       [[ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1., -10.,  -1.,  -1.]],\n\n       [[ -1.,  -1.,  -1., -10.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [ -1.,  -1.,  -1.,  -1.],\n        [100., -10.,  -1.,  -1.]],\n\n       [[-10.,  -1.,  -1., -10.],\n        [-10.,  -1.,  -1.,  -1.],\n        [-10., 100.,  -1.,  -1.],\n        [  0.,   0.,   0.,   0.]]])\n\n\n\nfor i in range(4):\n    print(f\"action = {i}\\n\"\n          f\"action-value function =\\n {q[:,:,i]}\\n\"\n          )\n\naction = 0\naction-value function =\n [[ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1. 100.]\n [-10. -10. -10.   0.]]\n\naction = 1\naction-value function =\n [[ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1. 100.   0.]]\n\naction = 2\naction-value function =\n [[-10. -10. -10. -10.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.   0.]]\n\naction = 3\naction-value function =\n [[-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.   0.]]\n\n\n\n- 환경을 이해하기 위한 기록 (2) – 이렇게하면 count를 따로 기록할 필요 없음\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a] # 풀이한 답\n    q_realistic = agent.rewards[i] # 실제 답\n    diff = q_realistic - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값이라고 하자\n    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백 * 학습률\n\n\nfor i in range(4):\n    print(\n        f\"action = {i}\\n\"\n        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n    )\n\naction = 0\naction-value function =\n [[ -1.   -1.   -1.   -1. ]\n [ -1.   -1.   -1.   -1. ]\n [ -1.   -1.   -1.   98.8]\n [-10.  -10.   -9.8   0. ]]\n\naction = 1\naction-value function =\n [[ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1.  98.   0.]]\n\naction = 2\naction-value function =\n [[-10. -10. -10. -10.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.   0.]]\n\naction = 3\naction-value function =\n [[-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.   0.]]"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#환경의-깊은-이해-좀-더-고차원적인-이해",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#환경의-깊은-이해-좀-더-고차원적인-이해",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "환경의 깊은 이해 (좀 더 고차원적인 이해)",
    "text": "환경의 깊은 이해 (좀 더 고차원적인 이해)\n- action=1 일때 각 state의 가치 (=기대보상)\n\nq[:,:,1]\n\narray([[90.17518032, 92.09062756, 93.7852859 , -9.99999742],\n       [92.09728427, 94.03932857, 95.98886378, -9.99999402],\n       [94.03773517, 96.00052959, 97.98357145, -9.99984316],\n       [95.80256645, 97.89107976, 99.96670104,  0.        ]])\n\n\n- 분석1\n\nq[3,2][1] \n\n97.97234528464163\n\n\n\n상태 (3,2)에서 행동 1을 하게 되면 100의 보상을 얻으므로 기대보상값은 100근처 -&gt; 합리적임\n\n- 분석2\n\nq[3,1][1] \n\n-0.9979836949402369\n\n\n\n상태 (3,1)에서 행동1을 하게 되면 -1의 보상을 얻으므로 기대보상값은 -1 근처 -&gt; 합리적일까?\n\n- 비판: 분석2는 합리적인듯 하지만 data를 분석한뒤는 그다지 합리적이지 못함\n- 상황상상\n\n빈 종이를 줌\n빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 or 1)\n0을 쓸 때와 1을 쓸 때의 보상은 다름\n그런데 무수히 많은 데이터를 분석한 결과 0을 쓰면 0원을 보상으로 주고, 1을 쓰면 10만원을 보상으로 준다는 것을 “알게 되었음”\n빈 종이의 가치는 5만원인가? 아니면 10만원인가? –&gt; 10만원 아니야?\n\n- 직관: 생각해보니 현재 \\(s=(3,1)\\), \\(a=1\\)에서 추정된(esimated) 값은 q[3,1][1]이지만1, 현실적으로는 “실제보상(-1)과 잠재적보상(100)”을 동시에 고려해야하는게 합리적인듯\n1 즉 잠재적 보상은 고려되어있지 않음\nq_esimated = q[3,1][1]\nq_esimated\n\n-0.9979836949402369\n\n\n\nq_realistic = (-1) + 0.99 * (100) \nq_realistic\n\n98.0\n\n\n\n여기에서 0.99 는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를 결정하는 가중치” 이다.\n1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이 = 십만원 으로 생각한다는 의미)\n\n- 즉 \\(q(s,a)\\)는 모든 \\(s,a\\)에 대하여\n\\[q(s,a) \\approx \\text{reward}(s,a) + 0.99\\times \\max_{a} q(s',a)\\]\n가 성립한다면 \\(q(s,a)\\) 타당하게 추정된 것이라 볼 수 있다. 좀 더 정확하게는\n\\[q(s,a) \\approx \\begin{cases} \\text{reward}(s,a) & \\text{terminated}  \\\\ \\text{reward}(s,a)+ 0.99\\times \\max_{a} q(s',a) & \\text{not terminated} \\end{cases} \\]\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    xx,yy = agent.next_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a]\n    if agent.terminations[i]:\n        q_realistic = agent.rewards[i] \n    else:\n        q_future = q[xx,yy,:].max()\n        q_realistic = agent.rewards[i] + 0.99*q_future # 이걸 관측했다고 치는거임\n    diff = q_realistic - q_estimated \n    q[x,y,a] = q_estimated + 0.1 * diff\n\n\nfor i in range(4):\n    print(\n        f\"action = {i}\\n\"\n        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n    )\n\naction = 0\naction-value function =\n [[ 90.2  92.1  94.   95.8]\n [ 92.1  94.   96.   98. ]\n [ 93.8  95.9  98.  100. ]\n [-10.  -10.  -10.    0. ]]\n\naction = 1\naction-value function =\n [[ 90.2  92.1  93.8 -10. ]\n [ 92.1  94.   96.  -10. ]\n [ 94.   96.   98.  -10. ]\n [ 95.8  97.9 100.    0. ]]\n\naction = 2\naction-value function =\n [[-10.  -10.  -10.  -10. ]\n [ 88.3  90.2  92.1  93.2]\n [ 90.2  92.1  94.   95.4]\n [ 92.1  94.   96.    0. ]]\n\naction = 3\naction-value function =\n [[-10.   88.3  90.2  92.1]\n [-10.   90.2  92.1  94. ]\n [-10.   92.1  94.   95.9]\n [-10.   93.6  95.2   0. ]]"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#행동-전략-수립",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#행동-전략-수립",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "행동 전략 수립",
    "text": "행동 전략 수립\n- 상태 (0,0)에 있다고 가정해보자.\n\nq[0,0,:]\n\narray([ 90.1758421 ,  90.17518032, -10.        , -10.        ])\n\n\n\n행동 0 혹은 1을 하는게 유리함.\n\n- 상태 (2,3)에 있다고 가정해보자.\n\nq[2,3,:]\n\narray([99.98955043, -9.99984316, 95.41160224, 95.91986523])\n\n\n\n행동 0을 하는게 유리함.\n\n- 상태 (3,2)에 있다고 가정해보자.\n\nq[3,2,:] \n\narray([-9.9966701 , 99.96670104, 95.95262746, 95.21718289])\n\n\n\n행동 1을 하는게 유리함.\n\n- 각 상태에서 최적은 action은 아래와 같다.\n\nq[0,0,:].argmax()\n\n0\n\n\n\nq[2,3,:].argmax()\n\n0\n\n\n\nq[3,2,:].argmax()\n\n1\n\n\n- 전략(=정책)을 정리해보자.\n\npolicy = np.array(['?????']*16).reshape(4,4)\npolicy\n\narray([['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????']], dtype='&lt;U5')\n\n\n\ndirections = {0: 'down', 1: 'right', 2:'up', 3:'left'}\n\n\nfor i in range(4):\n    for j in range(4):\n        policy[i,j] = directions[q[i,j,:].argmax()]\npolicy\n\narray([['down', 'down', 'down', 'down'],\n       ['right', 'right', 'down', 'down'],\n       ['right', 'right', 'right', 'down'],\n       ['right', 'right', 'right', 'down']], dtype='&lt;U5')\n\n\n- 요약: 값이 큰 쪽으로 이동\n\nq.max(axis=-1)\n\narray([[90.1758421 , 92.09658562, 94.03281934, 95.77518033],\n       [92.09728427, 94.03932857, 96.0009127 , 97.97374788],\n       [94.03773517, 96.00052959, 97.98357145, 99.98955043],\n       [95.80256645, 97.89107976, 99.96670104,  0.        ]])"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent2-클래스-구현-run",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent2-클래스-구현-run",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "Agent2 클래스 구현 + Run",
    "text": "Agent2 클래스 구현 + Run\n\n# learn 추가\nclass Agent2(Agent1):\n    def __init__(self,env):\n        super().__init__(env)\n        self.q = np.zeros([4,4,4])\n    def act(self):\n        if self.n_experiences &lt; 3000: \n            self.action = self.action_space.sample()\n        else: \n            x,y = self.current_state\n            self.action = self.q[x,y,:].argmax()\n    def learn(self): # make q\n        x,y = self.current_state\n        xx,yy = self.next_state\n        a = self.action\n        q_estimated = self.q[x,y,a]\n        if agent.terminated:\n            q_realistic = self.reward\n        else:\n            q_future = q[xx,yy,:].max()\n            q_realistic = self.reward + 0.99*q_future\n        # q_observed 와 q_estimated를 점점 비슷하게 만들어주는 역할\n        diff = q_realistic - q_estimated\n        self.q[x,y,a] = q_estimated + 0.1 * diff \n\n\nenv = GridWorld()\nagent = Agent2(env)\nfor _ in range(2000):\n    ### 1. 본질적인 코드\n    agent.current_state = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act()\n        env.agent_action = agent.action\n        # step2: env &lt;&lt; agent \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience()\n        # step3: 데이터저장 및 학습\n        agent.learn()        \n        # step4: 다음 iteration 준비 + 종료조건체크\n        agent.current_state = agent.next_state \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    ## 2. 비본질적 코드\n    if (agent.n_episode % 100) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_experiences: {agent.n_experiences}'\n        )\n\nEpisode 100 Score: -8.30    Playtime:  3.70 n_experiences: 370\nEpisode 200 Score: -10.66   Playtime:  2.76 n_experiences: 646\nEpisode 300 Score: -12.63   Playtime:  3.63 n_experiences: 1009\nEpisode 400 Score: -6.53    Playtime:  3.03 n_experiences: 1312\nEpisode 500 Score: -10.20   Playtime:  3.40 n_experiences: 1652\nEpisode 600 Score: -11.89   Playtime:  2.89 n_experiences: 1941\nEpisode 700 Score: -12.92   Playtime:  3.92 n_experiences: 2333\nEpisode 800 Score: -10.03   Playtime:  3.23 n_experiences: 2656\nEpisode 900 Score: -10.12   Playtime:  3.32 n_experiences: 2988\nEpisode 1000    Score: -48.86   Playtime:  48.59    n_experiences: 7847\nEpisode 1100    Score: -50.00   Playtime:  50.00    n_experiences: 12847\nEpisode 1200    Score: -50.00   Playtime:  50.00    n_experiences: 17847\nEpisode 1300    Score: -50.00   Playtime:  50.00    n_experiences: 22847\nEpisode 1400    Score: -50.00   Playtime:  50.00    n_experiences: 27847\nEpisode 1500    Score: -50.00   Playtime:  50.00    n_experiences: 32847\nEpisode 1600    Score: -50.00   Playtime:  50.00    n_experiences: 37847\nEpisode 1700    Score: -50.00   Playtime:  50.00    n_experiences: 42847\nEpisode 1800    Score: -50.00   Playtime:  50.00    n_experiences: 47847\nEpisode 1900    Score: -50.00   Playtime:  50.00    n_experiences: 52847\nEpisode 2000    Score: -50.00   Playtime:  50.00    n_experiences: 57847\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent3-클래스-구현-run",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#agent3-클래스-구현-run",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "Agent3 클래스 구현 + Run",
    "text": "Agent3 클래스 구현 + Run\n\nclass Agent3(Agent2):\n    def __init__(self,env):\n        super().__init__(env)\n        self.eps = 0\n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else: \n            x,y = self.current_state\n            self.action = self.q[x,y,:].argmax()\n\n\nenv = GridWorld()\nagent = Agent3(env)\nagent.eps = 1 \nfor _ in range(5000):\n    ### 1. 본질적인 코드\n    agent.current_state = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act()\n        env.agent_action = agent.action\n        # step2: env &lt;&lt; agent \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience()\n        # step3: 데이터저장 및 학습\n        agent.learn()        \n        # step4: 다음 iteration 준비 + 종료조건체크\n        agent.current_state = agent.next_state \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps * 0.999\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 1000) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_experiences: {agent.eps}'\n        )\n\nEpisode 1000    Score:  35.77   Playtime:  5.83 n_experiences: 0.3676954247709635\nEpisode 2000    Score:  78.15   Playtime:  6.35 n_experiences: 0.1351999253974994\nEpisode 3000    Score:  85.34   Playtime:  5.76 n_experiences: 0.04971239399803625\nEpisode 4000    Score:  92.77   Playtime:  6.03 n_experiences: 0.018279019827489446\nEpisode 5000    Score:  91.76   Playtime:  5.94 n_experiences: 0.006721111959865607\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#최종-agent-클래스-구현-run",
    "href": "posts/1_Essays/2023-08-29-강화학습(2) -- 4x4 grid.html#최종-agent-클래스-구현-run",
    "title": "[Essays] 강화학습(2) – 4x4 grid",
    "section": "최종 Agent 클래스 구현 + Run",
    "text": "최종 Agent 클래스 구현 + Run\n\n# learn 추가\nclass Agent(Agent3):\n    # agent &gt;&gt; env \n    def __rshift__(self,env):\n        self.act()\n        env.agent_action = self.action\n    # agent &lt;&lt; env \n    def __lshift__(self,env): \n        self.next_state, self.reward, self.terminated = env.step(env.agent_action)\n        self.save_experience()\n\n\nenv = GridWorld()\nagent = Agent(env)\nagent.eps = 1 \nfor _ in range(5000):\n    ### 1. 본질적인 코드\n    agent.current_state = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(50):\n        ## step1 \n        agent &gt;&gt; env \n        ## step2 \n        agent &lt;&lt; env \n        ## step3 \n        agent.learn()    \n        ## step4 \n        agent.current_state = agent.next_state \n        ## step5 \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps*0.999\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 1000) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_eps: {agent.eps}'\n        )\n\nEpisode 1000    Score:  39.50   Playtime:  6.50 n_eps: 0.3676954247709635\nEpisode 2000    Score:  71.99   Playtime:  5.91 n_eps: 0.1351999253974994\nEpisode 3000    Score:  92.76   Playtime:  6.04 n_eps: 0.04971239399803625\nEpisode 4000    Score:  92.82   Playtime:  5.98 n_eps: 0.018279019827489446\nEpisode 5000    Score:  93.86   Playtime:  6.04 n_eps: 0.006721111959865607\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-08-23-강화학습(1) -- bandit.html",
    "href": "posts/1_Essays/2023-08-23-강화학습(1) -- bandit.html",
    "title": "[Essays] 강화학습(1) – bandit",
    "section": "",
    "text": "환경셋팅\n- 설치\n!pip install -q swig\n!pip install gymnasium\n!pip install gymnasium[box2d]\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nIntro\n- 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를 학습하는 과업\n- 딥마인드: breakout \\(\\to\\) 알파고\nhttps://www.youtube.com/watch?v=TmPfTpjtdgg\n- 강화학습의 미래? (이거 잘하면 먹고 살 수 있을까?)\n\n\nGame1: 벤딧문제\n- 문제설명: 두개의 버튼이 있다. 버튼0을 누르면 1의 보상을, 버튼1를 누르면 100의 보상을 준다고 가정\n- 어떤 행동을 해야할까? –&gt; ?? 아는게없음 –&gt; 일단 “아무거나” 눌러보자\n\naction_space = ['button0','button1']\naction = np.random.choice(action_space)\naction\n\n'button1'\n\n\n- 보상은 아래와 같은 방식으로 받을 것이다.\n\nif action == 'button0': \n    reward = 1 \nelse:\n    reward = 100\n\n- 아무거나 10번 버튼을 눌러보면 다음과 같은 결과가 나온다.\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 'button0': \n        reward = 1 \n    else:\n        reward = 100\n    print(action,reward)\n\nbutton1 100\nbutton0 1\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton0 1\nbutton0 1\nbutton1 100\nbutton1 100\n\n\n- 깨달았음: button0을 누르면 1점을 받고, button1을 누르면 100점을 받는 “환경”이구나? \\(\\to\\) button1을 누르는 “동작”을 해야하는 상황이구나?\n\n여기에서 \\(\\to\\) 의 과정을 체계화 시킨 학문이 강화학습이다.\n\n\naction_space = ['button0','button1']\nfor _ in range(10):\n    #action = np.random.choice(action_space) # 무지한자의 행동 (찍어)\n    action = action_space[1] # 깨달은자의 행동 (button1을 눌러)\n    if action == 'button0': \n        reward = 1 \n    else:\n        reward = 100\n    print(action,reward)\n\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\n\n\n- 강화학습: 환경의 이해 \\(\\to\\) 뭘 해야 하는지 깨달음\n위의 과정이 잘 되었다는 의미로 사용하는 문장들\n\n강화학습이 성공적으로 잘 되었다.\n환경이 해결되었다.\n에이전트가 환경의 과제를 완료했다.\n에이전트가 환경에서 성공적으로 학습했다.\n에이전트가 올바른 행동을 학습했다.\n게임 클리어 (비공식)\n\n- 환경이 해결되었는지 나타내는 지표를 정해야겠다 \\(\\to\\) 주어진 상황을 게임처럼 이해하고, 게임의 클리어조건을 설정\n\n첫 생각: button1을 누르면 클리어 아니야?\n두번째 생각: 아니지? 우연히 누를수도 있잖아.\n게임클리어조건: 최근 20번의 보상이 1900 이상이면 게임이 클리어 되었다고 보자!1\n\n1 button1을 눌러야하는건 맞지만 20번에 실수한번정도는 눈감아 주자- 무지한자 – 게임을 클리어 할 수 없다.\n\naction_space = ['button0','button1']\nrewards = []\nfor i in range(50): # 1000번해도 못깸\n    action = np.random.choice(action_space) # 무지한자의 행동 (찍어)\n    #action = action_space[1] # 깨달은자의 행동 (button1을 눌러)\n    if action == 'button0': \n        reward = 1 \n        rewards.append(reward)\n    else:\n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {i+1}\\t\"\n        f\"action = {action}\\t\"\n        f\"reward = {reward}\\t\"\n        f\"rwd20 = {sum(rewards[-20:])}\"\n    )\n    if np.sum(rewards[-20:])&gt;1900:\n        break\n    \n\nn_try = 1   action = button1    reward = 100    rwd20 = 100\nn_try = 2   action = button1    reward = 100    rwd20 = 200\nn_try = 3   action = button1    reward = 100    rwd20 = 300\nn_try = 4   action = button0    reward = 1  rwd20 = 301\nn_try = 5   action = button1    reward = 100    rwd20 = 401\nn_try = 6   action = button1    reward = 100    rwd20 = 501\nn_try = 7   action = button0    reward = 1  rwd20 = 502\nn_try = 8   action = button1    reward = 100    rwd20 = 602\nn_try = 9   action = button0    reward = 1  rwd20 = 603\nn_try = 10  action = button1    reward = 100    rwd20 = 703\nn_try = 11  action = button0    reward = 1  rwd20 = 704\nn_try = 12  action = button0    reward = 1  rwd20 = 705\nn_try = 13  action = button0    reward = 1  rwd20 = 706\nn_try = 14  action = button1    reward = 100    rwd20 = 806\nn_try = 15  action = button1    reward = 100    rwd20 = 906\nn_try = 16  action = button0    reward = 1  rwd20 = 907\nn_try = 17  action = button1    reward = 100    rwd20 = 1007\nn_try = 18  action = button1    reward = 100    rwd20 = 1107\nn_try = 19  action = button0    reward = 1  rwd20 = 1108\nn_try = 20  action = button0    reward = 1  rwd20 = 1109\nn_try = 21  action = button1    reward = 100    rwd20 = 1109\nn_try = 22  action = button0    reward = 1  rwd20 = 1010\nn_try = 23  action = button1    reward = 100    rwd20 = 1010\nn_try = 24  action = button1    reward = 100    rwd20 = 1109\nn_try = 25  action = button1    reward = 100    rwd20 = 1109\nn_try = 26  action = button0    reward = 1  rwd20 = 1010\nn_try = 27  action = button1    reward = 100    rwd20 = 1109\nn_try = 28  action = button1    reward = 100    rwd20 = 1109\nn_try = 29  action = button1    reward = 100    rwd20 = 1208\nn_try = 30  action = button1    reward = 100    rwd20 = 1208\nn_try = 31  action = button1    reward = 100    rwd20 = 1307\nn_try = 32  action = button0    reward = 1  rwd20 = 1307\nn_try = 33  action = button1    reward = 100    rwd20 = 1406\nn_try = 34  action = button1    reward = 100    rwd20 = 1406\nn_try = 35  action = button0    reward = 1  rwd20 = 1307\nn_try = 36  action = button0    reward = 1  rwd20 = 1307\nn_try = 37  action = button0    reward = 1  rwd20 = 1208\nn_try = 38  action = button0    reward = 1  rwd20 = 1109\nn_try = 39  action = button1    reward = 100    rwd20 = 1208\nn_try = 40  action = button1    reward = 100    rwd20 = 1307\nn_try = 41  action = button0    reward = 1  rwd20 = 1208\nn_try = 42  action = button0    reward = 1  rwd20 = 1208\nn_try = 43  action = button1    reward = 100    rwd20 = 1208\nn_try = 44  action = button1    reward = 100    rwd20 = 1208\nn_try = 45  action = button1    reward = 100    rwd20 = 1208\nn_try = 46  action = button1    reward = 100    rwd20 = 1307\nn_try = 47  action = button0    reward = 1  rwd20 = 1208\nn_try = 48  action = button0    reward = 1  rwd20 = 1109\nn_try = 49  action = button0    reward = 1  rwd20 = 1010\nn_try = 50  action = button1    reward = 100    rwd20 = 1010\n\n\n- 깨달은자 – 게임 클리어\n\naction_space = ['button0','button1']\nrewards = []\nfor i in range(50): # 1000번해도 못깸\n    #action = np.random.choice(action_space) # 무지한자의 행동 (찍어)\n    action = action_space[1] # 깨달은자의 행동 (button1을 눌러)\n    if action == 'button0': \n        reward = 1 \n        rewards.append(reward)\n    else:\n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {i+1}\\t\"\n        f\"action = {action}\\t\"\n        f\"reward = {reward}\\t\"\n        f\"rwd20 = {sum(rewards[-20:])}\"\n    )\n    if np.sum(rewards[-20:])&gt;1900:\n        break\n\nn_try = 1   action = button1    reward = 100    rwd20 = 100\nn_try = 2   action = button1    reward = 100    rwd20 = 200\nn_try = 3   action = button1    reward = 100    rwd20 = 300\nn_try = 4   action = button1    reward = 100    rwd20 = 400\nn_try = 5   action = button1    reward = 100    rwd20 = 500\nn_try = 6   action = button1    reward = 100    rwd20 = 600\nn_try = 7   action = button1    reward = 100    rwd20 = 700\nn_try = 8   action = button1    reward = 100    rwd20 = 800\nn_try = 9   action = button1    reward = 100    rwd20 = 900\nn_try = 10  action = button1    reward = 100    rwd20 = 1000\nn_try = 11  action = button1    reward = 100    rwd20 = 1100\nn_try = 12  action = button1    reward = 100    rwd20 = 1200\nn_try = 13  action = button1    reward = 100    rwd20 = 1300\nn_try = 14  action = button1    reward = 100    rwd20 = 1400\nn_try = 15  action = button1    reward = 100    rwd20 = 1500\nn_try = 16  action = button1    reward = 100    rwd20 = 1600\nn_try = 17  action = button1    reward = 100    rwd20 = 1700\nn_try = 18  action = button1    reward = 100    rwd20 = 1800\nn_try = 19  action = button1    reward = 100    rwd20 = 1900\nn_try = 20  action = button1    reward = 100    rwd20 = 2000\n\n\n\n\n수정1: action_space의 수정\n\naction_space =  gym.spaces.Discrete(2)\naction_space\n\nDiscrete(2)\n\n\n- 좋은점1: sample\n\nfor _ in range(10):\n    print(action_space.sample())\n\n0\n1\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n- 좋은점2: in\n\n0 in action_space\n\nTrue\n\n\n\n1 in action_space\n\nTrue\n\n\n\n2 in action_space\n\nFalse\n\n\n- 코드 1차수정\n\naction_space = gym.spaces.Discrete(2)\nrewards = []\nfor i in range(50): # 1000번해도 못깸\n    action = action_space.sample() # 무지한자의 행동 (찍어)\n    #action = 1 # 깨달은자의 행동 (button1을 눌러)\n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else:\n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {i+1}\\t\"\n        f\"action = {action}\\t\"\n        f\"reward = {reward}\\t\"\n        f\"rwd20 = {sum(rewards[-20:])}\"\n    )\n    if np.sum(rewards[-20:])&gt;1900:\n        break\n\nn_try = 1   action = 0  reward = 1  rwd20 = 1\nn_try = 2   action = 0  reward = 1  rwd20 = 2\nn_try = 3   action = 0  reward = 1  rwd20 = 3\nn_try = 4   action = 0  reward = 1  rwd20 = 4\nn_try = 5   action = 0  reward = 1  rwd20 = 5\nn_try = 6   action = 1  reward = 100    rwd20 = 105\nn_try = 7   action = 1  reward = 100    rwd20 = 205\nn_try = 8   action = 1  reward = 100    rwd20 = 305\nn_try = 9   action = 1  reward = 100    rwd20 = 405\nn_try = 10  action = 0  reward = 1  rwd20 = 406\nn_try = 11  action = 0  reward = 1  rwd20 = 407\nn_try = 12  action = 1  reward = 100    rwd20 = 507\nn_try = 13  action = 0  reward = 1  rwd20 = 508\nn_try = 14  action = 0  reward = 1  rwd20 = 509\nn_try = 15  action = 0  reward = 1  rwd20 = 510\nn_try = 16  action = 0  reward = 1  rwd20 = 511\nn_try = 17  action = 1  reward = 100    rwd20 = 611\nn_try = 18  action = 0  reward = 1  rwd20 = 612\nn_try = 19  action = 0  reward = 1  rwd20 = 613\nn_try = 20  action = 1  reward = 100    rwd20 = 713\nn_try = 21  action = 0  reward = 1  rwd20 = 713\nn_try = 22  action = 1  reward = 100    rwd20 = 812\nn_try = 23  action = 1  reward = 100    rwd20 = 911\nn_try = 24  action = 0  reward = 1  rwd20 = 911\nn_try = 25  action = 1  reward = 100    rwd20 = 1010\nn_try = 26  action = 1  reward = 100    rwd20 = 1010\nn_try = 27  action = 0  reward = 1  rwd20 = 911\nn_try = 28  action = 1  reward = 100    rwd20 = 911\nn_try = 29  action = 1  reward = 100    rwd20 = 911\nn_try = 30  action = 0  reward = 1  rwd20 = 911\nn_try = 31  action = 0  reward = 1  rwd20 = 911\nn_try = 32  action = 1  reward = 100    rwd20 = 911\nn_try = 33  action = 0  reward = 1  rwd20 = 911\nn_try = 34  action = 0  reward = 1  rwd20 = 911\nn_try = 35  action = 1  reward = 100    rwd20 = 1010\nn_try = 36  action = 0  reward = 1  rwd20 = 1010\nn_try = 37  action = 0  reward = 1  rwd20 = 911\nn_try = 38  action = 0  reward = 1  rwd20 = 911\nn_try = 39  action = 0  reward = 1  rwd20 = 911\nn_try = 40  action = 0  reward = 1  rwd20 = 812\nn_try = 41  action = 1  reward = 100    rwd20 = 911\nn_try = 42  action = 1  reward = 100    rwd20 = 911\nn_try = 43  action = 0  reward = 1  rwd20 = 812\nn_try = 44  action = 0  reward = 1  rwd20 = 812\nn_try = 45  action = 1  reward = 100    rwd20 = 812\nn_try = 46  action = 1  reward = 100    rwd20 = 812\nn_try = 47  action = 0  reward = 1  rwd20 = 812\nn_try = 48  action = 1  reward = 100    rwd20 = 812\nn_try = 49  action = 1  reward = 100    rwd20 = 812\nn_try = 50  action = 1  reward = 100    rwd20 = 911\n\n\n\n\n수정2: Env 클래스\n- env 생성\n\nclass Bandit:\n    def step(self,action):\n        if action == 0: \n            return 1\n        else:\n            return 100\n\n\nenv = Bandit()\nrewards = []\nfor i in range(50): # 1000번해도 못깸\n    #action = action_space.sample() # 무지한자의 행동 (찍어)\n    action = 1 # 깨달은자의 행동 (button1을 눌러)\n    reward = env.step(action)\n    rewards.append(reward)\n    print(\n        f\"n_try = {i+1}\\t\"\n        f\"action = {action}\\t\"\n        f\"reward = {reward}\\t\"\n        f\"rwd20 = {sum(rewards[-20:])}\"\n    )\n    if np.sum(rewards[-20:])&gt;1900:\n        break\n\nn_try = 1   action = 1  reward = 100    rwd20 = 100\nn_try = 2   action = 1  reward = 100    rwd20 = 200\nn_try = 3   action = 1  reward = 100    rwd20 = 300\nn_try = 4   action = 1  reward = 100    rwd20 = 400\nn_try = 5   action = 1  reward = 100    rwd20 = 500\nn_try = 6   action = 1  reward = 100    rwd20 = 600\nn_try = 7   action = 1  reward = 100    rwd20 = 700\nn_try = 8   action = 1  reward = 100    rwd20 = 800\nn_try = 9   action = 1  reward = 100    rwd20 = 900\nn_try = 10  action = 1  reward = 100    rwd20 = 1000\nn_try = 11  action = 1  reward = 100    rwd20 = 1100\nn_try = 12  action = 1  reward = 100    rwd20 = 1200\nn_try = 13  action = 1  reward = 100    rwd20 = 1300\nn_try = 14  action = 1  reward = 100    rwd20 = 1400\nn_try = 15  action = 1  reward = 100    rwd20 = 1500\nn_try = 16  action = 1  reward = 100    rwd20 = 1600\nn_try = 17  action = 1  reward = 100    rwd20 = 1700\nn_try = 18  action = 1  reward = 100    rwd20 = 1800\nn_try = 19  action = 1  reward = 100    rwd20 = 1900\nn_try = 20  action = 1  reward = 100    rwd20 = 2000\n\n\n\n\n수정3: Agent 클래스\n- Agent 클래스를 만들자 (액션을 하고, 환경에서 받은 reward를 간직하도록)\n\nclass Agent1:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2)\n        self.action = None\n        self.reward = None\n        self.actions = []\n        self.rewards = []\n    def act(self):\n        # self.action = self.action_space.sample() # 무지한 자\n        self.action = 1 # 깨달은 자 \n    def save_experience(self): \n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n\n— 대충 아래와 같은 느낌으로 돌아가요 —\n시점0: init\n\nenv = Bandit()\nagent = Agent1()\n\n\nagent.action, agent.reward\n\n(None, None)\n\n\n시점1: agent &gt;&gt; env\n\nagent.act()\n\n\nagent.action, agent.reward\n\n(1, None)\n\n\n\nenv.agent_action = agent.action \n\n시점2: agent &lt;&lt; env\n\nagent.reward = env.step(env.agent_action) \n# agent.reward = env.step(agent.action) 도 같은 결과를 주는 코드임!!\n\n\nagent.action, agent.reward, env.agent_action\n\n(1, 100, 1)\n\n\n\nagent.actions, agent.rewards\n\n([], [])\n\n\n\nagent.save_experience()\n\n\nagent.actions, agent.rewards\n\n([1], [100])\n\n\n— 전체코드 —\n\nenv = Bandit()\nagent = Agent1()\nfor i in range(50):\n    ## 1. 본질적인 코드\n    # step1: agent &gt;&gt; env \n    agent.act()\n    env.agent_action = agent.action \n    # step2: agnet &lt;&lt; env \n    agent.reward = env.step(env.agent_action) \n    agent.save_experience()\n    \n    ## 2. 비본질적 코드 \n    print(\n        f\"n_try = {i+1}\\t\"\n        f\"action = {agent.action}\\t\"\n        f\"reward = {agent.reward}\\t\"\n        f\"rwd20 = {sum(agent.rewards[-20:])}\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;1900:\n        break\n\nn_try = 1   action = 1  reward = 100    rwd20 = 100\nn_try = 2   action = 1  reward = 100    rwd20 = 200\nn_try = 3   action = 1  reward = 100    rwd20 = 300\nn_try = 4   action = 1  reward = 100    rwd20 = 400\nn_try = 5   action = 1  reward = 100    rwd20 = 500\nn_try = 6   action = 1  reward = 100    rwd20 = 600\nn_try = 7   action = 1  reward = 100    rwd20 = 700\nn_try = 8   action = 1  reward = 100    rwd20 = 800\nn_try = 9   action = 1  reward = 100    rwd20 = 900\nn_try = 10  action = 1  reward = 100    rwd20 = 1000\nn_try = 11  action = 1  reward = 100    rwd20 = 1100\nn_try = 12  action = 1  reward = 100    rwd20 = 1200\nn_try = 13  action = 1  reward = 100    rwd20 = 1300\nn_try = 14  action = 1  reward = 100    rwd20 = 1400\nn_try = 15  action = 1  reward = 100    rwd20 = 1500\nn_try = 16  action = 1  reward = 100    rwd20 = 1600\nn_try = 17  action = 1  reward = 100    rwd20 = 1700\nn_try = 18  action = 1  reward = 100    rwd20 = 1800\nn_try = 19  action = 1  reward = 100    rwd20 = 1900\nn_try = 20  action = 1  reward = 100    rwd20 = 2000\n\n\n\n위의 코드를 거의 뼈대로 사용합니당!\n\n\n\n학습과정 포함\n- Game1에 대한 생각:\n\n사실 강화학습은 “환경을 이해 \\(\\to\\) 뭘 해야 하는지 깨달음” 의 과정에서 화살표를 수식화 하는 과정이다.\n지금까지 살펴보면 Game1은 환경(env)을 이해하는 순간 에이전트가 최적의 행동(action)2을 직관적으로 결정하였으므로 학습의 과정이 포함되었다고 볼 수 없다.\n\n2 button1을 누른다- 지금까지의 코드 복습\n\n클래스를 선언하는 부분\n\nEnv클래스의 선언\nAgent클래스의 선언\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하며 게임을 진행\n\n메인코드: (1) agent &gt;&gt; env (2) agent &lt;&lt; env (3) 데이터저장\n비본질적코드: 학습과정 display, 학습종료조건\n\n\n- 앞으로 구성할 코드의 형태: 에이전트가 데이터를 보고 스스로 button1을 눌러야 한다는 것을 추론하면 좋겠음\n\n클래스를 선언하는 부분\n\nEnv클래스의 선언\nAgent클래스의 선언: 학습의 과정 포함 \\(\\to\\) act함수 수정\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하며 게임을 진행\n\n메인코드: (1) agent &gt;&gt; env (2) agent &lt;&lt; env (3) 데이터저장 (4) 학습\n비본질적코드: 학습과정 display, 학습종료조건\n\n\n- 에이전트가 학습을 어떻게 하는가?3\n3 행동을 이런식으로 하도록 “전략(=정책)”을 설정하는 것은 상식적이다. 그렇지만 유일한 해결책은 아님!!\n\\(\\pi(0) = \\frac{Q_t(1)}{Q_t(1)+Q_t(0)}\\)\n\\(\\pi(1) = \\frac{Q_t(0)}{Q_t(1)+Q_t(0)}\\)\n\n여기에서 각각의 기호는 아래를 의미한다.\n\n\\(\\pi(0)\\): 에이전트가 action = 0 을 할 확률, 즉 에이전트가 button0을 누를 확률\n\\(\\pi(1)\\): 에이전트가 action = 1 을 할 확률, 즉 에이전트가 button1을 누를 확률\n\\(Q_t(0)\\): (\\(t\\)시점까지 파악한) action = 0 을 하였을 경우 에이전트가 환경으로 받은 보상의 평균값\n\\(Q_t(1)\\): (\\(t\\)시점까지 파악한) action = 1 을 하였을 경우 에이전트가 환경으로 받은 보상의 평균값\n\n- 걱정: \\(t=0\\)이면 어쩌지? \\(t=1\\)이면 어쩌지?.. \\(\\to\\) 잡기술1: 일정 시간동안은 랜덤액션을 하면서 데이터를 쌓자.\n- 쌓은 데이터를 바탕으로 환경을 이해하고 action을 뽑는 코드\n\nagent.actions = [0,0,1,0,0,1,1,1,1,0]\nagent.actions \n\n[0, 0, 1, 0, 0, 1, 1, 1, 1, 0]\n\n\n\nagent.rewards = [1,0.9,105,1.2,1,95,100,101,90,1]\nagent.rewards\n\n[1, 0.9, 105, 1.2, 1, 95, 100, 101, 90, 1]\n\n\n\nactions = np.array(agent.actions)\nrewards = np.array(agent.rewards)\n\n\nq0= rewards[actions==0].mean()\nq1= rewards[actions==1].mean()\n\n\nagent.q = np.array([q0,q1])\nagent.q\n\narray([ 1.02, 98.2 ])\n\n\n\naction = np.random.choice([0,1],p=agent.q/agent.q.sum())\naction\n\n1\n\n\n- 코드를 구현해보자.\n\n# 게임을 푸는 전략을 바꾸는 것이지 게임자체를 바꿀게 아니니까 수정할게 없죠?\nclass Bandit():\n    def step(self,action):\n        if action == 0: \n            return 1\n        else:\n            return 100\n\n\n# learn 추가\nclass Agent:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2)\n        self.action = None\n        self.reward = None\n        self.actions = []\n        self.rewards = []\n        self.n_experiences = 0\n        self.q = np.array([0,0])\n    def act(self):\n        if self.n_experiences &lt; 30:\n            self.action = self.action_space.sample()\n        else:\n            self.action = np.random.choice([0, 1], p=self.q/self.q.sum())\n    def save_experience(self): \n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experiences = self.n_experiences + 1 \n    def learn(self):\n        if self.n_experiences &lt; 30: \n            pass \n        else:\n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0= rewards[actions==0].mean() \n            q1= rewards[actions==1].mean()        \n            self.q = np.array([q0,q1])\n\n\nenv = Bandit()\nagent = Agent()\nfor i in range(60):\n    ## 본질적인 코드\n    # step1: agent &gt;&gt; env \n    agent.act()\n    env.agent_action = agent.action \n    # step2: agnet &lt;&lt; env \n    agent.reward = env.step(env.agent_action) \n    agent.save_experience()\n    # step3: learn\n    agent.learn()\n    \n    ## 비본질적 코드 \n    print(\n        f\"n_try = {agent.n_experiences}\\t\"\n        f\"action = {agent.action}\\t\"\n        f\"reward = {agent.reward}\\t\"\n        f\"rwd20 = {np.sum(agent.rewards[-20:])}\\t\"\n        f\"q = {agent.q}\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;1900:\n        break\n\nn_try = 1   action = 1  reward = 100    rwd20 = 100 q = [0 0]\nn_try = 2   action = 1  reward = 100    rwd20 = 200 q = [0 0]\nn_try = 3   action = 1  reward = 100    rwd20 = 300 q = [0 0]\nn_try = 4   action = 0  reward = 1  rwd20 = 301 q = [0 0]\nn_try = 5   action = 0  reward = 1  rwd20 = 302 q = [0 0]\nn_try = 6   action = 1  reward = 100    rwd20 = 402 q = [0 0]\nn_try = 7   action = 0  reward = 1  rwd20 = 403 q = [0 0]\nn_try = 8   action = 1  reward = 100    rwd20 = 503 q = [0 0]\nn_try = 9   action = 0  reward = 1  rwd20 = 504 q = [0 0]\nn_try = 10  action = 0  reward = 1  rwd20 = 505 q = [0 0]\nn_try = 11  action = 1  reward = 100    rwd20 = 605 q = [0 0]\nn_try = 12  action = 1  reward = 100    rwd20 = 705 q = [0 0]\nn_try = 13  action = 1  reward = 100    rwd20 = 805 q = [0 0]\nn_try = 14  action = 1  reward = 100    rwd20 = 905 q = [0 0]\nn_try = 15  action = 1  reward = 100    rwd20 = 1005    q = [0 0]\nn_try = 16  action = 0  reward = 1  rwd20 = 1006    q = [0 0]\nn_try = 17  action = 0  reward = 1  rwd20 = 1007    q = [0 0]\nn_try = 18  action = 0  reward = 1  rwd20 = 1008    q = [0 0]\nn_try = 19  action = 1  reward = 100    rwd20 = 1108    q = [0 0]\nn_try = 20  action = 0  reward = 1  rwd20 = 1109    q = [0 0]\nn_try = 21  action = 0  reward = 1  rwd20 = 1010    q = [0 0]\nn_try = 22  action = 1  reward = 100    rwd20 = 1010    q = [0 0]\nn_try = 23  action = 0  reward = 1  rwd20 = 911 q = [0 0]\nn_try = 24  action = 0  reward = 1  rwd20 = 911 q = [0 0]\nn_try = 25  action = 0  reward = 1  rwd20 = 911 q = [0 0]\nn_try = 26  action = 1  reward = 100    rwd20 = 911 q = [0 0]\nn_try = 27  action = 1  reward = 100    rwd20 = 1010    q = [0 0]\nn_try = 28  action = 0  reward = 1  rwd20 = 911 q = [0 0]\nn_try = 29  action = 0  reward = 1  rwd20 = 911 q = [0 0]\nn_try = 30  action = 0  reward = 1  rwd20 = 911 q = [  1. 100.]\nn_try = 31  action = 1  reward = 100    rwd20 = 911 q = [  1. 100.]\nn_try = 32  action = 1  reward = 100    rwd20 = 911 q = [  1. 100.]\nn_try = 33  action = 1  reward = 100    rwd20 = 911 q = [  1. 100.]\nn_try = 34  action = 1  reward = 100    rwd20 = 911 q = [  1. 100.]\nn_try = 35  action = 1  reward = 100    rwd20 = 911 q = [  1. 100.]\nn_try = 36  action = 1  reward = 100    rwd20 = 1010    q = [  1. 100.]\nn_try = 37  action = 1  reward = 100    rwd20 = 1109    q = [  1. 100.]\nn_try = 38  action = 1  reward = 100    rwd20 = 1208    q = [  1. 100.]\nn_try = 39  action = 1  reward = 100    rwd20 = 1208    q = [  1. 100.]\nn_try = 40  action = 1  reward = 100    rwd20 = 1307    q = [  1. 100.]\nn_try = 41  action = 1  reward = 100    rwd20 = 1406    q = [  1. 100.]\nn_try = 42  action = 1  reward = 100    rwd20 = 1406    q = [  1. 100.]\nn_try = 43  action = 1  reward = 100    rwd20 = 1505    q = [  1. 100.]\nn_try = 44  action = 1  reward = 100    rwd20 = 1604    q = [  1. 100.]\nn_try = 45  action = 1  reward = 100    rwd20 = 1703    q = [  1. 100.]\nn_try = 46  action = 1  reward = 100    rwd20 = 1703    q = [  1. 100.]\nn_try = 47  action = 1  reward = 100    rwd20 = 1703    q = [  1. 100.]\nn_try = 48  action = 1  reward = 100    rwd20 = 1802    q = [  1. 100.]\nn_try = 49  action = 1  reward = 100    rwd20 = 1901    q = [  1. 100.]"
  },
  {
    "objectID": "posts/1_Essays/2023-08-30-강화학습(3) -- LunarLander.html",
    "href": "posts/1_Essays/2023-08-30-강화학습(3) -- LunarLander.html",
    "title": "[Essays] 강화학습(3) – LunarLander",
    "section": "",
    "text": "imports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch\nimport collections\nimport IPython\nimport random\n\n\n\n예비학습\n- collections.deque의 기능\n\na = collections.deque([1,2,3],maxlen=5)\na\n\ndeque([1, 2, 3], maxlen=5)\n\n\n\na.append(4)\n\n\na\n\ndeque([1, 2, 3, 4], maxlen=5)\n\n\n\na.append(5)\n\n\na\n\ndeque([1, 2, 3, 4, 5], maxlen=5)\n\n\n\na.append(6)\n\n\na\n\ndeque([2, 3, 4, 5, 6], maxlen=5)\n\n\n- 단점?: numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\na+1\n\nTypeError: can only concatenate deque (not \"int\") to deque\n\n\n- 그렇지만 언제든지 numpy화 가능\n\nnp.array(a)+1\n\narray([3, 4, 5, 6, 7])\n\n\n- collection.deque는 리플레이 버퍼를 구현할때 유리한 자료구조이다.\n\n(우리가 했던) 기존방식: 모든 데이터를 저장하며 한번의 경험씩 학습함\n리플레이버퍼: 최근의 데이터를 저장하며 여러경험을 샘플링하여 학습함\n리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을 저장하고 무작위로 재사용하여 안정성이 높다, “저장 -&gt; 학습 -&gt; 저장” 순으로 반드시 실시간학습을 할 필요가 없어 병렬처리에 용이하다, 강화학습에서의 연속된 경험은 상관관계가 있을 수 있는데 무작위 샘플로 이를 제거할 수 있다 등등~\n\n\n\nGame3: LunarLander\n- 환경생성\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\n\n- state_space\n\nenv.observation_space\n\nBox([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n 1.       ], (8,), float32)\n\n\n- action_space\n\nenv.action_space\n\nDiscrete(4)\n\n\n- env.reset()\n\nenv.reset()\n\n(array([-0.00357695,  1.4113593 , -0.3623264 ,  0.01951814,  0.00415163,\n         0.08207229,  0.        ,  0.        ], dtype=float32),\n {})\n\n\n- env.render()\n\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f3e2ff5b110&gt;\n\n\n\n\n\n- env.step\n\nfor _ in range(5):\n    env.step(0)\n    env.step(2)\n    env.step(2)\n    env.step(1)\n\n\n0: 정지\n1: 왼쪽\n2: 위로\n3: 오른쪽\nhttps://gymnasium.farama.org/environments/box2d/lunar_lander/\n\n\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f3e2c22fcd0&gt;\n\n\n\n\n\n\n\n시각화\n\nenv.step(0)\n\n(array([-0.08772536,  1.4151428 , -0.48479563, -0.03591808,  0.18723553,\n         0.25663763,  0.        ,  0.        ], dtype=float32),\n -1.361842085847286,\n False,\n False,\n {})\n\n\n\ncurrent_states = collections.deque(maxlen=50)\nactions = collections.deque(maxlen=50)\nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50)\nims = []\ncurrent_state, _ = env.reset()\n\nfor t in range(500):\n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n    im = env.render()\n    \n    current_states.append(current_state)\n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward) \n    terminations.append(terminated)\n    ims.append(im)    \n\n    current_state = next_state \n    if terminated: break\n\n\ndef show(ims,jump=10):\n    ims = ims[::jump]\n    fig = plt.Figure()\n    ax = fig.subplots()\n    def update(i):\n       ax.imshow(ims[i])\n    ani = FuncAnimation(fig,update,frames=len(ims))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\nshow(ims,jump=10)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nq_net\n- 데이터를 모아보자.\n\ncurrent_states = collections.deque(maxlen=50) \nactions = collections.deque(maxlen=50) \nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50) \n\ncurrent_state, _ = env.reset()\nfor t in range(500): \n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n\n    current_states.append(current_state) \n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward)\n    terminations.append(terminated) \n    \n    current_state = next_state \n    if terminated: break \n\n- 이전코드에서 아래에 대응하는 부분을 잘 구현하면 된다.\n## 1. q[x,y,a] \nagent.q = np.zeros([4,4,4])  \n\n## 2. q_estimate  \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = reward + 0.99 * q_future\nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimate 와 q_realistic 를 비슷하게 만들어주는 역할을 하는 코드 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff\n1. q_net를 설정\n\nq_net = torch.nn.Sequential(\n    torch.nn.Linear(8,128), # 8개의 상태공간 \n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64), \n    torch.nn.ReLU(),\n    torch.nn.Linear(64,32), \n    torch.nn.ReLU(),\n    torch.nn.Linear(32,4) # 4개의 action값들 \n)\n\n\ntorch.tensor(current_state)\n\ntensor([-2.0107e-01,  1.8814e-02,  3.0546e-01, -1.0702e-01,  1.4925e+00,\n        -2.4794e-04,  0.0000e+00,  0.0000e+00])\n\n\n\nq_net(torch.tensor(current_state))\n\ntensor([ 0.0673, -0.1615,  0.0230,  0.0122], grad_fn=&lt;AddBackward0&gt;)\n\n\n1. q_net를 설정 (배치버전)\n– get batch –\n\nbatch_size = 4\nidx = np.random.randint(0,50,size=batch_size) \nidx\n\narray([19,  6, 30, 36])\n\n\n\ntorch.tensor(current_states)[idx]\n\ntensor([[-0.1059,  0.8952, -0.2577, -0.9382,  0.2779,  0.3141,  0.0000,  0.0000],\n        [-0.0771,  1.1378, -0.2146, -0.7420,  0.0873,  0.1995,  0.0000,  0.0000],\n        [-0.1361,  0.6269, -0.3092, -1.2115,  0.5054,  0.4266,  0.0000,  0.0000],\n        [-0.1544,  0.4519, -0.3573, -1.3554,  0.6506,  0.5228,  0.0000,  0.0000]])\n\n\n\ntorch.tensor(np.array(current_states))[idx]\n\ntensor([[-0.1059,  0.8952, -0.2577, -0.9382,  0.2779,  0.3141,  0.0000,  0.0000],\n        [-0.0771,  1.1378, -0.2146, -0.7420,  0.0873,  0.1995,  0.0000,  0.0000],\n        [-0.1361,  0.6269, -0.3092, -1.2115,  0.5054,  0.4266,  0.0000,  0.0000],\n        [-0.1544,  0.4519, -0.3573, -1.3554,  0.6506,  0.5228,  0.0000,  0.0000]])\n\n\n\ncurrent_states_batch = torch.tensor(np.array(current_states))[idx].float()\nactions_batch = torch.tensor(np.array(actions))[idx].reshape(batch_size,1)\nrewards_batch = torch.tensor(np.array(rewards))[idx].reshape(batch_size,-1).float()\nnext_states_batch = torch.tensor(np.array(next_states))[idx].float()\nterminations_batch = torch.tensor(np.array(terminations))[idx].reshape(batch_size,-1) \n\n\n~terminations_batch\n\ntensor([[True],\n        [True],\n        [True],\n        [True]])\n\n\n– q_net –\n\nq_net(current_states_batch)\n\ntensor([[ 0.0216, -0.1580,  0.0078,  0.0697],\n        [ 0.0217, -0.1601,  0.0112,  0.0705],\n        [ 0.0228, -0.1545,  0.0090,  0.0693],\n        [ 0.0258, -0.1564,  0.0129,  0.0672]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n2. q_estimated의 구현\n\nq_net(current_states_batch), actions_batch\n\n(tensor([[ 0.0216, -0.1580,  0.0078,  0.0697],\n         [ 0.0217, -0.1601,  0.0112,  0.0705],\n         [ 0.0228, -0.1545,  0.0090,  0.0693],\n         [ 0.0258, -0.1564,  0.0129,  0.0672]], grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[0],\n         [1],\n         [0],\n         [2]]))\n\n\n\nq_estimated = q_net(current_states_batch).gather(1,actions_batch)\nq_estimated\n\ntensor([[ 0.0216],\n        [-0.1601],\n        [ 0.0228],\n        [ 0.0129]], grad_fn=&lt;GatherBackward0&gt;)\n\n\n3. q_realistic = reward + 0.99 * q_future 의 구현\nq_future\n\nq_future = q_net(next_states_batch).max(axis=1)[0].reshape(batch_size,1)\nq_future\n\ntensor([[0.0698],\n        [0.0711],\n        [0.0690],\n        [0.0660]], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\n\nq_realistic = rewards_batch + q_future * 0.99 * (~terminations_batch)\n\n4. q_estimated \\(\\approx\\) q_realistic 가 되도록 만드는 장치\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야해요\noptimizer = torch.optim.Adam(q_net.parameters(), lr=0.0001)\nfor _ in range(2000):\n    ~~\n    ~~\n    q_estimated = ...\n    q_observed = ... \n    loss = torch.nn.functional.mse_loss(q_estimated, q_observed)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\npolicy\n\neps = 0.5 \nif np.random.rand() &gt; eps:\n    action = q_net(torch.tensor(current_state)).argmax().item()\nelse:\n    action = env.action_space.sample()\n\n\naction\n\n0\n\n\n\n\nAgent 클래스 + run\n\nclass Agent():\n    def __init__(self,env):\n        self.eps = 0\n        self.n_experiences = 0\n        self.n_episode = 0\n        self.score = 0\n        self.scores = [] \n        self.playtimes = []\n        self.batch_size = 64\n        self.buffer_size = 5000 \n        self.action_space = env.action_space\n        #self.state_space = env.observation_space\n\n        # Q-Network\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,128), \n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,4)\n        ) \n        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n\n        # ReplayBuffer\n        self.memory = collections.deque(maxlen=self.buffer_size)\n        self.current_states = collections.deque(maxlen=self.buffer_size)\n        self.actions = collections.deque(maxlen=self.buffer_size)\n        self.rewards = collections.deque(maxlen=self.buffer_size)\n        self.next_states = collections.deque(maxlen=self.buffer_size)\n        self.terminations = collections.deque(maxlen=self.buffer_size)\n       \n    def save_experience(self):\n        \"\"\"Add a new experience to memory.\"\"\"\n        self.current_states.append(self.current_state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences = self.n_experiences+1\n        self.score += self.reward\n    \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else:\n            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n            \n    def get_batch(self):\n        idx = np.random.randint(0,self.buffer_size,size=self.batch_size) \n        self.current_states_batch = torch.tensor(np.array(self.current_states))[idx].float()\n        self.actions_batch = torch.tensor(np.array(self.actions))[idx].reshape(self.batch_size,1)\n        self.rewards_batch = torch.tensor(np.array(self.rewards))[idx].reshape(self.batch_size,-1).float()\n        self.next_states_batch = torch.tensor(np.array(self.next_states))[idx].float()\n        self.terminations_batch = torch.tensor(np.array(self.terminations))[idx].reshape(self.batch_size,-1) \n    \n    def learn(self):\n        if self.n_experiences &lt; self.buffer_size:\n            pass\n        else: \n            self.get_batch()\n            q_estimated = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n            q_future = self.q_net(self.next_states_batch).detach().max(1)[0].reshape(self.batch_size,1)\n            q_realistic = self.rewards_batch + 0.99 * q_future * (~self.terminations_batch)\n\n            loss = torch.nn.functional.mse_loss(q_estimated, q_realistic)\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\nagent = Agent(env)\nagent.eps = 1.0 \nfor _ in range(2000):\n    ### 1. 본질적인 코드\n    agent.current_state, _  = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(500):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated, _,_ = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps*0.995\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 10) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_eps: {agent.eps}\\t'\n            f'n_experiences: {agent.n_experiences}\\t'\n        )\n    if np.mean(agent.scores[-100:])&gt;=200.0:\n        break\n\nEpisode 10  Score: -137.90  Playtime:  97.10    n_eps: 0.9511101304657719   n_experiences: 971  \nEpisode 20  Score: -195.78  Playtime:  96.60    n_eps: 0.9046104802746175   n_experiences: 1932 \nEpisode 30  Score: -196.55  Playtime:  92.40    n_eps: 0.8603841919146962   n_experiences: 2772 \nEpisode 40  Score: -201.82  Playtime:  90.53    n_eps: 0.8183201210226743   n_experiences: 3621 \nEpisode 50  Score: -217.03  Playtime:  90.06    n_eps: 0.778312557068642    n_experiences: 4503 \nEpisode 60  Score: -224.53  Playtime:  90.13    n_eps: 0.7402609576967045   n_experiences: 5408 \nEpisode 70  Score: -216.98  Playtime:  90.06    n_eps: 0.7040696960536299   n_experiences: 6304 \nEpisode 80  Score: -228.78  Playtime:  94.11    n_eps: 0.6696478204705644   n_experiences: 7529 \nEpisode 90  Score: -220.73  Playtime:  97.60    n_eps: 0.6369088258938781   n_experiences: 8784 \nEpisode 100 Score: -217.57  Playtime:  103.18   n_eps: 0.6057704364907278   n_experiences: 10318    \nEpisode 110 Score: -218.36  Playtime:  110.26   n_eps: 0.5761543988830038   n_experiences: 11997    \nEpisode 120 Score: -205.50  Playtime:  118.04   n_eps: 0.547986285490042    n_experiences: 13736    \nEpisode 130 Score: -200.74  Playtime:  126.74   n_eps: 0.5211953074858876   n_experiences: 15446    \nEpisode 140 Score: -191.62  Playtime:  135.60   n_eps: 0.49571413690105054  n_experiences: 17181    \nEpisode 150 Score: -173.48  Playtime:  142.64   n_eps: 0.47147873742168567  n_experiences: 18767    \nEpisode 160 Score: -155.38  Playtime:  151.56   n_eps: 0.4484282034609769   n_experiences: 20564    \nEpisode 170 Score: -146.95  Playtime:  165.12   n_eps: 0.42650460709830135  n_experiences: 22816    \nEpisode 180 Score: -120.43  Playtime:  190.67   n_eps: 0.40565285250151817  n_experiences: 26596    \nEpisode 190 Score: -108.96  Playtime:  211.01   n_eps: 0.3858205374665315   n_experiences: 29885    \nEpisode 200 Score: -94.70   Playtime:  229.29   n_eps: 0.3669578217261671   n_experiences: 33247    \nEpisode 210 Score: -84.55   Playtime:  241.50   n_eps: 0.34901730169741024  n_experiences: 36147    \nEpisode 220 Score: -72.07   Playtime:  260.91   n_eps: 0.33195389135223546  n_experiences: 39827    \nEpisode 230 Score: -53.77   Playtime:  290.32   n_eps: 0.3157247089126454   n_experiences: 44478    \nEpisode 240 Score: -43.37   Playtime:  308.80   n_eps: 0.30028896908517405  n_experiences: 48061    \nEpisode 250 Score: -35.43   Playtime:  333.85   n_eps: 0.285607880564032    n_experiences: 52152    \nEpisode 260 Score: -20.80   Playtime:  363.92   n_eps: 0.27164454854530906  n_experiences: 56956    \nEpisode 270 Score: -4.65    Playtime:  376.17   n_eps: 0.2583638820072446   n_experiences: 60433    \nEpisode 280 Score:  6.64    Playtime:  378.81   n_eps: 0.2457325055235537   n_experiences: 64477    \nEpisode 290 Score:  17.44   Playtime:  387.17   n_eps: 0.23371867538818816  n_experiences: 68602    \nEpisode 300 Score:  26.12   Playtime:  393.60   n_eps: 0.22229219984074702  n_experiences: 72607    \nEpisode 310 Score:  31.50   Playtime:  393.22   n_eps: 0.21142436319205632  n_experiences: 75469    \nEpisode 320 Score:  38.37   Playtime:  395.83   n_eps: 0.2010878536592394   n_experiences: 79410    \nEpisode 330 Score:  43.81   Playtime:  392.23   n_eps: 0.1912566947289212   n_experiences: 83701    \nEpisode 340 Score:  54.40   Playtime:  399.65   n_eps: 0.18190617987607657  n_experiences: 88026    \nEpisode 350 Score:  68.59   Playtime:  408.74   n_eps: 0.1730128104744653   n_experiences: 93026    \nEpisode 360 Score:  56.54   Playtime:  382.49   n_eps: 0.16455423674261854  n_experiences: 95205    \nEpisode 370 Score:  52.59   Playtime:  381.52   n_eps: 0.15650920157696743  n_experiences: 98585    \nEpisode 380 Score:  56.71   Playtime:  385.11   n_eps: 0.14885748713096328  n_experiences: 102988   \nEpisode 390 Score:  55.14   Playtime:  393.86   n_eps: 0.14157986400593744  n_experiences: 107988   \nEpisode 400 Score:  40.80   Playtime:  378.01   n_eps: 0.1346580429260134   n_experiences: 110408   \nEpisode 410 Score:  15.73   Playtime:  373.27   n_eps: 0.12807462877562611  n_experiences: 112796   \nEpisode 420 Score:  9.92    Playtime:  380.20   n_eps: 0.12181307688414106  n_experiences: 117430   \nEpisode 430 Score:  11.66   Playtime:  387.29   n_eps: 0.11585765144771248  n_experiences: 122430   \nEpisode 440 Score:  13.24   Playtime:  391.23   n_eps: 0.11019338598389174  n_experiences: 127149   \nEpisode 450 Score:  16.40   Playtime:  390.34   n_eps: 0.10480604571960442  n_experiences: 132060   \nEpisode 460 Score:  34.93   Playtime:  409.24   n_eps: 0.0996820918179746   n_experiences: 136129   \nEpisode 470 Score:  46.96   Playtime:  423.49   n_eps: 0.09480864735409487  n_experiences: 140934   \nEpisode 480 Score:  56.14   Playtime:  410.33   n_eps: 0.09017346495423652  n_experiences: 144021   \nEpisode 490 Score:  72.96   Playtime:  388.74   n_eps: 0.08576489601717459  n_experiences: 146862   \nEpisode 500 Score:  108.96  Playtime:  403.26   n_eps: 0.08157186144027828  n_experiences: 150734   \nEpisode 510 Score:  144.19  Playtime:  419.33   n_eps: 0.07758382377679894  n_experiences: 154729   \nEpisode 520 Score:  165.65  Playtime:  408.21   n_eps: 0.07379076075438468  n_experiences: 158251   \nEpisode 530 Score:  169.62  Playtime:  391.44   n_eps: 0.07018314008827135  n_experiences: 161574   \nEpisode 540 Score:  181.53  Playtime:  387.68   n_eps: 0.0667518955258533   n_experiences: 165917   \nEpisode 550 Score:  187.00  Playtime:  373.46   n_eps: 0.06348840406243188  n_experiences: 169406   \nEpisode 560 Score:  197.04  Playtime:  369.71   n_eps: 0.06038446427088321  n_experiences: 173100   \n\n\n\nagent2 = Agent(env)\nagent2.current_state,_ = env.reset()\n\nagent2.q_net = agent.q_net\nagent2.terminated = False\nims = [] \nims.append(env.render())\nfor t in range(500):\n    agent2.act()\n    agent2.next_state, agent2.reward, agent2.terminated,_ , _ = env.step(agent2.action)\n    agent2.current_state = agent2.next_state \n    ims.append(env.render())\n    if agent2.terminated: break \n\n\nshow(ims,jump=10)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-09-09-asdf.html",
    "href": "posts/1_Essays/2023-09-09-asdf.html",
    "title": "신록예찬's Blog",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv('./ecg/ptbdb_abnormal.csv', header=None)\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n\n\n\n\n0\n0.932233\n0.869679\n0.886186\n0.929626\n0.908775\n0.933970\n0.801043\n0.749783\n0.687229\n0.635100\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n1.000000\n0.606941\n0.384181\n0.254237\n0.223567\n0.276836\n0.253430\n0.184826\n0.153349\n0.121872\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2\n1.000000\n0.951613\n0.923963\n0.853303\n0.791859\n0.734255\n0.672043\n0.685100\n0.670507\n0.667435\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.977819\n0.899261\n0.230129\n0.032348\n0.142329\n0.223660\n0.328096\n0.367837\n0.381701\n0.389094\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n4\n0.935618\n0.801661\n0.805815\n1.000000\n0.722741\n0.480789\n0.454829\n0.319834\n0.266874\n0.308411\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10501\n0.981409\n1.000000\n0.559171\n0.287093\n0.196639\n0.204862\n0.215946\n0.243833\n0.242760\n0.250268\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n10502\n0.906250\n0.922379\n0.878024\n0.810484\n0.712702\n0.667339\n0.608871\n0.527218\n0.480847\n0.442540\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n10503\n1.000000\n0.867971\n0.674122\n0.470332\n0.296987\n0.169307\n0.077664\n0.081392\n0.074868\n0.089779\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n10504\n1.000000\n0.984672\n0.658888\n0.556394\n0.446809\n0.395790\n0.315260\n0.276367\n0.261039\n0.258522\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n10505\n0.997886\n0.700317\n0.464059\n0.318182\n0.233615\n0.184989\n0.124207\n0.082981\n0.087738\n0.063953\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n10506 rows × 188 columns\n\n\n\n\ndf.iloc[0].plot()\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls5.html",
    "href": "posts/2_Studies/PyG/ls5.html",
    "title": "[PyG] lesson5: Learning Methods on Graphs",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls5.ipynb\n\n\nRef\n\nhttps://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n\n\n\ndata\n\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\n\n\ndataset[0]\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\nimport torch_torch_geometric\n\n\ntorch_geometric.data.Data\n\nNameError: name 'torch_geometric' is not defined\n\n\n\ndataset[0].edge_index.shape\n\ntorch.Size([2, 10556])\n\n\n\ndataset[0].y.dtype\n\ntorch.int64\n\n\n\ndataset[0].train_mask\n\ntensor([ True,  True,  True,  ..., False, False, False])\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, 16)\n        self.conv2 = GCNConv(16, dataset.num_classes)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = GCN().to(device)\ndata = dataset[0].to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\nmodel.train()\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask]) # train에 대한 loss만 따로 처리해야함\n    loss.backward()\n    optimizer.step()\n\n\nout.shape # 카테고리가 7개\n\ntorch.Size([2708, 7])\n\n\n\ndata.y.unique() # 카테고리가 7개\n\ntensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.8050\n\n\n\n\nFRAUD data에 활용?\n\n고객정보별로 그래프생성\n커다란 하나의 그래프 생성\n\n\n\nmodel 분석\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1433, 16)\n  (conv2): GCNConv(16, 7)\n)\n\n\n\ndataset.data\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\n2708개의 노드가 있음 \\(\\to\\) 이걸 observation으로 해석해야함.\n각 노드에는 1433개의 특징(X)과 0-6까지의 label(y)이 연결되어 있음.\n2708개의 노드는 각각 tr,val,test로 나누어짐\n\n\nb,W = list(model.conv1.parameters())\n\n\nW,W.shape # 1433개의 특징을 16개로 줄임\n\n(Parameter containing:\n tensor([[ 0.0086, -0.0062, -0.1213,  ...,  0.1313, -0.0251,  0.0856],\n         [-0.0033, -0.0384, -0.1193,  ...,  0.0362, -0.1130,  0.0426],\n         [ 0.0118,  0.0722,  0.0481,  ..., -0.0677,  0.0497,  0.0095],\n         ...,\n         [ 0.0158,  0.1560, -0.0517,  ..., -0.0317,  0.1272,  0.0131],\n         [-0.0259,  0.0146, -0.0539,  ...,  0.0069, -0.0665,  0.0016],\n         [ 0.0191, -0.1023,  0.0411,  ..., -0.0415,  0.0125,  0.0015]],\n        device='cuda:0', requires_grad=True),\n torch.Size([16, 1433]))\n\n\n\nb,b.shape\n\n(Parameter containing:\n tensor([0.2343, 0.2136, 0.1581, 0.1914, 0.3095, 0.0851, 0.3675, 0.2414, 0.3099,\n         0.2571, 0.1340, 0.2693, 0.1588, 0.2984, 0.1877, 0.2014],\n        device='cuda:0', requires_grad=True),\n torch.Size([16]))\n\n\n\n\nGCNConv?? Kipf and Welling (2016)\n\nKipf, Thomas N, and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907.\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html\n\n아래의 논문에서 제안되었음.. (레퍼수가..)\nhttps://arxiv.org/abs/1609.02907\n대략적인 설명을 캡쳐하면 아래와 같음\n\n\n\nimage.png\n\n\n여기에서\n\n\\({\\bf A}\\)는 연결정보를 의미\n\\(\\hat{\\bf A}\\)는 연결정보에 자기자신의 노드를 추가\n\\({\\bf D}\\)는 \\({\\bf A}\\)를 표준화하기 위한 매트릭스\n결국 \\(\\hat{\\bf D}^{-1/2}\\hat{\\bf A}\\hat{\\bf D}^{-1/2}\\) 는 통째로 연결정보에 대한 matrix\n\\(\\hat{\\bf D}^{-1/2}\\hat{\\bf A}\\hat{\\bf D}^{-1/2}{\\bf X}\\) 는 통째로 \\({\\bf X}\\)를 평행이동한것을 의미 (혹은 그 비슷한 것을 의미)\n\\({\\bf \\Theta}\\)는 weight를 곱하는 과정임"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html",
    "href": "posts/2_Studies/PyG/ls1.html",
    "title": "[PyG] lesson1: 자료형",
    "section": "",
    "text": "import torch\nimport torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제1-아래와-같은-그래프자료를-고려하자.",
    "href": "posts/2_Studies/PyG/ls1.html#예제1-아래와-같은-그래프자료를-고려하자.",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제1: 아래와 같은 그래프자료를 고려하자.",
    "text": "예제1: 아래와 같은 그래프자료를 고려하자.\n\n- 이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index) # torch_geometric.data.Data는 그래프자료형을 만드는 클래스\n\n- data 의 자료형\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n- data의 __str__\n\ndata\n\nData(x=[3, 1], edge_index=[2, 4])\n\n\n\nx=[3, 1]: 이 자료는 3개의 노드가 있으며, 각 노드에는 1개의 feature가 있음\nedge_index=[2, 4]: \\({\\cal E}\\)는 총 4개의 원소가 있음.\n\n- 각 노드의 feature를 확인하는 방법 (즉 \\(f:{\\cal V} \\to \\mathbb{R}^k\\)를 확인하는 방법)\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n- \\({\\cal E}\\)를 확인하는 방법\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])\n\n\n-\n\nlen(data)\n\n2"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제2-잘못된-사용",
    "href": "posts/2_Studies/PyG/ls1.html#예제2-잘못된-사용",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제2: 잘못된 사용",
    "text": "예제2: 잘못된 사용\n- edge_index는 예제1과 같이 \\([2,|{\\cal E}|]\\) 의 shape으로 넣어야 한다. 그렇지 않으면 에러가 난다.\n\nedge_index = torch.tensor([[0, 1],\n                           [1, 0],\n                           [1, 2],\n                           [2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index)\n\n\n#data.validate(raise_on_error=True)\ndata.validate()\n\nValueError: 'edge_index' needs to be of shape [2, num_edges] in 'Data' (found torch.Size([4, 2]))"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls1.html#예제3-예제2의-수정",
    "href": "posts/2_Studies/PyG/ls1.html#예제3-예제2의-수정",
    "title": "[PyG] lesson1: 자료형",
    "section": "예제3: 예제2의 수정",
    "text": "예제3: 예제2의 수정\n- edge_index의 shape이 \\([|{\\cal E}|,2]\\) 꼴로 저장되어 있었을 경우 트랜스포즈이후 countiguous()함수를 사용하면 된다.1\n1 그런데 그냥 transpose만 해도되는것 같음\nedge_index = torch.tensor([[0, 1],\n                           [1, 0],\n                           [1, 2],\n                           [2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n\ndata = torch_geometric.data.Data(\n    x=x, \n    edge_index=edge_index.t().contiguous()\n)\n\n\n#data.validate(raise_on_error=True)\ndata.validate()\n\nTrue"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls4.html",
    "href": "posts/2_Studies/PyG/ls4.html",
    "title": "[PyG] lesson4: Data Transform???",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls4.ipynb\n\n\nRef\n\nhttps://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n\n\n\n데이터 변환\n\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'])\n\ndataset[0]\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])\n\n\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'],\n                    pre_transform=T.KNNGraph(k=6))\n\ndataset[0]\n\n/home/cgb2/anaconda3/envs/pyg/lib/python3.10/site-packages/torch_geometric/data/dataset.py:209: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, make sure to delete '/tmp/ShapeNet/processed' first\n  warnings.warn(\n\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])\n\n\n\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import ShapeNet\n\ndataset = ShapeNet(root='/tmp/ShapeNet', categories=['Airplane'],\n                    pre_transform=T.KNNGraph(k=6),\n                    transform=T.RandomJitter(0.01))\n\ndataset[0]\n\nData(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#abstract",
    "href": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#abstract",
    "title": "[Review] 사기탐지 + 그래프 관련연구",
    "section": "Abstract",
    "text": "Abstract\n이 논문에서는 최근 몇 년간 그래프 신경망(Graph Neural Networks, GNNs)이 사기 탐지 문제에 널리 적용되어, 다양한 관계를 통해 이웃 정보를 집계함으로써 노드의 수상성을 밝혀내는 것을 소개하고 있습니다. 그러나 몇몇 이전 연구들은 사기꾼들의 가장 큰 문제인 위장 행위(camouflage behavior)에 주목하지 않았습니다. 이러한 위장 행위는 GNN 기반 사기 탐지기의 집계 과정에서 성능을 저하시킬 수 있습니다. 따라서 이 논문에서는 최근의 경험적 연구를 기반으로 두 가지 유형의 위장 행위, 즉 특성 위장과 관계 위장을 소개하고 있습니다. 기존의 GNN은 이러한 두 가지 위장 행위에 대응하지 않았기 때문에 사기 탐지 문제에서 성능이 떨어지는 것입니다. 이에 대응하여 새로운 모델인 CAmouflage-REsistant GNN (CARE-GNN)을 제안하고 있으며, 이 모델은 위장 행위에 대응하기 위해 세 가지 독특한 모듈을 포함하고 있습니다. 구체적으로, 먼저 정보성이 있는 이웃 노드를 찾기 위해 레이블 기반 유사도 측정 방법을 고안합니다. 그런 다음, 강화 학습 (Reinforcement Learning, RL)을 활용하여 선택할 최적의 이웃 수를 결정합니다. 마지막으로, 다양한 관계 사이에서 선택된 이웃들을 함께 집계합니다. 두 개의 실제 사기 데이터셋에 대한 포괄적인 실험을 통해 RL 알고리즘의 효과를 입증하였습니다. 제안된 CARE-GNN은 최첨단 GNN 및 GNN 기반 사기 탐지기보다 뛰어난 성능을 보여줍니다. 또한, 모든 GNN 기반 사기 탐지기를 통합하여 오픈 소스 도구 상자(https://github.com/YingtongDou/CARE-GNN)로 제공하고 있습니다. CARE-GNN 코드와 데이터셋을 이용할 수 있습니다.]"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#introduction",
    "href": "posts/2_Studies/Reviews/2023-07-19-사기탐지 + 그래프 관련연구.html#introduction",
    "title": "[Review] 사기탐지 + 그래프 관련연구",
    "section": "Introduction",
    "text": "Introduction\n인터넷 서비스의 번창과 함께 다양한 유형의 사기 행위도 발생하고 있습니다 [14]. 사기꾼들은 일반 사용자로 위장하여 안티 사기 시스템을 우회하고 불명확한 정보를 퍼뜨리거나 최종 사용자의 개인정보를 빼앗습니다 [32]. 이러한 사기 행위를 탐지하기 위해 그래프 기반 방법이 학계 [7, 21, 38]와 산업계 [2, 28, 50] 모두에서 효과적인 접근 방법이 되었습니다. 그래프 기반 방법은 서로 다른 관계로 엔티티를 연결하고, 동일한 목표를 가진 사기꾼들은 서로 연결되기 때문에 이러한 엔티티들의 수상성을 그래프 수준에서 드러낼 수 있습니다 [1].\n최근에는 그래프 신경망(Graph Neural Networks, GNNs)의 발전으로 많은 GNN 기반 사기 탐지기들이 제안되었습니다. (예: GCN [17], GAT [34], 그리고 GraphSAGE [12]) 이들은 의견 사기 [19, 25, 39], 금융 사기 [23, 24, 37], 모바일 사기 [41], 그리고 사이버 범죄 [48]를 탐지하기 위해 사용됩니다. 기존의 전통적인 그래프 기반 접근 방법과는 달리, GNN 기반 방법은 이웃 정보를 집계하여 중심 노드의 표현을 학습합니다. 이들은 end-to-end 및 반지도 학습 방식으로 훈련될 수 있으며, 이는 많은 특성 엔지니어링과 데이터 주석 비용을 절약할 수 있습니다.\n그러나 기존의 GNN 기반 사기 탐지 연구들은 GNN을 제한적인 범위에서만 적용하면서 사기꾼들의 위장 행위를 무시하고 있습니다. 이러한 위장 행위는 연구자들 [8, 15, 16, 49]과 실무자들 [2, 19, 41] 양쪽에서 큰 관심을 받고 있습니다. 한편, 이론적인 연구들은 그래프에 노이즈가 있는 노드와 엣지가 있는 경우 GNN의 한계와 취약점을 입증하고 있습니다 [3, 4, 13, 33]. 따라서, 위장된 사기꾼들에 대응하지 못한다면 GNN 기반 사기 탐지기의 성능을 저하시킬 수 있습니다. 최근 몇몇 연구들 [4, 9, 13, 25, 41]은 비슷한 도전에 주목했지만, 이들의 해결책은 사기 탐지 문제에 적합하지 않거나 GNN의 end-to-end 학습 방식을 파괴하는 경우가 있습니다."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-003-타이타닉, 첫 제출.html#a.-test-답을-모름-제출해야-알-수-있음",
    "href": "posts/2_Studies/MP/02wk-003-타이타닉, 첫 제출.html#a.-test-답을-모름-제출해야-알-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "A. test – 답을 모름, 제출해야 알 수 있음",
    "text": "A. test – 답을 모름, 제출해야 알 수 있음\n- 제출결과는 리더보드에서 확인할 수 있음."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-003-타이타닉, 첫 제출.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "href": "posts/2_Studies/MP/02wk-003-타이타닉, 첫 제출.html#b.-train-스스로-풀어보고-채점할-수-있음",
    "title": "02wk-003: 타이타닉, 첫 제출",
    "section": "B. train – 스스로 풀어보고 채점할 수 있음",
    "text": "B. train – 스스로 풀어보고 채점할 수 있음\n- 캐글에서 code \\(\\to\\) New Notebook 클릭\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n\ntr=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntst=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# 예비학습 – accuracy의 계산\n\nimport pandas as pd \n\n\ndf = pd.DataFrame({'Surv':[1,0,1,1,0],'Sex':['f','m','f','m','m']})\n\n- Surv 열의 선택\n\ndf.Surv\n\n0    1\n1    0\n2    1\n3    1\n4    0\nName: Surv, dtype: int64\n\n\n- Sex 열의 선택\n\ndf.Sex\n\n0    f\n1    m\n2    f\n3    m\n4    m\nName: Sex, dtype: object\n\n\n- Sex == f이면 생존(1), 그렇지 않으면 사망(0)이라고 예측\n\n(df.Sex == 'f')*1\n\n0    1\n1    0\n2    1\n3    0\n4    0\nName: Sex, dtype: int64\n\n\n- 결과를 정리하면 아래와 같음\n\npd.DataFrame({'real': df.Surv, 'estimate': (df.Sex == 'f')*1})\n\n\n\n\n\n\n\n\nreal\nestimate\n\n\n\n\n0\n1\n1\n\n\n1\n0\n0\n\n\n2\n1\n1\n\n\n3\n1\n0\n\n\n4\n0\n0\n\n\n\n\n\n\n\n- accuracy를 손으로 계산하면 \\(\\frac{4}{5}=0.8\\).\n- 컴퓨터로 accuracy를 계산한다면\n\n(df.Surv == (df.Sex == 'f')*1).sum()/5 # 방법1\n\n0.8\n\n\n\n(df.Surv == (df.Sex == 'f')*1).mean() # 방법2\n\n0.8\n\n\n#\n- 실제자료의 accuracy를 구해보자.\n\ntr = pd.read_csv('./titanic/train.csv')\n\n\n(tr.Survived == (tr.Sex == 'female')).mean()\n\n0.7867564534231201"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html",
    "title": "02wk-006: 타이타닉, 피처엔지니어링",
    "section": "",
    "text": "from autogluon.tabular import TabularDataset, TabularPredictor"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#a.-데이터",
    "title": "02wk-006: 타이타닉, 피처엔지니어링",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 불러오기\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, 피처엔지니어링",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_162557/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#c.-적합fit",
    "title": "02wk-006: 타이타닉, 피처엔지니어링",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_162557/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.30 GB / 490.57 GB (62.4%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    126454.6 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.17s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.55s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.72s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.2s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.41s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.57s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_162557/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f6db8118f40&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.029358  1.749107                0.029358           1.749107            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.030523  2.127434                0.001165           0.378327            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.080968  2.807139                0.080968           2.807139            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029404  1.393675                0.029404           1.393675            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.029581  0.459240                0.029581           0.459240            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.060613  1.018996                0.060613           1.018996            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.091852  4.290427                0.091852           4.290427            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.074267  0.364517                0.074267           0.364517            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.081210  0.296924                0.081210           0.296924            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.029181  0.403180                0.029181           0.403180            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.079142  0.342071                0.079142           0.342071            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.100002  0.599436                0.100002           0.599436            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001733  0.003381                0.001733           0.003381            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.004235  0.084162                0.004235           0.084162            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.029358\n1.749107\n0.029358\n1.749107\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.030523\n2.127434\n0.001165\n0.378327\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.080968\n2.807139\n0.080968\n2.807139\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029404\n1.393675\n0.029404\n1.393675\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.029581\n0.459240\n0.029581\n0.459240\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.060613\n1.018996\n0.060613\n1.018996\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.091852\n4.290427\n0.091852\n4.290427\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.074267\n0.364517\n0.074267\n0.364517\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.081210\n0.296924\n0.081210\n0.296924\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.029181\n0.403180\n0.029181\n0.403180\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.079142\n0.342071\n0.079142\n0.342071\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.100002\n0.599436\n0.100002\n0.599436\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001733\n0.003381\n0.001733\n0.003381\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.004235\n0.084162\n0.004235\n0.084162\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, 피처엔지니어링.html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, 피처엔지니어링",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n#tst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01_invalidformat.csv\")\ntst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-007-아나콘다 가상환경 설정.html",
    "href": "posts/2_Studies/MP/01wk-007-아나콘다 가상환경 설정.html",
    "title": "01wk-007: 아나콘다 가상환경 설정",
    "section": "",
    "text": "강의영상\n\n\n가상환경 설정\nconda create -n mp python=3.10\nconda activate mp\npip install autogluon && pip install autogluon.tabular[tabpfn,vowpalwabbit,imodels,skex,skl2onnx]\npip install pycaret[full]\nconda install -c huggingface transformers\npip install kaggle \nconda install -c conda-forge notebook"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "",
    "text": "Logging into Kaggle for the first time can be daunting. Our competitions often have large cash prizes, public leaderboards, and involve complex data. Nevertheless, we really think all data scientists can rapidly learn from machine learning competitions and meaningfully contribute to our community. To give you a clear understanding of how our platform works and a mental model of the type of learning you could do on Kaggle, we’ve created a Getting Started tutorial for the Titanic competition. It walks you through the initial steps required to get your first decent submission on the leaderboard. By the end of the tutorial, you’ll also have a solid understanding of how to use Kaggle’s online coding environment, where you’ll have trained your own machine learning model.\nSo if this is your first time entering a Kaggle competition, regardless of whether you: - have experience with handling large datasets, - haven’t done much coding, - are newer to data science, or - are relatively experienced (but are just unfamiliar with Kaggle’s platform),\nyou’re in the right place!\n\n\nIn this section, you’ll learn more about the competition and make your first submission.\n\n\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\n\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\n\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died.\n\n\n\n\n\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\n\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\n\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\n\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\n#train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data = pd.read_csv(\"./titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\n#test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data = pd.read_csv(\"./titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)\n\n\n\n\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\n\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us.\n\n\n\n\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!\n\n\n\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-1-get-started",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-1-get-started",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 1: Get started",
    "text": "Part 1: Get started\nIn this section, you’ll learn more about the competition and make your first submission.\n\nJoin the competition!\nThe first thing to do is to join the competition! Open a new window with the competition page, and click on the “Join Competition” button, if you haven’t already. (If you see a “Submit Predictions” button instead of a “Join Competition” button, you have already joined the competition, and don’t need to do so again.)\n\nThis takes you to the rules acceptance page. You must accept the competition rules in order to participate. These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details. Then, click on “I Understand and Accept” to indicate that you will abide by the competition rules.\n\n\nThe challenge\nThe competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\n\n\nThe data\nTo take a look at the competition data, click on the Data tab at the top of the competition page. Then, scroll down to find the list of files.\nThere are three files in the data: (1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n(1) train.csv\ntrain.csv contains the details of a subset of the passengers on board (891 passengers, to be exact – where each passenger gets a different row in the table). To investigate this data, click on the name of the file on the left of the screen. Once you’ve done this, you can view all of the data in the window.\n\nThe values in the second column (“Survived”) can be used to determine whether each passenger survived or not: - if it’s a “1”, the passenger survived. - if it’s a “0”, the passenger died.\nFor instance, the first passenger listed in train.csv is Mr. Owen Harris Braund. He was 22 years old when he died on the Titanic.\n\n\n(2) test.csv\nUsing the patterns you find in train.csv, you have to predict whether the other 418 passengers on board (in test.csv) survived.\nClick on test.csv (on the left of the screen) to examine its contents. Note that test.csv does not have a “Survived” column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition!\n\n\n(3) gender_submission.csv\nThe gender_submission.csv file is provided as an example that shows how you should structure your predictions. It predicts that all female passengers survived, and all male passengers died. Your hypotheses regarding survival will probably be different, which will lead to a different submission file. But, just like this file, your submission should have: - a “PassengerId” column containing the IDs of each passenger from test.csv. - a “Survived” column (that you will create!) with a “1” for the rows where you think the passenger survived, and a “0” where you predict that the passenger died."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-2-your-coding-environment",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-2-your-coding-environment",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 2: Your coding environment",
    "text": "Part 2: Your coding environment\nIn this section, you’ll train your own machine learning model to improve your predictions. If you’ve never written code before or don’t have any experience with machine learning, don’t worry! We don’t assume any prior experience in this tutorial.\n\nThe Notebook\nThe first thing to do is to create a Kaggle Notebook where you’ll store all of your code. You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer. (If you are interested in deep learning, we also offer free GPU access!)\nBegin by clicking on the Code tab on the competition page. Then, click on “New Notebook”.\n\nYour notebook will take a few seconds to load. In the top left corner, you can see the name of your notebook – something like “kernel2daed3cd79”.\n\nYou can edit the name by clicking on it. Change it to something more descriptive, like “Getting Started with Titanic”.\n\n\n\nYour first lines of code\nWhen you start a new notebook, it has two gray boxes for storing code. We refer to these gray boxes as “code cells”.\n\nThe first code cell already has some code in it. To run this code, put your cursor in the code cell. (If your cursor is in the right place, you’ll notice a blue vertical line to the left of the gray box.) Then, either hit the play button (which appears to the left of the blue line), or hit [Shift] + [Enter] on your keyboard.\nIf the code runs successfully, three lines of output are returned. Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nThis shows us where the competition data is stored, so that we can load the files into the notebook. We’ll do that next.\n\n\nLoad the data\nThe second code cell in your notebook now appears below the three lines of output with the file locations.\n\nType the two lines of code below into your second code cell. Then, once you’re done, either click on the blue play button, or hit [Shift] + [Enter].\n\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntrain_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\nYour code should return the output above, which corresponds to the first five rows of the table in train.csv. It’s very important that you see this output in your notebook before proceeding with the tutorial! &gt; If your code does not produce this output, double-check that your code is identical to the two lines above. And, make sure your cursor is in the code cell before hitting [Shift] + [Enter].\nThe code that you’ve just written is in the Python programming language. It uses a Python “module” called pandas (abbreviated as pd) to load the table from the train.csv file into the notebook. To do this, we needed to plug in the location of the file (which we saw was /kaggle/input/titanic/train.csv).\n&gt; If you’re not already familiar with Python (and pandas), the code shouldn’t make sense to you – but don’t worry! The point of this tutorial is to (quickly!) make your first submission to the competition. At the end of the tutorial, we suggest resources to continue your learning.\nAt this point, you should have at least three code cells in your notebook.\n\nCopy the code below into the third code cell of your notebook to load the contents of the test.csv file. Don’t forget to click on the play button (or hit [Shift] + [Enter])!\n\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nAs before, make sure that you see the output above in your notebook before continuing.\nOnce all of the code runs successfully, all of the data (in train.csv and test.csv) is loaded in the notebook. (The code above shows only the first 5 rows of each table, but all of the data is there – all 891 rows of train.csv and all 418 rows of test.csv!)"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-3-your-first-submission",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-3-your-first-submission",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 3: Your first submission",
    "text": "Part 3: Your first submission\nRemember our goal: we want to find patterns in train.csv that help us predict whether the passengers in test.csv survived.\nIt might initially feel overwhelming to look for patterns, when there’s so much data to sort through. So, we’ll start simple.\n\nExplore a pattern\nRemember that the sample submission file in gender_submission.csv assumes that all female passengers survived (and all male passengers died).\nIs this a reasonable first guess? We’ll check if this pattern holds true in the data (in train.csv).\nCopy the code below into a new code cell. Then, run the cell.\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)/len(women)\n\nprint(\"% of women who survived:\", rate_women)\n\n% of women who survived: 0.7420382165605095\n\n\n\n여성의 생존률을 구하는 코드입니다, 이전에 accuracy를 구하던 테크닉을 활용하면 아래의 코드도 가능합니다\n\ntrain_data[train_data.Sex == 'female'].Survived.mean()\n\n0.7420382165605095\n\n\n\nBefore moving on, make sure that your code returns the output above. The code above calculates the percentage of female passengers (in train.csv) who survived.\nThen, run the code below in another code cell:\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)/len(men)\n\nprint(\"% of men who survived:\", rate_men)\n\n% of men who survived: 0.18890814558058924\n\n\nThe code above calculates the percentage of male passengers (in train.csv) who survived.\nFrom this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in gender_submission.csv is not a bad first guess!\nBut at the end of the day, this gender-based submission bases its predictions on only a single column. As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we’ll use machine learning to automate this for us."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#your-first-machine-learning-model",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#your-first-machine-learning-model",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Your first machine learning model",
    "text": "Your first machine learning model\nWe’ll build what’s known as a random forest model. This model is constructed of several “trees” (there are three trees in the picture below, but we’ll construct 100!) that will individually consider each passenger’s data and vote on whether the individual survived. Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\n\nThe code cell below looks for patterns in four different columns (“Pclass”, “Sex”, “SibSp”, and “Parch”) of the data. It constructs the trees in the random forest model based on patterns in the train.csv file, before generating predictions for the passengers in test.csv. The code also saves these new predictions in a CSV file submission.csv.\nCopy this code into your notebook, and run it in a new code cell.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\nMake sure that your notebook outputs the same message above (Your submission was successfully saved!) before moving on. &gt; Again, don’t worry if this code doesn’t make sense to you! For now, we’ll focus on how to generate and submit predictions.\nOnce you’re ready, click on the “Save Version” button in the top right corner of your notebook. This will generate a pop-up window.\n- Ensure that the “Save and Run All” option is selected, and then click on the “Save” button. - This generates a window in the bottom left corner of the notebook. After it has finished running, click on the number to the right of the “Save Version” button. This pulls up a list of versions on the right of the screen. Click on the ellipsis (…) to the right of the most recent version, and select Open in Viewer.\n- Click on the Data tab on the top of the screen. Then, click on the “Submit” button to submit your results.\n\nCongratulations for making your first submission to a Kaggle competition! Within ten minutes, you should receive a message providing your spot on the leaderboard. Great work!"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-4-learn-more",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#part-4-learn-more",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "Part 4: Learn more!",
    "text": "Part 4: Learn more!\nIf you’re interested in learning more, we strongly suggest our (3-hour) Intro to Machine Learning course, which will help you fully understand all of the code that we’ve presented here. You’ll also know enough to generate even better predictions!"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#a.-alexis-cook의-분석은-train에서-얼마나-잘-맞출까",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?",
    "text": "A. Alexis Cook의 분석은 train에서 얼마나 잘 맞출까?\n- 원래코드\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission_AlexisCook.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nYour submission was successfully saved!\n\n\n\nlen(predictions), len(X_test)\n\n(418, 418)\n\n\n- 이렇게 수정하면 될 듯\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\nlen(predictions),len(y)\n\n(891, 891)\n\n\n\n(predictions == y).mean()\n\n0.8159371492704826"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#b.-alexis-cook의-코드를-수정해보자",
    "href": "posts/2_Studies/MP/02wk-004-타이타닉, Alexis Cook의 코드.html#b.-alexis-cook의-코드를-수정해보자",
    "title": "02wk-004: 타이타닉, Alexis Cook의 코드",
    "section": "B. Alexis Cook의 코드를 수정해보자!",
    "text": "B. Alexis Cook의 코드를 수정해보자!\n- 코드를 수정해보자.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=5000, max_depth=1000, random_state=1)\nmodel.fit(X, y)\n\n####\n\npredictions = model.predict(X)\n\n\n(predictions == y).mean()\n\n0.8170594837261503\n\n\n\n내가 만든게 더 좋은데??\n\n- 이것도 제출결과로 만들어보자.\n\npredictions = model.predict(X_test)\n\n- 아래와 같이 제출하면 에러가 발생\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정_submission.csv\")\n\n- 아래와 같이 제출파일을 저장해야 한다.\n\npd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\\\n.assign(Survived=predictions)\\\n.to_csv(\"AlexisCook수정2_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step1",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step1",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step1",
    "text": "Step1\n- 계정페이지 https://www.kaggle.com/settings/account \\(\\to\\) API \\(\\to\\) Create New Token"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step2",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step2",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step2",
    "text": "Step2\n- 다운로드한 kaggle.json 파일을 업로드"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step3",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step3",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step3",
    "text": "Step3\n- 코랩에서 아래를 실행\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \".\"\n\n\n!kaggle competitions download -c titanic\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\nDownloading titanic.zip to /home/cgb2/Dropbox/03_yechan3/posts/1_Essays\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.65MB/s]\n\n\n\n!unzip titanic.zip -d ./data\n\nArchive:  titanic.zip\n  inflating: ./data/gender_submission.csv  \n  inflating: ./data/test.csv         \n  inflating: ./data/train.csv        \n\n\n\n!rm bike-sharing-demand.zip\n!rm -rf data\n\nrm: cannot remove 'bike-sharing-demand.zip': No such file or directory\n\n\n\n!ls ./data \n\nsampleSubmission.csv  test.csv  train.csv"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step1-1",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step1-1",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step1",
    "text": "Step1\n- 계정페이지 https://www.kaggle.com/settings/account \\(\\to\\) API \\(\\to\\) Create New Token"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step2-1",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step2-1",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step2",
    "text": "Step2\n- 다운로드한 kaggle.json 파일을 업로드"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step3-1",
    "href": "posts/2_Studies/MP/01wk-006-코랩에서 분석.html#step3-1",
    "title": "01wk-006: 코랩에서 분석",
    "section": "Step3",
    "text": "Step3\n- 코랩에서 아래를 실행\n\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \".\"\n\n\n!kaggle competitions download -c titanic\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 ./kaggle.json'\nDownloading titanic.zip to /home/cgb2/Dropbox/03_yechan3/posts/1_Essays\n  0%|                                               | 0.00/34.1k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 5.65MB/s]\n\n\n\n!unzip titanic.zip -d ./data\n\nArchive:  titanic.zip\n  inflating: ./data/gender_submission.csv  \n  inflating: ./data/test.csv         \n  inflating: ./data/train.csv        \n\n\n\n!rm bike-sharing-demand.zip\n!rm -rf data\n\nrm: cannot remove 'bike-sharing-demand.zip': No such file or directory\n\n\n\n!ls ./data \n\nsampleSubmission.csv  test.csv  train.csv"
  },
  {
    "objectID": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "href": "posts/2_Studies/PyTorchLightning/2022-11-29-Lesson1.html",
    "title": "[PL] Lesson1: 단순선형회귀",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport pytorch_lightning as pl \n\n\nref\nref: https://guebin.github.io/DL2022/posts/II.%20DNN/2022-09-20-3wk-2.html\n\n\nRegression 1: CPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1)\n\nGPU available: True (cuda), used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n/home/cgb4/anaconda3/envs/py37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1767: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n  category=PossibleUserWarning,\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')\n\n\n\n\n\n\nRegression 2: GPU\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\")\n\n\nx= torch.tensor(df.x).reshape(-1,1).float()\ny= torch.tensor(df.y).reshape(-1,1).float()\n\n\nds = torch.utils.data.TensorDataset(x,y)\ndl = torch.utils.data.DataLoader(ds,batch_size=100)\n\n\nclass NetLO(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linr = torch.nn.Linear(1,1)\n        self.loss_fn = torch.nn.MSELoss()\n    def forward(self,x):\n        yhat = self.linr(x)\n        return yhat\n    def configure_optimizers(self):\n        optimizr = torch.optim.SGD(self.parameters(), lr=0.1)\n        return optimizr \n    def training_step(self,batch,batch_idx):\n        x,y = batch\n        yhat = self(x)\n        loss = self.loss_fn(yhat,y) \n        return loss \n\n\nnet = NetLO()\n\n\ntrnr = pl.Trainer(max_epochs=1, accelerator='gpu', devices=1)\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n\n\n\nnet.linr.bias.data = torch.tensor([-5.0])\nnet.linr.weight.data = torch.tensor([[10.0]])\n\n\ntrnr.fit(net, train_dataloaders=dl) \n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name    | Type    | Params\n------------------------------------\n0 | linr    | Linear  | 2     \n1 | loss_fn | MSELoss | 0     \n------------------------------------\n2         Trainable params\n0         Non-trainable params\n2         Total params\n0.000     Total estimated model params size (MB)\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\nnet.linr.weight, net.linr.bias\n\n(Parameter containing:\n tensor([[8.8111]], requires_grad=True),\n Parameter containing:\n tensor([-3.6577], requires_grad=True))\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#kronecker-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Kronecker product",
    "text": "Kronecker product\n크로네커곱의 정의는 아래와 같다.\n\\[{\\bf A} \\otimes {\\bf B}\n=\\begin{bmatrix}\na_{11}{\\bf B} & a_{12}{\\bf B} & \\dots & a_{1m}{\\bf B} \\\\\na_{21}{\\bf B} & a_{22}{\\bf B} & \\dots & a_{2m}{\\bf B} \\\\\n\\dots & \\dots & \\dots & \\dots \\\\\na_{n1}{\\bf B} & a_{n2}{\\bf B} & \\dots & a_{nm}{\\bf B} \\\\\n\\end{bmatrix}\\]\n두 행렬 \\({\\bf A}_{m\\times n}\\), \\({\\bf B}_{p\\times q}\\)의 크로네커곱 \\({\\bf A}\\otimes {\\bf B}\\)의 차원은 \\(mp \\times nq\\) 가 된다. 계산예시는 아래와 같다.\n\n\n\n위키에서 긁은 예제, 글씨가 좀 작음\n\n\n크로네커곱에 대한 성질들이 위키에 많이 있으니 참고하면 좋다.\n(예제1)\n\nA= [1 2\n    3 4]\nB= [0 5\n    6 7]\nC = kron(A, B)\n\n4×4 Matrix{Int64}:\n  0   5   0  10\n  6   7  12  14\n  0  15   0  20\n 18  21  24  28\n\n\n(예제2)\n\nA= [1 -4 7; -2 3 3]\nB= [8 -9 -6 -5; 1 -3 -4 7; 2 8 -8 -3; 1 2 -5 -1]\nC = kron(A, B)\n\n8×12 Matrix{Int64}:\n   8   -9  -6   -5  -32   36   24   20  56  -63  -42  -35\n   1   -3  -4    7   -4   12   16  -28   7  -21  -28   49\n   2    8  -8   -3   -8  -32   32   12  14   56  -56  -21\n   1    2  -5   -1   -4   -8   20    4   7   14  -35   -7\n -16   18  12   10   24  -27  -18  -15  24  -27  -18  -15\n  -2    6   8  -14    3   -9  -12   21   3   -9  -12   21\n  -4  -16  16    6    6   24  -24   -9   6   24  -24   -9\n  -2   -4  10    2    3    6  -15   -3   3    6  -15   -3"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#khatrirao-product",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Khatri–Rao product",
    "text": "Khatri–Rao product\n카트리-라오곱은 매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 같은 차원의 블락매트릭스로 정의될때 각 서브매트릭스의 크로네커 곱으로 정의된다. 정의와 계산예시는 아래와 같다.\n\n\n\n예시1: 위키에서 긁은 그림\n\n\n또 다른 계산예시는 아래와 같다. 이 예제는 중요하니까 구현해보자.\n\n\n\n예시2: 위키에서 긁은 그림\n\n\n(예제1)\n\nC= [1 2 3 \n    4 5 6 \n    7 8 9] \nD= [1 4 7\n    2 5 8\n    3 6 9]\n\n3×3 Matrix{Int64}:\n 1  4  7\n 2  5  8\n 3  6  9\n\n\n\nhcat([kron(C[:,i],D[:,i]) for i in 1:3]...)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81\n\n\n이건 자주 쓸일이 있을것 같으니까 함수로 저장하자.\n\ncolumnwise_kron = \n(C,D) -&gt; hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#181 (generic function with 1 method)\n\n\n\ncolumnwise_kron(C,D)\n\n9×3 Matrix{Int64}:\n  1   8  21\n  2  10  24\n  3  12  27\n  4  20  42\n  8  25  48\n 12  30  54\n  7  32  63\n 14  40  72\n 21  48  81"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프-표현",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프 표현",
    "text": "그래프 표현\n아래의 그림을 살펴보자.\n\n\n\n그래프의 개념을 이해하는 필요한 그림, 일단 오른쪽의 \\({\\bf S}\\)는 무시할 것\n\n\n오른쪽의 \\({\\bf S}\\)는 무시하고 왼쪽의 그래프만 살펴보자. 이 그림에는 6개의 노드가 있고 각각의 노드는 저 마다의 연결구조를 가진다. 이러한 연결구조는 \\({\\bf G}=({\\bf N},{\\bf E})\\) 으로 표현할 수 있는데 여기에서 \\({\\bf N}\\)은 노드들의 집합이고 \\({\\bf E}\\)는 엣지들의 집합이다.1 보통 \\({\\cal E}\\)는 복잡하므로 연결정보를 매트릭스 \\({\\bf E}\\)로 표현하는데 이러한 \\({\\bf E}\\)를 인접행렬이라고 부른다. 인접행렬의 각 원소는 \\(E_{ij}= \\begin{cases} 1 & (i,j) \\in {\\cal E} \\\\ 0 & o.w \\end{cases}\\) 와 같이 정의한다. 이 그림의 경우 \\({\\cal N}\\) 와 \\({\\cal E}\\), \\({\\bf E}\\) 는 아래와 같다.\n1 노드 \\(i\\)에서 노드 \\(j\\)로 향하는 연결이 있다면 \\((i,j) \\in {\\cal E}\\)이다.\n\\({\\cal N}=\\{1,2,3,4,5,6\\}\\)\n\\({\\bf E}=\\begin{bmatrix} 0 & 1 & 0 & 0 & 1 & 0 \\\\ 1 & 0 & 1 & 0 & 1 & 0\\\\ 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\({\\cal E} = \\{(i,j) : E_{ij}=1 \\}\\)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#gso",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "GSO",
    "text": "GSO\n후에 자세히 서술하겠지만 전통적인 시계열분석기법을 그래프신호로 확장하기 위해서는 단지 퓨리에변환 대신에 그래프퓨리에 변환을 사용하면 된다. 즉 퓨리에변환을 일반화한 그래프퓨리에변환을 잘 정의하면 된다.\n전통적인 신호처리 영역에서의 퓨리에변환은 시계열자료의 인접성을 의미하는 행렬 \\({\\bf B}\\)2의 고유행렬의 켤레전치로 정의할 수 있다. 이를 이용하면 그래프 퓨리에변환은 그래프자료의 인접성을 의미하는 행렬3의 고유행렬의 켤레전치로 정의할 수 있음을 유추할 수 있다. 즉 비유클리드 자료에서도 \\({\\bf B}\\)에 대응하는 어떠한 매트릭스가 정의되어야 하는데 (그리고 이 매트릭스는 그래프자료의 인접성에 대한 정보가 있어야 한다) 이 매트릭스를 \\({\\bf S}\\)라고 정의하고 grahp shift operator (GSO) 라고 이름 붙인다.\n2 원래는 평행이동을 의미하는 행렬이지만, 이걸 인접성을 의미하는 행렬로 해석할 수도 있다. 어차피 인접한 곳으로 이동할 수 있으니까..3 예를들면 인접행렬 \\({\\bf E}\\)와 같은 행렬주어진 그래프 \\({\\cal G}=({\\cal N},{\\cal E})\\) 에 대하여 GSO \\({\\bf S}\\)는 \\({\\bf E}+{\\bf I}\\)의 값이 1인 영역에만 값이 있는 어떠한 행렬이다. 다시 아래의 그림을 생각하여 보자.\n\n\n\nGSO의 개념을 이해하는데 필요한 그림\n\n\n왼쪽그래프의 GSO는 오른쪽과 같은 행렬 \\({\\bf S}\\)가 된다. 이제 \\({\\bf S}\\) 의 고유벡터행렬을 구한 뒤에 그것의 켤레전치를 \\({\\bf GFT}\\) 행렬로 정의하면 될 것 같다. 문제는 “\\({\\bf S}\\)의 고유벡터행렬이 항상 존재하는가?” 인데, 사실 이게 항상 존재한다는 보장이 없다. 즉 \\({\\bf S}\\)의 고유벡터 행렬이 존재 안할 수도 있다. 따라서 GSO \\({\\bf S}\\)가 고유분해가능하다는 조건이 추가적으로 필요한데 이러한 조건을 만족하는 GSO를 normal GSO라고 부른다. 우리는 당연히 normal GSO에 대해서만 관심이 있으므로 앞으로 특별한 언급이 없는한 GSO는 모두 normal GSO라고 가정한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#periodogram-correlogram-and-ls-estimator",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "Periodogram, correlogram, and LS estimator",
    "text": "Periodogram, correlogram, and LS estimator\nFrom \\({\\bf C}_{\\tilde{\\bf x}}:= \\mathbb{E}\\left[\\tilde{\\bf x}\\tilde{\\bf x}^H \\right]=\\mathbb{E}\\left[({\\bf V}^H{\\bf x})({\\bf V}^H{\\bf x})^H \\right]=\\text{diag}({\\bf p})\\) it follows that one may express the PSD as \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\). That is, the PSD is given by the expected value of the squared frequency components of the random process. This leads to a natural approach for the estimation of \\({\\bf p}\\) from a finite set of \\(R\\) realizations of the process \\({\\bf x}\\). Indeed, we compute the \\({\\bf GFT} \\tilde{\\bf x}_r = {\\bf V}^H{\\bf x}_r\\) of each observed signal \\({\\bf x}_r\\) and estimate \\({\\bf p}\\) as\n\\[\n\\hat{\\bf p}_{pg}:= \\frac{1}{R}\\sum_{r=1}^R|\\tilde{\\bf x}_r|^2=\\frac{1}{R}\\sum_{r=1}^{R}|{\\bf V}^H{\\bf x}_{r}|^2.\n\\]\nThe estimator \\(\\hat{\\bf p}_{pg}\\) is termed periodogram due to its evident similarity with its homonym5 in classical estimation. It is simple to show that \\({\\bf p}_{pg}\\) is an unbiased estimator, that is, \\(\\mathbb{E}[\\hat{\\bf p}_{pg}]= {\\bf p}\\). A more detailed analysis of the performance of \\(\\hat{\\bf p}_{pg}\\), for the case where the observations are Gaussian, is given in Proposition 12.1.6\n5 동음이의어6 Proposition 12.1은 뒤에 다루는데 \\(\\hat{\\bf p}_{pg}\\)의 분산에 대한 서술이 있음. 분산은 \\(\\mathbb{V}[\\hat{\\bf p}_{pg}]=\\frac{2}{R}\\text{diag}^2({\\bf p})\\)와 같음An alternative nonparametric estimation scheme, denominated correlogram, can be devised by starting from the definition of \\({\\bf p}\\) in\n\\[{\\bf p}:=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big).\\]\nNamely, one may substitute \\({\\bf C}_{\\bf x}\\) in above equation by the sample covariance \\(\\hat{\\bf C}_{\\bf x} = \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\) computed based on the available observations to obtain\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\nNotice that the matrix \\({\\bf V}^H\\hat{\\bf C}_{\\bf x}{\\bf V}\\) is in general, not diagonal because the eigenbasis of \\(\\hat{\\bf C}_{\\bf x}\\) differs from \\({\\bf V}\\), the eigenbasis of \\({\\bf C}_{\\bf x}\\). Nonetheless, we keep only the diagonal elements \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x}{\\bf v}_i\\) for \\(i = 1, \\dots , N\\) as our PSD estimator. It can be shown that the correlogram \\({\\bf p}_{cg}\\) and the periodogram \\({\\bf p}_{pg}\\) lead to identical estimators, as is the case in classical signal processing.\nThe correlogram can also be interpreted as an LS estimator. The decomposition in \\({\\bf C}_{\\bf x}={\\bf V}\\text{diag}({\\bf p}){\\bf V}^H\\) allows a linear parameterization of the covariance matrix \\({\\bf C}_{\\bf x}\\) as\n\\[\n{\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H.\n\\]\nThis linear parametrization will also be useful for the sampling schemes developed in Section 12.4. Vectorizing \\({\\bf C}_{\\bf x}\\) in \\({\\bf C}_{\\bf x}({\\bf p})=\\sum_{i=1}^N p_i{\\bf v}_i{\\bf v}_i^H\\) results in a set of \\(N^2\\) equations in \\({\\bf p}\\)\n\\[\n{\\bf c}_{\\bf x} = \\text{vec}({\\bf C}_{\\bf x})=\\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf G}_{np}{\\bf p},\n\\]\nwhere \\(\\text{vec}({\\bf v}_i{\\bf v}_i^H)={\\bf v}_i^\\ast \\otimes {\\bf v}_i\\). Relying on the Khatri-Rao product, we then form the \\(N^2 \\times N\\) matrix \\({\\bf G}_{np}\\) as\n\\[\n{\\bf G}_{np}:= \\left[{\\bf v}_1^\\ast \\otimes {\\bf v}_1, \\dots, {\\bf v}_N^\\ast \\otimes {\\bf v}_N \\right] = {\\bf V}^\\ast \\odot {\\bf V}.\n\\]\n\nHere \\(\\otimes\\) denote the Kronecker matrix product and \\(\\odot\\) denote the Khatri-Rao matrix product.\n\nUsing the sample covariance matrix \\(\\hat{\\bf C}_{\\bf x}\\) as an estimate of \\({\\bf C}_{\\bf x}\\), we can match the estimated covariance vector \\(\\hat{\\bf c}_{\\bf x}=\\text{vec}(\\hat{\\bf C}_{\\bf x})\\) to the true covariance vector \\({\\bf c}_{\\bf x}\\) in the LS sense as\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\nIn other words, the LS estimator minimizes the squared error \\(\\text{tr}\\left[\\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)^T \\big(\\hat{\\bf C}_{\\bf x} − \\hat{\\bf C}_{\\bf x}({\\bf p})\\big)\\right]\\). From expression \\(\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\) it can be shown that the \\(i\\)th element of \\(\\hat{\\bf p}_{ls}\\) is \\({\\bf v}_i^H \\hat{\\bf C}_{\\bf x} {\\bf v}_i\\). Combining this with Eq.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right]\\]\nwe get that the LS estimator \\(\\hat{\\bf p}_{ls}\\) and the correlogram \\(\\hat{\\bf p}_{cg}\\) —and hence the periodogram as well— are all identical estimators. The estimators derived in this subsection do not assume any data distribution and are well suited for cases where the data probability density function is not available. In what follows, we provide performance bounds for these estimators under the condition that the observed signals are Gaussian."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#정상시계열을-분석하는-두-가지-흐름-acf와-psd",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD",
    "text": "정상시계열을 분석하는 두 가지 흐름, ACF와 PSD\n\n전통적인 분석방법\n클래식한 정상시계열은 유한차수의 ARMA로 근사할 수 있음이 알려져 있다7. 유한차수의 ARMA의 계수 \\(p\\),\\(q\\)를 적절하게 추정하기 위해서는 시계열 \\({\\bf x}\\)를 SACF plot 혹은 SPACF plot 을 이용하면 된다. 이때 SACF 혹은 SPACF 의 그림을 살펴보고 적절한 모형을 선택하기 위해서는 유한차수 ARMA의 이론적 ACF의 모양을 알면 되는데,8 이를 바꾸어서 말하면 결국 정상시계열 \\({\\bf x}\\)의 모든 정보는 ACF에 들어있다는 의미가 된다. 즉 정상시계열은 ACF만 잘 추정하면 모든 것이 해결된다.\n7 Wold’s theorem8 예를들어 “coef가 0.9인 AR(1)의 경우 lag=1 에 대한 이론적 ACF값이 0.9, lag=2에 대한 ACF값이 0.81, … 와 같이 되더라~” 하는식의그런데 ACF의 모든 정보는 다시 아래의 행렬에 들어있다.\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^T]\\]\n여기에서 \\({\\bf x}\\)는 realization이 아니라 확률벡터를 의미함을 유의하자.9 따라서 정상시계열의 경우 \\({\\bf C}_{\\bf x}\\)를 잘 추정하면 모든것이 해결된다고 생각하면 된다.\n9 보통 수리통계에서는 확률변수를 \\(X\\) realization을 \\(x\\)로 표현하지만 여기에서는 매트릭스를 대문자로 쓰고 있어서 그런식으로 표현하기 어렵다, 그래서 그때 그때 이것이 확률변수인지 realization인지 따져봐야 한다\n참고: 정상시계열의 경우 ACF 만 정확하게 알아도 (반대로 PACF만 정확하게 알아도) 이론상 모든 모형을 특정할 수 있다. 즉 정상시계열의 모형을 특정하기 위해서는 ACF plot, PACF plot 하나만 있어도 충분하다. (Wold’s Thm은 떠올리면 모든 정상시계열은 무한MA로 유니크하게 표현할 수 있는데, 이는 PACF plot을 가지고 모든 정상시계열을 유니크하게 특정할 수 있다는 것을 의미한다) 다만 좀 더 모형을 특정하는 과정을 용이하게 하기 위해서 실전에서는 SACF plot 과 SPACF plot 을 함께 보는 것이 유리하다.\n\n(예제) AR(1) 모형\n왜 ACF의 모든정보를 \\({\\bf C}_{\\bf x}\\)로 부터 알수 있는지 코드를 통하여 실습하여 보자. (바로 이해된다면 사실 이 예제는 스킵해도 무방함) 아래와 같은 모형을 가정하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n여기에서 \\(\\epsilon_t\\)는 서로 독립인 표준정규분포를 따른다. 이 모형에서 길이가 100인 시계열을 임의로 발생시키자.\n\nx = zeros(100*1000)\nx[1] = randn()\nfor t in 2:100\n    x[t] = 0.5*x[t-1] + randn()\nend\n\n모형에서 생성된 하나의 시계열을 시각화 하면 아래와 같다.\n\nplot(x) # 그냥 그려본것임. 별 의미는 없음\n\n\n\n\nlag=1일 경우 이 시계열의 SACF를 계산하면 아래와 같다.\n\nx[1:99] .* x[2:100]\n\n99-element Vector{Float64}:\n  1.587897526021493\n  1.130306190921068\n  0.5698214432110668\n  0.4648189302568683\n  0.3099446153360606\n  0.36362604534744775\n  0.8191871414624922\n -0.1720390842292145\n -0.06301214708310766\n  0.026414715508855904\n -0.007988283356933327\n -0.04178812545299474\n  0.22453267567940685\n  ⋮\n  3.931333581073927\n  1.315564948810858\n  0.9096080102581454\n  0.5410986320348997\n  0.29627801400693676\n  1.0673283524686212\n -1.0394649044573636\n  2.80195248208142\n  4.152973765526384\n  2.316315764368524\n  0.978758337765867\n -0.5840281943972468\n\n\n\n이 계산결과는 각 \\(t\\)에 대하여 \\(x_{t-1}x_t\\) 를 계산한 것과 같다.\n\n이 수열들의 평균은 아래와 같다.\n\nx[1:99] .* x[2:100] |&gt; mean\n\n0.5835563885014224\n\n\n\n이 계산결과는 \\(\\frac{1}{99}\\sum_{t=2}^{100} x_{t-1}x_t\\)를 계산한 것과 같다.\n\n이론적인 값인 0.5 근처의 값이 잘 나옴을 알 수 있다.\nlag=2일 경우도 마찬가지로 구할 수 있다.\n\nx[1:98] .* x[3:100] |&gt; mean\n\n0.38420263596668275\n\n\n이러한 숫자들은 그런데 \\({\\bf x}{\\bf x}^T\\)를 이용하여서도 구할 수 있다.10\n10 참고로 여기에서 \\({\\bf x}\\)는 확률벡터가 아니라 realization을 의미함\nx*x'\n\n100×100 Matrix{Float64}:\n  0.760108    1.5879      0.541064   …  -1.57394    -0.472676    0.939172\n  1.5879      3.31719     1.13031       -3.28802    -0.987441    1.96197\n  0.541064    1.13031     0.385143      -1.12037    -0.336463    0.668527\n  0.800507    1.67229     0.569821      -1.65759    -0.497799    0.989089\n  0.441361    0.922022    0.314172      -0.913915   -0.274462    0.545336\n  0.533784    1.1151      0.379961   …  -1.10529    -0.331936    0.659531\n  0.517803    1.08171     0.368586      -1.0722     -0.321998    0.639786\n  1.20252     2.51212     0.855987      -2.49003    -0.747794    1.48581\n -0.108745   -0.227173   -0.0774074      0.225175    0.0676234  -0.134363\n  0.440444    0.920106    0.313519      -0.912016   -0.273892    0.544203\n  0.0455859   0.0952309   0.0324492  …  -0.0943935  -0.0283478   0.0563249\n -0.133198   -0.278257   -0.0948139      0.27581     0.0828298  -0.164577\n  0.238468    0.498169    0.169748      -0.493789   -0.148292    0.294646\n  ⋮                                  ⋱                          \n  2.04697     4.2762      1.45708       -4.2386     -1.27291     2.52919\n  0.488514    1.02053     0.347736      -1.01155    -0.303784    0.603596\n  1.41531     2.95665     1.00746    …  -2.93065    -0.880119    1.74873\n  0.290602    0.60708     0.206858      -0.601742   -0.180712    0.359062\n  0.774954    1.61891     0.551632      -1.60468    -0.481908    0.957516\n  1.04688     2.18698     0.745197      -2.16775    -0.651007    1.2935\n -0.754723   -1.57665    -0.537231       1.56279     0.469328   -0.932519\n -2.82194    -5.89516    -2.00873    …   5.84333     1.75484    -3.48673\n -1.11863    -2.33686    -0.796269       2.31632     0.695624   -1.38215\n -1.57394    -3.28802    -1.12037        3.25911     0.978758   -1.94472\n -0.472676   -0.987441   -0.336463       0.978758    0.293936   -0.584028\n  0.939172    1.96197     0.668527      -1.94472    -0.584028    1.16042\n\n\n여기에서 각 원소들이 의미하는 바는 아래와 같다.\n\n대각선의 원소: \\(x_t^2,~ t=1,2,\\dots,100\\) 을 의미\n대각선 한칸 위, 혹은 한칸 아래: \\(x_{t-1} x_t~ t=2,3,\\dots,100\\) 을 의미\n대각선 두칸 위, 혹은 두칸 아래: \\(x_{t-2} x_t~ t=3,4,\\dots,100\\) 을 의미\n\n\n\n\nx*x'의 계산결과를 캡쳐한 그림, 이것은 \\(\\hat{\\bf C}_{\\bf x}\\)를 의미함\n\n\n확인해보자.\nlag=1, 스크린샷의 노란색\n\n(x[1:99] .* x[2:100])[1:5]\n\n5-element Vector{Float64}:\n 1.587897526021493\n 1.130306190921068\n 0.5698214432110668\n 0.4648189302568683\n 0.3099446153360606\n\n\n\nlag1에 해당하는 숫자들임. 이는 스크린샷에서 노란색으로 표현된 1.589, 1.13031, 0.569821 … 등과 일치한다.\n\nlag=2, 스크린샷의 빨간색\n\n(x[1:98] .* x[3:100])[1:5]\n\n5-element Vector{Float64}:\n 0.5410642277088621\n 1.6722932576420804\n 0.3141719983177106\n 0.5621541352252872\n 0.30066534927151267\n\n\n\nlag2에 해당하는 숫자들임. 이는 스크린샷에서 빨간색으로 표현된 숫자들인 0.54164, 1.67229, 0.31417 … 등과 일치한다.\n\n\n\n스펙트럼 방법\n지금까지는 정상시계열일 경우 ACF를 이용한 간단한 분석방법을 다시 복습했다. 그리고 \\({\\bf C}_{\\bf x}\\)가 ACF를 구함에 필요한 모든정보를 가지고 있음을 이해했다. 한편 \\({\\bf C}_{\\bf x}\\)은 positive definite matrix 이므로 아래와 같이 분해가능하다.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식표현을 잘 해석하면 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf V}\\)와 \\({\\bf p}\\)에 담겨있다는 사실을 이해할 수 있다. 그런데 정상시계열일 경우 한정하여 \\({\\bf C}_{x}\\)의 고유벡터행렬은 \\({\\bf B}\\)의 고유벡터행렬과 일치한다는 사실을 알고 있다. 따라서 \\({\\bf V}\\)는 \\({\\bf B}\\)로 부터 그냥 알 수 있는 정보이다. 따라서 \\({\\bf C}_{\\bf x}\\)의 모든 정보는 \\({\\bf p}\\)에 담겨있다는 사실을 알 수 있다. 이는 적절한 \\({\\bf p}\\)를 추정하는 일은 적절한 \\({\\bf C}_{\\bf x}\\)를 추정하는 것과 같다는 사실을 알려준다.\n요약하면 아래와 같다.\n\n임의의 정상시계열은 이론적인 ACF (혹은 PACF)를 잘 추정하면 유니크하게 특정할 수 있다. (Wold’s Thm)\nACF를 잘 추정한다는 말은 \\({\\bf C}_{\\bf x}\\)를 잘 추정한다는 의미이다.\n그런데 \\({\\bf p}\\)를 잘 추정하면 \\({\\bf C}_{\\bf x}\\)를 잘 추정하는 일이 된다.\n따라서 임의의 정상시계열은 \\({\\bf p}\\)를 잘 추정하면 유니크하게 특정할 수 있다는 결론을 얻는다.\n\n여기에서 \\({\\bf p}\\)를 power spectral density 라고 부른다. 일반적으로 정상시계열을 분석하기 위해서는 \\({\\bf C}_{\\bf x}\\)를 특정하거나, \\({\\bf p}\\)를 특정하면 되는데 여기에서 \\({\\bf p}\\)를 특정한뒤 \\({\\bf p}\\)로 부터 \\({\\bf C}\\)를 역으로 해석하는 방법론을 spectral analysis라고 부른다. 경우에 따라서 \\({\\bf C}_{\\bf x}\\)를 특정하는 것이 용이할 수도 있지만 \\({\\bf p}\\)를 특정하고 해석하는 것이 용이할 때도 있다.\n그렇다면 주어진 시계열 \\({\\bf x}\\)에 대하여 \\({\\bf p}\\)를 어떻게 구할까? 직관적으로 생각하면 단순히 아래의 알고리즘으로 구하면 된다는 것을 알 수 있다.\n\n\\({\\bf C}_{\\bf x}\\)를 알아낸다.\n\\({\\bf C}_{\\bf x}\\)를 고유분해하여 \\({\\bf p}\\)를 구한다.\n\n또 다른 방법으로는 교재에 소개된 바 있는 아래의 수식을 이용하는 것이다.11\n11 이 수식이 성립하는 이유는 조금 손으로 써보면 금방 알 수 있음\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\]\n이것을 이용하면 아래와 같은 알고리즘을 떠올릴 수 있다.\n\n\\({\\bf B}\\)의 고유벡터행렬 \\({\\bf V}\\)를 구하고 \\({\\bf V}^H{\\bf x}\\)를 계산한다.\n계산된 결과를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n그런데 \\({\\bf V}^H{\\bf x}= {\\bf DFT} \\cdot {\\bf x}\\) 이므로 1의 과정을 아래와 같이 바꾸어 서술할 수 있다.\n\n\\({\\bf x}\\)를 퓨리에변환하여 \\(\\tilde{\\bf x} = {\\bf DFT} \\cdot {\\bf x}\\) 를 계산한다.\n\\(\\tilde{\\bf x}\\)를 원소별로 제곱하여 \\({\\bf p}\\)를 얻는다.\n\n즉 임의의 시계열을 퓨리에변환한 뒤 제곱하면 \\({\\bf p}\\)를 얻을 수 있다.\n(예제2) – 하나의 realization에서 \\(\\hat{\\bf p}\\)를 구해보자.\n(예제1에 이어서) 아래의 모형에서 생성된 \\({\\bf x}\\)를 다시 고려하자.\n\\[x_{t} = 0.5 x_{t-1} +\\epsilon_t\\]\n\nplot(x)\n\n\n\n\n이 자료의 PSD \\({\\bf p}\\)는 아래와 같이 구할 수 있다.\n단계1: \\({\\bf x}\\)의 DFT를 계산\n\nx̃ = fft(x) \n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\\({\\bf B}\\)를 설정하고 고유값분해 하기 귀찮아서 그냥 DFT해주는 패키지 사용함\n\n단계2: \\(\\hat{\\bf p}\\)를 계산\n\np̂ = abs.(x̃).^2\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n참고\nfft(x) 대신에 아래의 코드를 이용해도 된다.\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\nV'x\n\n100-element Vector{ComplexF64}:\n  -5.756917285643587 + 0.0im\n -19.082672090492103 - 1.0178306444775291im\n   14.23050682476898 - 11.867854578090007im\n  3.8980118254428824 + 1.2603018602424476im\n  -16.15797305318818 + 27.48824632227092im\n   12.32574209329044 - 1.5134316695905325im\n  3.9542122497256385 + 15.369129638224617im\n    9.51693811050782 + 19.371467179753516im\n  -19.38292930624826 + 9.495062886234233im\n -7.8539348514784155 + 4.134711886071595im\n -14.072349901900417 - 5.945064076174276im\n -14.596266922162371 + 3.447776409279244im\n   5.857720447482956 + 5.7388951128385735im\n                     ⋮\n   5.857720447482839 - 5.738895112838781im\n -14.596266922162307 - 3.4477764092792627im\n  -14.07234990190023 + 5.945064076174198im\n  -7.853934851478599 - 4.134711886071242im\n -19.382929306248577 - 9.49506288623372im\n   9.516938110507212 - 19.371467179753736im\n  3.9542122497250025 - 15.369129638224603im\n  12.325742093290597 + 1.5134316695903638im\n  -16.15797305318867 - 27.488246322270854im\n  3.8980118254424903 - 1.2603018602428118im\n  14.230506824769146 + 11.867854578089572im\n   -19.0826720904922 + 1.0178306444775123im\n\n\n진짜 똑같은지 확인\n\nfft(x)\n\n100-element Vector{ComplexF64}:\n  -5.756917285643583 + 0.0im\n   -19.0826720904921 - 1.0178306444775302im\n  14.230506824768984 - 11.867854578089997im\n  3.8980118254428726 + 1.2603018602424614im\n -16.157973053188194 + 27.488246322270918im\n   12.32574209329046 - 1.5134316695905219im\n    3.95421224972561 + 15.369129638224624im\n   9.516938110507798 + 19.371467179753544im\n  -19.38292930624831 + 9.49506288623419im\n  -7.853934851478428 + 4.134711886071571im\n -14.072349901900408 - 5.945064076174294im\n -14.596266922162355 + 3.447776409279256im\n   5.857720447482927 + 5.738895112838594im\n                     ⋮\n   5.857720447482924 - 5.738895112838594im\n -14.596266922162352 - 3.4477764092792564im\n -14.072349901900408 + 5.945064076174294im\n   -7.85393485147843 - 4.134711886071569im\n -19.382929306248315 - 9.49506288623419im\n   9.516938110507798 - 19.37146717975354im\n  3.9542122497256105 - 15.36912963822462im\n  12.325742093290458 + 1.5134316695905206im\n -16.157973053188194 - 27.488246322270925im\n  3.8980118254428717 - 1.260301860242461im\n  14.230506824768984 + 11.867854578089993im\n -19.082672090492103 + 1.0178306444775296im\n\n\n\n\n전통적인 방법과 스펙트럼 방법의 비교\n시계열자료의 전통적인 분석과 spectral analysis는 대충 아래의 과정으로 비교 설명할 수 있다.\n\n\n\n\n\n\n\n\n단계\n전통적인 방법\n스펙트럴 분석\n\n\n\n\n1\n\\({\\bf x}\\)의 plot을 그려봄\n\\({\\bf x}\\)의 plot을 그려봄\n\n\n2\nSACF plot, SPACF plot 을 그려봄\nPSD plot을 그려봄\n\n\n3\nACF를 추정 (=ARMA(\\(p\\),\\(q\\))에 대응하는 파라메터를 추정)\n\\({\\bf p}\\)를 추정\n\n\n4\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n추정된 파라메터를 바탕으로 여러가지 분석 수행\n\n\n\n눈여겨 볼 점은 PSD plot의 존재이다. 전통적인 시계열에서 SACF plot 과 비슷하게 스펙트럼 방법에서 시계열을 분석하기 위해 필요한 매우 중요한 시각화 이다. 간단하게 비교를 하면 아래와 같다.\nSACF plot\n\nx축: lag=0, lag=1, ….\ny축: lag에 대응하는 상관계수값\n\nPSD plot\n\nx축: \\(\\Omega=\\big\\{\\frac{k}{N}:~ \\text{for}~ k=0,\\dots, N-1\\big\\}\\), 정규화된 freq를 의미함\ny축: 주파수에 대응하는 power값\n\n전통적인 방법에 비하여 스펙트럴 분석이 가지는 장점은 위의 표에서 소개한 일반적인 분석루틴이 시계열이 아닌 그래프신호로 쉽게 확장가능 하다는 점이다12. 따라서 앞으로는 전통적인 시계열 분석방법 대신 스펙트럴 분석만을 다룰 것이다. 스펙트럴 분석의 핵심적인 부분은 \\({\\bf p}\\)를 추정하는 방법과 추정량의 점근적 성질들을 파악하는 것이다. 이 포스트에서는 \\({\\bf p}\\)를 추정하는 방법만을 다룬다.\n12 퓨리에 변환대신에 그래프 퓨리에 변환을 이용하기만 하면된다"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#그래프신호에서의-psd의-추정",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "그래프신호에서의 PSD의 추정",
    "text": "그래프신호에서의 PSD의 추정\n이제 그래프 신호에서 \\({\\bf p}\\)를 추정하는 방법에 대하여 살펴보자. 그래프이동변환 (Graph Shift Operator, GSO)13 \\({\\bf S}={\\bf V}{\\bf \\Lambda}{\\bf V}^H\\)에 대하여 정상인 시계열 \\({\\bf x}\\)를 고려한다. 이 신호의 그래프퓨리에 변환14은 아래와 같이 구할 수 있다.\n13 Back shift operator의 일반화 버전14 좀 더 정확하게는 \\({\\bf V}^H\\) 에 대한 그래프 변환이라고 한다\\[\\tilde{\\bf x}={\\bf GFT} {\\bf x} = {\\bf V}^H{\\bf x}\\]\n여기에서 \\(\\tilde{\\bf x}\\)를 \\({\\bf x}\\)의 주파수응답(frequency representation)이라고 부른다.15 우리는 아래의 수식에서 \\({\\bf p}\\)의 값에 관심이 있다.\n15 이 \\(\\tilde{\\bf x}\\)를 그냥 graph Fourier transform이라고 부르는 사람도 많다. 즉 그래프퓨리에변환이 (1) 변환매트릭스 \\({\\bf GFT}\\)자체를 지칭할때도 있고 (2) 트랜스폼된 결과 \\(\\tilde{\\bf x}\\)를 지칭할때도 있음. 교재에서는 변환은 graph Fourier transform, 그리고 변환된 결과는 \\({\\bf x}\\)의 주파수응답이라고 한다.\\[{\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\]\n여기에서 \\({\\bf p}\\)를 PSD (power spectrum density) 라고 한다. \\({\\bf p}\\)가 포함된 표현식은 위의 수식 이외에도 2개가 더 있다. 이를 모두 요약하면 아래와 같다16\n16 약간의 계산을 통하면 1,2,3이 쉽게 같은 수식임을 알 수 있음\n\\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)17\n\\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n\\({\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\)\n\n17 이 수식을 살짝 정리하면 \\({\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\) 와 같이 보다 예쁜 수식을 얻을 수 있음위의 표현중 3.에서 \\({\\bf c}_{\\bf x}\\)은 \\({\\bf C}_x\\)를 벡터화한 것이며 \\({\\bf G}_{np}\\)는 \\({\\bf V}^\\ast\\) 와 \\({\\bf V}\\)를 열별-크로네커곱 (column-wise Kronecker product) 이다. 이때 \\({\\bf G}_{np}\\)의 정의가 조금 생소하니 한번 계산하여 보자.\n(예제) 아래와 같은 GSO \\({\\bf B}\\)를 고려하자.\n\nB= [0 1 0 0 \n    0 0 1 0 \n    0 0 0 1 \n    1 0 0 0]\n\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  1  0\n 0  0  0  1\n 1  0  0  0\n\n\n이러한 GSO에 대하여 \\({\\bf G}_{np}\\)는 아래와 같이 구할 수 있다.\n(1) \\({\\bf V}\\)를 정의\n\nV = [i*j for i in 0:3 for j in 0:3] |&gt; \n    x -&gt; reshape(x,(4,4)) .|&gt; \n    x -&gt; exp(im * (2π/4) * x) \n\n4×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n16×4 Matrix{ComplexF64}:\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im  -1.83697e-16-1.0im              5.51091e-16+1.0im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im          -1.0+1.22465e-16im             -1.0+3.67394e-16im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n 1.0+0.0im   6.12323e-17+1.0im             -1.83697e-16-1.0im\n 1.0+0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n 1.0+0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0+0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0+0.0im           1.0+0.0im          …           1.0+0.0im\n\n\n위에서 언급한 표현식 1,2,3 을 이용하면 \\({\\bf p}\\)를 추정하는 세 가지 방법을 각각 정의할 수 있다. 하나씩 살펴보자.\n\n1. \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 수식 \\({\\bf C}_{\\bf x}={\\bf V} \\text{diag}({\\bf p}){\\bf V}^H\\)를 적당히 변형하면 아래를 얻을 수 있다.\n\\[{\\bf p}=\\text{diag}\\big({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V} \\big)\\]\n여기에서\n\\[{\\bf C}_{\\bf x}=\\mathbb{E}[{\\bf x}{\\bf x}^H]\\approx \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_t{\\bf x}_r^H\\]\n이므로 이 수식에 근거하여 \\({\\bf p}\\)을 추정한다면 아래와 같이 할 수 있다.\n\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H\\big[ \\frac{1}{R}\\sum_{r=1}^R{\\bf x}_r{\\bf x}_r^H\\big]{\\bf V} \\right].\\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면18, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n18 대부분은 관측한 시계열이 하나겠지..\\[\\hat{\\bf p}_{cg}:= \\text{diag}\\left({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V} \\right):=\\text{diag}\\left[{\\bf V}^H{\\bf x}_r{\\bf x}_r^H{\\bf V} \\right].\\]\n\n주의: 여기에서 \\({\\bf V}^H {\\bf C}_{\\bf x}{\\bf V}\\) 는 항상 대각행렬이지만 \\({\\bf V}^H \\hat{\\bf C}_{\\bf x}{\\bf V}\\) 은 대각행렬이 아닐수도 있음을 유의하자. 즉 이론적인 모수는 대각행렬이지만 sample version은 대각행렬이 아닐 수 있다. 대각선이 아닌 원소는 버리면 된다.)\n\n\n아이디어: 혹시 대각선이 아닌 원소들을 이용하여 오차항 \\(\\epsilon_t\\)의 분산을 추정할 수도 있지 않을까? 이미 연구가 있겠지?\n\n(예제)\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n\np̂ = diag(V' * (x*x') * V)\n\n100-element Vector{ComplexF64}:\n 33.142096633741986 + 0.0im\n 365.18435333408354 + 1.5376069362644531e-13im\n  343.3532967764883 + 6.904176529646917e-14im\n 16.782856970223083 - 3.5538396658301444e-14im\n 1016.6837790613963 + 5.475049904926759e-15im\n 154.21439356883144 + 6.4512443306088e-14im\n  251.8459403524346 + 2.1316282072803006e-14im\n 465.82585169550384 + 1.816929057526117e-13im\n 465.85416770456044 + 4.1584439183295984e-14im\n    78.780135032089 + 1.3472456770553478e-14im\n 233.37481863133462 + 6.315728724701355e-14im\n 224.93817023139385 - 3.472109560086835e-14im\n  67.24780595702241 + 7.105427357601002e-14im\n                    ⋮\n   67.2478059570233 + 6.384723798533952e-14im\n 224.93817023139195 + 1.9727655769954595e-14im\n 233.37481863132837 - 2.1872689567834747e-14im\n    78.780135032089 + 1.917599080404094e-14im\n 465.85416770456294 + 4.808950231511622e-14im\n 465.82585169550094 - 4.890486289860305e-14im\n  251.8459403524291 + 2.0146681724568905e-14im\n  154.2143935688347 - 1.0948596967617507e-13im\n 1016.6837790614081 + 1.2114814701286432e-13im\n  16.78285697022108 + 2.376159104534641e-14im\n 343.35329677648286 + 1.1310381241837407e-14im\n 365.18435333408746 + 4.574214786667376e-14im\n\n\n\n\n2. \\({\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf p}=\\mathbb{E}\\left[|{\\bf V}^H{\\bf x}|^2\\right]\\approx \\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n따라서 \\(\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2\\) 를 PSD \\({\\bf p}\\)에 대한 추정량이라고 생각할 수 있다. 이러한 추정량을 기호로 \\(\\hat{\\bf p}_{pg}\\)라고 정의하고 periodogram이라고 부른다. 즉\n\\[\\hat{\\bf p}_{pg}=\\frac{1}{R}\\sum_{r=1}^{R} |{\\bf V}^H {\\bf x}_r|^2 \\]\n만약에 확률과정 \\({\\bf x}\\)에서 관측한 시계열이 \\({\\bf x}_r\\) 하나라면, 즉 \\(R=1\\) 이라면 단순히 아래와 같이 쓸 수 있다.\n\\[\\hat{\\bf p}_{pg}=|{\\bf V}^H {\\bf x}_r|^2 \\]\n즉 이 경우 \\(\\hat{\\bf p}_{pg}\\)는 단순히 관측시계열 \\({\\bf x}_r\\)의 그래프 퓨리에 변환 \\(\\tilde{\\bf x}={\\bf V}^H{\\bf x}_r\\) 결과에 절대값을 취하고 제곱한 것과 같다.\n(예제)\n스펙트럼방법챕터 예제2에서 이미 보여준 적 있다. 주어진 시계열 \\({\\bf x}\\)에 대하여 \\(\\hat{\\bf p}_{pg}\\)를 구하는 방법을 요약하면 아래와 같다.\n\nx̃ = fft(x) # 단계1: GFT, 이 신호는 시계열이라서 GFT대신에 DFT를 써도 된다.\np̂ = abs.(x̃).^2 # 단계2: hat p\n\n100-element Vector{Float64}:\n   33.14209663374188\n  365.18435333408365\n  343.3532967764883\n   16.782856970223087\n 1016.6837790613963\n  154.21439356883184\n  251.84594035243464\n  465.8258516955045\n  465.85416770456163\n   78.78013503208902\n  233.37481863133453\n  224.93817023139349\n   67.24780595702228\n    ⋮\n   67.24780595702225\n  224.93817023139337\n  233.37481863133453\n   78.78013503208902\n  465.8541677045618\n  465.8258516955044\n  251.84594035243452\n  154.2143935688318\n 1016.6837790613968\n   16.782856970223072\n  343.3532967764882\n  365.1843533340838\n\n\n\n\n3. \\({\\bf c}_{\\bf x} = {\\bf G}_{np} {\\bf p}\\)\n확률과정 \\({\\bf x}\\)에서 \\(R\\)개의 realization \\(\\{{\\bf x}_1 \\dots {\\bf x}_R\\}\\) 을 관측하였다고 하자. 아래의 수식을 관찰하자.\n\\[{\\bf C}_{\\bf x} = {\\bf V} \\text{diag}({\\bf p}) {\\bf V}^H\\]\n이 수식으로부터 아래를 얻을 수 있다.\n\\[{\\bf c}_{\\bf x} = \\sum_{i=1}^{N}p_i \\text{vec}({\\bf v}_i{\\bf v}_i^H) = {\\bf G}_{np} {\\bf p}\\]\n여기에서 \\({\\bf c}_{\\bf x}\\) 대신에 \\(\\hat{\\bf c}_{\\bf x}\\) 를 대입하면 아래와 같이 생각할 수 있다.\n\\[\\hat{\\bf c}_{\\bf x} \\approx  {\\bf G}_{np} {\\bf p}\\]\n이 문제는 아래와 같은 회귀모형으로 생각할 수 있다.\n\n\n\n\n\n\n\n\n\n회귀모형\n우리의 문제\n\n\n\n\n모형\n\\({\\bf y} \\approx {\\bf X}{\\boldsymbol \\beta}\\)\n\\(\\hat{\\bf c}_{\\bf x} \\approx {\\bf G}_{np}{\\bf p}\\)\n\n\n설명변수\n\\({\\bf X}\\)19\n\\({\\bf G}_{np}\\)20\n\n\n반응변수\n\\({\\bf y}\\)21\n\\(\\hat{\\bf c}_{\\bf x}\\)22\n\n\n추정하고 싶은 파라메터\n\\({\\boldsymbol \\beta}\\)23\n\\(\\hat{\\bf p}\\)24\n\n\n오차항\n대부분 정규분포를 가정\n??? 모르겠는데??\n\n\n\n19 (n,p) matrix20 (N²,N) matrix21 (n,1) col-vector22 (N²,1) col-vector23 (p,1) col-vector24 (N,1) col-vector회귀분석에서 아래의 수식이 익숙하다면\n\\[\n\\hat{\\boldsymbol \\beta}_{ls} = \\underset{\\boldsymbol \\beta}{\\operatorname{argmin}} \\|{\\bf y}-{\\bf X}{\\boldsymbol \\beta}\\|_2^2=({\\bf X}^T{\\bf X})^{-1}{\\bf X}^T{\\bf y}.\n\\]\n\\({\\bf p}\\)를 추정하기 위한 아래의 수식도 쉽게 이해할 수 있다. (의문: 그런데 왜 MSE를 손실함수로 쓰고 있는 거야? 오차항이 설마 정규분포?)\n\\[\n\\hat{\\bf p}_{ls} = \\underset{\\bf p}{\\operatorname{argmin}} \\|\\hat{\\bf c}_{\\bf x}-{\\bf G}_{np}{\\bf p}\\|_2^2=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}.\n\\]\n(예제)\n(1) \\({\\bf V}\\)를 정의\n\nN = 100 \nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x)\n\n100×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.728969-0.684547im      0.728969+0.684547im\n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im   …  0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im      0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im   …  0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im       0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n\n\n(2) \\({\\bf G}_{np}={\\bf V}^{\\ast} \\odot {\\bf V}\\), 여기에서 \\(\\odot\\)은 열별-크로네커곱을 의미한다.\n\n# columnwise_kron은 위에서 정의한적 있음~\nGₙₚ = columnwise_kron(conj(V),V)\n\n10000×100 Matrix{ComplexF64}:\n 1.0+0.0im       1.0+0.0im        …       1.0+0.0im\n 1.0+0.0im  0.998027+0.0627905im     0.998027-0.0627905im\n 1.0+0.0im  0.992115+0.125333im      0.992115-0.125333im\n 1.0+0.0im  0.982287+0.187381im      0.982287-0.187381im\n 1.0+0.0im  0.968583+0.24869im       0.968583-0.24869im\n 1.0+0.0im  0.951057+0.309017im   …  0.951057-0.309017im\n 1.0+0.0im  0.929776+0.368125im      0.929776-0.368125im\n 1.0+0.0im  0.904827+0.425779im      0.904827-0.425779im\n 1.0+0.0im  0.876307+0.481754im      0.876307-0.481754im\n 1.0+0.0im  0.844328+0.535827im      0.844328-0.535827im\n 1.0+0.0im  0.809017+0.587785im   …  0.809017-0.587785im\n 1.0+0.0im  0.770513+0.637424im      0.770513-0.637424im\n 1.0+0.0im  0.728969+0.684547im      0.728969-0.684547im\n    ⋮                             ⋱  \n 1.0+0.0im  0.770513-0.637424im      0.770513+0.637424im\n 1.0+0.0im  0.809017-0.587785im      0.809017+0.587785im\n 1.0+0.0im  0.844328-0.535827im   …  0.844328+0.535827im\n 1.0+0.0im  0.876307-0.481754im      0.876307+0.481754im\n 1.0+0.0im  0.904827-0.425779im      0.904827+0.425779im\n 1.0+0.0im  0.929776-0.368125im      0.929776+0.368125im\n 1.0+0.0im  0.951057-0.309017im      0.951057+0.309017im\n 1.0+0.0im  0.968583-0.24869im    …  0.968583+0.24869im\n 1.0+0.0im  0.982287-0.187381im      0.982287+0.187381im\n 1.0+0.0im  0.992115-0.125333im      0.992115+0.125333im\n 1.0+0.0im  0.998027-0.0627905im     0.998027+0.0627905im\n 1.0+0.0im       1.0+0.0im                1.0+0.0im\n\n\n(3) \\(\\hat{\\bf p}_{ls}=({\\bf G}_{np}^H{\\bf G}_{np})^{-1}{\\bf G}_{np}^H\\hat{\\bf c}_{\\bf x}\\)\n\nĉₓ = vec(x*x')\np̂ = inv(Gₙₚ' * Gₙₚ) * Gₙₚ' * ĉₓ \n\n100-element Vector{ComplexF64}:\n  0.003314209663374193 - 2.7356277964988863e-19im\n   0.03651843533340838 - 4.01518191768058e-18im\n   0.03433532967764885 + 2.515448157755484e-17im\n 0.0016782856970223292 - 1.0070028487673847e-17im\n   0.10166837790613971 + 3.1129277935880596e-18im\n  0.015421439356883134 + 9.403422807142065e-18im\n  0.025184594035243472 - 3.993782799800785e-18im\n   0.04658258516955039 - 1.850761436988587e-18im\n   0.04658541677045607 + 1.1559103895961936e-17im\n  0.007878013503208905 + 3.559698092088507e-18im\n  0.023337481863133468 + 2.6204945155857973e-18im\n   0.02249381702313939 + 5.304406111488559e-18im\n  0.006724780595702225 - 1.655564138463681e-17im\n                       ⋮\n  0.006724780595702329 + 1.8121162053534517e-18im\n  0.022493817023139205 - 1.0461976779111972e-17im\n   0.02333748186313285 - 6.792203007975684e-18im\n  0.007878013503208907 - 2.3575339315335667e-18im\n  0.046585416770456294 + 1.5392042695643853e-17im\n  0.046582585169550106 - 1.123245521985718e-17im\n  0.025184594035242928 + 1.1628578774983873e-18im\n  0.015421439356883466 + 5.864828990948797e-18im\n   0.10166837790614085 + 2.2712943512935246e-17im\n 0.0016782856970221013 + 4.829637376114682e-18im\n   0.03433532967764831 + 3.3208196889839756e-19im\n  0.036518435333408754 + 1.3795822112205515e-17im\n\n\n\n?? 뭔가 스케일이 안맞음\n\n\nN^2 * p̂\n\n100-element Vector{ComplexF64}:\n  33.14209663374193 - 2.7356277964988864e-15im\n 365.18435333408377 - 4.0151819176805797e-14im\n  343.3532967764885 + 2.515448157755484e-13im\n 16.782856970223293 - 1.0070028487673847e-13im\n 1016.6837790613971 + 3.1129277935880596e-14im\n 154.21439356883135 + 9.403422807142065e-14im\n 251.84594035243472 - 3.9937827998007846e-14im\n  465.8258516955039 - 1.850761436988587e-14im\n  465.8541677045607 + 1.1559103895961937e-13im\n  78.78013503208905 + 3.559698092088507e-14im\n 233.37481863133468 + 2.6204945155857973e-14im\n 224.93817023139388 + 5.304406111488559e-14im\n  67.24780595702225 - 1.655564138463681e-13im\n                    ⋮\n  67.24780595702329 + 1.8121162053534517e-14im\n 224.93817023139206 - 1.0461976779111972e-13im\n  233.3748186313285 - 6.792203007975684e-14im\n  78.78013503208906 - 2.3575339315335666e-14im\n 465.85416770456294 + 1.5392042695643854e-13im\n 465.82585169550106 - 1.123245521985718e-13im\n 251.84594035242927 + 1.1628578774983874e-14im\n 154.21439356883465 + 5.864828990948797e-14im\n 1016.6837790614085 + 2.2712943512935246e-13im\n  16.78285697022101 + 4.8296373761146824e-14im\n  343.3532967764831 + 3.3208196889839758e-15im\n  365.1843533340875 + 1.3795822112205514e-13im\n\n\n\n\\(N^2\\)를 곱해주니까 아까부터 구하던 값이 그대로 잘 나옴. (\\({\\bf DFT}\\) 혹은 \\({\\bf GFT}\\)를 정의할때 \\(\\frac{1}{\\sqrt N}\\)으로 스케일링 하느냐 마느냐 차이때문에 생기는 현상임)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "href": "posts/2_Studies/CGSP/2022-12-27-Chap-12.2.1~12.3.1.html#의문점",
    "title": "[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators",
    "section": "의문점",
    "text": "의문점\n아래의 그림을 살펴보자.\n\n\n\n그림12.3(교재에서 긁어온 그림): Power spectral density estimation. All estimators are based on the same random process defined on the Karate club network (Zachary 1977). (A) Periodogram estimation with different numbers of observations. (B) Windowed average periodogram from a single realization and a different number of windows. (C) Windowed average periodogram for four windows and a varying number of realizations. (D) Parametric MA estimation for 1 and 10 realizations.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n이 그림은 다양한 방법으로 true PSD \\({\\bf p}\\)를 추정한 결과를 나타내는 PSD plot 이다25. 우리가 적용한 방법은 (A)에서 \\(R=1\\)일 경우이다. 보는것 처럼 true PSD 를 놀라울 정도로 제대로 추정하지 못한다26. 만약에 우리가 모형에서 하나의 시계열이 아니라 1000개의 정도의 시계열을 관측하였다면 좀 더 합리적으로 추정할 수 있다. 그런데 사실 하나의 모형에서 1000개씩이나 되는 시계열을 관측하는 일은 현실적으로 불가능하다27 따라서 우리는 비교적 적은 \\(R\\)에서 합리적인 PSD의 추정치를 이끌어내야 한다. 그림 (B),(C)는 상대적으로 적은 \\(R\\)에 대해 \\({\\bf p}\\)를 추정하는 windowed periodogram 을 이용하여 PSD를 추정한 결과이다. (C)를 살펴보면 \\(R=1\\) 일경우 \\({\\bf p}\\)를 추정한 값들이 나와있는데 (A)와 비교하면 꽤 합리적으로 보인다.\n25 x축이 freq, y축이 PSD26 맞추는게 없는 것 같은데?27 그리고 대부분 \\(R=1\\)이지..28 약간 바이어스가 있어보이긴 하는데, 우연히 생긴건지 이론적으로 항상 생기는 건지는 잘 모르겠네?29 이런걸 세미파라메트릭 모형이라고 해요30 그래서 플랏을 보면서 적당한 ARMA를 찾을 필요도 없고, AIC 니 BIC 를 따져가면서 모형선택을 할 필요도 없고31 적합이후에 잔차분석 같은거 안해도 된다는 의미문제는 (A)-(C)에서 제안된 방법 모두가 (D)에 제시된 전통적인 방법에 비하여 퍼포먼스가 떨어진다는 것이다. (D)는 parametric 모형을 사용한 결과이다. 파라메트릭 방법이므로 특정 모델을 한정하고 거기에 대응하는 한두개의 모수만 추정하면 되므로 추정이 잘 된다.28 반면 (A)-(C)의 경우 한 두개의 파라메터가 아니라 \\({\\bf p}\\)의 모든 원소를 추정해야하므로 추정할 파라메터가 데이터의 수 \\(N\\)과 같다29. 따라서 추정치의 분산이 크다. 사실 이것은 파라메트릭 방법과 세미파라메트릭 방법이라는 구조적인 차이때문에 어쩔 수 없는 것 같다. 그래도 세미파라메트릭 방법은 머리아프게 모델링을 할 필요가 없고30 내가 적합한 모델이 맞는지 확인할 필요도 없다31는 장점이 있다.\n아래는 나름 PSD를 추정하는 신기술인 것 같다.\n\n\n\n그림12.4(교재에서 긁어온 그림): PSD estimation from a subset of nodes. Estimators are based on a random process defined on the Karate club network (Zachary 1977). (A) Graph sampling for nonparametric PSD estimation. Here, 20 out of 34 nodes are observed. The sampled nodes are highlighted by the circles around the nodes. (B) Nonparametric PSD estimation based on observations from 20 nodes and 100 data snapshots. (C) Graph sampling for parametric MA PSD estimation. Here, 4 out of 34 nodes are observed. (D) Parametric MA PSD estimation based on observations from 4 nodes and 100 data snapshots.\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73.\n\n\n\n그래프신호의 sub-sampling을 이용하는 것 같은데 교재의 뒤쪽에 서술되어있다. \\(R=100\\)임을 고려하여도 퍼포먼스가 좋은 편인듯 하다32.\n\n\n32 내 생각엔 이게 핵심 기술인 것 같음"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "",
    "text": "using LinearAlgebra, FFTW"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#cyclic-shfit-operator-bf-b",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Cyclic shfit operator \\({\\bf B}\\)",
    "text": "Cyclic shfit operator \\({\\bf B}\\)\nThe matrix \\({\\bf B}\\) representing the periodic shift is\n\nB= [0 0 0 0 1\n    1 0 0 0 0 \n    0 1 0 0 0\n    0 0 1 0 0\n    0 0 0 1 0]\n\n5×5 Matrix{Int64}:\n 0  0  0  0  1\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n\n\nThis matrix is the cyclic shift.\nnote: \\({\\bf B}\\) is orthogonal matrix.\n\nB'B\n\n5×5 Matrix{Int64}:\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n\n\n(ex1) Define \\({\\bf s}\\) as\n\ns = [1,2,3,4,5]\ns\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nObserve that\n\nB*s\n\n5-element Vector{Int64}:\n 5\n 1\n 2\n 3\n 4\n\n\n\nB^2*s\n\n5-element Vector{Int64}:\n 4\n 5\n 1\n 2\n 3\n\n\n\nB^3*s\n\n5-element Vector{Int64}:\n 3\n 4\n 5\n 1\n 2\n\n\nThus we can interprete the matrix \\({\\bf B}\\) as cyclic shift operator such that\n\\[\n{\\bf B}s_n =s_{n-1}\n\\]\nfor \\(n=1,\\dots, N-1\\) and \\({\\bf B}s_0 =s_N\\).\nnote: \\({\\bf B}\\)는 시계열에서 다루는 backshift operator 와 비슷함."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\nThe matrix \\({\\bf B}\\) can be expressed as\n\\({\\bf B}={\\bf DFT}^\\ast \\cdot {\\bf \\Lambda} \\cdot {\\bf DFT}\\)\nwhere \\({\\bf DFT}\\) is unitary and symmetric matrix and \\(\\bf \\Lambda\\) is diagonal matrix.\n\nλ, Ψ = eigen(B)\n\nEigen{ComplexF64, ComplexF64, Matrix{ComplexF64}, Vector{ComplexF64}}\nvalues:\n5-element Vector{ComplexF64}:\n -0.8090169943749472 - 0.5877852522924725im\n -0.8090169943749472 + 0.5877852522924725im\n 0.30901699437494734 - 0.9510565162951536im\n 0.30901699437494734 + 0.9510565162951536im\n  0.9999999999999998 + 0.0im\nvectors:\n5×5 Matrix{ComplexF64}:\n  0.138197+0.425325im   0.138197-0.425325im  …  0.447214+0.0im\n -0.361803-0.262866im  -0.361803+0.262866im     0.447214+0.0im\n  0.447214-0.0im        0.447214+0.0im          0.447214+0.0im\n -0.361803+0.262866im  -0.361803-0.262866im     0.447214+0.0im\n  0.138197-0.425325im   0.138197+0.425325im     0.447214+0.0im\n\n\n\nB ≈ Ψ * Diagonal(λ) * Ψ'\n\ntrue\n\n\nDefine \\({\\boldsymbol \\Psi}^\\ast={\\bf DFT}\\).\n\nDFT = Ψ'\n\n5×5 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n  0.138197-0.425325im  -0.361803+0.262866im  …  0.138197+0.425325im\n  0.138197+0.425325im  -0.361803-0.262866im     0.138197-0.425325im\n -0.361803-0.262866im  -0.361803+0.262866im     0.138197-0.425325im\n -0.361803+0.262866im  -0.361803-0.262866im     0.138197+0.425325im\n  0.447214-0.0im        0.447214-0.0im          0.447214-0.0im\n\n\nNote that the eigenvalues are not ordered in julia.\n\nλ[5], exp(-im* 2π/5 * 0)\n\n(0.9999999999999998 + 0.0im, 1.0 - 0.0im)\n\n\n\nλ[3], exp(-im* 2π/5 * 1)\n\n(0.30901699437494734 - 0.9510565162951536im, 0.30901699437494745 - 0.9510565162951535im)\n\n\n\nλ[1], exp(-im* 2π/5 * 2)\n\n(-0.8090169943749472 - 0.5877852522924725im, -0.8090169943749473 - 0.5877852522924732im)\n\n\n\nλ[2], exp(-im* 2π/5 * 3)\n\n(-0.8090169943749472 + 0.5877852522924725im, -0.8090169943749475 + 0.587785252292473im)"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-components-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral components and Frequencies",
    "text": "Spectral components and Frequencies\nWe remark:\n(1) Spectral components: For \\(k = 0,1,2,\\dots, N-1\\), the \\(k\\)-th column of \\({\\bf DFT}^\\ast\\) is defined by\n\\[\\Psi_k:=\\frac{1}{\\sqrt{N}}\\begin{bmatrix} 1 \\\\ e^{j\\frac{2\\pi}{N}k} \\\\ e^{j\\frac{2\\pi}{N}2k} \\\\ e^{j\\frac{2\\pi}{N}3k} \\\\  \\dots \\\\ e^{j\\frac{2\\pi}{N}(N-1)k} \\end{bmatrix}.\\]\nNote that \\(\\Psi_k\\) can be also interpreted as \\(\\ell\\)-th eigenvector of \\({\\bf A}\\) correspoding \\(\\lambda_\\ell = e^{-j\\frac{2\\pi}{N}k}\\). Those eigenvectors\n\\[\\big\\{{\\bf 1},\\Psi_1,\\Psi_2, \\dots, \\Psi_{N-1}\\big\\}\\]\nform a complete orthonomal basis of \\(\\mathbb{C}^N\\). These vectors are called spectral components.\n(2) Frequencies: The diagonal entries of \\({\\bf \\Lambda}\\) are the eigenvalues of the time shift \\({\\bf B}\\). In Physics and in operator theory, these eigenvalues are the frequencies of the signal. In DSP it is more common to call frequencies\n\\[\\Omega_k=\\frac{-1}{2\\pi j}\\ln\\lambda_k=\\frac{-1}{2\\pi j}\\ln e^{-j \\frac{2\\pi}{N}k}=\\frac{k}{N}, \\quad k=0,1,2,\\dots,N-1.\\]\n\nThe \\(N\\) (time) frequencies \\(\\Omega_k\\) are all distinct, positive, equally spaced, and increasing from \\(0\\) to \\(\\frac{N-1}{N}\\). The spectral components are the complex exponential sinusiodal functions. For example, corresponding to the zero frequency is the DC spectral component (a vector whose entries are constant and all equal to \\(\\frac{1}{\\sqrt{N}}\\))."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft-1",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft-1",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT",
    "text": "DFT\n일반적으로 우리가 알고있는 DFT1는 아래와 같다. (이 그림은 위키피디아에서 캡쳐한 것이다)\n1 discrete Fourier transform\n\n\n그림1: 위키에서 긁어온 DFT의 정의\n\n\n즉 DFT는 임의의 신호 \\(\\{{\\bf x}_n\\}:=x_0,x_1,\\dots,x_{N-1}\\)를 적당한 규칙2에 따라서 \\(\\{{\\bf X}_k\\}:=X_0,X_1,\\dots,X_{N-1}\\)로 바꾸는 변환을 이라고 이해할 수 있다. 이때 사용되는 적당한 규칙은 구체적으로 아래의 수식을 의미한다.\n2 \\(X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\)\\[X_k = \\sum_{n=0}^{N-1}x_n\\cdot e^{-i\\frac{2\\pi}{N}kn}\\]\n그런데 매트릭스를 활용하면 위의 수식을 아래와 같이 표현할 수 있다.\n\\[\\begin{bmatrix} X_1 \\\\ X_2 \\\\ X_3 \\\\ \\dots \\\\ X_{N-1} \\end{bmatrix}\n=\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ \\dots \\\\ x_{N-1} \\end{bmatrix}\\]\n편의상 \\({\\bf X}\\)와 \\({\\bf x}\\)를 \\(N \\times 1\\) col-vec이라고 생각하고 DFT를 아래와 같은 matrix로 정의하자.\n\\[{\\bf DFT} = \\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n그러면\n\\[{\\bf X} = {\\bf DFT} \\cdot {\\bf x}\\]\n와 같이 표현할 수 있고 \\({\\bf x}\\)에서 \\({\\bf X}\\)로 바꾸는 과정을 단순히 \\({\\bf DFT}\\)행렬을 \\({\\bf x}\\)의 왼쪽에 곱하는 과정으로 이해할 수 있다.\n(참고) 사실 아래와 같이 \\({\\bf DFT}\\)를 정의하는 버전도 있다. (둘이 혼용해서 쓰인다)\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\n예제1 아래는 위키에서 긁어온 예제이다. 이 예제를 따라가보자.\n\n\n\n그림2: 위키에서 긁어온 예제이미지\n\n\n예제를 풀기위해서 우선 아래와 같은 벡터를 선언하다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n(풀이1)\n\\(4\\times 4\\)의 크기를 가지는 DFT행렬을 선언한다.\n(step1) 아래의 매트릭스 생성\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\n_DFT\n\n4×4 Matrix{Int64}:\n 0  0  0  0\n 0  1  2  3\n 0  2  4  6\n 0  3  6  9\n\n\n(step2) _DFT의 각 원소에 함수 \\(f: x \\to \\exp(-i\\frac{2\\pi}{4}x)\\)를 취함\n\nf = x -&gt; exp(-im * (2π/4) * x)\nDFT = _DFT .|&gt; f\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n이제 \\({\\bf X}\\)를 구하면 아래와 같다.\n\nDFT * x\n\n4-element Vector{ComplexF64}:\n                   2.0 + 0.0im\n   -1.9999999999999998 - 2.0000000000000004im\n 8.881784197001252e-16 - 1.9999999999999998im\n    3.9999999999999987 + 4.000000000000001im\n\n\n위키의 답이 잘 나옴\n(풀이2)\n참고로 아래와 같이 패키지를 이용하여 구할 수도 있다.\n\nfft(x)\n\n4-element Vector{ComplexF64}:\n  2.0 + 0.0im\n -2.0 - 2.0im\n  0.0 - 2.0im\n  4.0 + 4.0im"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#inverse-dft",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#inverse-dft",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Inverse DFT",
    "text": "Inverse DFT\n앞으로는 \\({\\bf DFT}\\)를 아래와 같이 정의하자.\n\\[{\\bf DFT} = \\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 1} & e^{-i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot 2} & e^{-i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{-i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{-i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n\\({\\bf DFT}\\)행렬에는 몇 가지 특징이 있다.\n특징1: 유니터리행렬이다. 즉 \\({\\bf DFT}^\\ast \\cdot {\\bf DFT} = {\\bf DFT}^\\ast \\cdot{\\bf DFT} = {\\bf I}\\) 이다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nf = x -&gt; exp(-im * (2π/4) * x)\nDFT = _DFT .|&gt; f\nDFT # 아까의 예제의 DFT!\n\n4×4 Matrix{ComplexF64}:\n 1.0-0.0im           1.0-0.0im          …           1.0-0.0im\n 1.0-0.0im   6.12323e-17-1.0im             -1.83697e-16+1.0im\n 1.0-0.0im          -1.0-1.22465e-16im             -1.0-3.67394e-16im\n 1.0-0.0im  -1.83697e-16+1.0im              5.51091e-16-1.0im\n\n\n\nDFT = (1/√4)*DFT # 새로운 DFT의 정의 \nDFT'DFT .|&gt; round # 유니터리행렬임을 확인!\n\n4×4 Matrix{ComplexF64}:\n  1.0+0.0im  -0.0-0.0im   0.0-0.0im   0.0-0.0im\n -0.0+0.0im   1.0+0.0im  -0.0-0.0im   0.0-0.0im\n  0.0+0.0im  -0.0+0.0im   1.0+0.0im  -0.0-0.0im\n  0.0+0.0im   0.0+0.0im  -0.0+0.0im   1.0+0.0im\n\n\n특징2: \\({\\bf DFT}\\)는 대칭행렬이다. 따라서 이 행렬의 켤레전치는 DFT의 각 원소에서 단순히 \\(i=\\sqrt{-1}\\) 대신에 \\(-i\\) 를 넣은 것과 같다.\n특징1-2를 조합하면 아래와 같이 \\({\\bf DFT}\\)에서 \\(i\\) 대신에 \\(-i\\)를 넣은 행렬이 변환 DFT를 취소시킬 수 있음을 이해할 수 있다. 3\n3 아래의 행렬은 \\({\\bf DFT}^\\ast\\) 혹은 \\({\\bf DFT}\\)의 conjugate matrix 혹은 \\({\\bf DFT}^{-1}\\)로 생각할 수 있음\\[\\frac{1}{\\sqrt{N}}\\begin{bmatrix}\n1 & 1 & 1 & \\dots & 1 \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 1} & e^{i \\frac{2\\pi}{N}\\cdot 2} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)}\\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot 2} & e^{i \\frac{2\\pi}{N}\\cdot 4} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)}\\\\\n\\dots & \\dots & \\dots & \\dots & \\dots \\\\\n1 & e^{i \\frac{2\\pi}{N}\\cdot (N-1)} & e^{i \\frac{2\\pi}{N}\\cdot 2(N-1)} & \\dots & e^{i \\frac{2\\pi}{N}\\cdot (N-1)^2}\n    \\end{bmatrix}\\]\n행렬 \\({\\bf DFT}\\)를 discrete Fourier transform으로 생각했듯이 위의 행렬을 inverse discrete Fourier transform으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#dft의-또-다른-정의",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "DFT의 또 다른 정의",
    "text": "DFT의 또 다른 정의\n이번에는 \\({\\bf DFT}\\)에 대한 다른 정의를 생각해보자. 우선 아래와 같은 행렬 \\({\\bf B}\\)를 고려하자.\n\nB= [0 0 0 1 \n    1 0 0 0 \n    0 1 0 0\n    0 0 1 0]\n\n4×4 Matrix{Int64}:\n 0  0  0  1\n 1  0  0  0\n 0  1  0  0\n 0  0  1  0\n\n\n이것은 길이가 4인 임의의 column vector를 아래로 한칸씩 이동시키는 매트릭스이다.\n\nx = [1, 2-im, -im, -1+2im]\n\n4-element Vector{Complex{Int64}}:\n  1 + 0im\n  2 - 1im\n  0 - 1im\n -1 + 2im\n\n\n\nB*x # 아래로 한칸이동 \n\n4-element Vector{Complex{Int64}}:\n -1 + 2im\n  1 + 0im\n  2 - 1im\n  0 - 1im\n\n\n\nB^2*x # 아래로 두칸이동, B^2*x = B*(Bx) 이므로 \n\n4-element Vector{Complex{Int64}}:\n  0 - 1im\n -1 + 2im\n  1 + 0im\n  2 - 1im\n\n\n한편 이 매트릭스 \\({\\bf B}\\)는 아래와 같이 고유분해가 가능하다.\n\\[ {\\bf B} = {\\bf \\Psi} {\\bf \\Lambda} {\\bf \\Psi}^\\ast\\]\n\n\\({\\bf \\Psi}\\): make \\(\\frac{1}{\\sqrt{N}}[e^{\\sqrt{-1} \\frac{2\\pi}{N} ij}~\\text{ for }~ i=0,1,2,\\dots,N-1~\\text{ for }~j=0,1,2,\\dots,N-1]\\) and apply reshape function with \\((N,N)\\).\n\\({\\bf \\Lambda}\\): make \\([e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}~\\text{ for }~ i=0,1,2\\dots,N-1]\\) and apply Diagonal function.\n\n\nN = 4 \nλ = [exp(-im * (2π/N) *i) for i in 0:(N-1)]\nΛ = Diagonal(λ)\n_Ψ = 1/√N *[exp(im * (2π/N) * i*j) for i in 0:(N-1) for j in 0:(N-1)]\nΨ = reshape(_Ψ, (N,N))\nB ≈ Ψ * Λ * Ψ'\n\ntrue\n\n\n그런데 위에서 정의된 \\({\\bf \\Psi}^\\ast\\)는 우리가 그전에 정의하였던 \\({\\bf DFT}\\)의 행렬과 같다.\n\n_DFT = reshape([i*j for i in 0:3 for j in 0:3], (4,4))\nDFT = _DFT .|&gt; (x -&gt; exp(-im * (2π/4) * x)) \nDFT = DFT * 1/√N\n\n4×4 Matrix{ComplexF64}:\n 0.5-0.0im           0.5-0.0im          …           0.5-0.0im\n 0.5-0.0im   3.06162e-17-0.5im             -9.18485e-17+0.5im\n 0.5-0.0im          -0.5-6.12323e-17im             -0.5-1.83697e-16im\n 0.5-0.0im  -9.18485e-17+0.5im              2.75546e-16-0.5im\n\n\n\nΨ' == DFT \n\ntrue\n\n\n결국 요약하면 길이가 \\(N\\)인 신호의 \\({\\bf DFT}\\)행렬은 아래의 과정으로 구할 수 있음을 알 수 있다.\n\nForward operator \\({\\bf A}\\)를 정의한다.\n\\({\\bf A}\\)의 고유벡터행렬 \\({\\bf \\Psi}\\)을 구한다. 4\n\\({\\bf \\Psi}\\)의 conjugate transpose matrix \\({\\bf \\Psi}^\\ast\\) 를 구한다. 이것이 \\({\\bf DFT}\\) matrix 이다. 5\n\n4 고유벡터행렬은 고유값 \\(e^{-\\sqrt{-1}\\frac{2\\pi}{N}i}\\)에 의하여 정렬되어 있어야 함.5 사실 이미 대칭행렬이므로 conjugate matrix만 구하면 된다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "href": "posts/2_Studies/CGSP/2022-12-24-Chap-8.3.html#spectral-component-and-frequencies",
    "title": "[CGSP] Chap 8.3: Discrete Fourier Transform",
    "section": "Spectral component and Frequencies",
    "text": "Spectral component and Frequencies\n\\({\\bf A}\\)의 고유벡터 \\({\\bf \\Psi}\\)의 각 column을 spectral component라고 부른다.\n\nψ₁ = Ψ[:,1] # ψ₁ is first spectral component \nψ₂ = Ψ[:,2] # ψ₂ is seconde spectral component \nψ₃ = Ψ[:,3] # ψ₃ is third spectral component \nψ₄ = Ψ[:,4] # ψ₄ is last spectral component\n\n그리고 아래와 같은 수열을 \\(\\Omega_{k}=\\frac{k}{N}\\)을 frequency 라고 부른다.\n\nN=4 \nΩ = [k/N for k in 0:(N-1)]\nΩ\n\n4-element Vector{Float64}:\n 0.0\n 0.25\n 0.5\n 0.75"
  },
  {
    "objectID": "4_notes.html",
    "href": "4_notes.html",
    "title": "Notes",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 9, 2000\n\n\n[Note] docker\n\n\n신록예찬 \n\n\n\n\nJan 8, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬 \n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬 \n\n\n\n\nJan 6, 2000\n\n\n[Note] 깃 익히기\n\n\n신록예찬 \n\n\n\n\nJan 5, 2000\n\n\n[Note] vi 익히기\n\n\n신록예찬 \n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬 \n\n\n\n\nJan 2, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅2\n\n\n신록예찬 \n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅 (old)\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "",
    "text": "using LinearAlgebra, DSP"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#simultaneously-diagonalizable",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Simultaneously Diagonalizable",
    "text": "Simultaneously Diagonalizable\n매트릭스 \\({\\bf A}\\)와 \\({\\bf B}\\)가 대각화 가능하다는 것은 아래의 표현을 만족하는 적당한 invertible matrix \\({\\bf \\Psi}_A\\), \\({\\bf \\Psi}_B\\)와 대각행렬 \\({\\bf \\Lambda}_A\\), \\({\\bf \\Lambda}_B\\)가 존재한다는 의미가 된다.\n\\[{\\bf A} = {\\bf V}_{A} {\\bf \\Lambda}_A {\\bf V}_{A}^{-1}\\]\n\\[{\\bf B} = {\\bf V}_{B} {\\bf \\Lambda}_B {\\bf V}_{B}^{-1}\\]\n그리고 만약에 \\({\\bf V}_{A}={\\bf V}_{B}\\)이라면 즉\n\\[{\\bf A} = {\\bf V} {\\bf \\Lambda}_A {\\bf V}^{-1}\\]\n\\[{\\bf B} = {\\bf V} {\\bf \\Lambda}_B {\\bf V}^{-1}\\]\n이라면 \\(\\{{\\bf A},{\\bf B}\\}\\)가 simultaneously diagonalzable 하다고 표현한다."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#commute",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Commute",
    "text": "Commute\n두 matrix \\({\\bf A}\\)와 \\({\\bf B}\\)에 대하여\n\\[{\\bf A}{\\bf B}= {\\bf B}{\\bf A}\\]\n인 관계가 성립하면 두 매트릭스가 commute 한다고 표현한다. 그런데 \\({\\bf A}{\\bf B}={\\bf A}{\\bf B}\\)의 조건은 \\({\\bf A}, {\\bf B}\\)가 동시대각화가능할 (simultaneously diagonalzable) 조건과 같다. 1 따라서 simultaneously diagonalzable 는 commute와 같은 말이라 생각해도 무방하다.\n1 필요충분조건이다.\n참고: 위키피디아.."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#shift-invariant-filter",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Shift Invariant Filter",
    "text": "Shift Invariant Filter\n\nref: Djuric and Richard (2018) Chap 8.3 의 내용 중 일부\n\nDjuric, Petar, and Cédric Richard. 2018. Cooperative and Graph Signal Processing: Principles and Applications. Academic Press.\n\nDefine the matrix \\({\\bf B}\\) as periodic shift matrix such that\n\\[\n{\\bf B} = \\begin{bmatrix}\n0 & 0 & 0 & \\dots  & 0 & 1 \\\\\n1 & 0 & 0 & \\dots & 0 & 0 \\\\\n0 & 1 & 0 & \\dots & 0 & 0 \\\\\n\\dots & \\dots & \\dots & \\dots & \\dots & \\dots\\\\\n0 & 0 & \\dots & 1 & 0 & 0 \\\\\n0 & 0 & \\dots & 0 & 1 & 0 \\\\\n\\end{bmatrix}.\\]\nA generic filter \\({\\boldsymbol h}\\) is given by its \\(z\\)-transform\n\\[h(z)=h_0z^0+h_1z^{-1}+\\cdots +h_{N-1}z^{-(N-1)}\\]\nwhere \\(s_{n-1}=z^{-1}s_n\\). In vector notation, and with respect to the standard basis \\({\\bf I}\\), the filter is represented by the matrix \\({\\bf H}\\), a polynomial in the cyclic shift\n\\[{\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+\\cdots+h_{N-1}{\\bf B}^{N-1}.\\]\nFilters are shift invariant iff\n\\[z\\cdot h(z) = h(z)\\cdot z\\]\nor from the matrix representation\n\\[{\\bf B}h({\\bf B})=h({\\bf B}){\\bf B}.\\]\nExample\nLet \\({\\bf B}\\) as\n\nB= [0 1 0 0 0 0 0\n    0 0 1 0 0 0 0 \n    0 0 0 1 0 0 0 \n    0 0 0 0 1 0 0 \n    0 0 0 0 0 1 0 \n    0 0 0 0 0 0 1 \n    1 0 0 0 0 0 0]\n\n7×7 Matrix{Int64}:\n 0  1  0  0  0  0  0\n 0  0  1  0  0  0  0\n 0  0  0  1  0  0  0\n 0  0  0  0  1  0  0\n 0  0  0  0  0  1  0\n 0  0  0  0  0  0  1\n 1  0  0  0  0  0  0\n\n\nDefine \\({\\boldsymbol h}\\) as\n\nh = [1/3,1/3,1/3]\n\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\n\nFurthermore define \\({\\bf H}=h({\\bf B})=h_0{\\bf B}^0+h_1{\\bf B}^1+h_2{\\bf B}^2\\)\n\nH = (1/3)*B^0 + (1/3)*B^1 + (1/3)*B^2 \n\n7×7 Matrix{Float64}:\n 0.333333  0.333333  0.333333  0.0       0.0       0.0       0.0\n 0.0       0.333333  0.333333  0.333333  0.0       0.0       0.0\n 0.0       0.0       0.333333  0.333333  0.333333  0.0       0.0\n 0.0       0.0       0.0       0.333333  0.333333  0.333333  0.0\n 0.0       0.0       0.0       0.0       0.333333  0.333333  0.333333\n 0.333333  0.0       0.0       0.0       0.0       0.333333  0.333333\n 0.333333  0.333333  0.0       0.0       0.0       0.0       0.333333\n\n\nObserve following:\n\nB*H == H*B \n\ntrue\n\n\nThus, filter \\({\\boldsymbol h}\\) is shift invariant filter and matrix \\({\\bf H}\\) is shift invariant operator.\nnote: \\({\\boldsymbol h}\\) is moving average filter.\nnote: for any \\({\\bf x}\\), \\({\\bf H}{\\bf x}\\) is definded by\n\\[\\left[\\frac{x_{n-1}+x_n+x_1}{3},\\frac{x_n+x_1+x_2}{3},\\dots,\\frac{x_{n-3}+x_{n-2}+x_n}{3}\\right].\\]\n\nx = [1,1,1,1,2,2,2]\nH*x\n\n7-element Vector{Float64}:\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666665\n 2.0\n 1.6666666666666665\n 1.3333333333333333\n\n\nnote: In some sense, the matrix \\({\\bf H}{\\bf x}\\) can be thought as generalized version of \\({\\boldsymbol h}\\star {\\bf x}\\) where \\(\\star\\) is convolution up to shift\n\nconv(h,x)\n\n9-element Vector{Float64}:\n 0.3333333333333334\n 0.6666666666666667\n 1.0\n 1.0\n 1.3333333333333333\n 1.6666666666666667\n 2.0\n 1.3333333333333333\n 0.6666666666666667\n\n\nFinally, we observe that, from the Cayley-Hamilton Theorem, \\({\\bf B}\\) satisfies its characteristic polynomial \\(\\Delta({\\bf B})\\), where \\(\\Delta(\\lambda)\\) is the determinant of \\(\\lambda{\\bf I}-{\\bf B}\\). The characteristic polynomial \\(\\Delta({\\bf B})\\) has degree \\(N\\), so, in DSP, as described so far, linear filters are (matrix) polynomial with degree at most \\(N-1\\).\n\n이 부분은 책에 써있길래 가져오긴 했는데, 무슨 의미인지 모르겠음"
  },
  {
    "objectID": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "href": "posts/2_Studies/CGSP/2022-12-26-Chap-12.2.html#coexisting-approaches",
    "title": "[CGSP] Chap 12.2: Weakly Stationary Graph Processes",
    "section": "Coexisting Approaches",
    "text": "Coexisting Approaches\nStationary graph processes were first defined and analyzed in (Girault 2015). The fundamental problem identified there is that GSOs do not preserve energy in general and therefore cannot be isometric (Gavili and Zhang 2017). This problem is addressed in (Girault, Gonçalves, and Fleury 2015) with the definition of an isometric graph shift that preserves the eigenvector space of the Laplacian GSO but modifies its eigenvalues.\n\nGirault, Benjamin. 2015. “Stationary Graph Signals Using an Isometric Graph Translation.” In 2015 23rd European Signal Processing Conference (EUSIPCO), 1516–20. IEEE.\n\nGavili, Adnan, and Xiao-Ping Zhang. 2017. “On the Shift Operator, Graph Frequency, and Optimal Filtering in Graph Signal Processing.” IEEE Transactions on Signal Processing 65 (23): 6303–18.\n\nGirault, Benjamin, Paulo Gonçalves, and Éric Fleury. 2015. “Translation on Graphs: An Isometric Shift Operator.” IEEE Signal Processing Letters 22 (12): 2416–20.\nA stationary graph process is then defined as one whose probability distributions are invariant with respect to multiplications with the isometric shift. One drawback of this approach is that the isometric shift is a complex-valued operator and has a sparsity structure (if any) different from \\({\\bf S}\\). By contrast, the vertex-based definition in\n\\[\\mathbb{E} \\bigg[ \\big({\\bf S}^a{\\bf x}\\big)\\Big(\\big({\\bf S}^H)^b {\\bf x}\\Big)^H  \\bigg]=\\mathbb{E}\\bigg[\\big({\\bf S}^{a+c}{\\bf x}\\big)\\Big(\\big({\\bf S}^H\\big)^{b-c}{\\bf x} \\Big)^H \\bigg]\\]\nis based on the original GSO \\({\\bf S}\\), which is local and real-valued. As a result, above Eq. provides intuition on the relations between stationarity and locality, which can be leveraged to develop stationarity tests or estimation schemes that work with local information. Graph stationarity was also studied in (Perraudin and Vandergheynst 2017) where the requirement of having a covariance matrix diagonalizable by the eigenvectors of the Laplacian GSO is adopted as a definition. This condition is shown to be equivalent to statistical invariance with respect to the translation operator introduced in (Shuman, Ricaud, and Vandergheynst 2016). When the shift \\({\\bf S}\\) coincides with the Laplacian of the graph and the eigenvalues of \\({\\bf S}\\) are all distinct, Definitions 12.1 and 12.2 are equivalent to those in Perraudin and Vandergheynst (2017). Hence, the definitions presented here differ from (Perraudin and Vandergheynst 2017) in that we consider general normal shifts instead of Laplacians and that we see Definition 12.1 as a definition, not a property. These are mathematically minor differences that are important in practice though; see Segarra et al. (2017) for more details.\n\nPerraudin, Nathanaël, and Pierre Vandergheynst. 2017. “Stationary Signal Processing on Graphs.” IEEE Transactions on Signal Processing 65 (13): 3462–77.\n\nShuman, David I, Benjamin Ricaud, and Pierre Vandergheynst. 2016. “Vertex-Frequency Analysis on Graphs.” Applied and Computational Harmonic Analysis 40 (2): 260–91.\n\nSegarra, Santiago, Antonio G Marques, Gonzalo Mateos, and Alejandro Ribeiro. 2017. “Network Topology Inference from Spectral Templates.” IEEE Transactions on Signal and Information Processing over Networks 3 (3): 467–83."
  },
  {
    "objectID": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "href": "posts/2_Studies/CGSP/2023-01-15-Chap-12.4.html",
    "title": "[CGSP] Chap 12.4: Node Subsampling for PSD Estimation",
    "section": "",
    "text": "using LinearAlgebra, Plots, FFTW, Statistics\n\n\ncolumnwise_kron = \n(C,D) -&gt; hcat([kron(C[:,i],D[:,i]) for i in 1:size(C)[2]]...)\n\n#49 (generic function with 1 method)\n\n\n\n12.4.1 The Sampling Problem\n아래와 같이 길이가 \\(N=10\\) 인 신호 \\({\\bf x}\\)를 고려하자.\n\nx = rand(10)\n\n10-element Vector{Float64}:\n 0.03235208758206609\n 0.5069925854414447\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n 0.24116013388795854\n 0.8439116925218157\n 0.6362602319916778\n 0.386069828675059\n 0.5313655894235898\n\n\n여기에서 1,3,4,5 번째 원소만 추출하여길이가 \\(K=4\\) 인 신호 \\({\\bf y}\\)를 만들고 싶다.\n\ny = x[[1,3,4,5]]\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n이 과정은 아래와 같이 수행할 수도 있다.\n\nΦ= [1 0 0 0 0 0 0 0 0 0\n    0 0 1 0 0 0 0 0 0 0\n    0 0 0 1 0 0 0 0 0 0\n    0 0 0 0 1 0 0 0 0 0]\n\n4×10 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0\n 0  0  0  1  0  0  0  0  0  0\n 0  0  0  0  1  0  0  0  0  0\n\n\n\nΦ*x\n\n4-element Vector{Float64}:\n 0.03235208758206609\n 0.5795228508497553\n 0.682832351742401\n 0.64422613488741\n\n\n즉 적당한 \\(K\\times N\\) selection matrix를 선언하여 subsampling을 수행할 수 있다. 이때 매트릭스 \\({\\bf \\Phi}\\)를 subsampling matrix 혹은 sparse sampling matrix 라고 부른다.\n\n\n12.4.2 Compressed LS Estimator\n\nN = 10\nV = [i*j for i in 0:(N-1) for j in 0:(N-1)] |&gt; \n    x -&gt; reshape(x,(N,N)) .|&gt; \n    x -&gt; exp(im * (2π/N) * x) \n\n10×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n\n\n\nG = columnwise_kron(conj(V),V)\n\n100×10 Matrix{ComplexF64}:\n 1.0+0.0im        1.0+0.0im          …        1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0+1.22465e-16im  …       -1.0+1.10218e-15im\n 1.0+0.0im  -0.809017-0.587785im        -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im   0.809017-0.587785im     …   0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n    ⋮                                ⋱  \n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n 1.0+0.0im   0.809017+0.587785im         0.809017-0.587785im\n 1.0+0.0im   0.809017+0.587785im     …   0.809017-0.587785im\n 1.0+0.0im   0.309017+0.951057im         0.309017-0.951057im\n 1.0+0.0im  -0.309017+0.951057im        -0.309017-0.951057im\n 1.0+0.0im  -0.809017+0.587785im        -0.809017-0.587785im\n 1.0+0.0im       -1.0-1.11022e-16im          -1.0+2.27596e-15im\n 1.0+0.0im  -0.809017-0.587785im     …  -0.809017+0.587785im\n 1.0+0.0im  -0.309017-0.951057im        -0.309017+0.951057im\n 1.0+0.0im   0.309017-0.951057im         0.309017+0.951057im\n 1.0+0.0im   0.809017-0.587785im         0.809017+0.587785im\n 1.0+0.0im        1.0+0.0im                   1.0+0.0im\n\n\n- 방법1\n\nĉx = vec(x*x')\np̂ = inv(G' * G) * G' * ĉx\n\n10-element Vector{ComplexF64}:\n    0.25854107856772546 + 2.245922875954761e-20im\n   0.004743491121735806 - 1.3138893409553828e-18im\n   0.006946482731189413 - 9.791191432641327e-19im\n   0.001721693617954179 - 1.9827974128203887e-18im\n   0.011344167525098774 + 2.6827005818057562e-19im\n 0.00012662617844242917 - 3.748573865136995e-20im\n   0.011344167525098762 + 2.7448152053954017e-18im\n  0.0017216936179541913 - 9.35534609073096e-19im\n   0.006946482731189404 + 1.954408900185458e-18im\n   0.004743491121735756 - 2.561030398375897e-18im\n\n\n- 방법2\n\nĉy = vec(y*y')\np̂ = (kron(Φ,Φ)*G)' * ĉy\n\n10-element Vector{ComplexF64}:\n   3.759462826821233 + 0.0im\n   2.765185174577697 - 2.0816681711721685e-17im\n   1.077337414764992 + 2.7755575615628914e-17im\n 0.11594812606807317 + 2.0816681711721685e-17im\n 0.08838298603932843 + 3.903127820947816e-17im\n 0.32863702713833354 + 4.622231866529366e-33im\n 0.08838298603932859 + 9.540979117872439e-18im\n  0.1159481260680729 - 2.0816681711721685e-17im\n  1.0773374147649915 + 0.0im\n  2.7651851745776965 - 2.0816681711721685e-17im"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-009-타이타닉, best_quality.html",
    "href": "posts/2_Studies/MP/02wk-009-타이타닉, best_quality.html",
    "title": "02wk-009: 타이타닉, best_quality",
    "section": "",
    "text": "from autogluon.tabular import TabularDataset, TabularPredictor\n\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")\n\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_033356/\"\n\n\n\npredictr.fit(tr,presets='best_quality')\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_033356/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.52 GB / 490.57 GB (62.5%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    122497.58 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6319   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.58s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.89s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.07s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8507   = Validation score   (accuracy)\n    2.73s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.94s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    4.36s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.42s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 22.95s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_033356/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f9da8145180&gt;\n\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.025074  1.894616                0.025074           1.894616            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.026225  2.306724                0.001151           0.412107            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.078130  2.732091                0.078130           2.732091            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029351  1.422157                0.029351           1.422157            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.033552  0.523090                0.033552           0.523090            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.044913  0.940203                0.044913           0.940203            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.085953  4.356180                0.085953           4.356180            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.070430  0.580106                0.070430           0.580106            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.108941  0.360885                0.108941           0.360885            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.028826  0.408408                0.028826           0.408408            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.070786  0.320610                0.070786           0.320610            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.070738  0.418201                0.070738           0.418201            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001816  0.003192                0.001816           0.003192            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.001942  0.003109                0.001942           0.003109            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.025074\n1.894616\n0.025074\n1.894616\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.026225\n2.306724\n0.001151\n0.412107\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.078130\n2.732091\n0.078130\n2.732091\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029351\n1.422157\n0.029351\n1.422157\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.033552\n0.523090\n0.033552\n0.523090\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.044913\n0.940203\n0.044913\n0.940203\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.085953\n4.356180\n0.085953\n4.356180\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.070430\n0.580106\n0.070430\n0.580106\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.108941\n0.360885\n0.108941\n0.360885\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.028826\n0.408408\n0.028826\n0.408408\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.070786\n0.320610\n0.070786\n0.320610\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.070738\n0.418201\n0.070738\n0.418201\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001816\n0.003192\n0.001816\n0.003192\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.001942\n0.003109\n0.001942\n0.003109\n1\nTrue\n1\n\n\n\n\n\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean()\n\n0.7867564534231201\n\n\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n\n#!conda install -c conda-forge pygraphviz -y\n\n\npredictr.plot_ensemble_model()\n\n'AutogluonModels/ag-20230910_033356/ensemble_model.png'\n\n\n\n#tst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01_invalidformat.csv\")\ntst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01.csv\",index=False)\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.025074  1.894616                0.025074           1.894616            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.026225  2.306724                0.001151           0.412107            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.078130  2.732091                0.078130           2.732091            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029351  1.422157                0.029351           1.422157            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.033552  0.523090                0.033552           0.523090            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.044913  0.940203                0.044913           0.940203            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.085953  4.356180                0.085953           4.356180            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.070430  0.580106                0.070430           0.580106            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.108941  0.360885                0.108941           0.360885            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.028826  0.408408                0.028826           0.408408            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.070786  0.320610                0.070786           0.320610            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.070738  0.418201                0.070738           0.418201            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001816  0.003192                0.001816           0.003192            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.001942  0.003109                0.001942           0.003109            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.025074\n1.894616\n0.025074\n1.894616\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.026225\n2.306724\n0.001151\n0.412107\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.078130\n2.732091\n0.078130\n2.732091\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029351\n1.422157\n0.029351\n1.422157\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.033552\n0.523090\n0.033552\n0.523090\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.044913\n0.940203\n0.044913\n0.940203\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.085953\n4.356180\n0.085953\n4.356180\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.070430\n0.580106\n0.070430\n0.580106\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.108941\n0.360885\n0.108941\n0.360885\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.028826\n0.408408\n0.028826\n0.408408\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.070786\n0.320610\n0.070786\n0.320610\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.070738\n0.418201\n0.070738\n0.418201\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001816\n0.003192\n0.001816\n0.003192\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.001942\n0.003109\n0.001942\n0.003109\n1\nTrue\n1"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-007-오버피팅.html",
    "href": "posts/2_Studies/MP/02wk-007-오버피팅.html",
    "title": "02wk-007: 오버피팅",
    "section": "",
    "text": "1. 개념"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-008-Autogluon, Fit.html",
    "href": "posts/2_Studies/MP/02wk-008-Autogluon, Fit.html",
    "title": "02wk-008: Autogluon, Fit",
    "section": "",
    "text": "1. Autogluon"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-009-Autoglueon에서 타이타닉 분석.html",
    "href": "posts/2_Studies/MP/01wk-009-Autoglueon에서 타이타닉 분석.html",
    "title": "01wk-009: 타이타닉 자료 분석",
    "section": "",
    "text": "imports\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport os\nos.environ['KAGGLE_CONFIG_DIR'] = \".\"\n\n\n\n데이터 download\n- 캐글에서 자료를 다운로드\n\n!chmod 600 ./kaggle.json\n!kaggle competitions download -c bike-sharing-demand\n!unzip bike-sharing-demand.zip -d ./data\n!rm bike-sharing-demand.zip\n\nDownloading bike-sharing-demand.zip to /home/cgb2/Dropbox/03_yechan3/posts/2_Studies/MP\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 430kB/s]\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 429kB/s]\nArchive:  bike-sharing-demand.zip\n  inflating: ./data/sampleSubmission.csv  \n  inflating: ./data/test.csv         \n  inflating: ./data/train.csv        \n\n\n- 잘 있나 확인\n\n!ls ./data\n\nsampleSubmission.csv  test.csv  train.csv\n\n\n\n!head ./data/train.csv\n\ndatetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n2011-01-01 00:00:00,1,0,0,1,9.84,14.395,81,0,3,13,16\n2011-01-01 01:00:00,1,0,0,1,9.02,13.635,80,0,8,32,40\n2011-01-01 02:00:00,1,0,0,1,9.02,13.635,80,0,5,27,32\n2011-01-01 03:00:00,1,0,0,1,9.84,14.395,75,0,3,10,13\n2011-01-01 04:00:00,1,0,0,1,9.84,14.395,75,0,0,1,1\n2011-01-01 05:00:00,1,0,0,2,9.84,12.88,75,6.0032,0,1,1\n2011-01-01 06:00:00,1,0,0,1,9.02,13.635,80,0,2,0,2\n2011-01-01 07:00:00,1,0,0,1,8.2,12.88,86,0,1,2,3\n2011-01-01 08:00:00,1,0,0,1,9.84,14.395,75,0,1,7,8\n\n\n\n!tail ./data/train.csv\n\n2012-12-19 14:00:00,4,0,1,1,17.22,21.21,50,12.998,33,185,218\n2012-12-19 15:00:00,4,0,1,1,17.22,21.21,50,19.0012,28,209,237\n2012-12-19 16:00:00,4,0,1,1,17.22,21.21,50,23.9994,37,297,334\n2012-12-19 17:00:00,4,0,1,1,16.4,20.455,50,26.0027,26,536,562\n2012-12-19 18:00:00,4,0,1,1,15.58,19.695,50,23.9994,23,546,569\n2012-12-19 19:00:00,4,0,1,1,15.58,19.695,50,26.0027,7,329,336\n2012-12-19 20:00:00,4,0,1,1,14.76,17.425,57,15.0013,10,231,241\n2012-12-19 21:00:00,4,0,1,1,13.94,15.91,61,15.0013,4,164,168\n2012-12-19 22:00:00,4,0,1,1,13.94,17.425,61,6.0032,12,117,129\n2012-12-19 23:00:00,4,0,1,1,13.12,16.665,66,8.9981,4,84,88\n\n\n\n\nautogluon을 이용한 첫 분석\n- autogluon 을 이용하여 data load\n\ntrain_data = TabularDataset('./data/train.csv').drop(['casual','registered'],axis=1)\ntest_data = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 10886 -&gt; 10886\nLoaded data from: ./data/test.csv | Columns = 9 / 9 | Rows = 6493 -&gt; 6493\n\n\n\ntrain_data.head(5)\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\ncount\n\n\n\n\n0\n2011-01-01 00:00:00\n1\n0\n0\n1\n9.84\n14.395\n81\n0.0\n16\n\n\n1\n2011-01-01 01:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0\n40\n\n\n2\n2011-01-01 02:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0\n32\n\n\n3\n2011-01-01 03:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0\n13\n\n\n4\n2011-01-01 04:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0\n1\n\n\n\n\n\n\n\n\ntest_data.head(5)\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\n\n\n\n\n0\n2011-01-20 00:00:00\n1\n0\n1\n1\n10.66\n11.365\n56\n26.0027\n\n\n1\n2011-01-20 01:00:00\n1\n0\n1\n1\n10.66\n13.635\n56\n0.0000\n\n\n2\n2011-01-20 02:00:00\n1\n0\n1\n1\n10.66\n13.635\n56\n0.0000\n\n\n3\n2011-01-20 03:00:00\n1\n0\n1\n1\n10.66\n12.880\n56\n11.0014\n\n\n4\n2011-01-20 04:00:00\n1\n0\n1\n1\n10.66\n12.880\n56\n11.0014\n\n\n\n\n\n\n\n\npredictor = TabularPredictor(label='count',problem_type='regression')\npredictor.fit(train_data)\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230904_133352/\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230904_133352/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   311.66 GB / 490.57 GB (63.5%)\nTrain Data Rows:    10886\nTrain Data Columns: 9\nLabel Column: count\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    122066.47 MB\n    Train Data (Original)  Memory Usage: 1.52 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 2 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting DatetimeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])                      : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                        : 5 | ['season', 'holiday', 'workingday', 'weather', 'humidity']\n        ('object', ['datetime_as_object']) : 1 | ['datetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                  : 3 | ['season', 'weather', 'humidity']\n        ('int', ['bool'])            : 2 | ['holiday', 'workingday']\n        ('int', ['datetime_as_int']) : 5 | ['datetime', 'datetime.year', 'datetime.month', 'datetime.day', 'datetime.dayofweek']\n    0.0s = Fit runtime\n    9 features in original data used to generate 13 features in processed data.\n    Train Data (Processed) Memory Usage: 0.98 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 9797, Val Rows: 1089\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -109.9845    = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    -92.5189     = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    -135.958     = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    -134.0804    = Validation score   (-root_mean_squared_error)\n    0.41s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -122.0128    = Validation score   (-root_mean_squared_error)\n    0.83s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    -134.2362    = Validation score   (-root_mean_squared_error)\n    2.7s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -128.4294    = Validation score   (-root_mean_squared_error)\n    0.44s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -136.518     = Validation score   (-root_mean_squared_error)\n    5.25s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    -135.0751    = Validation score   (-root_mean_squared_error)\n    0.42s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    -139.9605    = Validation score   (-root_mean_squared_error)\n    13.23s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    -132.1736    = Validation score   (-root_mean_squared_error)\n    0.68s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -92.5189     = Validation score   (-root_mean_squared_error)\n    0.21s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 25.48s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230904_133352/\")\n\n\n[1000]  valid_set's rmse: 136.065\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f5e7a19aa10&gt;\n\n\n\npredictions = predictor.predict(train_data)\n\n\npredictor.leaderboard(train_data)\n\n                  model  score_test   score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0        KNeighborsDist  -29.277866  -92.518908        0.018920       0.002748   0.010689                 0.018920                0.002748           0.010689            1       True          2\n1   WeightedEnsemble_L2  -29.277866  -92.518908        0.019943       0.003013   0.222456                 0.001024                0.000265           0.211767            2       True         12\n2       RandomForestMSE  -56.773851 -122.012844        0.205197       0.034002   0.831343                 0.205197                0.034002           0.831343            1       True          5\n3         ExtraTreesMSE  -59.892225 -128.429416        0.103312       0.034829   0.435451                 0.103312                0.034829           0.435451            1       True          7\n4        KNeighborsUnif  -81.605878 -109.984461        0.018461       0.002700   0.010497                 0.018461                0.002700           0.010497            1       True          1\n5               XGBoost  -87.981046 -135.075087        0.047721       0.005103   0.422865                 0.047721                0.005103           0.422865            1       True          9\n6         LightGBMLarge  -95.485492 -132.173561        0.033100       0.003929   0.681926                 0.033100                0.003929           0.681926            1       True         11\n7              LightGBM -105.679214 -134.080427        0.045547       0.004989   0.412630                 0.045547                0.004989           0.412630            1       True          4\n8              CatBoost -109.803887 -134.236163        0.005125       0.001562   2.696200                 0.005125                0.001562           2.696200            1       True          6\n9            LightGBMXT -118.022777 -135.958034        0.086194       0.008725   0.655559                 0.086194                0.008725           0.655559            1       True          3\n10      NeuralNetFastAI -127.771403 -136.518011        0.082512       0.009437   5.253613                 0.082512                0.009437           5.253613            1       True          8\n11       NeuralNetTorch -133.776883 -139.960481        0.019109       0.006050  13.230059                 0.019109                0.006050          13.230059            1       True         10\n\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nKNeighborsDist\n-29.277866\n-92.518908\n0.018920\n0.002748\n0.010689\n0.018920\n0.002748\n0.010689\n1\nTrue\n2\n\n\n1\nWeightedEnsemble_L2\n-29.277866\n-92.518908\n0.019943\n0.003013\n0.222456\n0.001024\n0.000265\n0.211767\n2\nTrue\n12\n\n\n2\nRandomForestMSE\n-56.773851\n-122.012844\n0.205197\n0.034002\n0.831343\n0.205197\n0.034002\n0.831343\n1\nTrue\n5\n\n\n3\nExtraTreesMSE\n-59.892225\n-128.429416\n0.103312\n0.034829\n0.435451\n0.103312\n0.034829\n0.435451\n1\nTrue\n7\n\n\n4\nKNeighborsUnif\n-81.605878\n-109.984461\n0.018461\n0.002700\n0.010497\n0.018461\n0.002700\n0.010497\n1\nTrue\n1\n\n\n5\nXGBoost\n-87.981046\n-135.075087\n0.047721\n0.005103\n0.422865\n0.047721\n0.005103\n0.422865\n1\nTrue\n9\n\n\n6\nLightGBMLarge\n-95.485492\n-132.173561\n0.033100\n0.003929\n0.681926\n0.033100\n0.003929\n0.681926\n1\nTrue\n11\n\n\n7\nLightGBM\n-105.679214\n-134.080427\n0.045547\n0.004989\n0.412630\n0.045547\n0.004989\n0.412630\n1\nTrue\n4\n\n\n8\nCatBoost\n-109.803887\n-134.236163\n0.005125\n0.001562\n2.696200\n0.005125\n0.001562\n2.696200\n1\nTrue\n6\n\n\n9\nLightGBMXT\n-118.022777\n-135.958034\n0.086194\n0.008725\n0.655559\n0.086194\n0.008725\n0.655559\n1\nTrue\n3\n\n\n10\nNeuralNetFastAI\n-127.771403\n-136.518011\n0.082512\n0.009437\n5.253613\n0.082512\n0.009437\n5.253613\n1\nTrue\n8\n\n\n11\nNeuralNetTorch\n-133.776883\n-139.960481\n0.019109\n0.006050\n13.230059\n0.019109\n0.006050\n13.230059\n1\nTrue\n10\n\n\n\n\n\n\n\n\nnp.sqrt(np.mean((train_data['count'] - predictions)**2))\n\n29.277866163077135\n\n\n\nplt.plot(train_data['count'][:200])\nplt.plot(predictions[:200])\n\n\n\n\n- 테스트도 잘 맞출까?\n\npredictor.predict(test_data) \n\n0        51.861626\n1        56.686295\n2        59.077763\n3        60.468826\n4        61.419930\n           ...    \n6488    287.868591\n6489    287.871124\n6490    287.873596\n6491    287.876099\n6492    287.878662\nName: count, Length: 6493, dtype: float32\n\n\n\n잘 맞춘지 어떻게 알지?\n\n\n!rm submit1.csv\ntest_data.assign(count= predictor.predict(test_data)).loc[:,['datetime','count']].to_csv('submit1.csv',index=False)\n\n\npredictor.predict(test_data)\n\n0        51.861626\n1        56.686295\n2        59.077763\n3        60.468826\n4        61.419930\n           ...    \n6488    287.868591\n6489    287.871124\n6490    287.873596\n6491    287.876099\n6492    287.878662\nName: count, Length: 6493, dtype: float32\n\n\n\n\nautogluon을 이용한 첫 분석\n- autogluon 을 이용하여 data load\n\ntrain_data = TabularDataset('./data/train.csv').drop(['casual','registered'],axis=1)\ntest_data = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 10886 -&gt; 10886\nLoaded data from: ./data/test.csv | Columns = 9 / 9 | Rows = 6493 -&gt; 6493\n\n\n\ntrain_data['count'].max()\n\n977\n\n\n\nnp.random.randint(1,977,size=6493)\n\narray([560, 630, 193, ..., 487, 526, 791])\n\n\n\ntest_data.assign(count=np.random.randint(1,977,size=6493))\\\n.loc[:,['datetime','count']]\\\n.to_csv('submit0.csv',index=False)\n\n\n!cat ./data/sampleSubmission.csv\n\ndatetime,count\n2011-01-20 00:00:00,0\n2011-01-20 01:00:00,0\n2011-01-20 02:00:00,0\n2011-01-20 03:00:00,0\n2011-01-20 04:00:00,0\n2011-01-20 05:00:00,0\n2011-01-20 06:00:00,0\n2011-01-20 07:00:00,0\n2011-01-20 08:00:00,0\n2011-01-20 09:00:00,0\n2011-01-20 10:00:00,0\n2011-01-20 11:00:00,0\n2011-01-20 12:00:00,0\n2011-01-20 13:00:00,0\n2011-01-20 14:00:00,0\n2011-01-20 15:00:00,0\n2011-01-20 16:00:00,0\n2011-01-20 17:00:00,0\n2011-01-20 18:00:00,0\n2011-01-20 19:00:00,0\n2011-01-20 20:00:00,0\n2011-01-20 21:00:00,0\n2011-01-20 22:00:00,0\n2011-01-20 23:00:00,0\n2011-01-21 00:00:00,0\n2011-01-21 01:00:00,0\n2011-01-21 02:00:00,0\n2011-01-21 03:00:00,0\n2011-01-21 04:00:00,0\n2011-01-21 05:00:00,0\n2011-01-21 06:00:00,0\n2011-01-21 07:00:00,0\n2011-01-21 08:00:00,0\n2011-01-21 09:00:00,0\n2011-01-21 10:00:00,0\n2011-01-21 11:00:00,0\n2011-01-21 12:00:00,0\n2011-01-21 13:00:00,0\n2011-01-21 14:00:00,0\n2011-01-21 15:00:00,0\n2011-01-21 16:00:00,0\n2011-01-21 17:00:00,0\n2011-01-21 18:00:00,0\n2011-01-21 19:00:00,0\n2011-01-21 20:00:00,0\n2011-01-21 21:00:00,0\n2011-01-21 22:00:00,0\n2011-01-21 23:00:00,0\n2011-01-22 00:00:00,0\n2011-01-22 01:00:00,0\n2011-01-22 02:00:00,0\n2011-01-22 03:00:00,0\n2011-01-22 04:00:00,0\n2011-01-22 06:00:00,0\n2011-01-22 07:00:00,0\n2011-01-22 08:00:00,0\n2011-01-22 09:00:00,0\n2011-01-22 10:00:00,0\n2011-01-22 11:00:00,0\n2011-01-22 12:00:00,0\n2011-01-22 13:00:00,0\n2011-01-22 14:00:00,0\n2011-01-22 15:00:00,0\n2011-01-22 16:00:00,0\n2011-01-22 17:00:00,0\n2011-01-22 18:00:00,0\n2011-01-22 19:00:00,0\n2011-01-22 20:00:00,0\n2011-01-22 21:00:00,0\n2011-01-22 22:00:00,0\n2011-01-22 23:00:00,0\n2011-01-23 00:00:00,0\n2011-01-23 01:00:00,0\n2011-01-23 02:00:00,0\n2011-01-23 03:00:00,0\n2011-01-23 05:00:00,0\n2011-01-23 06:00:00,0\n2011-01-23 07:00:00,0\n2011-01-23 08:00:00,0\n2011-01-23 09:00:00,0\n2011-01-23 10:00:00,0\n2011-01-23 11:00:00,0\n2011-01-23 12:00:00,0\n2011-01-23 13:00:00,0\n2011-01-23 14:00:00,0\n2011-01-23 15:00:00,0\n2011-01-23 16:00:00,0\n2011-01-23 17:00:00,0\n2011-01-23 18:00:00,0\n2011-01-23 19:00:00,0\n2011-01-23 20:00:00,0\n2011-01-23 21:00:00,0\n2011-01-23 22:00:00,0\n2011-01-23 23:00:00,0\n2011-01-24 00:00:00,0\n2011-01-24 01:00:00,0\n2011-01-24 03:00:00,0\n2011-01-24 04:00:00,0\n2011-01-24 05:00:00,0\n2011-01-24 06:00:00,0\n2011-01-24 07:00:00,0\n2011-01-24 08:00:00,0\n2011-01-24 09:00:00,0\n2011-01-24 10:00:00,0\n2011-01-24 11:00:00,0\n2011-01-24 12:00:00,0\n2011-01-24 13:00:00,0\n2011-01-24 14:00:00,0\n2011-01-24 15:00:00,0\n2011-01-24 16:00:00,0\n2011-01-24 17:00:00,0\n2011-01-24 18:00:00,0\n2011-01-24 19:00:00,0\n2011-01-24 20:00:00,0\n2011-01-24 21:00:00,0\n2011-01-24 22:00:00,0\n2011-01-24 23:00:00,0\n2011-01-25 00:00:00,0\n2011-01-25 01:00:00,0\n2011-01-25 02:00:00,0\n2011-01-25 04:00:00,0\n2011-01-25 05:00:00,0\n2011-01-25 06:00:00,0\n2011-01-25 07:00:00,0\n2011-01-25 08:00:00,0\n2011-01-25 09:00:00,0\n2011-01-25 10:00:00,0\n2011-01-25 11:00:00,0\n2011-01-25 12:00:00,0\n2011-01-25 13:00:00,0\n2011-01-25 14:00:00,0\n2011-01-25 15:00:00,0\n2011-01-25 16:00:00,0\n2011-01-25 17:00:00,0\n2011-01-25 18:00:00,0\n2011-01-25 19:00:00,0\n2011-01-25 20:00:00,0\n2011-01-25 21:00:00,0\n2011-01-25 22:00:00,0\n2011-01-25 23:00:00,0\n2011-01-26 00:00:00,0\n2011-01-26 01:00:00,0\n2011-01-26 02:00:00,0\n2011-01-26 05:00:00,0\n2011-01-26 06:00:00,0\n2011-01-26 07:00:00,0\n2011-01-26 08:00:00,0\n2011-01-26 09:00:00,0\n2011-01-26 10:00:00,0\n2011-01-26 11:00:00,0\n2011-01-26 12:00:00,0\n2011-01-26 13:00:00,0\n2011-01-26 14:00:00,0\n2011-01-26 15:00:00,0\n2011-01-26 16:00:00,0\n2011-01-26 17:00:00,0\n2011-01-27 16:00:00,0\n2011-01-27 17:00:00,0\n2011-01-27 18:00:00,0\n2011-01-27 19:00:00,0\n2011-01-27 20:00:00,0\n2011-01-27 21:00:00,0\n2011-01-27 22:00:00,0\n2011-01-27 23:00:00,0\n2011-01-28 00:00:00,0\n2011-01-28 01:00:00,0\n2011-01-28 02:00:00,0\n2011-01-28 03:00:00,0\n2011-01-28 05:00:00,0\n2011-01-28 06:00:00,0\n2011-01-28 07:00:00,0\n2011-01-28 08:00:00,0\n2011-01-28 09:00:00,0\n2011-01-28 10:00:00,0\n2011-01-28 11:00:00,0\n2011-01-28 12:00:00,0\n2011-01-28 13:00:00,0\n2011-01-28 14:00:00,0\n2011-01-28 15:00:00,0\n2011-01-28 16:00:00,0\n2011-01-28 17:00:00,0\n2011-01-28 18:00:00,0\n2011-01-28 19:00:00,0\n2011-01-28 20:00:00,0\n2011-01-28 21:00:00,0\n2011-01-28 22:00:00,0\n2011-01-28 23:00:00,0\n2011-01-29 00:00:00,0\n2011-01-29 01:00:00,0\n2011-01-29 02:00:00,0\n2011-01-29 03:00:00,0\n2011-01-29 04:00:00,0\n2011-01-29 06:00:00,0\n2011-01-29 07:00:00,0\n2011-01-29 08:00:00,0\n2011-01-29 09:00:00,0\n2011-01-29 10:00:00,0\n2011-01-29 11:00:00,0\n2011-01-29 12:00:00,0\n2011-01-29 13:00:00,0\n2011-01-29 14:00:00,0\n2011-01-29 15:00:00,0\n2011-01-29 16:00:00,0\n2011-01-29 17:00:00,0\n2011-01-29 18:00:00,0\n2011-01-29 19:00:00,0\n2011-01-29 20:00:00,0\n2011-01-29 21:00:00,0\n2011-01-29 22:00:00,0\n2011-01-29 23:00:00,0\n2011-01-30 00:00:00,0\n2011-01-30 01:00:00,0\n2011-01-30 02:00:00,0\n2011-01-30 03:00:00,0\n2011-01-30 04:00:00,0\n2011-01-30 05:00:00,0\n2011-01-30 07:00:00,0\n2011-01-30 08:00:00,0\n2011-01-30 09:00:00,0\n2011-01-30 10:00:00,0\n2011-01-30 11:00:00,0\n2011-01-30 12:00:00,0\n2011-01-30 13:00:00,0\n2011-01-30 14:00:00,0\n2011-01-30 15:00:00,0\n2011-01-30 16:00:00,0\n2011-01-30 17:00:00,0\n2011-01-30 18:00:00,0\n2011-01-30 19:00:00,0\n2011-01-30 20:00:00,0\n2011-01-30 21:00:00,0\n2011-01-30 22:00:00,0\n2011-01-30 23:00:00,0\n2011-01-31 00:00:00,0\n2011-01-31 01:00:00,0\n2011-01-31 02:00:00,0\n2011-01-31 03:00:00,0\n2011-01-31 04:00:00,0\n2011-01-31 05:00:00,0\n2011-01-31 06:00:00,0\n2011-01-31 07:00:00,0\n2011-01-31 08:00:00,0\n2011-01-31 09:00:00,0\n2011-01-31 10:00:00,0\n2011-01-31 11:00:00,0\n2011-01-31 12:00:00,0\n2011-01-31 13:00:00,0\n2011-01-31 14:00:00,0\n2011-01-31 15:00:00,0\n2011-01-31 16:00:00,0\n2011-01-31 17:00:00,0\n2011-01-31 18:00:00,0\n2011-01-31 19:00:00,0\n2011-01-31 20:00:00,0\n2011-01-31 21:00:00,0\n2011-01-31 22:00:00,0\n2011-01-31 23:00:00,0\n2011-02-20 00:00:00,0\n2011-02-20 01:00:00,0\n2011-02-20 02:00:00,0\n2011-02-20 03:00:00,0\n2011-02-20 04:00:00,0\n2011-02-20 06:00:00,0\n2011-02-20 07:00:00,0\n2011-02-20 08:00:00,0\n2011-02-20 09:00:00,0\n2011-02-20 10:00:00,0\n2011-02-20 11:00:00,0\n2011-02-20 12:00:00,0\n2011-02-20 13:00:00,0\n2011-02-20 14:00:00,0\n2011-02-20 15:00:00,0\n2011-02-20 16:00:00,0\n2011-02-20 17:00:00,0\n2011-02-20 18:00:00,0\n2011-02-20 19:00:00,0\n2011-02-20 20:00:00,0\n2011-02-20 21:00:00,0\n2011-02-20 22:00:00,0\n2011-02-20 23:00:00,0\n2011-02-21 00:00:00,0\n2011-02-21 01:00:00,0\n2011-02-21 02:00:00,0\n2011-02-21 03:00:00,0\n2011-02-21 04:00:00,0\n2011-02-21 05:00:00,0\n2011-02-21 06:00:00,0\n2011-02-21 07:00:00,0\n2011-02-21 08:00:00,0\n2011-02-21 09:00:00,0\n2011-02-21 10:00:00,0\n2011-02-21 11:00:00,0\n2011-02-21 12:00:00,0\n2011-02-21 13:00:00,0\n2011-02-21 14:00:00,0\n2011-02-21 15:00:00,0\n2011-02-21 16:00:00,0\n2011-02-21 17:00:00,0\n2011-02-21 18:00:00,0\n2011-02-21 19:00:00,0\n2011-02-21 20:00:00,0\n2011-02-21 21:00:00,0\n2011-02-21 22:00:00,0\n2011-02-21 23:00:00,0\n2011-02-22 06:00:00,0\n2011-02-22 07:00:00,0\n2011-02-22 08:00:00,0\n2011-02-22 09:00:00,0\n2011-02-22 10:00:00,0\n2011-02-22 11:00:00,0\n2011-02-22 12:00:00,0\n2011-02-22 13:00:00,0\n2011-02-22 14:00:00,0\n2011-02-22 15:00:00,0\n2011-02-22 16:00:00,0\n2011-02-22 17:00:00,0\n2011-02-22 18:00:00,0\n2011-02-22 19:00:00,0\n2011-02-22 20:00:00,0\n2011-02-22 21:00:00,0\n2011-02-22 22:00:00,0\n2011-02-22 23:00:00,0\n2011-02-23 00:00:00,0\n2011-02-23 01:00:00,0\n2011-02-23 02:00:00,0\n2011-02-23 03:00:00,0\n2011-02-23 05:00:00,0\n2011-02-23 06:00:00,0\n2011-02-23 07:00:00,0\n2011-02-23 08:00:00,0\n2011-02-23 09:00:00,0\n2011-02-23 10:00:00,0\n2011-02-23 11:00:00,0\n2011-02-23 12:00:00,0\n2011-02-23 13:00:00,0\n2011-02-23 14:00:00,0\n2011-02-23 15:00:00,0\n2011-02-23 16:00:00,0\n2011-02-23 17:00:00,0\n2011-02-23 18:00:00,0\n2011-02-23 19:00:00,0\n2011-02-23 20:00:00,0\n2011-02-23 21:00:00,0\n2011-02-23 22:00:00,0\n2011-02-23 23:00:00,0\n2011-02-24 00:00:00,0\n2011-02-24 01:00:00,0\n2011-02-24 02:00:00,0\n2011-02-24 03:00:00,0\n2011-02-24 05:00:00,0\n2011-02-24 06:00:00,0\n2011-02-24 07:00:00,0\n2011-02-24 08:00:00,0\n2011-02-24 09:00:00,0\n2011-02-24 10:00:00,0\n2011-02-24 11:00:00,0\n2011-02-24 12:00:00,0\n2011-02-24 13:00:00,0\n2011-02-24 14:00:00,0\n2011-02-24 15:00:00,0\n2011-02-24 16:00:00,0\n2011-02-24 17:00:00,0\n2011-02-24 18:00:00,0\n2011-02-24 19:00:00,0\n2011-02-24 20:00:00,0\n2011-02-24 21:00:00,0\n2011-02-24 22:00:00,0\n2011-02-24 23:00:00,0\n2011-02-25 00:00:00,0\n2011-02-25 01:00:00,0\n2011-02-25 02:00:00,0\n2011-02-25 03:00:00,0\n2011-02-25 05:00:00,0\n2011-02-25 06:00:00,0\n2011-02-25 07:00:00,0\n2011-02-25 08:00:00,0\n2011-02-25 09:00:00,0\n2011-02-25 10:00:00,0\n2011-02-25 11:00:00,0\n2011-02-25 12:00:00,0\n2011-02-25 13:00:00,0\n2011-02-25 14:00:00,0\n2011-02-25 15:00:00,0\n2011-02-25 16:00:00,0\n2011-02-25 17:00:00,0\n2011-02-25 18:00:00,0\n2011-02-25 19:00:00,0\n2011-02-25 20:00:00,0\n2011-02-25 21:00:00,0\n2011-02-25 22:00:00,0\n2011-02-25 23:00:00,0\n2011-02-26 00:00:00,0\n2011-02-26 01:00:00,0\n2011-02-26 02:00:00,0\n2011-02-26 03:00:00,0\n2011-02-26 04:00:00,0\n2011-02-26 05:00:00,0\n2011-02-26 06:00:00,0\n2011-02-26 07:00:00,0\n2011-02-26 08:00:00,0\n2011-02-26 09:00:00,0\n2011-02-26 10:00:00,0\n2011-02-26 11:00:00,0\n2011-02-26 12:00:00,0\n2011-02-26 13:00:00,0\n2011-02-26 14:00:00,0\n2011-02-26 15:00:00,0\n2011-02-26 16:00:00,0\n2011-02-26 17:00:00,0\n2011-02-26 18:00:00,0\n2011-02-26 19:00:00,0\n2011-02-26 20:00:00,0\n2011-02-26 21:00:00,0\n2011-02-26 22:00:00,0\n2011-02-26 23:00:00,0\n2011-02-27 00:00:00,0\n2011-02-27 01:00:00,0\n2011-02-27 02:00:00,0\n2011-02-27 03:00:00,0\n2011-02-27 04:00:00,0\n2011-02-27 06:00:00,0\n2011-02-27 07:00:00,0\n2011-02-27 08:00:00,0\n2011-02-27 09:00:00,0\n2011-02-27 10:00:00,0\n2011-02-27 11:00:00,0\n2011-02-27 12:00:00,0\n2011-02-27 13:00:00,0\n2011-02-27 14:00:00,0\n2011-02-27 15:00:00,0\n2011-02-27 16:00:00,0\n2011-02-27 17:00:00,0\n2011-02-27 18:00:00,0\n2011-02-27 19:00:00,0\n2011-02-27 20:00:00,0\n2011-02-27 21:00:00,0\n2011-02-27 22:00:00,0\n2011-02-27 23:00:00,0\n2011-02-28 00:00:00,0\n2011-02-28 01:00:00,0\n2011-02-28 03:00:00,0\n2011-02-28 05:00:00,0\n2011-02-28 06:00:00,0\n2011-02-28 07:00:00,0\n2011-02-28 08:00:00,0\n2011-02-28 09:00:00,0\n2011-02-28 10:00:00,0\n2011-02-28 11:00:00,0\n2011-02-28 12:00:00,0\n2011-02-28 13:00:00,0\n2011-02-28 14:00:00,0\n2011-02-28 15:00:00,0\n2011-02-28 16:00:00,0\n2011-02-28 17:00:00,0\n2011-02-28 18:00:00,0\n2011-02-28 19:00:00,0\n2011-02-28 20:00:00,0\n2011-02-28 21:00:00,0\n2011-02-28 22:00:00,0\n2011-02-28 23:00:00,0\n2011-03-20 00:00:00,0\n2011-03-20 01:00:00,0\n2011-03-20 02:00:00,0\n2011-03-20 03:00:00,0\n2011-03-20 04:00:00,0\n2011-03-20 05:00:00,0\n2011-03-20 06:00:00,0\n2011-03-20 07:00:00,0\n2011-03-20 08:00:00,0\n2011-03-20 09:00:00,0\n2011-03-20 10:00:00,0\n2011-03-20 11:00:00,0\n2011-03-20 12:00:00,0\n2011-03-20 13:00:00,0\n2011-03-20 14:00:00,0\n2011-03-20 15:00:00,0\n2011-03-20 16:00:00,0\n2011-03-20 17:00:00,0\n2011-03-20 18:00:00,0\n2011-03-20 19:00:00,0\n2011-03-20 20:00:00,0\n2011-03-20 21:00:00,0\n2011-03-20 22:00:00,0\n2011-03-20 23:00:00,0\n2011-03-21 00:00:00,0\n2011-03-21 01:00:00,0\n2011-03-21 02:00:00,0\n2011-03-21 03:00:00,0\n2011-03-21 05:00:00,0\n2011-03-21 06:00:00,0\n2011-03-21 07:00:00,0\n2011-03-21 08:00:00,0\n2011-03-21 09:00:00,0\n2011-03-21 10:00:00,0\n2011-03-21 11:00:00,0\n2011-03-21 12:00:00,0\n2011-03-21 13:00:00,0\n2011-03-21 14:00:00,0\n2011-03-21 15:00:00,0\n2011-03-21 16:00:00,0\n2011-03-21 17:00:00,0\n2011-03-21 18:00:00,0\n2011-03-21 19:00:00,0\n2011-03-21 20:00:00,0\n2011-03-21 21:00:00,0\n2011-03-21 22:00:00,0\n2011-03-21 23:00:00,0\n2011-03-22 00:00:00,0\n2011-03-22 01:00:00,0\n2011-03-22 02:00:00,0\n2011-03-22 03:00:00,0\n2011-03-22 04:00:00,0\n2011-03-22 05:00:00,0\n2011-03-22 06:00:00,0\n2011-03-22 07:00:00,0\n2011-03-22 08:00:00,0\n2011-03-22 09:00:00,0\n2011-03-22 10:00:00,0\n2011-03-22 11:00:00,0\n2011-03-22 12:00:00,0\n2011-03-22 13:00:00,0\n2011-03-22 14:00:00,0\n2011-03-22 15:00:00,0\n2011-03-22 16:00:00,0\n2011-03-22 17:00:00,0\n2011-03-22 18:00:00,0\n2011-03-22 19:00:00,0\n2011-03-22 20:00:00,0\n2011-03-22 21:00:00,0\n2011-03-22 22:00:00,0\n2011-03-22 23:00:00,0\n2011-03-23 00:00:00,0\n2011-03-23 01:00:00,0\n2011-03-23 02:00:00,0\n2011-03-23 03:00:00,0\n2011-03-23 05:00:00,0\n2011-03-23 06:00:00,0\n2011-03-23 07:00:00,0\n2011-03-23 08:00:00,0\n2011-03-23 09:00:00,0\n2011-03-23 10:00:00,0\n2011-03-23 11:00:00,0\n2011-03-23 12:00:00,0\n2011-03-23 13:00:00,0\n2011-03-23 14:00:00,0\n2011-03-23 15:00:00,0\n2011-03-23 16:00:00,0\n2011-03-23 17:00:00,0\n2011-03-23 18:00:00,0\n2011-03-23 19:00:00,0\n2011-03-23 20:00:00,0\n2011-03-23 21:00:00,0\n2011-03-23 22:00:00,0\n2011-03-23 23:00:00,0\n2011-03-24 00:00:00,0\n2011-03-24 01:00:00,0\n2011-03-24 02:00:00,0\n2011-03-24 03:00:00,0\n2011-03-24 04:00:00,0\n2011-03-24 05:00:00,0\n2011-03-24 06:00:00,0\n2011-03-24 07:00:00,0\n2011-03-24 08:00:00,0\n2011-03-24 09:00:00,0\n2011-03-24 10:00:00,0\n2011-03-24 11:00:00,0\n2011-03-24 12:00:00,0\n2011-03-24 13:00:00,0\n2011-03-24 14:00:00,0\n2011-03-24 15:00:00,0\n2011-03-24 16:00:00,0\n2011-03-24 17:00:00,0\n2011-03-24 18:00:00,0\n2011-03-24 19:00:00,0\n2011-03-24 20:00:00,0\n2011-03-24 21:00:00,0\n2011-03-24 22:00:00,0\n2011-03-24 23:00:00,0\n2011-03-25 00:00:00,0\n2011-03-25 01:00:00,0\n2011-03-25 02:00:00,0\n2011-03-25 03:00:00,0\n2011-03-25 04:00:00,0\n2011-03-25 05:00:00,0\n2011-03-25 06:00:00,0\n2011-03-25 07:00:00,0\n2011-03-25 08:00:00,0\n2011-03-25 09:00:00,0\n2011-03-25 10:00:00,0\n2011-03-25 11:00:00,0\n2011-03-25 12:00:00,0\n2011-03-25 13:00:00,0\n2011-03-25 14:00:00,0\n2011-03-25 15:00:00,0\n2011-03-25 16:00:00,0\n2011-03-25 17:00:00,0\n2011-03-25 18:00:00,0\n2011-03-25 19:00:00,0\n2011-03-25 20:00:00,0\n2011-03-25 21:00:00,0\n2011-03-25 22:00:00,0\n2011-03-25 23:00:00,0\n2011-03-26 00:00:00,0\n2011-03-26 01:00:00,0\n2011-03-26 02:00:00,0\n2011-03-26 03:00:00,0\n2011-03-26 04:00:00,0\n2011-03-26 05:00:00,0\n2011-03-26 06:00:00,0\n2011-03-26 07:00:00,0\n2011-03-26 08:00:00,0\n2011-03-26 09:00:00,0\n2011-03-26 10:00:00,0\n2011-03-26 11:00:00,0\n2011-03-26 12:00:00,0\n2011-03-26 13:00:00,0\n2011-03-26 14:00:00,0\n2011-03-26 15:00:00,0\n2011-03-26 16:00:00,0\n2011-03-26 17:00:00,0\n2011-03-26 18:00:00,0\n2011-03-26 19:00:00,0\n2011-03-26 20:00:00,0\n2011-03-26 21:00:00,0\n2011-03-26 22:00:00,0\n2011-03-26 23:00:00,0\n2011-03-27 00:00:00,0\n2011-03-27 01:00:00,0\n2011-03-27 02:00:00,0\n2011-03-27 03:00:00,0\n2011-03-27 04:00:00,0\n2011-03-27 06:00:00,0\n2011-03-27 07:00:00,0\n2011-03-27 08:00:00,0\n2011-03-27 09:00:00,0\n2011-03-27 10:00:00,0\n2011-03-27 11:00:00,0\n2011-03-27 12:00:00,0\n2011-03-27 13:00:00,0\n2011-03-27 14:00:00,0\n2011-03-27 15:00:00,0\n2011-03-27 16:00:00,0\n2011-03-27 17:00:00,0\n2011-03-27 18:00:00,0\n2011-03-27 19:00:00,0\n2011-03-27 20:00:00,0\n2011-03-27 21:00:00,0\n2011-03-27 22:00:00,0\n2011-03-27 23:00:00,0\n2011-03-28 00:00:00,0\n2011-03-28 01:00:00,0\n2011-03-28 02:00:00,0\n2011-03-28 03:00:00,0\n2011-03-28 05:00:00,0\n2011-03-28 06:00:00,0\n2011-03-28 07:00:00,0\n2011-03-28 08:00:00,0\n2011-03-28 09:00:00,0\n2011-03-28 10:00:00,0\n2011-03-28 11:00:00,0\n2011-03-28 12:00:00,0\n2011-03-28 13:00:00,0\n2011-03-28 14:00:00,0\n2011-03-28 15:00:00,0\n2011-03-28 16:00:00,0\n2011-03-28 17:00:00,0\n2011-03-28 18:00:00,0\n2011-03-28 19:00:00,0\n2011-03-28 20:00:00,0\n2011-03-28 21:00:00,0\n2011-03-28 22:00:00,0\n2011-03-28 23:00:00,0\n2011-03-29 00:00:00,0\n2011-03-29 01:00:00,0\n2011-03-29 02:00:00,0\n2011-03-29 03:00:00,0\n2011-03-29 04:00:00,0\n2011-03-29 05:00:00,0\n2011-03-29 06:00:00,0\n2011-03-29 07:00:00,0\n2011-03-29 08:00:00,0\n2011-03-29 09:00:00,0\n2011-03-29 10:00:00,0\n2011-03-29 11:00:00,0\n2011-03-29 12:00:00,0\n2011-03-29 13:00:00,0\n2011-03-29 14:00:00,0\n2011-03-29 15:00:00,0\n2011-03-29 16:00:00,0\n2011-03-29 17:00:00,0\n2011-03-29 18:00:00,0\n2011-03-29 19:00:00,0\n2011-03-29 20:00:00,0\n2011-03-29 21:00:00,0\n2011-03-29 22:00:00,0\n2011-03-29 23:00:00,0\n2011-03-30 00:00:00,0\n2011-03-30 01:00:00,0\n2011-03-30 02:00:00,0\n2011-03-30 03:00:00,0\n2011-03-30 04:00:00,0\n2011-03-30 05:00:00,0\n2011-03-30 06:00:00,0\n2011-03-30 07:00:00,0\n2011-03-30 08:00:00,0\n2011-03-30 09:00:00,0\n2011-03-30 10:00:00,0\n2011-03-30 11:00:00,0\n2011-03-30 12:00:00,0\n2011-03-30 13:00:00,0\n2011-03-30 14:00:00,0\n2011-03-30 15:00:00,0\n2011-03-30 16:00:00,0\n2011-03-30 17:00:00,0\n2011-03-30 18:00:00,0\n2011-03-30 19:00:00,0\n2011-03-30 20:00:00,0\n2011-03-30 21:00:00,0\n2011-03-30 22:00:00,0\n2011-03-30 23:00:00,0\n2011-03-31 00:00:00,0\n2011-03-31 01:00:00,0\n2011-03-31 02:00:00,0\n2011-03-31 03:00:00,0\n2011-03-31 04:00:00,0\n2011-03-31 05:00:00,0\n2011-03-31 06:00:00,0\n2011-03-31 07:00:00,0\n2011-03-31 08:00:00,0\n2011-03-31 09:00:00,0\n2011-03-31 10:00:00,0\n2011-03-31 11:00:00,0\n2011-03-31 12:00:00,0\n2011-03-31 13:00:00,0\n2011-03-31 14:00:00,0\n2011-03-31 15:00:00,0\n2011-03-31 16:00:00,0\n2011-03-31 17:00:00,0\n2011-03-31 18:00:00,0\n2011-03-31 19:00:00,0\n2011-03-31 20:00:00,0\n2011-03-31 21:00:00,0\n2011-03-31 22:00:00,0\n2011-03-31 23:00:00,0\n2011-04-20 00:00:00,0\n2011-04-20 01:00:00,0\n2011-04-20 02:00:00,0\n2011-04-20 03:00:00,0\n2011-04-20 04:00:00,0\n2011-04-20 05:00:00,0\n2011-04-20 06:00:00,0\n2011-04-20 07:00:00,0\n2011-04-20 08:00:00,0\n2011-04-20 09:00:00,0\n2011-04-20 10:00:00,0\n2011-04-20 11:00:00,0\n2011-04-20 12:00:00,0\n2011-04-20 13:00:00,0\n2011-04-20 14:00:00,0\n2011-04-20 15:00:00,0\n2011-04-20 16:00:00,0\n2011-04-20 17:00:00,0\n2011-04-20 18:00:00,0\n2011-04-20 19:00:00,0\n2011-04-20 20:00:00,0\n2011-04-20 21:00:00,0\n2011-04-20 22:00:00,0\n2011-04-20 23:00:00,0\n2011-04-21 00:00:00,0\n2011-04-21 01:00:00,0\n2011-04-21 02:00:00,0\n2011-04-21 03:00:00,0\n2011-04-21 04:00:00,0\n2011-04-21 05:00:00,0\n2011-04-21 06:00:00,0\n2011-04-21 07:00:00,0\n2011-04-21 08:00:00,0\n2011-04-21 09:00:00,0\n2011-04-21 10:00:00,0\n2011-04-21 11:00:00,0\n2011-04-21 12:00:00,0\n2011-04-21 13:00:00,0\n2011-04-21 14:00:00,0\n2011-04-21 15:00:00,0\n2011-04-21 16:00:00,0\n2011-04-21 17:00:00,0\n2011-04-21 18:00:00,0\n2011-04-21 19:00:00,0\n2011-04-21 20:00:00,0\n2011-04-21 21:00:00,0\n2011-04-21 22:00:00,0\n2011-04-21 23:00:00,0\n2011-04-22 00:00:00,0\n2011-04-22 01:00:00,0\n2011-04-22 02:00:00,0\n2011-04-22 03:00:00,0\n2011-04-22 04:00:00,0\n2011-04-22 05:00:00,0\n2011-04-22 06:00:00,0\n2011-04-22 07:00:00,0\n2011-04-22 08:00:00,0\n2011-04-22 09:00:00,0\n2011-04-22 10:00:00,0\n2011-04-22 11:00:00,0\n2011-04-22 12:00:00,0\n2011-04-22 13:00:00,0\n2011-04-22 14:00:00,0\n2011-04-22 15:00:00,0\n2011-04-22 16:00:00,0\n2011-04-22 17:00:00,0\n2011-04-22 18:00:00,0\n2011-04-22 19:00:00,0\n2011-04-22 20:00:00,0\n2011-04-22 21:00:00,0\n2011-04-22 22:00:00,0\n2011-04-22 23:00:00,0\n2011-04-23 00:00:00,0\n2011-04-23 01:00:00,0\n2011-04-23 02:00:00,0\n2011-04-23 03:00:00,0\n2011-04-23 04:00:00,0\n2011-04-23 05:00:00,0\n2011-04-23 06:00:00,0\n2011-04-23 07:00:00,0\n2011-04-23 08:00:00,0\n2011-04-23 09:00:00,0\n2011-04-23 10:00:00,0\n2011-04-23 11:00:00,0\n2011-04-23 12:00:00,0\n2011-04-23 13:00:00,0\n2011-04-23 14:00:00,0\n2011-04-23 15:00:00,0\n2011-04-23 16:00:00,0\n2011-04-23 17:00:00,0\n2011-04-23 18:00:00,0\n2011-04-23 19:00:00,0\n2011-04-23 20:00:00,0\n2011-04-23 21:00:00,0\n2011-04-23 22:00:00,0\n2011-04-23 23:00:00,0\n2011-04-24 00:00:00,0\n2011-04-24 01:00:00,0\n2011-04-24 02:00:00,0\n2011-04-24 03:00:00,0\n2011-04-24 04:00:00,0\n2011-04-24 05:00:00,0\n2011-04-24 06:00:00,0\n2011-04-24 07:00:00,0\n2011-04-24 08:00:00,0\n2011-04-24 09:00:00,0\n2011-04-24 10:00:00,0\n2011-04-24 11:00:00,0\n2011-04-24 12:00:00,0\n2011-04-24 13:00:00,0\n2011-04-24 14:00:00,0\n2011-04-24 15:00:00,0\n2011-04-24 16:00:00,0\n2011-04-24 17:00:00,0\n2011-04-24 18:00:00,0\n2011-04-24 19:00:00,0\n2011-04-24 20:00:00,0\n2011-04-24 21:00:00,0\n2011-04-24 22:00:00,0\n2011-04-24 23:00:00,0\n2011-04-25 00:00:00,0\n2011-04-25 01:00:00,0\n2011-04-25 02:00:00,0\n2011-04-25 03:00:00,0\n2011-04-25 04:00:00,0\n2011-04-25 05:00:00,0\n2011-04-25 06:00:00,0\n2011-04-25 07:00:00,0\n2011-04-25 08:00:00,0\n2011-04-25 09:00:00,0\n2011-04-25 10:00:00,0\n2011-04-25 11:00:00,0\n2011-04-25 12:00:00,0\n2011-04-25 13:00:00,0\n2011-04-25 14:00:00,0\n2011-04-25 15:00:00,0\n2011-04-25 16:00:00,0\n2011-04-25 17:00:00,0\n2011-04-25 18:00:00,0\n2011-04-25 19:00:00,0\n2011-04-25 20:00:00,0\n2011-04-25 21:00:00,0\n2011-04-25 22:00:00,0\n2011-04-25 23:00:00,0\n2011-04-26 00:00:00,0\n2011-04-26 01:00:00,0\n2011-04-26 02:00:00,0\n2011-04-26 03:00:00,0\n2011-04-26 04:00:00,0\n2011-04-26 05:00:00,0\n2011-04-26 06:00:00,0\n2011-04-26 07:00:00,0\n2011-04-26 08:00:00,0\n2011-04-26 09:00:00,0\n2011-04-26 10:00:00,0\n2011-04-26 11:00:00,0\n2011-04-26 12:00:00,0\n2011-04-26 13:00:00,0\n2011-04-26 14:00:00,0\n2011-04-26 15:00:00,0\n2011-04-26 16:00:00,0\n2011-04-26 17:00:00,0\n2011-04-26 18:00:00,0\n2011-04-26 19:00:00,0\n2011-04-26 20:00:00,0\n2011-04-26 21:00:00,0\n2011-04-26 22:00:00,0\n2011-04-26 23:00:00,0\n2011-04-27 00:00:00,0\n2011-04-27 01:00:00,0\n2011-04-27 02:00:00,0\n2011-04-27 03:00:00,0\n2011-04-27 04:00:00,0\n2011-04-27 05:00:00,0\n2011-04-27 06:00:00,0\n2011-04-27 07:00:00,0\n2011-04-27 08:00:00,0\n2011-04-27 09:00:00,0\n2011-04-27 10:00:00,0\n2011-04-27 11:00:00,0\n2011-04-27 12:00:00,0\n2011-04-27 13:00:00,0\n2011-04-27 14:00:00,0\n2011-04-27 15:00:00,0\n2011-04-27 16:00:00,0\n2011-04-27 17:00:00,0\n2011-04-27 18:00:00,0\n2011-04-27 19:00:00,0\n2011-04-27 20:00:00,0\n2011-04-27 21:00:00,0\n2011-04-27 22:00:00,0\n2011-04-27 23:00:00,0\n2011-04-28 00:00:00,0\n2011-04-28 01:00:00,0\n2011-04-28 02:00:00,0\n2011-04-28 03:00:00,0\n2011-04-28 04:00:00,0\n2011-04-28 05:00:00,0\n2011-04-28 06:00:00,0\n2011-04-28 07:00:00,0\n2011-04-28 08:00:00,0\n2011-04-28 09:00:00,0\n2011-04-28 10:00:00,0\n2011-04-28 11:00:00,0\n2011-04-28 12:00:00,0\n2011-04-28 13:00:00,0\n2011-04-28 14:00:00,0\n2011-04-28 15:00:00,0\n2011-04-28 16:00:00,0\n2011-04-28 17:00:00,0\n2011-04-28 18:00:00,0\n2011-04-28 19:00:00,0\n2011-04-28 20:00:00,0\n2011-04-28 21:00:00,0\n2011-04-28 22:00:00,0\n2011-04-28 23:00:00,0\n2011-04-29 00:00:00,0\n2011-04-29 01:00:00,0\n2011-04-29 02:00:00,0\n2011-04-29 03:00:00,0\n2011-04-29 04:00:00,0\n2011-04-29 05:00:00,0\n2011-04-29 06:00:00,0\n2011-04-29 07:00:00,0\n2011-04-29 08:00:00,0\n2011-04-29 09:00:00,0\n2011-04-29 10:00:00,0\n2011-04-29 11:00:00,0\n2011-04-29 12:00:00,0\n2011-04-29 13:00:00,0\n2011-04-29 14:00:00,0\n2011-04-29 15:00:00,0\n2011-04-29 16:00:00,0\n2011-04-29 17:00:00,0\n2011-04-29 18:00:00,0\n2011-04-29 19:00:00,0\n2011-04-29 20:00:00,0\n2011-04-29 21:00:00,0\n2011-04-29 22:00:00,0\n2011-04-29 23:00:00,0\n2011-04-30 00:00:00,0\n2011-04-30 01:00:00,0\n2011-04-30 02:00:00,0\n2011-04-30 03:00:00,0\n2011-04-30 04:00:00,0\n2011-04-30 05:00:00,0\n2011-04-30 06:00:00,0\n2011-04-30 07:00:00,0\n2011-04-30 08:00:00,0\n2011-04-30 09:00:00,0\n2011-04-30 10:00:00,0\n2011-04-30 11:00:00,0\n2011-04-30 12:00:00,0\n2011-04-30 13:00:00,0\n2011-04-30 14:00:00,0\n2011-04-30 15:00:00,0\n2011-04-30 16:00:00,0\n2011-04-30 17:00:00,0\n2011-04-30 18:00:00,0\n2011-04-30 19:00:00,0\n2011-04-30 20:00:00,0\n2011-04-30 21:00:00,0\n2011-04-30 22:00:00,0\n2011-04-30 23:00:00,0\n2011-05-20 00:00:00,0\n2011-05-20 01:00:00,0\n2011-05-20 02:00:00,0\n2011-05-20 03:00:00,0\n2011-05-20 04:00:00,0\n2011-05-20 05:00:00,0\n2011-05-20 06:00:00,0\n2011-05-20 07:00:00,0\n2011-05-20 08:00:00,0\n2011-05-20 09:00:00,0\n2011-05-20 10:00:00,0\n2011-05-20 11:00:00,0\n2011-05-20 12:00:00,0\n2011-05-20 13:00:00,0\n2011-05-20 14:00:00,0\n2011-05-20 15:00:00,0\n2011-05-20 16:00:00,0\n2011-05-20 17:00:00,0\n2011-05-20 18:00:00,0\n2011-05-20 19:00:00,0\n2011-05-20 20:00:00,0\n2011-05-20 21:00:00,0\n2011-05-20 22:00:00,0\n2011-05-20 23:00:00,0\n2011-05-21 00:00:00,0\n2011-05-21 01:00:00,0\n2011-05-21 02:00:00,0\n2011-05-21 03:00:00,0\n2011-05-21 04:00:00,0\n2011-05-21 05:00:00,0\n2011-05-21 06:00:00,0\n2011-05-21 07:00:00,0\n2011-05-21 08:00:00,0\n2011-05-21 09:00:00,0\n2011-05-21 10:00:00,0\n2011-05-21 11:00:00,0\n2011-05-21 12:00:00,0\n2011-05-21 13:00:00,0\n2011-05-21 14:00:00,0\n2011-05-21 15:00:00,0\n2011-05-21 16:00:00,0\n2011-05-21 17:00:00,0\n2011-05-21 18:00:00,0\n2011-05-21 19:00:00,0\n2011-05-21 20:00:00,0\n2011-05-21 21:00:00,0\n2011-05-21 22:00:00,0\n2011-05-21 23:00:00,0\n2011-05-22 00:00:00,0\n2011-05-22 01:00:00,0\n2011-05-22 02:00:00,0\n2011-05-22 03:00:00,0\n2011-05-22 04:00:00,0\n2011-05-22 05:00:00,0\n2011-05-22 06:00:00,0\n2011-05-22 07:00:00,0\n2011-05-22 08:00:00,0\n2011-05-22 09:00:00,0\n2011-05-22 10:00:00,0\n2011-05-22 11:00:00,0\n2011-05-22 12:00:00,0\n2011-05-22 13:00:00,0\n2011-05-22 14:00:00,0\n2011-05-22 15:00:00,0\n2011-05-22 16:00:00,0\n2011-05-22 17:00:00,0\n2011-05-22 18:00:00,0\n2011-05-22 19:00:00,0\n2011-05-22 20:00:00,0\n2011-05-22 21:00:00,0\n2011-05-22 22:00:00,0\n2011-05-22 23:00:00,0\n2011-05-23 00:00:00,0\n2011-05-23 01:00:00,0\n2011-05-23 02:00:00,0\n2011-05-23 03:00:00,0\n2011-05-23 04:00:00,0\n2011-05-23 05:00:00,0\n2011-05-23 06:00:00,0\n2011-05-23 07:00:00,0\n2011-05-23 08:00:00,0\n2011-05-23 09:00:00,0\n2011-05-23 10:00:00,0\n2011-05-23 11:00:00,0\n2011-05-23 12:00:00,0\n2011-05-23 13:00:00,0\n2011-05-23 14:00:00,0\n2011-05-23 15:00:00,0\n2011-05-23 16:00:00,0\n2011-05-23 17:00:00,0\n2011-05-23 18:00:00,0\n2011-05-23 19:00:00,0\n2011-05-23 20:00:00,0\n2011-05-23 21:00:00,0\n2011-05-23 22:00:00,0\n2011-05-23 23:00:00,0\n2011-05-24 00:00:00,0\n2011-05-24 01:00:00,0\n2011-05-24 02:00:00,0\n2011-05-24 03:00:00,0\n2011-05-24 04:00:00,0\n2011-05-24 05:00:00,0\n2011-05-24 06:00:00,0\n2011-05-24 07:00:00,0\n2011-05-24 08:00:00,0\n2011-05-24 09:00:00,0\n2011-05-24 10:00:00,0\n2011-05-24 11:00:00,0\n2011-05-24 12:00:00,0\n2011-05-24 13:00:00,0\n2011-05-24 14:00:00,0\n2011-05-24 15:00:00,0\n2011-05-24 16:00:00,0\n2011-05-24 17:00:00,0\n2011-05-24 18:00:00,0\n2011-05-24 19:00:00,0\n2011-05-24 20:00:00,0\n2011-05-24 21:00:00,0\n2011-05-24 22:00:00,0\n2011-05-24 23:00:00,0\n2011-05-25 00:00:00,0\n2011-05-25 01:00:00,0\n2011-05-25 02:00:00,0\n2011-05-25 03:00:00,0\n2011-05-25 04:00:00,0\n2011-05-25 05:00:00,0\n2011-05-25 06:00:00,0\n2011-05-25 07:00:00,0\n2011-05-25 08:00:00,0\n2011-05-25 09:00:00,0\n2011-05-25 10:00:00,0\n2011-05-25 11:00:00,0\n2011-05-25 12:00:00,0\n2011-05-25 13:00:00,0\n2011-05-25 14:00:00,0\n2011-05-25 15:00:00,0\n2011-05-25 16:00:00,0\n2011-05-25 17:00:00,0\n2011-05-25 18:00:00,0\n2011-05-25 19:00:00,0\n2011-05-25 20:00:00,0\n2011-05-25 21:00:00,0\n2011-05-25 22:00:00,0\n2011-05-25 23:00:00,0\n2011-05-26 00:00:00,0\n2011-05-26 01:00:00,0\n2011-05-26 02:00:00,0\n2011-05-26 03:00:00,0\n2011-05-26 04:00:00,0\n2011-05-26 05:00:00,0\n2011-05-26 06:00:00,0\n2011-05-26 07:00:00,0\n2011-05-26 08:00:00,0\n2011-05-26 09:00:00,0\n2011-05-26 10:00:00,0\n2011-05-26 11:00:00,0\n2011-05-26 12:00:00,0\n2011-05-26 13:00:00,0\n2011-05-26 14:00:00,0\n2011-05-26 15:00:00,0\n2011-05-26 16:00:00,0\n2011-05-26 17:00:00,0\n2011-05-26 18:00:00,0\n2011-05-26 19:00:00,0\n2011-05-26 20:00:00,0\n2011-05-26 21:00:00,0\n2011-05-26 22:00:00,0\n2011-05-26 23:00:00,0\n2011-05-27 00:00:00,0\n2011-05-27 01:00:00,0\n2011-05-27 02:00:00,0\n2011-05-27 03:00:00,0\n2011-05-27 04:00:00,0\n2011-05-27 05:00:00,0\n2011-05-27 06:00:00,0\n2011-05-27 07:00:00,0\n2011-05-27 08:00:00,0\n2011-05-27 09:00:00,0\n2011-05-27 10:00:00,0\n2011-05-27 11:00:00,0\n2011-05-27 12:00:00,0\n2011-05-27 13:00:00,0\n2011-05-27 14:00:00,0\n2011-05-27 15:00:00,0\n2011-05-27 16:00:00,0\n2011-05-27 17:00:00,0\n2011-05-27 18:00:00,0\n2011-05-27 19:00:00,0\n2011-05-27 20:00:00,0\n2011-05-27 21:00:00,0\n2011-05-27 22:00:00,0\n2011-05-27 23:00:00,0\n2011-05-28 00:00:00,0\n2011-05-28 01:00:00,0\n2011-05-28 02:00:00,0\n2011-05-28 03:00:00,0\n2011-05-28 04:00:00,0\n2011-05-28 05:00:00,0\n2011-05-28 06:00:00,0\n2011-05-28 07:00:00,0\n2011-05-28 08:00:00,0\n2011-05-28 09:00:00,0\n2011-05-28 10:00:00,0\n2011-05-28 11:00:00,0\n2011-05-28 12:00:00,0\n2011-05-28 13:00:00,0\n2011-05-28 14:00:00,0\n2011-05-28 15:00:00,0\n2011-05-28 16:00:00,0\n2011-05-28 17:00:00,0\n2011-05-28 18:00:00,0\n2011-05-28 19:00:00,0\n2011-05-28 20:00:00,0\n2011-05-28 21:00:00,0\n2011-05-28 22:00:00,0\n2011-05-28 23:00:00,0\n2011-05-29 00:00:00,0\n2011-05-29 01:00:00,0\n2011-05-29 02:00:00,0\n2011-05-29 03:00:00,0\n2011-05-29 04:00:00,0\n2011-05-29 05:00:00,0\n2011-05-29 06:00:00,0\n2011-05-29 07:00:00,0\n2011-05-29 08:00:00,0\n2011-05-29 09:00:00,0\n2011-05-29 10:00:00,0\n2011-05-29 11:00:00,0\n2011-05-29 12:00:00,0\n2011-05-29 13:00:00,0\n2011-05-29 14:00:00,0\n2011-05-29 15:00:00,0\n2011-05-29 16:00:00,0\n2011-05-29 17:00:00,0\n2011-05-29 18:00:00,0\n2011-05-29 19:00:00,0\n2011-05-29 20:00:00,0\n2011-05-29 21:00:00,0\n2011-05-29 22:00:00,0\n2011-05-29 23:00:00,0\n2011-05-30 00:00:00,0\n2011-05-30 01:00:00,0\n2011-05-30 02:00:00,0\n2011-05-30 03:00:00,0\n2011-05-30 04:00:00,0\n2011-05-30 05:00:00,0\n2011-05-30 06:00:00,0\n2011-05-30 07:00:00,0\n2011-05-30 08:00:00,0\n2011-05-30 09:00:00,0\n2011-05-30 10:00:00,0\n2011-05-30 11:00:00,0\n2011-05-30 12:00:00,0\n2011-05-30 13:00:00,0\n2011-05-30 14:00:00,0\n2011-05-30 15:00:00,0\n2011-05-30 16:00:00,0\n2011-05-30 17:00:00,0\n2011-05-30 18:00:00,0\n2011-05-30 19:00:00,0\n2011-05-30 20:00:00,0\n2011-05-30 21:00:00,0\n2011-05-30 22:00:00,0\n2011-05-30 23:00:00,0\n2011-05-31 00:00:00,0\n2011-05-31 01:00:00,0\n2011-05-31 02:00:00,0\n2011-05-31 03:00:00,0\n2011-05-31 04:00:00,0\n2011-05-31 05:00:00,0\n2011-05-31 06:00:00,0\n2011-05-31 07:00:00,0\n2011-05-31 08:00:00,0\n2011-05-31 09:00:00,0\n2011-05-31 10:00:00,0\n2011-05-31 11:00:00,0\n2011-05-31 12:00:00,0\n2011-05-31 13:00:00,0\n2011-05-31 14:00:00,0\n2011-05-31 15:00:00,0\n2011-05-31 16:00:00,0\n2011-05-31 17:00:00,0\n2011-05-31 18:00:00,0\n2011-05-31 19:00:00,0\n2011-05-31 20:00:00,0\n2011-05-31 21:00:00,0\n2011-05-31 22:00:00,0\n2011-05-31 23:00:00,0\n2011-06-20 00:00:00,0\n2011-06-20 01:00:00,0\n2011-06-20 02:00:00,0\n2011-06-20 03:00:00,0\n2011-06-20 04:00:00,0\n2011-06-20 05:00:00,0\n2011-06-20 06:00:00,0\n2011-06-20 07:00:00,0\n2011-06-20 08:00:00,0\n2011-06-20 09:00:00,0\n2011-06-20 10:00:00,0\n2011-06-20 11:00:00,0\n2011-06-20 12:00:00,0\n2011-06-20 13:00:00,0\n2011-06-20 14:00:00,0\n2011-06-20 15:00:00,0\n2011-06-20 16:00:00,0\n2011-06-20 17:00:00,0\n2011-06-20 18:00:00,0\n2011-06-20 19:00:00,0\n2011-06-20 20:00:00,0\n2011-06-20 21:00:00,0\n2011-06-20 22:00:00,0\n2011-06-20 23:00:00,0\n2011-06-21 00:00:00,0\n2011-06-21 01:00:00,0\n2011-06-21 02:00:00,0\n2011-06-21 03:00:00,0\n2011-06-21 04:00:00,0\n2011-06-21 05:00:00,0\n2011-06-21 06:00:00,0\n2011-06-21 07:00:00,0\n2011-06-21 08:00:00,0\n2011-06-21 09:00:00,0\n2011-06-21 10:00:00,0\n2011-06-21 11:00:00,0\n2011-06-21 12:00:00,0\n2011-06-21 13:00:00,0\n2011-06-21 14:00:00,0\n2011-06-21 15:00:00,0\n2011-06-21 16:00:00,0\n2011-06-21 17:00:00,0\n2011-06-21 18:00:00,0\n2011-06-21 19:00:00,0\n2011-06-21 20:00:00,0\n2011-06-21 21:00:00,0\n2011-06-21 22:00:00,0\n2011-06-21 23:00:00,0\n2011-06-22 00:00:00,0\n2011-06-22 01:00:00,0\n2011-06-22 02:00:00,0\n2011-06-22 03:00:00,0\n2011-06-22 04:00:00,0\n2011-06-22 05:00:00,0\n2011-06-22 06:00:00,0\n2011-06-22 07:00:00,0\n2011-06-22 08:00:00,0\n2011-06-22 09:00:00,0\n2011-06-22 10:00:00,0\n2011-06-22 11:00:00,0\n2011-06-22 12:00:00,0\n2011-06-22 13:00:00,0\n2011-06-22 14:00:00,0\n2011-06-22 15:00:00,0\n2011-06-22 16:00:00,0\n2011-06-22 17:00:00,0\n2011-06-22 18:00:00,0\n2011-06-22 19:00:00,0\n2011-06-22 20:00:00,0\n2011-06-22 21:00:00,0\n2011-06-22 22:00:00,0\n2011-06-22 23:00:00,0\n2011-06-23 00:00:00,0\n2011-06-23 01:00:00,0\n2011-06-23 02:00:00,0\n2011-06-23 03:00:00,0\n2011-06-23 04:00:00,0\n2011-06-23 05:00:00,0\n2011-06-23 06:00:00,0\n2011-06-23 07:00:00,0\n2011-06-23 08:00:00,0\n2011-06-23 09:00:00,0\n2011-06-23 10:00:00,0\n2011-06-23 11:00:00,0\n2011-06-23 12:00:00,0\n2011-06-23 13:00:00,0\n2011-06-23 14:00:00,0\n2011-06-23 15:00:00,0\n2011-06-23 16:00:00,0\n2011-06-23 17:00:00,0\n2011-06-23 18:00:00,0\n2011-06-23 19:00:00,0\n2011-06-23 20:00:00,0\n2011-06-23 21:00:00,0\n2011-06-23 22:00:00,0\n2011-06-23 23:00:00,0\n2011-06-24 00:00:00,0\n2011-06-24 01:00:00,0\n2011-06-24 02:00:00,0\n2011-06-24 03:00:00,0\n2011-06-24 04:00:00,0\n2011-06-24 05:00:00,0\n2011-06-24 06:00:00,0\n2011-06-24 07:00:00,0\n2011-06-24 08:00:00,0\n2011-06-24 09:00:00,0\n2011-06-24 10:00:00,0\n2011-06-24 11:00:00,0\n2011-06-24 12:00:00,0\n2011-06-24 13:00:00,0\n2011-06-24 14:00:00,0\n2011-06-24 15:00:00,0\n2011-06-24 16:00:00,0\n2011-06-24 17:00:00,0\n2011-06-24 18:00:00,0\n2011-06-24 19:00:00,0\n2011-06-24 20:00:00,0\n2011-06-24 21:00:00,0\n2011-06-24 22:00:00,0\n2011-06-24 23:00:00,0\n2011-06-25 00:00:00,0\n2011-06-25 01:00:00,0\n2011-06-25 02:00:00,0\n2011-06-25 03:00:00,0\n2011-06-25 04:00:00,0\n2011-06-25 05:00:00,0\n2011-06-25 06:00:00,0\n2011-06-25 07:00:00,0\n2011-06-25 08:00:00,0\n2011-06-25 09:00:00,0\n2011-06-25 10:00:00,0\n2011-06-25 11:00:00,0\n2011-06-25 12:00:00,0\n2011-06-25 13:00:00,0\n2011-06-25 14:00:00,0\n2011-06-25 15:00:00,0\n2011-06-25 16:00:00,0\n2011-06-25 17:00:00,0\n2011-06-25 18:00:00,0\n2011-06-25 19:00:00,0\n2011-06-25 20:00:00,0\n2011-06-25 21:00:00,0\n2011-06-25 22:00:00,0\n2011-06-25 23:00:00,0\n2011-06-26 00:00:00,0\n2011-06-26 01:00:00,0\n2011-06-26 02:00:00,0\n2011-06-26 03:00:00,0\n2011-06-26 04:00:00,0\n2011-06-26 05:00:00,0\n2011-06-26 06:00:00,0\n2011-06-26 07:00:00,0\n2011-06-26 08:00:00,0\n2011-06-26 09:00:00,0\n2011-06-26 10:00:00,0\n2011-06-26 11:00:00,0\n2011-06-26 12:00:00,0\n2011-06-26 13:00:00,0\n2011-06-26 14:00:00,0\n2011-06-26 15:00:00,0\n2011-06-26 16:00:00,0\n2011-06-26 17:00:00,0\n2011-06-26 18:00:00,0\n2011-06-26 19:00:00,0\n2011-06-26 20:00:00,0\n2011-06-26 21:00:00,0\n2011-06-26 22:00:00,0\n2011-06-26 23:00:00,0\n2011-06-27 00:00:00,0\n2011-06-27 01:00:00,0\n2011-06-27 02:00:00,0\n2011-06-27 03:00:00,0\n2011-06-27 04:00:00,0\n2011-06-27 05:00:00,0\n2011-06-27 06:00:00,0\n2011-06-27 07:00:00,0\n2011-06-27 08:00:00,0\n2011-06-27 09:00:00,0\n2011-06-27 10:00:00,0\n2011-06-27 11:00:00,0\n2011-06-27 12:00:00,0\n2011-06-27 13:00:00,0\n2011-06-27 14:00:00,0\n2011-06-27 15:00:00,0\n2011-06-27 16:00:00,0\n2011-06-27 17:00:00,0\n2011-06-27 18:00:00,0\n2011-06-27 19:00:00,0\n2011-06-27 20:00:00,0\n2011-06-27 21:00:00,0\n2011-06-27 22:00:00,0\n2011-06-27 23:00:00,0\n2011-06-28 00:00:00,0\n2011-06-28 01:00:00,0\n2011-06-28 02:00:00,0\n2011-06-28 03:00:00,0\n2011-06-28 04:00:00,0\n2011-06-28 05:00:00,0\n2011-06-28 06:00:00,0\n2011-06-28 07:00:00,0\n2011-06-28 08:00:00,0\n2011-06-28 09:00:00,0\n2011-06-28 10:00:00,0\n2011-06-28 11:00:00,0\n2011-06-28 12:00:00,0\n2011-06-28 13:00:00,0\n2011-06-28 14:00:00,0\n2011-06-28 15:00:00,0\n2011-06-28 16:00:00,0\n2011-06-28 17:00:00,0\n2011-06-28 18:00:00,0\n2011-06-28 19:00:00,0\n2011-06-28 20:00:00,0\n2011-06-28 21:00:00,0\n2011-06-28 22:00:00,0\n2011-06-28 23:00:00,0\n2011-06-29 00:00:00,0\n2011-06-29 01:00:00,0\n2011-06-29 02:00:00,0\n2011-06-29 03:00:00,0\n2011-06-29 04:00:00,0\n2011-06-29 05:00:00,0\n2011-06-29 06:00:00,0\n2011-06-29 07:00:00,0\n2011-06-29 08:00:00,0\n2011-06-29 09:00:00,0\n2011-06-29 10:00:00,0\n2011-06-29 11:00:00,0\n2011-06-29 12:00:00,0\n2011-06-29 13:00:00,0\n2011-06-29 14:00:00,0\n2011-06-29 15:00:00,0\n2011-06-29 16:00:00,0\n2011-06-29 17:00:00,0\n2011-06-29 18:00:00,0\n2011-06-29 19:00:00,0\n2011-06-29 20:00:00,0\n2011-06-29 21:00:00,0\n2011-06-29 22:00:00,0\n2011-06-29 23:00:00,0\n2011-06-30 00:00:00,0\n2011-06-30 01:00:00,0\n2011-06-30 02:00:00,0\n2011-06-30 03:00:00,0\n2011-06-30 04:00:00,0\n2011-06-30 05:00:00,0\n2011-06-30 06:00:00,0\n2011-06-30 07:00:00,0\n2011-06-30 08:00:00,0\n2011-06-30 09:00:00,0\n2011-06-30 10:00:00,0\n2011-06-30 11:00:00,0\n2011-06-30 12:00:00,0\n2011-06-30 13:00:00,0\n2011-06-30 14:00:00,0\n2011-06-30 15:00:00,0\n2011-06-30 16:00:00,0\n2011-06-30 17:00:00,0\n2011-06-30 18:00:00,0\n2011-06-30 19:00:00,0\n2011-06-30 20:00:00,0\n2011-06-30 21:00:00,0\n2011-06-30 22:00:00,0\n2011-06-30 23:00:00,0\n2011-07-20 00:00:00,0\n2011-07-20 01:00:00,0\n2011-07-20 02:00:00,0\n2011-07-20 03:00:00,0\n2011-07-20 04:00:00,0\n2011-07-20 05:00:00,0\n2011-07-20 06:00:00,0\n2011-07-20 07:00:00,0\n2011-07-20 08:00:00,0\n2011-07-20 09:00:00,0\n2011-07-20 10:00:00,0\n2011-07-20 11:00:00,0\n2011-07-20 12:00:00,0\n2011-07-20 13:00:00,0\n2011-07-20 14:00:00,0\n2011-07-20 15:00:00,0\n2011-07-20 16:00:00,0\n2011-07-20 17:00:00,0\n2011-07-20 18:00:00,0\n2011-07-20 19:00:00,0\n2011-07-20 20:00:00,0\n2011-07-20 21:00:00,0\n2011-07-20 22:00:00,0\n2011-07-20 23:00:00,0\n2011-07-21 00:00:00,0\n2011-07-21 01:00:00,0\n2011-07-21 02:00:00,0\n2011-07-21 03:00:00,0\n2011-07-21 04:00:00,0\n2011-07-21 05:00:00,0\n2011-07-21 06:00:00,0\n2011-07-21 07:00:00,0\n2011-07-21 08:00:00,0\n2011-07-21 09:00:00,0\n2011-07-21 10:00:00,0\n2011-07-21 11:00:00,0\n2011-07-21 12:00:00,0\n2011-07-21 13:00:00,0\n2011-07-21 14:00:00,0\n2011-07-21 15:00:00,0\n2011-07-21 16:00:00,0\n2011-07-21 17:00:00,0\n2011-07-21 18:00:00,0\n2011-07-21 19:00:00,0\n2011-07-21 20:00:00,0\n2011-07-21 21:00:00,0\n2011-07-21 22:00:00,0\n2011-07-21 23:00:00,0\n2011-07-22 00:00:00,0\n2011-07-22 01:00:00,0\n2011-07-22 02:00:00,0\n2011-07-22 03:00:00,0\n2011-07-22 04:00:00,0\n2011-07-22 05:00:00,0\n2011-07-22 06:00:00,0\n2011-07-22 07:00:00,0\n2011-07-22 08:00:00,0\n2011-07-22 09:00:00,0\n2011-07-22 10:00:00,0\n2011-07-22 11:00:00,0\n2011-07-22 12:00:00,0\n2011-07-22 13:00:00,0\n2011-07-22 14:00:00,0\n2011-07-22 15:00:00,0\n2011-07-22 16:00:00,0\n2011-07-22 17:00:00,0\n2011-07-22 18:00:00,0\n2011-07-22 19:00:00,0\n2011-07-22 20:00:00,0\n2011-07-22 21:00:00,0\n2011-07-22 22:00:00,0\n2011-07-22 23:00:00,0\n2011-07-23 00:00:00,0\n2011-07-23 01:00:00,0\n2011-07-23 02:00:00,0\n2011-07-23 03:00:00,0\n2011-07-23 04:00:00,0\n2011-07-23 05:00:00,0\n2011-07-23 06:00:00,0\n2011-07-23 07:00:00,0\n2011-07-23 08:00:00,0\n2011-07-23 09:00:00,0\n2011-07-23 10:00:00,0\n2011-07-23 11:00:00,0\n2011-07-23 12:00:00,0\n2011-07-23 13:00:00,0\n2011-07-23 14:00:00,0\n2011-07-23 15:00:00,0\n2011-07-23 16:00:00,0\n2011-07-23 17:00:00,0\n2011-07-23 18:00:00,0\n2011-07-23 19:00:00,0\n2011-07-23 20:00:00,0\n2011-07-23 21:00:00,0\n2011-07-23 22:00:00,0\n2011-07-23 23:00:00,0\n2011-07-24 00:00:00,0\n2011-07-24 01:00:00,0\n2011-07-24 02:00:00,0\n2011-07-24 03:00:00,0\n2011-07-24 04:00:00,0\n2011-07-24 05:00:00,0\n2011-07-24 06:00:00,0\n2011-07-24 07:00:00,0\n2011-07-24 08:00:00,0\n2011-07-24 09:00:00,0\n2011-07-24 10:00:00,0\n2011-07-24 11:00:00,0\n2011-07-24 12:00:00,0\n2011-07-24 13:00:00,0\n2011-07-24 14:00:00,0\n2011-07-24 15:00:00,0\n2011-07-24 16:00:00,0\n2011-07-24 17:00:00,0\n2011-07-24 18:00:00,0\n2011-07-24 19:00:00,0\n2011-07-24 20:00:00,0\n2011-07-24 21:00:00,0\n2011-07-24 22:00:00,0\n2011-07-24 23:00:00,0\n2011-07-25 00:00:00,0\n2011-07-25 01:00:00,0\n2011-07-25 02:00:00,0\n2011-07-25 03:00:00,0\n2011-07-25 04:00:00,0\n2011-07-25 05:00:00,0\n2011-07-25 06:00:00,0\n2011-07-25 07:00:00,0\n2011-07-25 08:00:00,0\n2011-07-25 09:00:00,0\n2011-07-25 10:00:00,0\n2011-07-25 11:00:00,0\n2011-07-25 12:00:00,0\n2011-07-25 13:00:00,0\n2011-07-25 14:00:00,0\n2011-07-25 15:00:00,0\n2011-07-25 16:00:00,0\n2011-07-25 17:00:00,0\n2011-07-25 18:00:00,0\n2011-07-25 19:00:00,0\n2011-07-25 20:00:00,0\n2011-07-25 21:00:00,0\n2011-07-25 22:00:00,0\n2011-07-25 23:00:00,0\n2011-07-26 00:00:00,0\n2011-07-26 01:00:00,0\n2011-07-26 02:00:00,0\n2011-07-26 03:00:00,0\n2011-07-26 04:00:00,0\n2011-07-26 05:00:00,0\n2011-07-26 06:00:00,0\n2011-07-26 07:00:00,0\n2011-07-26 08:00:00,0\n2011-07-26 09:00:00,0\n2011-07-26 10:00:00,0\n2011-07-26 11:00:00,0\n2011-07-26 12:00:00,0\n2011-07-26 13:00:00,0\n2011-07-26 14:00:00,0\n2011-07-26 15:00:00,0\n2011-07-26 16:00:00,0\n2011-07-26 17:00:00,0\n2011-07-26 18:00:00,0\n2011-07-26 19:00:00,0\n2011-07-26 20:00:00,0\n2011-07-26 21:00:00,0\n2011-07-26 22:00:00,0\n2011-07-26 23:00:00,0\n2011-07-27 00:00:00,0\n2011-07-27 01:00:00,0\n2011-07-27 02:00:00,0\n2011-07-27 03:00:00,0\n2011-07-27 04:00:00,0\n2011-07-27 05:00:00,0\n2011-07-27 06:00:00,0\n2011-07-27 07:00:00,0\n2011-07-27 08:00:00,0\n2011-07-27 09:00:00,0\n2011-07-27 10:00:00,0\n2011-07-27 11:00:00,0\n2011-07-27 12:00:00,0\n2011-07-27 13:00:00,0\n2011-07-27 14:00:00,0\n2011-07-27 15:00:00,0\n2011-07-27 16:00:00,0\n2011-07-27 17:00:00,0\n2011-07-27 18:00:00,0\n2011-07-27 19:00:00,0\n2011-07-27 20:00:00,0\n2011-07-27 21:00:00,0\n2011-07-27 22:00:00,0\n2011-07-27 23:00:00,0\n2011-07-28 00:00:00,0\n2011-07-28 01:00:00,0\n2011-07-28 02:00:00,0\n2011-07-28 03:00:00,0\n2011-07-28 04:00:00,0\n2011-07-28 05:00:00,0\n2011-07-28 06:00:00,0\n2011-07-28 07:00:00,0\n2011-07-28 08:00:00,0\n2011-07-28 09:00:00,0\n2011-07-28 10:00:00,0\n2011-07-28 11:00:00,0\n2011-07-28 12:00:00,0\n2011-07-28 13:00:00,0\n2011-07-28 14:00:00,0\n2011-07-28 15:00:00,0\n2011-07-28 16:00:00,0\n2011-07-28 17:00:00,0\n2011-07-28 18:00:00,0\n2011-07-28 19:00:00,0\n2011-07-28 20:00:00,0\n2011-07-28 21:00:00,0\n2011-07-28 22:00:00,0\n2011-07-28 23:00:00,0\n2011-07-29 00:00:00,0\n2011-07-29 01:00:00,0\n2011-07-29 02:00:00,0\n2011-07-29 03:00:00,0\n2011-07-29 04:00:00,0\n2011-07-29 05:00:00,0\n2011-07-29 06:00:00,0\n2011-07-29 07:00:00,0\n2011-07-29 08:00:00,0\n2011-07-29 09:00:00,0\n2011-07-29 10:00:00,0\n2011-07-29 11:00:00,0\n2011-07-29 12:00:00,0\n2011-07-29 13:00:00,0\n2011-07-29 14:00:00,0\n2011-07-29 15:00:00,0\n2011-07-29 16:00:00,0\n2011-07-29 17:00:00,0\n2011-07-29 18:00:00,0\n2011-07-29 19:00:00,0\n2011-07-29 20:00:00,0\n2011-07-29 21:00:00,0\n2011-07-29 22:00:00,0\n2011-07-29 23:00:00,0\n2011-07-30 00:00:00,0\n2011-07-30 01:00:00,0\n2011-07-30 02:00:00,0\n2011-07-30 03:00:00,0\n2011-07-30 04:00:00,0\n2011-07-30 05:00:00,0\n2011-07-30 06:00:00,0\n2011-07-30 07:00:00,0\n2011-07-30 08:00:00,0\n2011-07-30 09:00:00,0\n2011-07-30 10:00:00,0\n2011-07-30 11:00:00,0\n2011-07-30 12:00:00,0\n2011-07-30 13:00:00,0\n2011-07-30 14:00:00,0\n2011-07-30 15:00:00,0\n2011-07-30 16:00:00,0\n2011-07-30 17:00:00,0\n2011-07-30 18:00:00,0\n2011-07-30 19:00:00,0\n2011-07-30 20:00:00,0\n2011-07-30 21:00:00,0\n2011-07-30 22:00:00,0\n2011-07-30 23:00:00,0\n2011-07-31 00:00:00,0\n2011-07-31 01:00:00,0\n2011-07-31 02:00:00,0\n2011-07-31 03:00:00,0\n2011-07-31 04:00:00,0\n2011-07-31 05:00:00,0\n2011-07-31 06:00:00,0\n2011-07-31 07:00:00,0\n2011-07-31 08:00:00,0\n2011-07-31 09:00:00,0\n2011-07-31 10:00:00,0\n2011-07-31 11:00:00,0\n2011-07-31 12:00:00,0\n2011-07-31 13:00:00,0\n2011-07-31 14:00:00,0\n2011-07-31 15:00:00,0\n2011-07-31 16:00:00,0\n2011-07-31 17:00:00,0\n2011-07-31 18:00:00,0\n2011-07-31 19:00:00,0\n2011-07-31 20:00:00,0\n2011-07-31 21:00:00,0\n2011-07-31 22:00:00,0\n2011-07-31 23:00:00,0\n2011-08-20 00:00:00,0\n2011-08-20 01:00:00,0\n2011-08-20 02:00:00,0\n2011-08-20 03:00:00,0\n2011-08-20 04:00:00,0\n2011-08-20 05:00:00,0\n2011-08-20 06:00:00,0\n2011-08-20 07:00:00,0\n2011-08-20 08:00:00,0\n2011-08-20 09:00:00,0\n2011-08-20 10:00:00,0\n2011-08-20 11:00:00,0\n2011-08-20 12:00:00,0\n2011-08-20 13:00:00,0\n2011-08-20 14:00:00,0\n2011-08-20 15:00:00,0\n2011-08-20 16:00:00,0\n2011-08-20 17:00:00,0\n2011-08-20 18:00:00,0\n2011-08-20 19:00:00,0\n2011-08-20 20:00:00,0\n2011-08-20 21:00:00,0\n2011-08-20 22:00:00,0\n2011-08-20 23:00:00,0\n2011-08-21 00:00:00,0\n2011-08-21 01:00:00,0\n2011-08-21 02:00:00,0\n2011-08-21 03:00:00,0\n2011-08-21 04:00:00,0\n2011-08-21 05:00:00,0\n2011-08-21 06:00:00,0\n2011-08-21 07:00:00,0\n2011-08-21 08:00:00,0\n2011-08-21 09:00:00,0\n2011-08-21 10:00:00,0\n2011-08-21 11:00:00,0\n2011-08-21 12:00:00,0\n2011-08-21 13:00:00,0\n2011-08-21 14:00:00,0\n2011-08-21 15:00:00,0\n2011-08-21 16:00:00,0\n2011-08-21 17:00:00,0\n2011-08-21 18:00:00,0\n2011-08-21 19:00:00,0\n2011-08-21 20:00:00,0\n2011-08-21 21:00:00,0\n2011-08-21 22:00:00,0\n2011-08-21 23:00:00,0\n2011-08-22 00:00:00,0\n2011-08-22 01:00:00,0\n2011-08-22 02:00:00,0\n2011-08-22 03:00:00,0\n2011-08-22 04:00:00,0\n2011-08-22 05:00:00,0\n2011-08-22 06:00:00,0\n2011-08-22 07:00:00,0\n2011-08-22 08:00:00,0\n2011-08-22 09:00:00,0\n2011-08-22 10:00:00,0\n2011-08-22 11:00:00,0\n2011-08-22 12:00:00,0\n2011-08-22 13:00:00,0\n2011-08-22 14:00:00,0\n2011-08-22 15:00:00,0\n2011-08-22 16:00:00,0\n2011-08-22 17:00:00,0\n2011-08-22 18:00:00,0\n2011-08-22 19:00:00,0\n2011-08-22 20:00:00,0\n2011-08-22 21:00:00,0\n2011-08-22 22:00:00,0\n2011-08-22 23:00:00,0\n2011-08-23 00:00:00,0\n2011-08-23 01:00:00,0\n2011-08-23 02:00:00,0\n2011-08-23 03:00:00,0\n2011-08-23 04:00:00,0\n2011-08-23 05:00:00,0\n2011-08-23 06:00:00,0\n2011-08-23 07:00:00,0\n2011-08-23 08:00:00,0\n2011-08-23 09:00:00,0\n2011-08-23 10:00:00,0\n2011-08-23 11:00:00,0\n2011-08-23 12:00:00,0\n2011-08-23 13:00:00,0\n2011-08-23 14:00:00,0\n2011-08-23 15:00:00,0\n2011-08-23 16:00:00,0\n2011-08-23 17:00:00,0\n2011-08-23 18:00:00,0\n2011-08-23 19:00:00,0\n2011-08-23 20:00:00,0\n2011-08-23 21:00:00,0\n2011-08-23 22:00:00,0\n2011-08-23 23:00:00,0\n2011-08-24 00:00:00,0\n2011-08-24 01:00:00,0\n2011-08-24 02:00:00,0\n2011-08-24 03:00:00,0\n2011-08-24 04:00:00,0\n2011-08-24 05:00:00,0\n2011-08-24 06:00:00,0\n2011-08-24 07:00:00,0\n2011-08-24 08:00:00,0\n2011-08-24 09:00:00,0\n2011-08-24 10:00:00,0\n2011-08-24 11:00:00,0\n2011-08-24 12:00:00,0\n2011-08-24 13:00:00,0\n2011-08-24 14:00:00,0\n2011-08-24 15:00:00,0\n2011-08-24 16:00:00,0\n2011-08-24 17:00:00,0\n2011-08-24 18:00:00,0\n2011-08-24 19:00:00,0\n2011-08-24 20:00:00,0\n2011-08-24 21:00:00,0\n2011-08-24 22:00:00,0\n2011-08-24 23:00:00,0\n2011-08-25 00:00:00,0\n2011-08-25 01:00:00,0\n2011-08-25 02:00:00,0\n2011-08-25 03:00:00,0\n2011-08-25 04:00:00,0\n2011-08-25 05:00:00,0\n2011-08-25 06:00:00,0\n2011-08-25 07:00:00,0\n2011-08-25 08:00:00,0\n2011-08-25 09:00:00,0\n2011-08-25 10:00:00,0\n2011-08-25 11:00:00,0\n2011-08-25 12:00:00,0\n2011-08-25 13:00:00,0\n2011-08-25 14:00:00,0\n2011-08-25 15:00:00,0\n2011-08-25 16:00:00,0\n2011-08-25 17:00:00,0\n2011-08-25 18:00:00,0\n2011-08-25 19:00:00,0\n2011-08-25 20:00:00,0\n2011-08-25 21:00:00,0\n2011-08-25 22:00:00,0\n2011-08-25 23:00:00,0\n2011-08-26 00:00:00,0\n2011-08-26 01:00:00,0\n2011-08-26 02:00:00,0\n2011-08-26 03:00:00,0\n2011-08-26 04:00:00,0\n2011-08-26 05:00:00,0\n2011-08-26 06:00:00,0\n2011-08-26 07:00:00,0\n2011-08-26 08:00:00,0\n2011-08-26 09:00:00,0\n2011-08-26 10:00:00,0\n2011-08-26 11:00:00,0\n2011-08-26 12:00:00,0\n2011-08-26 13:00:00,0\n2011-08-26 14:00:00,0\n2011-08-26 15:00:00,0\n2011-08-26 16:00:00,0\n2011-08-26 17:00:00,0\n2011-08-26 18:00:00,0\n2011-08-26 19:00:00,0\n2011-08-26 20:00:00,0\n2011-08-26 21:00:00,0\n2011-08-26 22:00:00,0\n2011-08-26 23:00:00,0\n2011-08-27 00:00:00,0\n2011-08-27 01:00:00,0\n2011-08-27 02:00:00,0\n2011-08-27 03:00:00,0\n2011-08-27 04:00:00,0\n2011-08-27 05:00:00,0\n2011-08-27 06:00:00,0\n2011-08-27 07:00:00,0\n2011-08-27 08:00:00,0\n2011-08-27 09:00:00,0\n2011-08-27 10:00:00,0\n2011-08-27 11:00:00,0\n2011-08-27 12:00:00,0\n2011-08-27 13:00:00,0\n2011-08-27 14:00:00,0\n2011-08-27 15:00:00,0\n2011-08-27 16:00:00,0\n2011-08-27 17:00:00,0\n2011-08-28 07:00:00,0\n2011-08-28 08:00:00,0\n2011-08-28 09:00:00,0\n2011-08-28 10:00:00,0\n2011-08-28 11:00:00,0\n2011-08-28 12:00:00,0\n2011-08-28 13:00:00,0\n2011-08-28 14:00:00,0\n2011-08-28 15:00:00,0\n2011-08-28 16:00:00,0\n2011-08-28 17:00:00,0\n2011-08-28 18:00:00,0\n2011-08-28 19:00:00,0\n2011-08-28 20:00:00,0\n2011-08-28 21:00:00,0\n2011-08-28 22:00:00,0\n2011-08-28 23:00:00,0\n2011-08-29 00:00:00,0\n2011-08-29 01:00:00,0\n2011-08-29 02:00:00,0\n2011-08-29 03:00:00,0\n2011-08-29 04:00:00,0\n2011-08-29 05:00:00,0\n2011-08-29 06:00:00,0\n2011-08-29 07:00:00,0\n2011-08-29 08:00:00,0\n2011-08-29 09:00:00,0\n2011-08-29 10:00:00,0\n2011-08-29 11:00:00,0\n2011-08-29 12:00:00,0\n2011-08-29 13:00:00,0\n2011-08-29 14:00:00,0\n2011-08-29 15:00:00,0\n2011-08-29 16:00:00,0\n2011-08-29 17:00:00,0\n2011-08-29 18:00:00,0\n2011-08-29 19:00:00,0\n2011-08-29 20:00:00,0\n2011-08-29 21:00:00,0\n2011-08-29 22:00:00,0\n2011-08-29 23:00:00,0\n2011-08-30 00:00:00,0\n2011-08-30 01:00:00,0\n2011-08-30 02:00:00,0\n2011-08-30 03:00:00,0\n2011-08-30 04:00:00,0\n2011-08-30 05:00:00,0\n2011-08-30 06:00:00,0\n2011-08-30 07:00:00,0\n2011-08-30 08:00:00,0\n2011-08-30 09:00:00,0\n2011-08-30 10:00:00,0\n2011-08-30 11:00:00,0\n2011-08-30 12:00:00,0\n2011-08-30 13:00:00,0\n2011-08-30 14:00:00,0\n2011-08-30 15:00:00,0\n2011-08-30 16:00:00,0\n2011-08-30 17:00:00,0\n2011-08-30 18:00:00,0\n2011-08-30 19:00:00,0\n2011-08-30 20:00:00,0\n2011-08-30 21:00:00,0\n2011-08-30 22:00:00,0\n2011-08-30 23:00:00,0\n2011-08-31 00:00:00,0\n2011-08-31 01:00:00,0\n2011-08-31 02:00:00,0\n2011-08-31 03:00:00,0\n2011-08-31 04:00:00,0\n2011-08-31 05:00:00,0\n2011-08-31 06:00:00,0\n2011-08-31 07:00:00,0\n2011-08-31 08:00:00,0\n2011-08-31 09:00:00,0\n2011-08-31 10:00:00,0\n2011-08-31 11:00:00,0\n2011-08-31 12:00:00,0\n2011-08-31 13:00:00,0\n2011-08-31 14:00:00,0\n2011-08-31 15:00:00,0\n2011-08-31 16:00:00,0\n2011-08-31 17:00:00,0\n2011-08-31 18:00:00,0\n2011-08-31 19:00:00,0\n2011-08-31 20:00:00,0\n2011-08-31 21:00:00,0\n2011-08-31 22:00:00,0\n2011-08-31 23:00:00,0\n2011-09-20 00:00:00,0\n2011-09-20 01:00:00,0\n2011-09-20 02:00:00,0\n2011-09-20 03:00:00,0\n2011-09-20 04:00:00,0\n2011-09-20 05:00:00,0\n2011-09-20 06:00:00,0\n2011-09-20 07:00:00,0\n2011-09-20 08:00:00,0\n2011-09-20 09:00:00,0\n2011-09-20 10:00:00,0\n2011-09-20 11:00:00,0\n2011-09-20 12:00:00,0\n2011-09-20 13:00:00,0\n2011-09-20 14:00:00,0\n2011-09-20 15:00:00,0\n2011-09-20 16:00:00,0\n2011-09-20 17:00:00,0\n2011-09-20 18:00:00,0\n2011-09-20 19:00:00,0\n2011-09-20 20:00:00,0\n2011-09-20 21:00:00,0\n2011-09-20 22:00:00,0\n2011-09-20 23:00:00,0\n2011-09-21 00:00:00,0\n2011-09-21 01:00:00,0\n2011-09-21 02:00:00,0\n2011-09-21 03:00:00,0\n2011-09-21 04:00:00,0\n2011-09-21 05:00:00,0\n2011-09-21 06:00:00,0\n2011-09-21 07:00:00,0\n2011-09-21 08:00:00,0\n2011-09-21 09:00:00,0\n2011-09-21 10:00:00,0\n2011-09-21 11:00:00,0\n2011-09-21 12:00:00,0\n2011-09-21 13:00:00,0\n2011-09-21 14:00:00,0\n2011-09-21 15:00:00,0\n2011-09-21 16:00:00,0\n2011-09-21 17:00:00,0\n2011-09-21 18:00:00,0\n2011-09-21 19:00:00,0\n2011-09-21 20:00:00,0\n2011-09-21 21:00:00,0\n2011-09-21 22:00:00,0\n2011-09-21 23:00:00,0\n2011-09-22 00:00:00,0\n2011-09-22 01:00:00,0\n2011-09-22 02:00:00,0\n2011-09-22 03:00:00,0\n2011-09-22 04:00:00,0\n2011-09-22 05:00:00,0\n2011-09-22 06:00:00,0\n2011-09-22 07:00:00,0\n2011-09-22 08:00:00,0\n2011-09-22 09:00:00,0\n2011-09-22 10:00:00,0\n2011-09-22 11:00:00,0\n2011-09-22 12:00:00,0\n2011-09-22 13:00:00,0\n2011-09-22 14:00:00,0\n2011-09-22 15:00:00,0\n2011-09-22 16:00:00,0\n2011-09-22 17:00:00,0\n2011-09-22 18:00:00,0\n2011-09-22 19:00:00,0\n2011-09-22 20:00:00,0\n2011-09-22 21:00:00,0\n2011-09-22 22:00:00,0\n2011-09-22 23:00:00,0\n2011-09-23 00:00:00,0\n2011-09-23 01:00:00,0\n2011-09-23 02:00:00,0\n2011-09-23 03:00:00,0\n2011-09-23 04:00:00,0\n2011-09-23 05:00:00,0\n2011-09-23 06:00:00,0\n2011-09-23 07:00:00,0\n2011-09-23 08:00:00,0\n2011-09-23 09:00:00,0\n2011-09-23 10:00:00,0\n2011-09-23 11:00:00,0\n2011-09-23 12:00:00,0\n2011-09-23 13:00:00,0\n2011-09-23 14:00:00,0\n2011-09-23 15:00:00,0\n2011-09-23 16:00:00,0\n2011-09-23 17:00:00,0\n2011-09-23 18:00:00,0\n2011-09-23 19:00:00,0\n2011-09-23 20:00:00,0\n2011-09-23 21:00:00,0\n2011-09-23 22:00:00,0\n2011-09-23 23:00:00,0\n2011-09-24 00:00:00,0\n2011-09-24 01:00:00,0\n2011-09-24 02:00:00,0\n2011-09-24 03:00:00,0\n2011-09-24 04:00:00,0\n2011-09-24 05:00:00,0\n2011-09-24 06:00:00,0\n2011-09-24 07:00:00,0\n2011-09-24 08:00:00,0\n2011-09-24 09:00:00,0\n2011-09-24 10:00:00,0\n2011-09-24 11:00:00,0\n2011-09-24 12:00:00,0\n2011-09-24 13:00:00,0\n2011-09-24 14:00:00,0\n2011-09-24 15:00:00,0\n2011-09-24 16:00:00,0\n2011-09-24 17:00:00,0\n2011-09-24 18:00:00,0\n2011-09-24 19:00:00,0\n2011-09-24 20:00:00,0\n2011-09-24 21:00:00,0\n2011-09-24 22:00:00,0\n2011-09-24 23:00:00,0\n2011-09-25 00:00:00,0\n2011-09-25 01:00:00,0\n2011-09-25 02:00:00,0\n2011-09-25 03:00:00,0\n2011-09-25 04:00:00,0\n2011-09-25 05:00:00,0\n2011-09-25 06:00:00,0\n2011-09-25 07:00:00,0\n2011-09-25 08:00:00,0\n2011-09-25 09:00:00,0\n2011-09-25 10:00:00,0\n2011-09-25 11:00:00,0\n2011-09-25 12:00:00,0\n2011-09-25 13:00:00,0\n2011-09-25 14:00:00,0\n2011-09-25 15:00:00,0\n2011-09-25 16:00:00,0\n2011-09-25 17:00:00,0\n2011-09-25 18:00:00,0\n2011-09-25 19:00:00,0\n2011-09-25 20:00:00,0\n2011-09-25 21:00:00,0\n2011-09-25 22:00:00,0\n2011-09-25 23:00:00,0\n2011-09-26 00:00:00,0\n2011-09-26 01:00:00,0\n2011-09-26 02:00:00,0\n2011-09-26 03:00:00,0\n2011-09-26 04:00:00,0\n2011-09-26 05:00:00,0\n2011-09-26 06:00:00,0\n2011-09-26 07:00:00,0\n2011-09-26 08:00:00,0\n2011-09-26 09:00:00,0\n2011-09-26 10:00:00,0\n2011-09-26 11:00:00,0\n2011-09-26 12:00:00,0\n2011-09-26 13:00:00,0\n2011-09-26 14:00:00,0\n2011-09-26 15:00:00,0\n2011-09-26 16:00:00,0\n2011-09-26 17:00:00,0\n2011-09-26 18:00:00,0\n2011-09-26 19:00:00,0\n2011-09-26 20:00:00,0\n2011-09-26 21:00:00,0\n2011-09-26 22:00:00,0\n2011-09-26 23:00:00,0\n2011-09-27 00:00:00,0\n2011-09-27 01:00:00,0\n2011-09-27 02:00:00,0\n2011-09-27 03:00:00,0\n2011-09-27 04:00:00,0\n2011-09-27 05:00:00,0\n2011-09-27 06:00:00,0\n2011-09-27 07:00:00,0\n2011-09-27 08:00:00,0\n2011-09-27 09:00:00,0\n2011-09-27 10:00:00,0\n2011-09-27 11:00:00,0\n2011-09-27 12:00:00,0\n2011-09-27 13:00:00,0\n2011-09-27 14:00:00,0\n2011-09-27 15:00:00,0\n2011-09-27 16:00:00,0\n2011-09-27 17:00:00,0\n2011-09-27 18:00:00,0\n2011-09-27 19:00:00,0\n2011-09-27 20:00:00,0\n2011-09-27 21:00:00,0\n2011-09-27 22:00:00,0\n2011-09-27 23:00:00,0\n2011-09-28 00:00:00,0\n2011-09-28 01:00:00,0\n2011-09-28 02:00:00,0\n2011-09-28 03:00:00,0\n2011-09-28 04:00:00,0\n2011-09-28 05:00:00,0\n2011-09-28 06:00:00,0\n2011-09-28 07:00:00,0\n2011-09-28 08:00:00,0\n2011-09-28 09:00:00,0\n2011-09-28 10:00:00,0\n2011-09-28 11:00:00,0\n2011-09-28 12:00:00,0\n2011-09-28 13:00:00,0\n2011-09-28 14:00:00,0\n2011-09-28 15:00:00,0\n2011-09-28 16:00:00,0\n2011-09-28 17:00:00,0\n2011-09-28 18:00:00,0\n2011-09-28 19:00:00,0\n2011-09-28 20:00:00,0\n2011-09-28 21:00:00,0\n2011-09-28 22:00:00,0\n2011-09-28 23:00:00,0\n2011-09-29 00:00:00,0\n2011-09-29 01:00:00,0\n2011-09-29 02:00:00,0\n2011-09-29 03:00:00,0\n2011-09-29 04:00:00,0\n2011-09-29 05:00:00,0\n2011-09-29 06:00:00,0\n2011-09-29 07:00:00,0\n2011-09-29 08:00:00,0\n2011-09-29 09:00:00,0\n2011-09-29 10:00:00,0\n2011-09-29 11:00:00,0\n2011-09-29 12:00:00,0\n2011-09-29 13:00:00,0\n2011-09-29 14:00:00,0\n2011-09-29 15:00:00,0\n2011-09-29 16:00:00,0\n2011-09-29 17:00:00,0\n2011-09-29 18:00:00,0\n2011-09-29 19:00:00,0\n2011-09-29 20:00:00,0\n2011-09-29 21:00:00,0\n2011-09-29 22:00:00,0\n2011-09-29 23:00:00,0\n2011-09-30 00:00:00,0\n2011-09-30 01:00:00,0\n2011-09-30 02:00:00,0\n2011-09-30 03:00:00,0\n2011-09-30 04:00:00,0\n2011-09-30 05:00:00,0\n2011-09-30 06:00:00,0\n2011-09-30 07:00:00,0\n2011-09-30 08:00:00,0\n2011-09-30 09:00:00,0\n2011-09-30 10:00:00,0\n2011-09-30 11:00:00,0\n2011-09-30 12:00:00,0\n2011-09-30 13:00:00,0\n2011-09-30 14:00:00,0\n2011-09-30 15:00:00,0\n2011-09-30 16:00:00,0\n2011-09-30 17:00:00,0\n2011-09-30 18:00:00,0\n2011-09-30 19:00:00,0\n2011-09-30 20:00:00,0\n2011-09-30 21:00:00,0\n2011-09-30 22:00:00,0\n2011-09-30 23:00:00,0\n2011-10-20 00:00:00,0\n2011-10-20 01:00:00,0\n2011-10-20 02:00:00,0\n2011-10-20 03:00:00,0\n2011-10-20 04:00:00,0\n2011-10-20 05:00:00,0\n2011-10-20 06:00:00,0\n2011-10-20 07:00:00,0\n2011-10-20 08:00:00,0\n2011-10-20 09:00:00,0\n2011-10-20 10:00:00,0\n2011-10-20 11:00:00,0\n2011-10-20 12:00:00,0\n2011-10-20 13:00:00,0\n2011-10-20 14:00:00,0\n2011-10-20 15:00:00,0\n2011-10-20 16:00:00,0\n2011-10-20 17:00:00,0\n2011-10-20 18:00:00,0\n2011-10-20 19:00:00,0\n2011-10-20 20:00:00,0\n2011-10-20 21:00:00,0\n2011-10-20 22:00:00,0\n2011-10-20 23:00:00,0\n2011-10-21 00:00:00,0\n2011-10-21 01:00:00,0\n2011-10-21 02:00:00,0\n2011-10-21 03:00:00,0\n2011-10-21 04:00:00,0\n2011-10-21 05:00:00,0\n2011-10-21 06:00:00,0\n2011-10-21 07:00:00,0\n2011-10-21 08:00:00,0\n2011-10-21 09:00:00,0\n2011-10-21 10:00:00,0\n2011-10-21 11:00:00,0\n2011-10-21 12:00:00,0\n2011-10-21 13:00:00,0\n2011-10-21 14:00:00,0\n2011-10-21 15:00:00,0\n2011-10-21 16:00:00,0\n2011-10-21 17:00:00,0\n2011-10-21 18:00:00,0\n2011-10-21 19:00:00,0\n2011-10-21 20:00:00,0\n2011-10-21 21:00:00,0\n2011-10-21 22:00:00,0\n2011-10-21 23:00:00,0\n2011-10-22 00:00:00,0\n2011-10-22 01:00:00,0\n2011-10-22 02:00:00,0\n2011-10-22 03:00:00,0\n2011-10-22 04:00:00,0\n2011-10-22 05:00:00,0\n2011-10-22 06:00:00,0\n2011-10-22 07:00:00,0\n2011-10-22 08:00:00,0\n2011-10-22 09:00:00,0\n2011-10-22 10:00:00,0\n2011-10-22 11:00:00,0\n2011-10-22 12:00:00,0\n2011-10-22 13:00:00,0\n2011-10-22 14:00:00,0\n2011-10-22 15:00:00,0\n2011-10-22 16:00:00,0\n2011-10-22 17:00:00,0\n2011-10-22 18:00:00,0\n2011-10-22 19:00:00,0\n2011-10-22 20:00:00,0\n2011-10-22 21:00:00,0\n2011-10-22 22:00:00,0\n2011-10-22 23:00:00,0\n2011-10-23 00:00:00,0\n2011-10-23 01:00:00,0\n2011-10-23 02:00:00,0\n2011-10-23 03:00:00,0\n2011-10-23 04:00:00,0\n2011-10-23 05:00:00,0\n2011-10-23 06:00:00,0\n2011-10-23 07:00:00,0\n2011-10-23 08:00:00,0\n2011-10-23 09:00:00,0\n2011-10-23 10:00:00,0\n2011-10-23 11:00:00,0\n2011-10-23 12:00:00,0\n2011-10-23 13:00:00,0\n2011-10-23 14:00:00,0\n2011-10-23 15:00:00,0\n2011-10-23 16:00:00,0\n2011-10-23 17:00:00,0\n2011-10-23 18:00:00,0\n2011-10-23 19:00:00,0\n2011-10-23 20:00:00,0\n2011-10-23 21:00:00,0\n2011-10-23 22:00:00,0\n2011-10-23 23:00:00,0\n2011-10-24 00:00:00,0\n2011-10-24 01:00:00,0\n2011-10-24 02:00:00,0\n2011-10-24 03:00:00,0\n2011-10-24 04:00:00,0\n2011-10-24 05:00:00,0\n2011-10-24 06:00:00,0\n2011-10-24 07:00:00,0\n2011-10-24 08:00:00,0\n2011-10-24 09:00:00,0\n2011-10-24 10:00:00,0\n2011-10-24 11:00:00,0\n2011-10-24 12:00:00,0\n2011-10-24 13:00:00,0\n2011-10-24 14:00:00,0\n2011-10-24 15:00:00,0\n2011-10-24 16:00:00,0\n2011-10-24 17:00:00,0\n2011-10-24 18:00:00,0\n2011-10-24 19:00:00,0\n2011-10-24 20:00:00,0\n2011-10-24 21:00:00,0\n2011-10-24 22:00:00,0\n2011-10-24 23:00:00,0\n2011-10-25 00:00:00,0\n2011-10-25 01:00:00,0\n2011-10-25 02:00:00,0\n2011-10-25 03:00:00,0\n2011-10-25 04:00:00,0\n2011-10-25 05:00:00,0\n2011-10-25 06:00:00,0\n2011-10-25 07:00:00,0\n2011-10-25 08:00:00,0\n2011-10-25 09:00:00,0\n2011-10-25 10:00:00,0\n2011-10-25 11:00:00,0\n2011-10-25 12:00:00,0\n2011-10-25 13:00:00,0\n2011-10-25 14:00:00,0\n2011-10-25 15:00:00,0\n2011-10-25 16:00:00,0\n2011-10-25 17:00:00,0\n2011-10-25 18:00:00,0\n2011-10-25 19:00:00,0\n2011-10-25 20:00:00,0\n2011-10-25 21:00:00,0\n2011-10-25 22:00:00,0\n2011-10-25 23:00:00,0\n2011-10-26 00:00:00,0\n2011-10-26 01:00:00,0\n2011-10-26 02:00:00,0\n2011-10-26 03:00:00,0\n2011-10-26 04:00:00,0\n2011-10-26 05:00:00,0\n2011-10-26 06:00:00,0\n2011-10-26 07:00:00,0\n2011-10-26 08:00:00,0\n2011-10-26 09:00:00,0\n2011-10-26 10:00:00,0\n2011-10-26 11:00:00,0\n2011-10-26 12:00:00,0\n2011-10-26 13:00:00,0\n2011-10-26 14:00:00,0\n2011-10-26 15:00:00,0\n2011-10-26 16:00:00,0\n2011-10-26 17:00:00,0\n2011-10-26 18:00:00,0\n2011-10-26 19:00:00,0\n2011-10-26 20:00:00,0\n2011-10-26 21:00:00,0\n2011-10-26 22:00:00,0\n2011-10-26 23:00:00,0\n2011-10-27 00:00:00,0\n2011-10-27 01:00:00,0\n2011-10-27 02:00:00,0\n2011-10-27 03:00:00,0\n2011-10-27 04:00:00,0\n2011-10-27 05:00:00,0\n2011-10-27 06:00:00,0\n2011-10-27 07:00:00,0\n2011-10-27 08:00:00,0\n2011-10-27 09:00:00,0\n2011-10-27 10:00:00,0\n2011-10-27 11:00:00,0\n2011-10-27 12:00:00,0\n2011-10-27 13:00:00,0\n2011-10-27 14:00:00,0\n2011-10-27 15:00:00,0\n2011-10-27 16:00:00,0\n2011-10-27 17:00:00,0\n2011-10-27 18:00:00,0\n2011-10-27 19:00:00,0\n2011-10-27 20:00:00,0\n2011-10-27 21:00:00,0\n2011-10-27 22:00:00,0\n2011-10-27 23:00:00,0\n2011-10-28 00:00:00,0\n2011-10-28 01:00:00,0\n2011-10-28 02:00:00,0\n2011-10-28 03:00:00,0\n2011-10-28 04:00:00,0\n2011-10-28 05:00:00,0\n2011-10-28 06:00:00,0\n2011-10-28 07:00:00,0\n2011-10-28 08:00:00,0\n2011-10-28 09:00:00,0\n2011-10-28 10:00:00,0\n2011-10-28 11:00:00,0\n2011-10-28 12:00:00,0\n2011-10-28 13:00:00,0\n2011-10-28 14:00:00,0\n2011-10-28 15:00:00,0\n2011-10-28 16:00:00,0\n2011-10-28 17:00:00,0\n2011-10-28 18:00:00,0\n2011-10-28 19:00:00,0\n2011-10-28 20:00:00,0\n2011-10-28 21:00:00,0\n2011-10-28 22:00:00,0\n2011-10-28 23:00:00,0\n2011-10-29 00:00:00,0\n2011-10-29 01:00:00,0\n2011-10-29 02:00:00,0\n2011-10-29 03:00:00,0\n2011-10-29 04:00:00,0\n2011-10-29 05:00:00,0\n2011-10-29 06:00:00,0\n2011-10-29 07:00:00,0\n2011-10-29 08:00:00,0\n2011-10-29 09:00:00,0\n2011-10-29 10:00:00,0\n2011-10-29 11:00:00,0\n2011-10-29 12:00:00,0\n2011-10-29 13:00:00,0\n2011-10-29 14:00:00,0\n2011-10-29 15:00:00,0\n2011-10-29 16:00:00,0\n2011-10-29 17:00:00,0\n2011-10-29 18:00:00,0\n2011-10-29 19:00:00,0\n2011-10-29 20:00:00,0\n2011-10-29 21:00:00,0\n2011-10-29 22:00:00,0\n2011-10-29 23:00:00,0\n2011-10-30 00:00:00,0\n2011-10-30 01:00:00,0\n2011-10-30 02:00:00,0\n2011-10-30 03:00:00,0\n2011-10-30 04:00:00,0\n2011-10-30 05:00:00,0\n2011-10-30 06:00:00,0\n2011-10-30 07:00:00,0\n2011-10-30 08:00:00,0\n2011-10-30 09:00:00,0\n2011-10-30 10:00:00,0\n2011-10-30 11:00:00,0\n2011-10-30 12:00:00,0\n2011-10-30 13:00:00,0\n2011-10-30 14:00:00,0\n2011-10-30 15:00:00,0\n2011-10-30 16:00:00,0\n2011-10-30 17:00:00,0\n2011-10-30 18:00:00,0\n2011-10-30 19:00:00,0\n2011-10-30 20:00:00,0\n2011-10-30 21:00:00,0\n2011-10-30 22:00:00,0\n2011-10-30 23:00:00,0\n2011-10-31 00:00:00,0\n2011-10-31 01:00:00,0\n2011-10-31 02:00:00,0\n2011-10-31 03:00:00,0\n2011-10-31 04:00:00,0\n2011-10-31 05:00:00,0\n2011-10-31 06:00:00,0\n2011-10-31 07:00:00,0\n2011-10-31 08:00:00,0\n2011-10-31 09:00:00,0\n2011-10-31 10:00:00,0\n2011-10-31 11:00:00,0\n2011-10-31 12:00:00,0\n2011-10-31 13:00:00,0\n2011-10-31 14:00:00,0\n2011-10-31 15:00:00,0\n2011-10-31 16:00:00,0\n2011-10-31 17:00:00,0\n2011-10-31 18:00:00,0\n2011-10-31 19:00:00,0\n2011-10-31 20:00:00,0\n2011-10-31 21:00:00,0\n2011-10-31 22:00:00,0\n2011-10-31 23:00:00,0\n2011-11-20 00:00:00,0\n2011-11-20 01:00:00,0\n2011-11-20 02:00:00,0\n2011-11-20 03:00:00,0\n2011-11-20 04:00:00,0\n2011-11-20 05:00:00,0\n2011-11-20 06:00:00,0\n2011-11-20 07:00:00,0\n2011-11-20 08:00:00,0\n2011-11-20 09:00:00,0\n2011-11-20 10:00:00,0\n2011-11-20 11:00:00,0\n2011-11-20 12:00:00,0\n2011-11-20 13:00:00,0\n2011-11-20 14:00:00,0\n2011-11-20 15:00:00,0\n2011-11-20 16:00:00,0\n2011-11-20 17:00:00,0\n2011-11-20 18:00:00,0\n2011-11-20 19:00:00,0\n2011-11-20 20:00:00,0\n2011-11-20 21:00:00,0\n2011-11-20 22:00:00,0\n2011-11-20 23:00:00,0\n2011-11-21 00:00:00,0\n2011-11-21 01:00:00,0\n2011-11-21 02:00:00,0\n2011-11-21 03:00:00,0\n2011-11-21 04:00:00,0\n2011-11-21 05:00:00,0\n2011-11-21 06:00:00,0\n2011-11-21 07:00:00,0\n2011-11-21 08:00:00,0\n2011-11-21 09:00:00,0\n2011-11-21 10:00:00,0\n2011-11-21 11:00:00,0\n2011-11-21 12:00:00,0\n2011-11-21 13:00:00,0\n2011-11-21 14:00:00,0\n2011-11-21 15:00:00,0\n2011-11-21 16:00:00,0\n2011-11-21 17:00:00,0\n2011-11-21 18:00:00,0\n2011-11-21 19:00:00,0\n2011-11-21 20:00:00,0\n2011-11-21 21:00:00,0\n2011-11-21 22:00:00,0\n2011-11-21 23:00:00,0\n2011-11-22 00:00:00,0\n2011-11-22 01:00:00,0\n2011-11-22 02:00:00,0\n2011-11-22 03:00:00,0\n2011-11-22 04:00:00,0\n2011-11-22 05:00:00,0\n2011-11-22 06:00:00,0\n2011-11-22 07:00:00,0\n2011-11-22 08:00:00,0\n2011-11-22 09:00:00,0\n2011-11-22 10:00:00,0\n2011-11-22 11:00:00,0\n2011-11-22 12:00:00,0\n2011-11-22 13:00:00,0\n2011-11-22 14:00:00,0\n2011-11-22 15:00:00,0\n2011-11-22 16:00:00,0\n2011-11-22 17:00:00,0\n2011-11-22 18:00:00,0\n2011-11-22 19:00:00,0\n2011-11-22 20:00:00,0\n2011-11-22 21:00:00,0\n2011-11-22 22:00:00,0\n2011-11-22 23:00:00,0\n2011-11-23 00:00:00,0\n2011-11-23 01:00:00,0\n2011-11-23 02:00:00,0\n2011-11-23 03:00:00,0\n2011-11-23 04:00:00,0\n2011-11-23 05:00:00,0\n2011-11-23 06:00:00,0\n2011-11-23 07:00:00,0\n2011-11-23 08:00:00,0\n2011-11-23 09:00:00,0\n2011-11-23 10:00:00,0\n2011-11-23 11:00:00,0\n2011-11-23 12:00:00,0\n2011-11-23 13:00:00,0\n2011-11-23 14:00:00,0\n2011-11-23 15:00:00,0\n2011-11-23 16:00:00,0\n2011-11-23 17:00:00,0\n2011-11-23 18:00:00,0\n2011-11-23 19:00:00,0\n2011-11-23 20:00:00,0\n2011-11-23 21:00:00,0\n2011-11-23 22:00:00,0\n2011-11-23 23:00:00,0\n2011-11-24 00:00:00,0\n2011-11-24 01:00:00,0\n2011-11-24 02:00:00,0\n2011-11-24 03:00:00,0\n2011-11-24 04:00:00,0\n2011-11-24 05:00:00,0\n2011-11-24 06:00:00,0\n2011-11-24 07:00:00,0\n2011-11-24 08:00:00,0\n2011-11-24 09:00:00,0\n2011-11-24 10:00:00,0\n2011-11-24 11:00:00,0\n2011-11-24 12:00:00,0\n2011-11-24 13:00:00,0\n2011-11-24 14:00:00,0\n2011-11-24 15:00:00,0\n2011-11-24 16:00:00,0\n2011-11-24 17:00:00,0\n2011-11-24 18:00:00,0\n2011-11-24 19:00:00,0\n2011-11-24 20:00:00,0\n2011-11-24 21:00:00,0\n2011-11-24 22:00:00,0\n2011-11-24 23:00:00,0\n2011-11-25 00:00:00,0\n2011-11-25 01:00:00,0\n2011-11-25 02:00:00,0\n2011-11-25 03:00:00,0\n2011-11-25 04:00:00,0\n2011-11-25 05:00:00,0\n2011-11-25 06:00:00,0\n2011-11-25 07:00:00,0\n2011-11-25 08:00:00,0\n2011-11-25 09:00:00,0\n2011-11-25 10:00:00,0\n2011-11-25 11:00:00,0\n2011-11-25 12:00:00,0\n2011-11-25 13:00:00,0\n2011-11-25 14:00:00,0\n2011-11-25 15:00:00,0\n2011-11-25 16:00:00,0\n2011-11-25 17:00:00,0\n2011-11-25 18:00:00,0\n2011-11-25 19:00:00,0\n2011-11-25 20:00:00,0\n2011-11-25 21:00:00,0\n2011-11-25 22:00:00,0\n2011-11-25 23:00:00,0\n2011-11-26 00:00:00,0\n2011-11-26 01:00:00,0\n2011-11-26 02:00:00,0\n2011-11-26 03:00:00,0\n2011-11-26 04:00:00,0\n2011-11-26 05:00:00,0\n2011-11-26 06:00:00,0\n2011-11-26 07:00:00,0\n2011-11-26 08:00:00,0\n2011-11-26 09:00:00,0\n2011-11-26 10:00:00,0\n2011-11-26 11:00:00,0\n2011-11-26 12:00:00,0\n2011-11-26 13:00:00,0\n2011-11-26 14:00:00,0\n2011-11-26 15:00:00,0\n2011-11-26 16:00:00,0\n2011-11-26 17:00:00,0\n2011-11-26 18:00:00,0\n2011-11-26 19:00:00,0\n2011-11-26 20:00:00,0\n2011-11-26 21:00:00,0\n2011-11-26 22:00:00,0\n2011-11-26 23:00:00,0\n2011-11-27 00:00:00,0\n2011-11-27 01:00:00,0\n2011-11-27 02:00:00,0\n2011-11-27 03:00:00,0\n2011-11-27 04:00:00,0\n2011-11-27 05:00:00,0\n2011-11-27 06:00:00,0\n2011-11-27 07:00:00,0\n2011-11-27 08:00:00,0\n2011-11-27 09:00:00,0\n2011-11-27 10:00:00,0\n2011-11-27 11:00:00,0\n2011-11-27 12:00:00,0\n2011-11-27 13:00:00,0\n2011-11-27 14:00:00,0\n2011-11-27 15:00:00,0\n2011-11-27 16:00:00,0\n2011-11-27 17:00:00,0\n2011-11-27 18:00:00,0\n2011-11-27 19:00:00,0\n2011-11-27 20:00:00,0\n2011-11-27 21:00:00,0\n2011-11-27 22:00:00,0\n2011-11-27 23:00:00,0\n2011-11-28 00:00:00,0\n2011-11-28 01:00:00,0\n2011-11-28 03:00:00,0\n2011-11-28 04:00:00,0\n2011-11-28 05:00:00,0\n2011-11-28 06:00:00,0\n2011-11-28 07:00:00,0\n2011-11-28 08:00:00,0\n2011-11-28 09:00:00,0\n2011-11-28 10:00:00,0\n2011-11-28 11:00:00,0\n2011-11-28 12:00:00,0\n2011-11-28 13:00:00,0\n2011-11-28 14:00:00,0\n2011-11-28 15:00:00,0\n2011-11-28 16:00:00,0\n2011-11-28 17:00:00,0\n2011-11-28 18:00:00,0\n2011-11-28 19:00:00,0\n2011-11-28 20:00:00,0\n2011-11-28 21:00:00,0\n2011-11-28 22:00:00,0\n2011-11-28 23:00:00,0\n2011-11-29 00:00:00,0\n2011-11-29 01:00:00,0\n2011-11-29 02:00:00,0\n2011-11-29 03:00:00,0\n2011-11-29 04:00:00,0\n2011-11-29 05:00:00,0\n2011-11-29 06:00:00,0\n2011-11-29 07:00:00,0\n2011-11-29 08:00:00,0\n2011-11-29 09:00:00,0\n2011-11-29 10:00:00,0\n2011-11-29 11:00:00,0\n2011-11-29 12:00:00,0\n2011-11-29 13:00:00,0\n2011-11-29 14:00:00,0\n2011-11-29 15:00:00,0\n2011-11-29 16:00:00,0\n2011-11-29 17:00:00,0\n2011-11-29 18:00:00,0\n2011-11-29 19:00:00,0\n2011-11-29 20:00:00,0\n2011-11-29 21:00:00,0\n2011-11-29 22:00:00,0\n2011-11-29 23:00:00,0\n2011-11-30 00:00:00,0\n2011-11-30 01:00:00,0\n2011-11-30 02:00:00,0\n2011-11-30 03:00:00,0\n2011-11-30 04:00:00,0\n2011-11-30 05:00:00,0\n2011-11-30 06:00:00,0\n2011-11-30 07:00:00,0\n2011-11-30 08:00:00,0\n2011-11-30 09:00:00,0\n2011-11-30 10:00:00,0\n2011-11-30 11:00:00,0\n2011-11-30 12:00:00,0\n2011-11-30 13:00:00,0\n2011-11-30 14:00:00,0\n2011-11-30 15:00:00,0\n2011-11-30 16:00:00,0\n2011-11-30 17:00:00,0\n2011-11-30 18:00:00,0\n2011-11-30 19:00:00,0\n2011-11-30 20:00:00,0\n2011-11-30 21:00:00,0\n2011-11-30 22:00:00,0\n2011-11-30 23:00:00,0\n2011-12-20 00:00:00,0\n2011-12-20 01:00:00,0\n2011-12-20 02:00:00,0\n2011-12-20 03:00:00,0\n2011-12-20 04:00:00,0\n2011-12-20 05:00:00,0\n2011-12-20 06:00:00,0\n2011-12-20 07:00:00,0\n2011-12-20 08:00:00,0\n2011-12-20 09:00:00,0\n2011-12-20 10:00:00,0\n2011-12-20 11:00:00,0\n2011-12-20 12:00:00,0\n2011-12-20 13:00:00,0\n2011-12-20 14:00:00,0\n2011-12-20 15:00:00,0\n2011-12-20 16:00:00,0\n2011-12-20 17:00:00,0\n2011-12-20 18:00:00,0\n2011-12-20 19:00:00,0\n2011-12-20 20:00:00,0\n2011-12-20 21:00:00,0\n2011-12-20 22:00:00,0\n2011-12-20 23:00:00,0\n2011-12-21 00:00:00,0\n2011-12-21 01:00:00,0\n2011-12-21 02:00:00,0\n2011-12-21 03:00:00,0\n2011-12-21 04:00:00,0\n2011-12-21 05:00:00,0\n2011-12-21 06:00:00,0\n2011-12-21 07:00:00,0\n2011-12-21 08:00:00,0\n2011-12-21 09:00:00,0\n2011-12-21 10:00:00,0\n2011-12-21 11:00:00,0\n2011-12-21 12:00:00,0\n2011-12-21 13:00:00,0\n2011-12-21 14:00:00,0\n2011-12-21 15:00:00,0\n2011-12-21 16:00:00,0\n2011-12-21 17:00:00,0\n2011-12-21 18:00:00,0\n2011-12-21 19:00:00,0\n2011-12-21 20:00:00,0\n2011-12-21 21:00:00,0\n2011-12-21 22:00:00,0\n2011-12-21 23:00:00,0\n2011-12-22 00:00:00,0\n2011-12-22 01:00:00,0\n2011-12-22 02:00:00,0\n2011-12-22 03:00:00,0\n2011-12-22 04:00:00,0\n2011-12-22 05:00:00,0\n2011-12-22 06:00:00,0\n2011-12-22 07:00:00,0\n2011-12-22 08:00:00,0\n2011-12-22 09:00:00,0\n2011-12-22 10:00:00,0\n2011-12-22 11:00:00,0\n2011-12-22 12:00:00,0\n2011-12-22 13:00:00,0\n2011-12-22 14:00:00,0\n2011-12-22 15:00:00,0\n2011-12-22 16:00:00,0\n2011-12-22 17:00:00,0\n2011-12-22 18:00:00,0\n2011-12-22 19:00:00,0\n2011-12-22 20:00:00,0\n2011-12-22 21:00:00,0\n2011-12-22 22:00:00,0\n2011-12-22 23:00:00,0\n2011-12-23 00:00:00,0\n2011-12-23 01:00:00,0\n2011-12-23 02:00:00,0\n2011-12-23 03:00:00,0\n2011-12-23 04:00:00,0\n2011-12-23 05:00:00,0\n2011-12-23 06:00:00,0\n2011-12-23 07:00:00,0\n2011-12-23 08:00:00,0\n2011-12-23 09:00:00,0\n2011-12-23 10:00:00,0\n2011-12-23 11:00:00,0\n2011-12-23 12:00:00,0\n2011-12-23 13:00:00,0\n2011-12-23 14:00:00,0\n2011-12-23 15:00:00,0\n2011-12-23 16:00:00,0\n2011-12-23 17:00:00,0\n2011-12-23 18:00:00,0\n2011-12-23 19:00:00,0\n2011-12-23 20:00:00,0\n2011-12-23 21:00:00,0\n2011-12-23 22:00:00,0\n2011-12-23 23:00:00,0\n2011-12-24 00:00:00,0\n2011-12-24 01:00:00,0\n2011-12-24 02:00:00,0\n2011-12-24 03:00:00,0\n2011-12-24 04:00:00,0\n2011-12-24 05:00:00,0\n2011-12-24 06:00:00,0\n2011-12-24 07:00:00,0\n2011-12-24 08:00:00,0\n2011-12-24 09:00:00,0\n2011-12-24 10:00:00,0\n2011-12-24 11:00:00,0\n2011-12-24 12:00:00,0\n2011-12-24 13:00:00,0\n2011-12-24 14:00:00,0\n2011-12-24 15:00:00,0\n2011-12-24 16:00:00,0\n2011-12-24 17:00:00,0\n2011-12-24 18:00:00,0\n2011-12-24 19:00:00,0\n2011-12-24 20:00:00,0\n2011-12-24 21:00:00,0\n2011-12-24 22:00:00,0\n2011-12-24 23:00:00,0\n2011-12-25 00:00:00,0\n2011-12-25 01:00:00,0\n2011-12-25 02:00:00,0\n2011-12-25 03:00:00,0\n2011-12-25 05:00:00,0\n2011-12-25 06:00:00,0\n2011-12-25 07:00:00,0\n2011-12-25 08:00:00,0\n2011-12-25 09:00:00,0\n2011-12-25 10:00:00,0\n2011-12-25 11:00:00,0\n2011-12-25 12:00:00,0\n2011-12-25 13:00:00,0\n2011-12-25 14:00:00,0\n2011-12-25 15:00:00,0\n2011-12-25 16:00:00,0\n2011-12-25 17:00:00,0\n2011-12-25 18:00:00,0\n2011-12-25 19:00:00,0\n2011-12-25 20:00:00,0\n2011-12-25 21:00:00,0\n2011-12-25 22:00:00,0\n2011-12-25 23:00:00,0\n2011-12-26 00:00:00,0\n2011-12-26 01:00:00,0\n2011-12-26 02:00:00,0\n2011-12-26 04:00:00,0\n2011-12-26 05:00:00,0\n2011-12-26 06:00:00,0\n2011-12-26 07:00:00,0\n2011-12-26 08:00:00,0\n2011-12-26 09:00:00,0\n2011-12-26 10:00:00,0\n2011-12-26 11:00:00,0\n2011-12-26 12:00:00,0\n2011-12-26 13:00:00,0\n2011-12-26 14:00:00,0\n2011-12-26 15:00:00,0\n2011-12-26 16:00:00,0\n2011-12-26 17:00:00,0\n2011-12-26 18:00:00,0\n2011-12-26 19:00:00,0\n2011-12-26 20:00:00,0\n2011-12-26 21:00:00,0\n2011-12-26 22:00:00,0\n2011-12-26 23:00:00,0\n2011-12-27 00:00:00,0\n2011-12-27 01:00:00,0\n2011-12-27 02:00:00,0\n2011-12-27 03:00:00,0\n2011-12-27 04:00:00,0\n2011-12-27 05:00:00,0\n2011-12-27 06:00:00,0\n2011-12-27 07:00:00,0\n2011-12-27 08:00:00,0\n2011-12-27 09:00:00,0\n2011-12-27 10:00:00,0\n2011-12-27 11:00:00,0\n2011-12-27 12:00:00,0\n2011-12-27 13:00:00,0\n2011-12-27 14:00:00,0\n2011-12-27 15:00:00,0\n2011-12-27 16:00:00,0\n2011-12-27 17:00:00,0\n2011-12-27 18:00:00,0\n2011-12-27 19:00:00,0\n2011-12-27 20:00:00,0\n2011-12-27 21:00:00,0\n2011-12-27 22:00:00,0\n2011-12-27 23:00:00,0\n2011-12-28 00:00:00,0\n2011-12-28 01:00:00,0\n2011-12-28 02:00:00,0\n2011-12-28 03:00:00,0\n2011-12-28 05:00:00,0\n2011-12-28 06:00:00,0\n2011-12-28 07:00:00,0\n2011-12-28 08:00:00,0\n2011-12-28 09:00:00,0\n2011-12-28 10:00:00,0\n2011-12-28 11:00:00,0\n2011-12-28 12:00:00,0\n2011-12-28 13:00:00,0\n2011-12-28 14:00:00,0\n2011-12-28 15:00:00,0\n2011-12-28 16:00:00,0\n2011-12-28 17:00:00,0\n2011-12-28 18:00:00,0\n2011-12-28 19:00:00,0\n2011-12-28 20:00:00,0\n2011-12-28 21:00:00,0\n2011-12-28 22:00:00,0\n2011-12-28 23:00:00,0\n2011-12-29 00:00:00,0\n2011-12-29 01:00:00,0\n2011-12-29 02:00:00,0\n2011-12-29 03:00:00,0\n2011-12-29 04:00:00,0\n2011-12-29 05:00:00,0\n2011-12-29 06:00:00,0\n2011-12-29 07:00:00,0\n2011-12-29 08:00:00,0\n2011-12-29 09:00:00,0\n2011-12-29 10:00:00,0\n2011-12-29 11:00:00,0\n2011-12-29 12:00:00,0\n2011-12-29 13:00:00,0\n2011-12-29 14:00:00,0\n2011-12-29 15:00:00,0\n2011-12-29 16:00:00,0\n2011-12-29 17:00:00,0\n2011-12-29 18:00:00,0\n2011-12-29 19:00:00,0\n2011-12-29 20:00:00,0\n2011-12-29 21:00:00,0\n2011-12-29 22:00:00,0\n2011-12-29 23:00:00,0\n2011-12-30 00:00:00,0\n2011-12-30 01:00:00,0\n2011-12-30 02:00:00,0\n2011-12-30 03:00:00,0\n2011-12-30 04:00:00,0\n2011-12-30 05:00:00,0\n2011-12-30 06:00:00,0\n2011-12-30 07:00:00,0\n2011-12-30 08:00:00,0\n2011-12-30 09:00:00,0\n2011-12-30 10:00:00,0\n2011-12-30 11:00:00,0\n2011-12-30 12:00:00,0\n2011-12-30 13:00:00,0\n2011-12-30 14:00:00,0\n2011-12-30 15:00:00,0\n2011-12-30 16:00:00,0\n2011-12-30 17:00:00,0\n2011-12-30 18:00:00,0\n2011-12-30 19:00:00,0\n2011-12-30 20:00:00,0\n2011-12-30 21:00:00,0\n2011-12-30 22:00:00,0\n2011-12-30 23:00:00,0\n2011-12-31 00:00:00,0\n2011-12-31 01:00:00,0\n2011-12-31 02:00:00,0\n2011-12-31 03:00:00,0\n2011-12-31 04:00:00,0\n2011-12-31 05:00:00,0\n2011-12-31 06:00:00,0\n2011-12-31 07:00:00,0\n2011-12-31 08:00:00,0\n2011-12-31 09:00:00,0\n2011-12-31 10:00:00,0\n2011-12-31 11:00:00,0\n2011-12-31 12:00:00,0\n2011-12-31 13:00:00,0\n2011-12-31 14:00:00,0\n2011-12-31 15:00:00,0\n2011-12-31 16:00:00,0\n2011-12-31 17:00:00,0\n2011-12-31 18:00:00,0\n2011-12-31 19:00:00,0\n2011-12-31 20:00:00,0\n2011-12-31 21:00:00,0\n2011-12-31 22:00:00,0\n2011-12-31 23:00:00,0\n2012-01-20 00:00:00,0\n2012-01-20 01:00:00,0\n2012-01-20 02:00:00,0\n2012-01-20 03:00:00,0\n2012-01-20 04:00:00,0\n2012-01-20 05:00:00,0\n2012-01-20 06:00:00,0\n2012-01-20 07:00:00,0\n2012-01-20 08:00:00,0\n2012-01-20 09:00:00,0\n2012-01-20 10:00:00,0\n2012-01-20 11:00:00,0\n2012-01-20 12:00:00,0\n2012-01-20 13:00:00,0\n2012-01-20 14:00:00,0\n2012-01-20 15:00:00,0\n2012-01-20 16:00:00,0\n2012-01-20 17:00:00,0\n2012-01-20 18:00:00,0\n2012-01-20 19:00:00,0\n2012-01-20 20:00:00,0\n2012-01-20 21:00:00,0\n2012-01-20 22:00:00,0\n2012-01-20 23:00:00,0\n2012-01-21 00:00:00,0\n2012-01-21 01:00:00,0\n2012-01-21 02:00:00,0\n2012-01-21 03:00:00,0\n2012-01-21 04:00:00,0\n2012-01-21 05:00:00,0\n2012-01-21 06:00:00,0\n2012-01-21 07:00:00,0\n2012-01-21 08:00:00,0\n2012-01-21 09:00:00,0\n2012-01-21 10:00:00,0\n2012-01-21 11:00:00,0\n2012-01-21 12:00:00,0\n2012-01-21 13:00:00,0\n2012-01-21 14:00:00,0\n2012-01-21 15:00:00,0\n2012-01-21 16:00:00,0\n2012-01-21 17:00:00,0\n2012-01-21 18:00:00,0\n2012-01-21 19:00:00,0\n2012-01-21 20:00:00,0\n2012-01-21 21:00:00,0\n2012-01-21 22:00:00,0\n2012-01-21 23:00:00,0\n2012-01-22 00:00:00,0\n2012-01-22 01:00:00,0\n2012-01-22 02:00:00,0\n2012-01-22 03:00:00,0\n2012-01-22 04:00:00,0\n2012-01-22 05:00:00,0\n2012-01-22 06:00:00,0\n2012-01-22 07:00:00,0\n2012-01-22 08:00:00,0\n2012-01-22 09:00:00,0\n2012-01-22 10:00:00,0\n2012-01-22 11:00:00,0\n2012-01-22 12:00:00,0\n2012-01-22 13:00:00,0\n2012-01-22 14:00:00,0\n2012-01-22 15:00:00,0\n2012-01-22 16:00:00,0\n2012-01-22 17:00:00,0\n2012-01-22 18:00:00,0\n2012-01-22 19:00:00,0\n2012-01-22 20:00:00,0\n2012-01-22 21:00:00,0\n2012-01-22 22:00:00,0\n2012-01-22 23:00:00,0\n2012-01-23 00:00:00,0\n2012-01-23 01:00:00,0\n2012-01-23 02:00:00,0\n2012-01-23 03:00:00,0\n2012-01-23 04:00:00,0\n2012-01-23 05:00:00,0\n2012-01-23 06:00:00,0\n2012-01-23 07:00:00,0\n2012-01-23 08:00:00,0\n2012-01-23 09:00:00,0\n2012-01-23 10:00:00,0\n2012-01-23 11:00:00,0\n2012-01-23 12:00:00,0\n2012-01-23 13:00:00,0\n2012-01-23 14:00:00,0\n2012-01-23 15:00:00,0\n2012-01-23 16:00:00,0\n2012-01-23 17:00:00,0\n2012-01-23 18:00:00,0\n2012-01-23 19:00:00,0\n2012-01-23 20:00:00,0\n2012-01-23 21:00:00,0\n2012-01-23 22:00:00,0\n2012-01-23 23:00:00,0\n2012-01-24 00:00:00,0\n2012-01-24 01:00:00,0\n2012-01-24 02:00:00,0\n2012-01-24 03:00:00,0\n2012-01-24 04:00:00,0\n2012-01-24 05:00:00,0\n2012-01-24 06:00:00,0\n2012-01-24 07:00:00,0\n2012-01-24 08:00:00,0\n2012-01-24 09:00:00,0\n2012-01-24 10:00:00,0\n2012-01-24 11:00:00,0\n2012-01-24 12:00:00,0\n2012-01-24 13:00:00,0\n2012-01-24 14:00:00,0\n2012-01-24 15:00:00,0\n2012-01-24 16:00:00,0\n2012-01-24 17:00:00,0\n2012-01-24 18:00:00,0\n2012-01-24 19:00:00,0\n2012-01-24 20:00:00,0\n2012-01-24 21:00:00,0\n2012-01-24 22:00:00,0\n2012-01-24 23:00:00,0\n2012-01-25 00:00:00,0\n2012-01-25 01:00:00,0\n2012-01-25 02:00:00,0\n2012-01-25 03:00:00,0\n2012-01-25 04:00:00,0\n2012-01-25 05:00:00,0\n2012-01-25 06:00:00,0\n2012-01-25 07:00:00,0\n2012-01-25 08:00:00,0\n2012-01-25 09:00:00,0\n2012-01-25 10:00:00,0\n2012-01-25 11:00:00,0\n2012-01-25 12:00:00,0\n2012-01-25 13:00:00,0\n2012-01-25 14:00:00,0\n2012-01-25 15:00:00,0\n2012-01-25 16:00:00,0\n2012-01-25 17:00:00,0\n2012-01-25 18:00:00,0\n2012-01-25 19:00:00,0\n2012-01-25 20:00:00,0\n2012-01-25 21:00:00,0\n2012-01-25 22:00:00,0\n2012-01-25 23:00:00,0\n2012-01-26 00:00:00,0\n2012-01-26 01:00:00,0\n2012-01-26 02:00:00,0\n2012-01-26 03:00:00,0\n2012-01-26 04:00:00,0\n2012-01-26 05:00:00,0\n2012-01-26 06:00:00,0\n2012-01-26 07:00:00,0\n2012-01-26 08:00:00,0\n2012-01-26 09:00:00,0\n2012-01-26 10:00:00,0\n2012-01-26 11:00:00,0\n2012-01-26 12:00:00,0\n2012-01-26 13:00:00,0\n2012-01-26 14:00:00,0\n2012-01-26 15:00:00,0\n2012-01-26 16:00:00,0\n2012-01-26 17:00:00,0\n2012-01-26 18:00:00,0\n2012-01-26 19:00:00,0\n2012-01-26 20:00:00,0\n2012-01-26 21:00:00,0\n2012-01-26 22:00:00,0\n2012-01-26 23:00:00,0\n2012-01-27 00:00:00,0\n2012-01-27 01:00:00,0\n2012-01-27 02:00:00,0\n2012-01-27 03:00:00,0\n2012-01-27 04:00:00,0\n2012-01-27 05:00:00,0\n2012-01-27 06:00:00,0\n2012-01-27 07:00:00,0\n2012-01-27 08:00:00,0\n2012-01-27 09:00:00,0\n2012-01-27 10:00:00,0\n2012-01-27 11:00:00,0\n2012-01-27 12:00:00,0\n2012-01-27 13:00:00,0\n2012-01-27 14:00:00,0\n2012-01-27 15:00:00,0\n2012-01-27 16:00:00,0\n2012-01-27 17:00:00,0\n2012-01-27 18:00:00,0\n2012-01-27 19:00:00,0\n2012-01-27 20:00:00,0\n2012-01-27 21:00:00,0\n2012-01-27 22:00:00,0\n2012-01-27 23:00:00,0\n2012-01-28 00:00:00,0\n2012-01-28 01:00:00,0\n2012-01-28 02:00:00,0\n2012-01-28 03:00:00,0\n2012-01-28 04:00:00,0\n2012-01-28 05:00:00,0\n2012-01-28 06:00:00,0\n2012-01-28 07:00:00,0\n2012-01-28 08:00:00,0\n2012-01-28 09:00:00,0\n2012-01-28 10:00:00,0\n2012-01-28 11:00:00,0\n2012-01-28 12:00:00,0\n2012-01-28 13:00:00,0\n2012-01-28 14:00:00,0\n2012-01-28 15:00:00,0\n2012-01-28 16:00:00,0\n2012-01-28 17:00:00,0\n2012-01-28 18:00:00,0\n2012-01-28 19:00:00,0\n2012-01-28 20:00:00,0\n2012-01-28 21:00:00,0\n2012-01-28 22:00:00,0\n2012-01-28 23:00:00,0\n2012-01-29 00:00:00,0\n2012-01-29 01:00:00,0\n2012-01-29 02:00:00,0\n2012-01-29 03:00:00,0\n2012-01-29 04:00:00,0\n2012-01-29 05:00:00,0\n2012-01-29 06:00:00,0\n2012-01-29 07:00:00,0\n2012-01-29 08:00:00,0\n2012-01-29 09:00:00,0\n2012-01-29 10:00:00,0\n2012-01-29 11:00:00,0\n2012-01-29 12:00:00,0\n2012-01-29 13:00:00,0\n2012-01-29 14:00:00,0\n2012-01-29 15:00:00,0\n2012-01-29 16:00:00,0\n2012-01-29 17:00:00,0\n2012-01-29 18:00:00,0\n2012-01-29 19:00:00,0\n2012-01-29 20:00:00,0\n2012-01-29 21:00:00,0\n2012-01-29 22:00:00,0\n2012-01-29 23:00:00,0\n2012-01-30 00:00:00,0\n2012-01-30 01:00:00,0\n2012-01-30 02:00:00,0\n2012-01-30 03:00:00,0\n2012-01-30 04:00:00,0\n2012-01-30 05:00:00,0\n2012-01-30 06:00:00,0\n2012-01-30 07:00:00,0\n2012-01-30 08:00:00,0\n2012-01-30 09:00:00,0\n2012-01-30 10:00:00,0\n2012-01-30 11:00:00,0\n2012-01-30 12:00:00,0\n2012-01-30 13:00:00,0\n2012-01-30 14:00:00,0\n2012-01-30 15:00:00,0\n2012-01-30 16:00:00,0\n2012-01-30 17:00:00,0\n2012-01-30 18:00:00,0\n2012-01-30 19:00:00,0\n2012-01-30 20:00:00,0\n2012-01-30 21:00:00,0\n2012-01-30 22:00:00,0\n2012-01-30 23:00:00,0\n2012-01-31 00:00:00,0\n2012-01-31 01:00:00,0\n2012-01-31 02:00:00,0\n2012-01-31 03:00:00,0\n2012-01-31 04:00:00,0\n2012-01-31 05:00:00,0\n2012-01-31 06:00:00,0\n2012-01-31 07:00:00,0\n2012-01-31 08:00:00,0\n2012-01-31 09:00:00,0\n2012-01-31 10:00:00,0\n2012-01-31 11:00:00,0\n2012-01-31 12:00:00,0\n2012-01-31 13:00:00,0\n2012-01-31 14:00:00,0\n2012-01-31 15:00:00,0\n2012-01-31 16:00:00,0\n2012-01-31 17:00:00,0\n2012-01-31 18:00:00,0\n2012-01-31 19:00:00,0\n2012-01-31 20:00:00,0\n2012-01-31 21:00:00,0\n2012-01-31 22:00:00,0\n2012-01-31 23:00:00,0\n2012-02-20 00:00:00,0\n2012-02-20 01:00:00,0\n2012-02-20 02:00:00,0\n2012-02-20 03:00:00,0\n2012-02-20 05:00:00,0\n2012-02-20 06:00:00,0\n2012-02-20 07:00:00,0\n2012-02-20 08:00:00,0\n2012-02-20 09:00:00,0\n2012-02-20 10:00:00,0\n2012-02-20 11:00:00,0\n2012-02-20 12:00:00,0\n2012-02-20 13:00:00,0\n2012-02-20 14:00:00,0\n2012-02-20 15:00:00,0\n2012-02-20 16:00:00,0\n2012-02-20 17:00:00,0\n2012-02-20 18:00:00,0\n2012-02-20 19:00:00,0\n2012-02-20 20:00:00,0\n2012-02-20 21:00:00,0\n2012-02-20 22:00:00,0\n2012-02-20 23:00:00,0\n2012-02-21 00:00:00,0\n2012-02-21 01:00:00,0\n2012-02-21 02:00:00,0\n2012-02-21 04:00:00,0\n2012-02-21 05:00:00,0\n2012-02-21 06:00:00,0\n2012-02-21 07:00:00,0\n2012-02-21 08:00:00,0\n2012-02-21 09:00:00,0\n2012-02-21 10:00:00,0\n2012-02-21 11:00:00,0\n2012-02-21 12:00:00,0\n2012-02-21 13:00:00,0\n2012-02-21 14:00:00,0\n2012-02-21 15:00:00,0\n2012-02-21 16:00:00,0\n2012-02-21 17:00:00,0\n2012-02-21 18:00:00,0\n2012-02-21 19:00:00,0\n2012-02-21 20:00:00,0\n2012-02-21 21:00:00,0\n2012-02-21 22:00:00,0\n2012-02-21 23:00:00,0\n2012-02-22 00:00:00,0\n2012-02-22 01:00:00,0\n2012-02-22 02:00:00,0\n2012-02-22 03:00:00,0\n2012-02-22 04:00:00,0\n2012-02-22 05:00:00,0\n2012-02-22 06:00:00,0\n2012-02-22 07:00:00,0\n2012-02-22 08:00:00,0\n2012-02-22 09:00:00,0\n2012-02-22 10:00:00,0\n2012-02-22 11:00:00,0\n2012-02-22 12:00:00,0\n2012-02-22 13:00:00,0\n2012-02-22 14:00:00,0\n2012-02-22 15:00:00,0\n2012-02-22 16:00:00,0\n2012-02-22 17:00:00,0\n2012-02-22 18:00:00,0\n2012-02-22 19:00:00,0\n2012-02-22 20:00:00,0\n2012-02-22 21:00:00,0\n2012-02-22 22:00:00,0\n2012-02-22 23:00:00,0\n2012-02-23 00:00:00,0\n2012-02-23 01:00:00,0\n2012-02-23 02:00:00,0\n2012-02-23 03:00:00,0\n2012-02-23 04:00:00,0\n2012-02-23 05:00:00,0\n2012-02-23 06:00:00,0\n2012-02-23 07:00:00,0\n2012-02-23 08:00:00,0\n2012-02-23 09:00:00,0\n2012-02-23 10:00:00,0\n2012-02-23 11:00:00,0\n2012-02-23 12:00:00,0\n2012-02-23 13:00:00,0\n2012-02-23 14:00:00,0\n2012-02-23 15:00:00,0\n2012-02-23 16:00:00,0\n2012-02-23 17:00:00,0\n2012-02-23 18:00:00,0\n2012-02-23 19:00:00,0\n2012-02-23 20:00:00,0\n2012-02-23 21:00:00,0\n2012-02-23 22:00:00,0\n2012-02-23 23:00:00,0\n2012-02-24 00:00:00,0\n2012-02-24 01:00:00,0\n2012-02-24 02:00:00,0\n2012-02-24 03:00:00,0\n2012-02-24 04:00:00,0\n2012-02-24 05:00:00,0\n2012-02-24 06:00:00,0\n2012-02-24 07:00:00,0\n2012-02-24 08:00:00,0\n2012-02-24 09:00:00,0\n2012-02-24 10:00:00,0\n2012-02-24 11:00:00,0\n2012-02-24 12:00:00,0\n2012-02-24 13:00:00,0\n2012-02-24 14:00:00,0\n2012-02-24 15:00:00,0\n2012-02-24 16:00:00,0\n2012-02-24 17:00:00,0\n2012-02-24 18:00:00,0\n2012-02-24 19:00:00,0\n2012-02-24 20:00:00,0\n2012-02-24 21:00:00,0\n2012-02-24 22:00:00,0\n2012-02-24 23:00:00,0\n2012-02-25 00:00:00,0\n2012-02-25 01:00:00,0\n2012-02-25 02:00:00,0\n2012-02-25 03:00:00,0\n2012-02-25 04:00:00,0\n2012-02-25 05:00:00,0\n2012-02-25 06:00:00,0\n2012-02-25 07:00:00,0\n2012-02-25 08:00:00,0\n2012-02-25 09:00:00,0\n2012-02-25 10:00:00,0\n2012-02-25 11:00:00,0\n2012-02-25 12:00:00,0\n2012-02-25 13:00:00,0\n2012-02-25 14:00:00,0\n2012-02-25 15:00:00,0\n2012-02-25 16:00:00,0\n2012-02-25 17:00:00,0\n2012-02-25 18:00:00,0\n2012-02-25 19:00:00,0\n2012-02-25 20:00:00,0\n2012-02-25 21:00:00,0\n2012-02-25 22:00:00,0\n2012-02-25 23:00:00,0\n2012-02-26 00:00:00,0\n2012-02-26 01:00:00,0\n2012-02-26 02:00:00,0\n2012-02-26 03:00:00,0\n2012-02-26 04:00:00,0\n2012-02-26 05:00:00,0\n2012-02-26 06:00:00,0\n2012-02-26 07:00:00,0\n2012-02-26 08:00:00,0\n2012-02-26 09:00:00,0\n2012-02-26 10:00:00,0\n2012-02-26 11:00:00,0\n2012-02-26 12:00:00,0\n2012-02-26 13:00:00,0\n2012-02-26 14:00:00,0\n2012-02-26 15:00:00,0\n2012-02-26 16:00:00,0\n2012-02-26 17:00:00,0\n2012-02-26 18:00:00,0\n2012-02-26 19:00:00,0\n2012-02-26 20:00:00,0\n2012-02-26 21:00:00,0\n2012-02-26 22:00:00,0\n2012-02-26 23:00:00,0\n2012-02-27 00:00:00,0\n2012-02-27 01:00:00,0\n2012-02-27 02:00:00,0\n2012-02-27 03:00:00,0\n2012-02-27 04:00:00,0\n2012-02-27 05:00:00,0\n2012-02-27 06:00:00,0\n2012-02-27 07:00:00,0\n2012-02-27 08:00:00,0\n2012-02-27 09:00:00,0\n2012-02-27 10:00:00,0\n2012-02-27 11:00:00,0\n2012-02-27 12:00:00,0\n2012-02-27 13:00:00,0\n2012-02-27 14:00:00,0\n2012-02-27 15:00:00,0\n2012-02-27 16:00:00,0\n2012-02-27 17:00:00,0\n2012-02-27 18:00:00,0\n2012-02-27 19:00:00,0\n2012-02-27 20:00:00,0\n2012-02-27 21:00:00,0\n2012-02-27 22:00:00,0\n2012-02-27 23:00:00,0\n2012-02-28 00:00:00,0\n2012-02-28 01:00:00,0\n2012-02-28 02:00:00,0\n2012-02-28 03:00:00,0\n2012-02-28 04:00:00,0\n2012-02-28 05:00:00,0\n2012-02-28 06:00:00,0\n2012-02-28 07:00:00,0\n2012-02-28 08:00:00,0\n2012-02-28 09:00:00,0\n2012-02-28 10:00:00,0\n2012-02-28 11:00:00,0\n2012-02-28 12:00:00,0\n2012-02-28 13:00:00,0\n2012-02-28 14:00:00,0\n2012-02-28 15:00:00,0\n2012-02-28 16:00:00,0\n2012-02-28 17:00:00,0\n2012-02-28 18:00:00,0\n2012-02-28 19:00:00,0\n2012-02-28 20:00:00,0\n2012-02-28 21:00:00,0\n2012-02-28 22:00:00,0\n2012-02-28 23:00:00,0\n2012-02-29 00:00:00,0\n2012-02-29 01:00:00,0\n2012-02-29 02:00:00,0\n2012-02-29 03:00:00,0\n2012-02-29 05:00:00,0\n2012-02-29 06:00:00,0\n2012-02-29 07:00:00,0\n2012-02-29 08:00:00,0\n2012-02-29 09:00:00,0\n2012-02-29 10:00:00,0\n2012-02-29 11:00:00,0\n2012-02-29 12:00:00,0\n2012-02-29 13:00:00,0\n2012-02-29 14:00:00,0\n2012-02-29 15:00:00,0\n2012-02-29 16:00:00,0\n2012-02-29 17:00:00,0\n2012-02-29 18:00:00,0\n2012-02-29 19:00:00,0\n2012-02-29 20:00:00,0\n2012-02-29 21:00:00,0\n2012-02-29 22:00:00,0\n2012-02-29 23:00:00,0\n2012-03-20 00:00:00,0\n2012-03-20 01:00:00,0\n2012-03-20 02:00:00,0\n2012-03-20 03:00:00,0\n2012-03-20 04:00:00,0\n2012-03-20 05:00:00,0\n2012-03-20 06:00:00,0\n2012-03-20 07:00:00,0\n2012-03-20 08:00:00,0\n2012-03-20 09:00:00,0\n2012-03-20 10:00:00,0\n2012-03-20 11:00:00,0\n2012-03-20 12:00:00,0\n2012-03-20 13:00:00,0\n2012-03-20 14:00:00,0\n2012-03-20 15:00:00,0\n2012-03-20 16:00:00,0\n2012-03-20 17:00:00,0\n2012-03-20 18:00:00,0\n2012-03-20 19:00:00,0\n2012-03-20 20:00:00,0\n2012-03-20 21:00:00,0\n2012-03-20 22:00:00,0\n2012-03-20 23:00:00,0\n2012-03-21 00:00:00,0\n2012-03-21 01:00:00,0\n2012-03-21 02:00:00,0\n2012-03-21 03:00:00,0\n2012-03-21 04:00:00,0\n2012-03-21 05:00:00,0\n2012-03-21 06:00:00,0\n2012-03-21 07:00:00,0\n2012-03-21 08:00:00,0\n2012-03-21 09:00:00,0\n2012-03-21 10:00:00,0\n2012-03-21 11:00:00,0\n2012-03-21 12:00:00,0\n2012-03-21 13:00:00,0\n2012-03-21 14:00:00,0\n2012-03-21 15:00:00,0\n2012-03-21 16:00:00,0\n2012-03-21 17:00:00,0\n2012-03-21 18:00:00,0\n2012-03-21 19:00:00,0\n2012-03-21 20:00:00,0\n2012-03-21 21:00:00,0\n2012-03-21 22:00:00,0\n2012-03-21 23:00:00,0\n2012-03-22 00:00:00,0\n2012-03-22 01:00:00,0\n2012-03-22 02:00:00,0\n2012-03-22 03:00:00,0\n2012-03-22 04:00:00,0\n2012-03-22 05:00:00,0\n2012-03-22 06:00:00,0\n2012-03-22 07:00:00,0\n2012-03-22 08:00:00,0\n2012-03-22 09:00:00,0\n2012-03-22 10:00:00,0\n2012-03-22 11:00:00,0\n2012-03-22 12:00:00,0\n2012-03-22 13:00:00,0\n2012-03-22 14:00:00,0\n2012-03-22 15:00:00,0\n2012-03-22 16:00:00,0\n2012-03-22 17:00:00,0\n2012-03-22 18:00:00,0\n2012-03-22 19:00:00,0\n2012-03-22 20:00:00,0\n2012-03-22 21:00:00,0\n2012-03-22 22:00:00,0\n2012-03-22 23:00:00,0\n2012-03-23 00:00:00,0\n2012-03-23 01:00:00,0\n2012-03-23 02:00:00,0\n2012-03-23 03:00:00,0\n2012-03-23 04:00:00,0\n2012-03-23 05:00:00,0\n2012-03-23 06:00:00,0\n2012-03-23 07:00:00,0\n2012-03-23 08:00:00,0\n2012-03-23 09:00:00,0\n2012-03-23 10:00:00,0\n2012-03-23 11:00:00,0\n2012-03-23 12:00:00,0\n2012-03-23 13:00:00,0\n2012-03-23 14:00:00,0\n2012-03-23 15:00:00,0\n2012-03-23 16:00:00,0\n2012-03-23 17:00:00,0\n2012-03-23 18:00:00,0\n2012-03-23 19:00:00,0\n2012-03-23 20:00:00,0\n2012-03-23 21:00:00,0\n2012-03-23 22:00:00,0\n2012-03-23 23:00:00,0\n2012-03-24 00:00:00,0\n2012-03-24 01:00:00,0\n2012-03-24 02:00:00,0\n2012-03-24 03:00:00,0\n2012-03-24 04:00:00,0\n2012-03-24 05:00:00,0\n2012-03-24 06:00:00,0\n2012-03-24 07:00:00,0\n2012-03-24 08:00:00,0\n2012-03-24 09:00:00,0\n2012-03-24 10:00:00,0\n2012-03-24 11:00:00,0\n2012-03-24 12:00:00,0\n2012-03-24 13:00:00,0\n2012-03-24 14:00:00,0\n2012-03-24 15:00:00,0\n2012-03-24 16:00:00,0\n2012-03-24 17:00:00,0\n2012-03-24 18:00:00,0\n2012-03-24 19:00:00,0\n2012-03-24 20:00:00,0\n2012-03-24 21:00:00,0\n2012-03-24 22:00:00,0\n2012-03-24 23:00:00,0\n2012-03-25 00:00:00,0\n2012-03-25 01:00:00,0\n2012-03-25 02:00:00,0\n2012-03-25 03:00:00,0\n2012-03-25 04:00:00,0\n2012-03-25 05:00:00,0\n2012-03-25 06:00:00,0\n2012-03-25 07:00:00,0\n2012-03-25 08:00:00,0\n2012-03-25 09:00:00,0\n2012-03-25 10:00:00,0\n2012-03-25 11:00:00,0\n2012-03-25 12:00:00,0\n2012-03-25 13:00:00,0\n2012-03-25 14:00:00,0\n2012-03-25 15:00:00,0\n2012-03-25 16:00:00,0\n2012-03-25 17:00:00,0\n2012-03-25 18:00:00,0\n2012-03-25 19:00:00,0\n2012-03-25 20:00:00,0\n2012-03-25 21:00:00,0\n2012-03-25 22:00:00,0\n2012-03-25 23:00:00,0\n2012-03-26 00:00:00,0\n2012-03-26 01:00:00,0\n2012-03-26 02:00:00,0\n2012-03-26 03:00:00,0\n2012-03-26 04:00:00,0\n2012-03-26 05:00:00,0\n2012-03-26 06:00:00,0\n2012-03-26 07:00:00,0\n2012-03-26 08:00:00,0\n2012-03-26 09:00:00,0\n2012-03-26 10:00:00,0\n2012-03-26 11:00:00,0\n2012-03-26 12:00:00,0\n2012-03-26 13:00:00,0\n2012-03-26 14:00:00,0\n2012-03-26 15:00:00,0\n2012-03-26 16:00:00,0\n2012-03-26 17:00:00,0\n2012-03-26 18:00:00,0\n2012-03-26 19:00:00,0\n2012-03-26 20:00:00,0\n2012-03-26 21:00:00,0\n2012-03-26 22:00:00,0\n2012-03-26 23:00:00,0\n2012-03-27 00:00:00,0\n2012-03-27 01:00:00,0\n2012-03-27 02:00:00,0\n2012-03-27 03:00:00,0\n2012-03-27 04:00:00,0\n2012-03-27 05:00:00,0\n2012-03-27 06:00:00,0\n2012-03-27 07:00:00,0\n2012-03-27 08:00:00,0\n2012-03-27 09:00:00,0\n2012-03-27 10:00:00,0\n2012-03-27 11:00:00,0\n2012-03-27 12:00:00,0\n2012-03-27 13:00:00,0\n2012-03-27 14:00:00,0\n2012-03-27 15:00:00,0\n2012-03-27 16:00:00,0\n2012-03-27 17:00:00,0\n2012-03-27 18:00:00,0\n2012-03-27 19:00:00,0\n2012-03-27 20:00:00,0\n2012-03-27 21:00:00,0\n2012-03-27 22:00:00,0\n2012-03-27 23:00:00,0\n2012-03-28 00:00:00,0\n2012-03-28 01:00:00,0\n2012-03-28 02:00:00,0\n2012-03-28 03:00:00,0\n2012-03-28 04:00:00,0\n2012-03-28 05:00:00,0\n2012-03-28 06:00:00,0\n2012-03-28 07:00:00,0\n2012-03-28 08:00:00,0\n2012-03-28 09:00:00,0\n2012-03-28 10:00:00,0\n2012-03-28 11:00:00,0\n2012-03-28 12:00:00,0\n2012-03-28 13:00:00,0\n2012-03-28 14:00:00,0\n2012-03-28 15:00:00,0\n2012-03-28 16:00:00,0\n2012-03-28 17:00:00,0\n2012-03-28 18:00:00,0\n2012-03-28 19:00:00,0\n2012-03-28 20:00:00,0\n2012-03-28 21:00:00,0\n2012-03-28 22:00:00,0\n2012-03-28 23:00:00,0\n2012-03-29 00:00:00,0\n2012-03-29 01:00:00,0\n2012-03-29 02:00:00,0\n2012-03-29 03:00:00,0\n2012-03-29 04:00:00,0\n2012-03-29 05:00:00,0\n2012-03-29 06:00:00,0\n2012-03-29 07:00:00,0\n2012-03-29 08:00:00,0\n2012-03-29 09:00:00,0\n2012-03-29 10:00:00,0\n2012-03-29 11:00:00,0\n2012-03-29 12:00:00,0\n2012-03-29 13:00:00,0\n2012-03-29 14:00:00,0\n2012-03-29 15:00:00,0\n2012-03-29 16:00:00,0\n2012-03-29 17:00:00,0\n2012-03-29 18:00:00,0\n2012-03-29 19:00:00,0\n2012-03-29 20:00:00,0\n2012-03-29 21:00:00,0\n2012-03-29 22:00:00,0\n2012-03-29 23:00:00,0\n2012-03-30 00:00:00,0\n2012-03-30 01:00:00,0\n2012-03-30 02:00:00,0\n2012-03-30 03:00:00,0\n2012-03-30 04:00:00,0\n2012-03-30 05:00:00,0\n2012-03-30 06:00:00,0\n2012-03-30 07:00:00,0\n2012-03-30 08:00:00,0\n2012-03-30 09:00:00,0\n2012-03-30 10:00:00,0\n2012-03-30 11:00:00,0\n2012-03-30 12:00:00,0\n2012-03-30 13:00:00,0\n2012-03-30 14:00:00,0\n2012-03-30 15:00:00,0\n2012-03-30 16:00:00,0\n2012-03-30 17:00:00,0\n2012-03-30 18:00:00,0\n2012-03-30 19:00:00,0\n2012-03-30 20:00:00,0\n2012-03-30 21:00:00,0\n2012-03-30 22:00:00,0\n2012-03-30 23:00:00,0\n2012-03-31 00:00:00,0\n2012-03-31 01:00:00,0\n2012-03-31 02:00:00,0\n2012-03-31 03:00:00,0\n2012-03-31 04:00:00,0\n2012-03-31 05:00:00,0\n2012-03-31 06:00:00,0\n2012-03-31 07:00:00,0\n2012-03-31 08:00:00,0\n2012-03-31 09:00:00,0\n2012-03-31 10:00:00,0\n2012-03-31 11:00:00,0\n2012-03-31 12:00:00,0\n2012-03-31 13:00:00,0\n2012-03-31 14:00:00,0\n2012-03-31 15:00:00,0\n2012-03-31 16:00:00,0\n2012-03-31 17:00:00,0\n2012-03-31 18:00:00,0\n2012-03-31 19:00:00,0\n2012-03-31 20:00:00,0\n2012-03-31 21:00:00,0\n2012-03-31 22:00:00,0\n2012-03-31 23:00:00,0\n2012-04-20 00:00:00,0\n2012-04-20 01:00:00,0\n2012-04-20 02:00:00,0\n2012-04-20 03:00:00,0\n2012-04-20 04:00:00,0\n2012-04-20 05:00:00,0\n2012-04-20 06:00:00,0\n2012-04-20 07:00:00,0\n2012-04-20 08:00:00,0\n2012-04-20 09:00:00,0\n2012-04-20 10:00:00,0\n2012-04-20 11:00:00,0\n2012-04-20 12:00:00,0\n2012-04-20 13:00:00,0\n2012-04-20 14:00:00,0\n2012-04-20 15:00:00,0\n2012-04-20 16:00:00,0\n2012-04-20 17:00:00,0\n2012-04-20 18:00:00,0\n2012-04-20 19:00:00,0\n2012-04-20 20:00:00,0\n2012-04-20 21:00:00,0\n2012-04-20 22:00:00,0\n2012-04-20 23:00:00,0\n2012-04-21 00:00:00,0\n2012-04-21 01:00:00,0\n2012-04-21 02:00:00,0\n2012-04-21 03:00:00,0\n2012-04-21 04:00:00,0\n2012-04-21 05:00:00,0\n2012-04-21 06:00:00,0\n2012-04-21 07:00:00,0\n2012-04-21 08:00:00,0\n2012-04-21 09:00:00,0\n2012-04-21 10:00:00,0\n2012-04-21 11:00:00,0\n2012-04-21 12:00:00,0\n2012-04-21 13:00:00,0\n2012-04-21 14:00:00,0\n2012-04-21 15:00:00,0\n2012-04-21 16:00:00,0\n2012-04-21 17:00:00,0\n2012-04-21 18:00:00,0\n2012-04-21 19:00:00,0\n2012-04-21 20:00:00,0\n2012-04-21 21:00:00,0\n2012-04-21 22:00:00,0\n2012-04-21 23:00:00,0\n2012-04-22 00:00:00,0\n2012-04-22 01:00:00,0\n2012-04-22 02:00:00,0\n2012-04-22 03:00:00,0\n2012-04-22 04:00:00,0\n2012-04-22 05:00:00,0\n2012-04-22 06:00:00,0\n2012-04-22 07:00:00,0\n2012-04-22 08:00:00,0\n2012-04-22 09:00:00,0\n2012-04-22 10:00:00,0\n2012-04-22 11:00:00,0\n2012-04-22 12:00:00,0\n2012-04-22 13:00:00,0\n2012-04-22 14:00:00,0\n2012-04-22 15:00:00,0\n2012-04-22 16:00:00,0\n2012-04-22 17:00:00,0\n2012-04-22 18:00:00,0\n2012-04-22 19:00:00,0\n2012-04-22 20:00:00,0\n2012-04-22 21:00:00,0\n2012-04-22 22:00:00,0\n2012-04-22 23:00:00,0\n2012-04-23 00:00:00,0\n2012-04-23 01:00:00,0\n2012-04-23 02:00:00,0\n2012-04-23 03:00:00,0\n2012-04-23 04:00:00,0\n2012-04-23 05:00:00,0\n2012-04-23 06:00:00,0\n2012-04-23 07:00:00,0\n2012-04-23 08:00:00,0\n2012-04-23 09:00:00,0\n2012-04-23 10:00:00,0\n2012-04-23 11:00:00,0\n2012-04-23 12:00:00,0\n2012-04-23 13:00:00,0\n2012-04-23 14:00:00,0\n2012-04-23 15:00:00,0\n2012-04-23 16:00:00,0\n2012-04-23 17:00:00,0\n2012-04-23 18:00:00,0\n2012-04-23 19:00:00,0\n2012-04-23 20:00:00,0\n2012-04-23 21:00:00,0\n2012-04-23 22:00:00,0\n2012-04-23 23:00:00,0\n2012-04-24 00:00:00,0\n2012-04-24 01:00:00,0\n2012-04-24 02:00:00,0\n2012-04-24 03:00:00,0\n2012-04-24 04:00:00,0\n2012-04-24 05:00:00,0\n2012-04-24 06:00:00,0\n2012-04-24 07:00:00,0\n2012-04-24 08:00:00,0\n2012-04-24 09:00:00,0\n2012-04-24 10:00:00,0\n2012-04-24 11:00:00,0\n2012-04-24 12:00:00,0\n2012-04-24 13:00:00,0\n2012-04-24 14:00:00,0\n2012-04-24 15:00:00,0\n2012-04-24 16:00:00,0\n2012-04-24 17:00:00,0\n2012-04-24 18:00:00,0\n2012-04-24 19:00:00,0\n2012-04-24 20:00:00,0\n2012-04-24 21:00:00,0\n2012-04-24 22:00:00,0\n2012-04-24 23:00:00,0\n2012-04-25 00:00:00,0\n2012-04-25 01:00:00,0\n2012-04-25 02:00:00,0\n2012-04-25 03:00:00,0\n2012-04-25 04:00:00,0\n2012-04-25 05:00:00,0\n2012-04-25 06:00:00,0\n2012-04-25 07:00:00,0\n2012-04-25 08:00:00,0\n2012-04-25 09:00:00,0\n2012-04-25 10:00:00,0\n2012-04-25 11:00:00,0\n2012-04-25 12:00:00,0\n2012-04-25 13:00:00,0\n2012-04-25 14:00:00,0\n2012-04-25 15:00:00,0\n2012-04-25 16:00:00,0\n2012-04-25 17:00:00,0\n2012-04-25 18:00:00,0\n2012-04-25 19:00:00,0\n2012-04-25 20:00:00,0\n2012-04-25 21:00:00,0\n2012-04-25 22:00:00,0\n2012-04-25 23:00:00,0\n2012-04-26 00:00:00,0\n2012-04-26 01:00:00,0\n2012-04-26 02:00:00,0\n2012-04-26 03:00:00,0\n2012-04-26 04:00:00,0\n2012-04-26 05:00:00,0\n2012-04-26 06:00:00,0\n2012-04-26 07:00:00,0\n2012-04-26 08:00:00,0\n2012-04-26 09:00:00,0\n2012-04-26 10:00:00,0\n2012-04-26 11:00:00,0\n2012-04-26 12:00:00,0\n2012-04-26 13:00:00,0\n2012-04-26 14:00:00,0\n2012-04-26 15:00:00,0\n2012-04-26 16:00:00,0\n2012-04-26 17:00:00,0\n2012-04-26 18:00:00,0\n2012-04-26 19:00:00,0\n2012-04-26 20:00:00,0\n2012-04-26 21:00:00,0\n2012-04-26 22:00:00,0\n2012-04-26 23:00:00,0\n2012-04-27 00:00:00,0\n2012-04-27 01:00:00,0\n2012-04-27 02:00:00,0\n2012-04-27 03:00:00,0\n2012-04-27 04:00:00,0\n2012-04-27 05:00:00,0\n2012-04-27 06:00:00,0\n2012-04-27 07:00:00,0\n2012-04-27 08:00:00,0\n2012-04-27 09:00:00,0\n2012-04-27 10:00:00,0\n2012-04-27 11:00:00,0\n2012-04-27 12:00:00,0\n2012-04-27 13:00:00,0\n2012-04-27 14:00:00,0\n2012-04-27 15:00:00,0\n2012-04-27 16:00:00,0\n2012-04-27 17:00:00,0\n2012-04-27 18:00:00,0\n2012-04-27 19:00:00,0\n2012-04-27 20:00:00,0\n2012-04-27 21:00:00,0\n2012-04-27 22:00:00,0\n2012-04-27 23:00:00,0\n2012-04-28 00:00:00,0\n2012-04-28 01:00:00,0\n2012-04-28 02:00:00,0\n2012-04-28 03:00:00,0\n2012-04-28 04:00:00,0\n2012-04-28 05:00:00,0\n2012-04-28 06:00:00,0\n2012-04-28 07:00:00,0\n2012-04-28 08:00:00,0\n2012-04-28 09:00:00,0\n2012-04-28 10:00:00,0\n2012-04-28 11:00:00,0\n2012-04-28 12:00:00,0\n2012-04-28 13:00:00,0\n2012-04-28 14:00:00,0\n2012-04-28 15:00:00,0\n2012-04-28 16:00:00,0\n2012-04-28 17:00:00,0\n2012-04-28 18:00:00,0\n2012-04-28 19:00:00,0\n2012-04-28 20:00:00,0\n2012-04-28 21:00:00,0\n2012-04-28 22:00:00,0\n2012-04-28 23:00:00,0\n2012-04-29 00:00:00,0\n2012-04-29 01:00:00,0\n2012-04-29 02:00:00,0\n2012-04-29 03:00:00,0\n2012-04-29 04:00:00,0\n2012-04-29 05:00:00,0\n2012-04-29 06:00:00,0\n2012-04-29 07:00:00,0\n2012-04-29 08:00:00,0\n2012-04-29 09:00:00,0\n2012-04-29 10:00:00,0\n2012-04-29 11:00:00,0\n2012-04-29 12:00:00,0\n2012-04-29 13:00:00,0\n2012-04-29 14:00:00,0\n2012-04-29 15:00:00,0\n2012-04-29 16:00:00,0\n2012-04-29 17:00:00,0\n2012-04-29 18:00:00,0\n2012-04-29 19:00:00,0\n2012-04-29 20:00:00,0\n2012-04-29 21:00:00,0\n2012-04-29 22:00:00,0\n2012-04-29 23:00:00,0\n2012-04-30 00:00:00,0\n2012-04-30 01:00:00,0\n2012-04-30 02:00:00,0\n2012-04-30 03:00:00,0\n2012-04-30 04:00:00,0\n2012-04-30 05:00:00,0\n2012-04-30 06:00:00,0\n2012-04-30 07:00:00,0\n2012-04-30 08:00:00,0\n2012-04-30 09:00:00,0\n2012-04-30 10:00:00,0\n2012-04-30 11:00:00,0\n2012-04-30 12:00:00,0\n2012-04-30 13:00:00,0\n2012-04-30 14:00:00,0\n2012-04-30 15:00:00,0\n2012-04-30 16:00:00,0\n2012-04-30 17:00:00,0\n2012-04-30 18:00:00,0\n2012-04-30 19:00:00,0\n2012-04-30 20:00:00,0\n2012-04-30 21:00:00,0\n2012-04-30 22:00:00,0\n2012-04-30 23:00:00,0\n2012-05-20 00:00:00,0\n2012-05-20 01:00:00,0\n2012-05-20 02:00:00,0\n2012-05-20 03:00:00,0\n2012-05-20 04:00:00,0\n2012-05-20 05:00:00,0\n2012-05-20 06:00:00,0\n2012-05-20 07:00:00,0\n2012-05-20 08:00:00,0\n2012-05-20 09:00:00,0\n2012-05-20 10:00:00,0\n2012-05-20 11:00:00,0\n2012-05-20 12:00:00,0\n2012-05-20 13:00:00,0\n2012-05-20 14:00:00,0\n2012-05-20 15:00:00,0\n2012-05-20 16:00:00,0\n2012-05-20 17:00:00,0\n2012-05-20 18:00:00,0\n2012-05-20 19:00:00,0\n2012-05-20 20:00:00,0\n2012-05-20 21:00:00,0\n2012-05-20 22:00:00,0\n2012-05-20 23:00:00,0\n2012-05-21 00:00:00,0\n2012-05-21 01:00:00,0\n2012-05-21 02:00:00,0\n2012-05-21 03:00:00,0\n2012-05-21 04:00:00,0\n2012-05-21 05:00:00,0\n2012-05-21 06:00:00,0\n2012-05-21 07:00:00,0\n2012-05-21 08:00:00,0\n2012-05-21 09:00:00,0\n2012-05-21 10:00:00,0\n2012-05-21 11:00:00,0\n2012-05-21 12:00:00,0\n2012-05-21 13:00:00,0\n2012-05-21 14:00:00,0\n2012-05-21 15:00:00,0\n2012-05-21 16:00:00,0\n2012-05-21 17:00:00,0\n2012-05-21 18:00:00,0\n2012-05-21 19:00:00,0\n2012-05-21 20:00:00,0\n2012-05-21 21:00:00,0\n2012-05-21 22:00:00,0\n2012-05-21 23:00:00,0\n2012-05-22 00:00:00,0\n2012-05-22 01:00:00,0\n2012-05-22 02:00:00,0\n2012-05-22 03:00:00,0\n2012-05-22 04:00:00,0\n2012-05-22 05:00:00,0\n2012-05-22 06:00:00,0\n2012-05-22 07:00:00,0\n2012-05-22 08:00:00,0\n2012-05-22 09:00:00,0\n2012-05-22 10:00:00,0\n2012-05-22 11:00:00,0\n2012-05-22 12:00:00,0\n2012-05-22 13:00:00,0\n2012-05-22 14:00:00,0\n2012-05-22 15:00:00,0\n2012-05-22 16:00:00,0\n2012-05-22 17:00:00,0\n2012-05-22 18:00:00,0\n2012-05-22 19:00:00,0\n2012-05-22 20:00:00,0\n2012-05-22 21:00:00,0\n2012-05-22 22:00:00,0\n2012-05-22 23:00:00,0\n2012-05-23 00:00:00,0\n2012-05-23 01:00:00,0\n2012-05-23 02:00:00,0\n2012-05-23 03:00:00,0\n2012-05-23 04:00:00,0\n2012-05-23 05:00:00,0\n2012-05-23 06:00:00,0\n2012-05-23 07:00:00,0\n2012-05-23 08:00:00,0\n2012-05-23 09:00:00,0\n2012-05-23 10:00:00,0\n2012-05-23 11:00:00,0\n2012-05-23 12:00:00,0\n2012-05-23 13:00:00,0\n2012-05-23 14:00:00,0\n2012-05-23 15:00:00,0\n2012-05-23 16:00:00,0\n2012-05-23 17:00:00,0\n2012-05-23 18:00:00,0\n2012-05-23 19:00:00,0\n2012-05-23 20:00:00,0\n2012-05-23 21:00:00,0\n2012-05-23 22:00:00,0\n2012-05-23 23:00:00,0\n2012-05-24 00:00:00,0\n2012-05-24 01:00:00,0\n2012-05-24 02:00:00,0\n2012-05-24 03:00:00,0\n2012-05-24 04:00:00,0\n2012-05-24 05:00:00,0\n2012-05-24 06:00:00,0\n2012-05-24 07:00:00,0\n2012-05-24 08:00:00,0\n2012-05-24 09:00:00,0\n2012-05-24 10:00:00,0\n2012-05-24 11:00:00,0\n2012-05-24 12:00:00,0\n2012-05-24 13:00:00,0\n2012-05-24 14:00:00,0\n2012-05-24 15:00:00,0\n2012-05-24 16:00:00,0\n2012-05-24 17:00:00,0\n2012-05-24 18:00:00,0\n2012-05-24 19:00:00,0\n2012-05-24 20:00:00,0\n2012-05-24 21:00:00,0\n2012-05-24 22:00:00,0\n2012-05-24 23:00:00,0\n2012-05-25 00:00:00,0\n2012-05-25 01:00:00,0\n2012-05-25 02:00:00,0\n2012-05-25 03:00:00,0\n2012-05-25 04:00:00,0\n2012-05-25 05:00:00,0\n2012-05-25 06:00:00,0\n2012-05-25 07:00:00,0\n2012-05-25 08:00:00,0\n2012-05-25 09:00:00,0\n2012-05-25 10:00:00,0\n2012-05-25 11:00:00,0\n2012-05-25 12:00:00,0\n2012-05-25 13:00:00,0\n2012-05-25 14:00:00,0\n2012-05-25 15:00:00,0\n2012-05-25 16:00:00,0\n2012-05-25 17:00:00,0\n2012-05-25 18:00:00,0\n2012-05-25 19:00:00,0\n2012-05-25 20:00:00,0\n2012-05-25 21:00:00,0\n2012-05-25 22:00:00,0\n2012-05-25 23:00:00,0\n2012-05-26 00:00:00,0\n2012-05-26 01:00:00,0\n2012-05-26 02:00:00,0\n2012-05-26 03:00:00,0\n2012-05-26 04:00:00,0\n2012-05-26 05:00:00,0\n2012-05-26 06:00:00,0\n2012-05-26 07:00:00,0\n2012-05-26 08:00:00,0\n2012-05-26 09:00:00,0\n2012-05-26 10:00:00,0\n2012-05-26 11:00:00,0\n2012-05-26 12:00:00,0\n2012-05-26 13:00:00,0\n2012-05-26 14:00:00,0\n2012-05-26 15:00:00,0\n2012-05-26 16:00:00,0\n2012-05-26 17:00:00,0\n2012-05-26 18:00:00,0\n2012-05-26 19:00:00,0\n2012-05-26 20:00:00,0\n2012-05-26 21:00:00,0\n2012-05-26 22:00:00,0\n2012-05-26 23:00:00,0\n2012-05-27 00:00:00,0\n2012-05-27 01:00:00,0\n2012-05-27 02:00:00,0\n2012-05-27 03:00:00,0\n2012-05-27 04:00:00,0\n2012-05-27 05:00:00,0\n2012-05-27 06:00:00,0\n2012-05-27 07:00:00,0\n2012-05-27 08:00:00,0\n2012-05-27 09:00:00,0\n2012-05-27 10:00:00,0\n2012-05-27 11:00:00,0\n2012-05-27 12:00:00,0\n2012-05-27 13:00:00,0\n2012-05-27 14:00:00,0\n2012-05-27 15:00:00,0\n2012-05-27 16:00:00,0\n2012-05-27 17:00:00,0\n2012-05-27 18:00:00,0\n2012-05-27 19:00:00,0\n2012-05-27 20:00:00,0\n2012-05-27 21:00:00,0\n2012-05-27 22:00:00,0\n2012-05-27 23:00:00,0\n2012-05-28 00:00:00,0\n2012-05-28 01:00:00,0\n2012-05-28 02:00:00,0\n2012-05-28 03:00:00,0\n2012-05-28 04:00:00,0\n2012-05-28 05:00:00,0\n2012-05-28 06:00:00,0\n2012-05-28 07:00:00,0\n2012-05-28 08:00:00,0\n2012-05-28 09:00:00,0\n2012-05-28 10:00:00,0\n2012-05-28 11:00:00,0\n2012-05-28 12:00:00,0\n2012-05-28 13:00:00,0\n2012-05-28 14:00:00,0\n2012-05-28 15:00:00,0\n2012-05-28 16:00:00,0\n2012-05-28 17:00:00,0\n2012-05-28 18:00:00,0\n2012-05-28 19:00:00,0\n2012-05-28 20:00:00,0\n2012-05-28 21:00:00,0\n2012-05-28 22:00:00,0\n2012-05-28 23:00:00,0\n2012-05-29 00:00:00,0\n2012-05-29 01:00:00,0\n2012-05-29 02:00:00,0\n2012-05-29 03:00:00,0\n2012-05-29 04:00:00,0\n2012-05-29 05:00:00,0\n2012-05-29 06:00:00,0\n2012-05-29 07:00:00,0\n2012-05-29 08:00:00,0\n2012-05-29 09:00:00,0\n2012-05-29 10:00:00,0\n2012-05-29 11:00:00,0\n2012-05-29 12:00:00,0\n2012-05-29 13:00:00,0\n2012-05-29 14:00:00,0\n2012-05-29 15:00:00,0\n2012-05-29 16:00:00,0\n2012-05-29 17:00:00,0\n2012-05-29 18:00:00,0\n2012-05-29 19:00:00,0\n2012-05-29 20:00:00,0\n2012-05-29 21:00:00,0\n2012-05-29 22:00:00,0\n2012-05-29 23:00:00,0\n2012-05-30 00:00:00,0\n2012-05-30 01:00:00,0\n2012-05-30 02:00:00,0\n2012-05-30 03:00:00,0\n2012-05-30 04:00:00,0\n2012-05-30 05:00:00,0\n2012-05-30 06:00:00,0\n2012-05-30 07:00:00,0\n2012-05-30 08:00:00,0\n2012-05-30 09:00:00,0\n2012-05-30 10:00:00,0\n2012-05-30 11:00:00,0\n2012-05-30 12:00:00,0\n2012-05-30 13:00:00,0\n2012-05-30 14:00:00,0\n2012-05-30 15:00:00,0\n2012-05-30 16:00:00,0\n2012-05-30 17:00:00,0\n2012-05-30 18:00:00,0\n2012-05-30 19:00:00,0\n2012-05-30 20:00:00,0\n2012-05-30 21:00:00,0\n2012-05-30 22:00:00,0\n2012-05-30 23:00:00,0\n2012-05-31 00:00:00,0\n2012-05-31 01:00:00,0\n2012-05-31 02:00:00,0\n2012-05-31 03:00:00,0\n2012-05-31 04:00:00,0\n2012-05-31 05:00:00,0\n2012-05-31 06:00:00,0\n2012-05-31 07:00:00,0\n2012-05-31 08:00:00,0\n2012-05-31 09:00:00,0\n2012-05-31 10:00:00,0\n2012-05-31 11:00:00,0\n2012-05-31 12:00:00,0\n2012-05-31 13:00:00,0\n2012-05-31 14:00:00,0\n2012-05-31 15:00:00,0\n2012-05-31 16:00:00,0\n2012-05-31 17:00:00,0\n2012-05-31 18:00:00,0\n2012-05-31 19:00:00,0\n2012-05-31 20:00:00,0\n2012-05-31 21:00:00,0\n2012-05-31 22:00:00,0\n2012-05-31 23:00:00,0\n2012-06-20 00:00:00,0\n2012-06-20 01:00:00,0\n2012-06-20 02:00:00,0\n2012-06-20 03:00:00,0\n2012-06-20 04:00:00,0\n2012-06-20 05:00:00,0\n2012-06-20 06:00:00,0\n2012-06-20 07:00:00,0\n2012-06-20 08:00:00,0\n2012-06-20 09:00:00,0\n2012-06-20 10:00:00,0\n2012-06-20 11:00:00,0\n2012-06-20 12:00:00,0\n2012-06-20 13:00:00,0\n2012-06-20 14:00:00,0\n2012-06-20 15:00:00,0\n2012-06-20 16:00:00,0\n2012-06-20 17:00:00,0\n2012-06-20 18:00:00,0\n2012-06-20 19:00:00,0\n2012-06-20 20:00:00,0\n2012-06-20 21:00:00,0\n2012-06-20 22:00:00,0\n2012-06-20 23:00:00,0\n2012-06-21 00:00:00,0\n2012-06-21 01:00:00,0\n2012-06-21 02:00:00,0\n2012-06-21 03:00:00,0\n2012-06-21 04:00:00,0\n2012-06-21 05:00:00,0\n2012-06-21 06:00:00,0\n2012-06-21 07:00:00,0\n2012-06-21 08:00:00,0\n2012-06-21 09:00:00,0\n2012-06-21 10:00:00,0\n2012-06-21 11:00:00,0\n2012-06-21 12:00:00,0\n2012-06-21 13:00:00,0\n2012-06-21 14:00:00,0\n2012-06-21 15:00:00,0\n2012-06-21 16:00:00,0\n2012-06-21 17:00:00,0\n2012-06-21 18:00:00,0\n2012-06-21 19:00:00,0\n2012-06-21 20:00:00,0\n2012-06-21 21:00:00,0\n2012-06-21 22:00:00,0\n2012-06-21 23:00:00,0\n2012-06-22 00:00:00,0\n2012-06-22 01:00:00,0\n2012-06-22 02:00:00,0\n2012-06-22 03:00:00,0\n2012-06-22 04:00:00,0\n2012-06-22 05:00:00,0\n2012-06-22 06:00:00,0\n2012-06-22 07:00:00,0\n2012-06-22 08:00:00,0\n2012-06-22 09:00:00,0\n2012-06-22 10:00:00,0\n2012-06-22 11:00:00,0\n2012-06-22 12:00:00,0\n2012-06-22 13:00:00,0\n2012-06-22 14:00:00,0\n2012-06-22 15:00:00,0\n2012-06-22 16:00:00,0\n2012-06-22 17:00:00,0\n2012-06-22 18:00:00,0\n2012-06-22 19:00:00,0\n2012-06-22 20:00:00,0\n2012-06-22 21:00:00,0\n2012-06-22 22:00:00,0\n2012-06-22 23:00:00,0\n2012-06-23 00:00:00,0\n2012-06-23 01:00:00,0\n2012-06-23 02:00:00,0\n2012-06-23 03:00:00,0\n2012-06-23 04:00:00,0\n2012-06-23 05:00:00,0\n2012-06-23 06:00:00,0\n2012-06-23 07:00:00,0\n2012-06-23 08:00:00,0\n2012-06-23 09:00:00,0\n2012-06-23 10:00:00,0\n2012-06-23 11:00:00,0\n2012-06-23 12:00:00,0\n2012-06-23 13:00:00,0\n2012-06-23 14:00:00,0\n2012-06-23 15:00:00,0\n2012-06-23 16:00:00,0\n2012-06-23 17:00:00,0\n2012-06-23 18:00:00,0\n2012-06-23 19:00:00,0\n2012-06-23 20:00:00,0\n2012-06-23 21:00:00,0\n2012-06-23 22:00:00,0\n2012-06-23 23:00:00,0\n2012-06-24 00:00:00,0\n2012-06-24 01:00:00,0\n2012-06-24 02:00:00,0\n2012-06-24 03:00:00,0\n2012-06-24 04:00:00,0\n2012-06-24 05:00:00,0\n2012-06-24 06:00:00,0\n2012-06-24 07:00:00,0\n2012-06-24 08:00:00,0\n2012-06-24 09:00:00,0\n2012-06-24 10:00:00,0\n2012-06-24 11:00:00,0\n2012-06-24 12:00:00,0\n2012-06-24 13:00:00,0\n2012-06-24 14:00:00,0\n2012-06-24 15:00:00,0\n2012-06-24 16:00:00,0\n2012-06-24 17:00:00,0\n2012-06-24 18:00:00,0\n2012-06-24 19:00:00,0\n2012-06-24 20:00:00,0\n2012-06-24 21:00:00,0\n2012-06-24 22:00:00,0\n2012-06-24 23:00:00,0\n2012-06-25 00:00:00,0\n2012-06-25 01:00:00,0\n2012-06-25 02:00:00,0\n2012-06-25 03:00:00,0\n2012-06-25 04:00:00,0\n2012-06-25 05:00:00,0\n2012-06-25 06:00:00,0\n2012-06-25 07:00:00,0\n2012-06-25 08:00:00,0\n2012-06-25 09:00:00,0\n2012-06-25 10:00:00,0\n2012-06-25 11:00:00,0\n2012-06-25 12:00:00,0\n2012-06-25 13:00:00,0\n2012-06-25 14:00:00,0\n2012-06-25 15:00:00,0\n2012-06-25 16:00:00,0\n2012-06-25 17:00:00,0\n2012-06-25 18:00:00,0\n2012-06-25 19:00:00,0\n2012-06-25 20:00:00,0\n2012-06-25 21:00:00,0\n2012-06-25 22:00:00,0\n2012-06-25 23:00:00,0\n2012-06-26 00:00:00,0\n2012-06-26 01:00:00,0\n2012-06-26 02:00:00,0\n2012-06-26 03:00:00,0\n2012-06-26 04:00:00,0\n2012-06-26 05:00:00,0\n2012-06-26 06:00:00,0\n2012-06-26 07:00:00,0\n2012-06-26 08:00:00,0\n2012-06-26 09:00:00,0\n2012-06-26 10:00:00,0\n2012-06-26 11:00:00,0\n2012-06-26 12:00:00,0\n2012-06-26 13:00:00,0\n2012-06-26 14:00:00,0\n2012-06-26 15:00:00,0\n2012-06-26 16:00:00,0\n2012-06-26 17:00:00,0\n2012-06-26 18:00:00,0\n2012-06-26 19:00:00,0\n2012-06-26 20:00:00,0\n2012-06-26 21:00:00,0\n2012-06-26 22:00:00,0\n2012-06-26 23:00:00,0\n2012-06-27 00:00:00,0\n2012-06-27 01:00:00,0\n2012-06-27 02:00:00,0\n2012-06-27 03:00:00,0\n2012-06-27 04:00:00,0\n2012-06-27 05:00:00,0\n2012-06-27 06:00:00,0\n2012-06-27 07:00:00,0\n2012-06-27 08:00:00,0\n2012-06-27 09:00:00,0\n2012-06-27 10:00:00,0\n2012-06-27 11:00:00,0\n2012-06-27 12:00:00,0\n2012-06-27 13:00:00,0\n2012-06-27 14:00:00,0\n2012-06-27 15:00:00,0\n2012-06-27 16:00:00,0\n2012-06-27 17:00:00,0\n2012-06-27 18:00:00,0\n2012-06-27 19:00:00,0\n2012-06-27 20:00:00,0\n2012-06-27 21:00:00,0\n2012-06-27 22:00:00,0\n2012-06-27 23:00:00,0\n2012-06-28 00:00:00,0\n2012-06-28 01:00:00,0\n2012-06-28 02:00:00,0\n2012-06-28 03:00:00,0\n2012-06-28 04:00:00,0\n2012-06-28 05:00:00,0\n2012-06-28 06:00:00,0\n2012-06-28 07:00:00,0\n2012-06-28 08:00:00,0\n2012-06-28 09:00:00,0\n2012-06-28 10:00:00,0\n2012-06-28 11:00:00,0\n2012-06-28 12:00:00,0\n2012-06-28 13:00:00,0\n2012-06-28 14:00:00,0\n2012-06-28 15:00:00,0\n2012-06-28 16:00:00,0\n2012-06-28 17:00:00,0\n2012-06-28 18:00:00,0\n2012-06-28 19:00:00,0\n2012-06-28 20:00:00,0\n2012-06-28 21:00:00,0\n2012-06-28 22:00:00,0\n2012-06-28 23:00:00,0\n2012-06-29 00:00:00,0\n2012-06-29 01:00:00,0\n2012-06-29 02:00:00,0\n2012-06-29 03:00:00,0\n2012-06-29 04:00:00,0\n2012-06-29 05:00:00,0\n2012-06-29 06:00:00,0\n2012-06-29 07:00:00,0\n2012-06-29 08:00:00,0\n2012-06-29 09:00:00,0\n2012-06-29 10:00:00,0\n2012-06-29 11:00:00,0\n2012-06-29 12:00:00,0\n2012-06-29 13:00:00,0\n2012-06-29 14:00:00,0\n2012-06-29 15:00:00,0\n2012-06-29 16:00:00,0\n2012-06-29 17:00:00,0\n2012-06-29 18:00:00,0\n2012-06-29 19:00:00,0\n2012-06-29 20:00:00,0\n2012-06-29 21:00:00,0\n2012-06-29 22:00:00,0\n2012-06-29 23:00:00,0\n2012-06-30 00:00:00,0\n2012-06-30 01:00:00,0\n2012-06-30 02:00:00,0\n2012-06-30 03:00:00,0\n2012-06-30 04:00:00,0\n2012-06-30 05:00:00,0\n2012-06-30 06:00:00,0\n2012-06-30 07:00:00,0\n2012-06-30 08:00:00,0\n2012-06-30 09:00:00,0\n2012-06-30 10:00:00,0\n2012-06-30 11:00:00,0\n2012-06-30 12:00:00,0\n2012-06-30 13:00:00,0\n2012-06-30 14:00:00,0\n2012-06-30 15:00:00,0\n2012-06-30 16:00:00,0\n2012-06-30 17:00:00,0\n2012-06-30 18:00:00,0\n2012-06-30 19:00:00,0\n2012-06-30 20:00:00,0\n2012-06-30 21:00:00,0\n2012-06-30 22:00:00,0\n2012-06-30 23:00:00,0\n2012-07-20 00:00:00,0\n2012-07-20 01:00:00,0\n2012-07-20 02:00:00,0\n2012-07-20 03:00:00,0\n2012-07-20 04:00:00,0\n2012-07-20 05:00:00,0\n2012-07-20 06:00:00,0\n2012-07-20 07:00:00,0\n2012-07-20 08:00:00,0\n2012-07-20 09:00:00,0\n2012-07-20 10:00:00,0\n2012-07-20 11:00:00,0\n2012-07-20 12:00:00,0\n2012-07-20 13:00:00,0\n2012-07-20 14:00:00,0\n2012-07-20 15:00:00,0\n2012-07-20 16:00:00,0\n2012-07-20 17:00:00,0\n2012-07-20 18:00:00,0\n2012-07-20 19:00:00,0\n2012-07-20 20:00:00,0\n2012-07-20 21:00:00,0\n2012-07-20 22:00:00,0\n2012-07-20 23:00:00,0\n2012-07-21 00:00:00,0\n2012-07-21 01:00:00,0\n2012-07-21 02:00:00,0\n2012-07-21 03:00:00,0\n2012-07-21 04:00:00,0\n2012-07-21 05:00:00,0\n2012-07-21 06:00:00,0\n2012-07-21 07:00:00,0\n2012-07-21 08:00:00,0\n2012-07-21 09:00:00,0\n2012-07-21 10:00:00,0\n2012-07-21 11:00:00,0\n2012-07-21 12:00:00,0\n2012-07-21 13:00:00,0\n2012-07-21 14:00:00,0\n2012-07-21 15:00:00,0\n2012-07-21 16:00:00,0\n2012-07-21 17:00:00,0\n2012-07-21 18:00:00,0\n2012-07-21 19:00:00,0\n2012-07-21 20:00:00,0\n2012-07-21 21:00:00,0\n2012-07-21 22:00:00,0\n2012-07-21 23:00:00,0\n2012-07-22 00:00:00,0\n2012-07-22 01:00:00,0\n2012-07-22 02:00:00,0\n2012-07-22 03:00:00,0\n2012-07-22 04:00:00,0\n2012-07-22 05:00:00,0\n2012-07-22 06:00:00,0\n2012-07-22 07:00:00,0\n2012-07-22 08:00:00,0\n2012-07-22 09:00:00,0\n2012-07-22 10:00:00,0\n2012-07-22 11:00:00,0\n2012-07-22 12:00:00,0\n2012-07-22 13:00:00,0\n2012-07-22 14:00:00,0\n2012-07-22 15:00:00,0\n2012-07-22 16:00:00,0\n2012-07-22 17:00:00,0\n2012-07-22 18:00:00,0\n2012-07-22 19:00:00,0\n2012-07-22 20:00:00,0\n2012-07-22 21:00:00,0\n2012-07-22 22:00:00,0\n2012-07-22 23:00:00,0\n2012-07-23 00:00:00,0\n2012-07-23 01:00:00,0\n2012-07-23 02:00:00,0\n2012-07-23 03:00:00,0\n2012-07-23 04:00:00,0\n2012-07-23 05:00:00,0\n2012-07-23 06:00:00,0\n2012-07-23 07:00:00,0\n2012-07-23 08:00:00,0\n2012-07-23 09:00:00,0\n2012-07-23 10:00:00,0\n2012-07-23 11:00:00,0\n2012-07-23 12:00:00,0\n2012-07-23 13:00:00,0\n2012-07-23 14:00:00,0\n2012-07-23 15:00:00,0\n2012-07-23 16:00:00,0\n2012-07-23 17:00:00,0\n2012-07-23 18:00:00,0\n2012-07-23 19:00:00,0\n2012-07-23 20:00:00,0\n2012-07-23 21:00:00,0\n2012-07-23 22:00:00,0\n2012-07-23 23:00:00,0\n2012-07-24 00:00:00,0\n2012-07-24 01:00:00,0\n2012-07-24 02:00:00,0\n2012-07-24 03:00:00,0\n2012-07-24 04:00:00,0\n2012-07-24 05:00:00,0\n2012-07-24 06:00:00,0\n2012-07-24 07:00:00,0\n2012-07-24 08:00:00,0\n2012-07-24 09:00:00,0\n2012-07-24 10:00:00,0\n2012-07-24 11:00:00,0\n2012-07-24 12:00:00,0\n2012-07-24 13:00:00,0\n2012-07-24 14:00:00,0\n2012-07-24 15:00:00,0\n2012-07-24 16:00:00,0\n2012-07-24 17:00:00,0\n2012-07-24 18:00:00,0\n2012-07-24 19:00:00,0\n2012-07-24 20:00:00,0\n2012-07-24 21:00:00,0\n2012-07-24 22:00:00,0\n2012-07-24 23:00:00,0\n2012-07-25 00:00:00,0\n2012-07-25 01:00:00,0\n2012-07-25 02:00:00,0\n2012-07-25 03:00:00,0\n2012-07-25 04:00:00,0\n2012-07-25 05:00:00,0\n2012-07-25 06:00:00,0\n2012-07-25 07:00:00,0\n2012-07-25 08:00:00,0\n2012-07-25 09:00:00,0\n2012-07-25 10:00:00,0\n2012-07-25 11:00:00,0\n2012-07-25 12:00:00,0\n2012-07-25 13:00:00,0\n2012-07-25 14:00:00,0\n2012-07-25 15:00:00,0\n2012-07-25 16:00:00,0\n2012-07-25 17:00:00,0\n2012-07-25 18:00:00,0\n2012-07-25 19:00:00,0\n2012-07-25 20:00:00,0\n2012-07-25 21:00:00,0\n2012-07-25 22:00:00,0\n2012-07-25 23:00:00,0\n2012-07-26 00:00:00,0\n2012-07-26 01:00:00,0\n2012-07-26 02:00:00,0\n2012-07-26 03:00:00,0\n2012-07-26 04:00:00,0\n2012-07-26 05:00:00,0\n2012-07-26 06:00:00,0\n2012-07-26 07:00:00,0\n2012-07-26 08:00:00,0\n2012-07-26 09:00:00,0\n2012-07-26 10:00:00,0\n2012-07-26 11:00:00,0\n2012-07-26 12:00:00,0\n2012-07-26 13:00:00,0\n2012-07-26 14:00:00,0\n2012-07-26 15:00:00,0\n2012-07-26 16:00:00,0\n2012-07-26 17:00:00,0\n2012-07-26 18:00:00,0\n2012-07-26 19:00:00,0\n2012-07-26 20:00:00,0\n2012-07-26 21:00:00,0\n2012-07-26 22:00:00,0\n2012-07-26 23:00:00,0\n2012-07-27 00:00:00,0\n2012-07-27 01:00:00,0\n2012-07-27 02:00:00,0\n2012-07-27 03:00:00,0\n2012-07-27 04:00:00,0\n2012-07-27 05:00:00,0\n2012-07-27 06:00:00,0\n2012-07-27 07:00:00,0\n2012-07-27 08:00:00,0\n2012-07-27 09:00:00,0\n2012-07-27 10:00:00,0\n2012-07-27 11:00:00,0\n2012-07-27 12:00:00,0\n2012-07-27 13:00:00,0\n2012-07-27 14:00:00,0\n2012-07-27 15:00:00,0\n2012-07-27 16:00:00,0\n2012-07-27 17:00:00,0\n2012-07-27 18:00:00,0\n2012-07-27 19:00:00,0\n2012-07-27 20:00:00,0\n2012-07-27 21:00:00,0\n2012-07-27 22:00:00,0\n2012-07-27 23:00:00,0\n2012-07-28 00:00:00,0\n2012-07-28 01:00:00,0\n2012-07-28 02:00:00,0\n2012-07-28 03:00:00,0\n2012-07-28 04:00:00,0\n2012-07-28 05:00:00,0\n2012-07-28 06:00:00,0\n2012-07-28 07:00:00,0\n2012-07-28 08:00:00,0\n2012-07-28 09:00:00,0\n2012-07-28 10:00:00,0\n2012-07-28 11:00:00,0\n2012-07-28 12:00:00,0\n2012-07-28 13:00:00,0\n2012-07-28 14:00:00,0\n2012-07-28 15:00:00,0\n2012-07-28 16:00:00,0\n2012-07-28 17:00:00,0\n2012-07-28 18:00:00,0\n2012-07-28 19:00:00,0\n2012-07-28 20:00:00,0\n2012-07-28 21:00:00,0\n2012-07-28 22:00:00,0\n2012-07-28 23:00:00,0\n2012-07-29 00:00:00,0\n2012-07-29 01:00:00,0\n2012-07-29 02:00:00,0\n2012-07-29 03:00:00,0\n2012-07-29 04:00:00,0\n2012-07-29 05:00:00,0\n2012-07-29 06:00:00,0\n2012-07-29 07:00:00,0\n2012-07-29 08:00:00,0\n2012-07-29 09:00:00,0\n2012-07-29 10:00:00,0\n2012-07-29 11:00:00,0\n2012-07-29 12:00:00,0\n2012-07-29 13:00:00,0\n2012-07-29 14:00:00,0\n2012-07-29 15:00:00,0\n2012-07-29 16:00:00,0\n2012-07-29 17:00:00,0\n2012-07-29 18:00:00,0\n2012-07-29 19:00:00,0\n2012-07-29 20:00:00,0\n2012-07-29 21:00:00,0\n2012-07-29 22:00:00,0\n2012-07-29 23:00:00,0\n2012-07-30 00:00:00,0\n2012-07-30 01:00:00,0\n2012-07-30 02:00:00,0\n2012-07-30 03:00:00,0\n2012-07-30 04:00:00,0\n2012-07-30 05:00:00,0\n2012-07-30 06:00:00,0\n2012-07-30 07:00:00,0\n2012-07-30 08:00:00,0\n2012-07-30 09:00:00,0\n2012-07-30 10:00:00,0\n2012-07-30 11:00:00,0\n2012-07-30 12:00:00,0\n2012-07-30 13:00:00,0\n2012-07-30 14:00:00,0\n2012-07-30 15:00:00,0\n2012-07-30 16:00:00,0\n2012-07-30 17:00:00,0\n2012-07-30 18:00:00,0\n2012-07-30 19:00:00,0\n2012-07-30 20:00:00,0\n2012-07-30 21:00:00,0\n2012-07-30 22:00:00,0\n2012-07-30 23:00:00,0\n2012-07-31 00:00:00,0\n2012-07-31 01:00:00,0\n2012-07-31 02:00:00,0\n2012-07-31 03:00:00,0\n2012-07-31 04:00:00,0\n2012-07-31 05:00:00,0\n2012-07-31 06:00:00,0\n2012-07-31 07:00:00,0\n2012-07-31 08:00:00,0\n2012-07-31 09:00:00,0\n2012-07-31 10:00:00,0\n2012-07-31 11:00:00,0\n2012-07-31 12:00:00,0\n2012-07-31 13:00:00,0\n2012-07-31 14:00:00,0\n2012-07-31 15:00:00,0\n2012-07-31 16:00:00,0\n2012-07-31 17:00:00,0\n2012-07-31 18:00:00,0\n2012-07-31 19:00:00,0\n2012-07-31 20:00:00,0\n2012-07-31 21:00:00,0\n2012-07-31 22:00:00,0\n2012-07-31 23:00:00,0\n2012-08-20 00:00:00,0\n2012-08-20 01:00:00,0\n2012-08-20 02:00:00,0\n2012-08-20 03:00:00,0\n2012-08-20 04:00:00,0\n2012-08-20 05:00:00,0\n2012-08-20 06:00:00,0\n2012-08-20 07:00:00,0\n2012-08-20 08:00:00,0\n2012-08-20 09:00:00,0\n2012-08-20 10:00:00,0\n2012-08-20 11:00:00,0\n2012-08-20 12:00:00,0\n2012-08-20 13:00:00,0\n2012-08-20 14:00:00,0\n2012-08-20 15:00:00,0\n2012-08-20 16:00:00,0\n2012-08-20 17:00:00,0\n2012-08-20 18:00:00,0\n2012-08-20 19:00:00,0\n2012-08-20 20:00:00,0\n2012-08-20 21:00:00,0\n2012-08-20 22:00:00,0\n2012-08-20 23:00:00,0\n2012-08-21 00:00:00,0\n2012-08-21 01:00:00,0\n2012-08-21 02:00:00,0\n2012-08-21 03:00:00,0\n2012-08-21 04:00:00,0\n2012-08-21 05:00:00,0\n2012-08-21 06:00:00,0\n2012-08-21 07:00:00,0\n2012-08-21 08:00:00,0\n2012-08-21 09:00:00,0\n2012-08-21 10:00:00,0\n2012-08-21 11:00:00,0\n2012-08-21 12:00:00,0\n2012-08-21 13:00:00,0\n2012-08-21 14:00:00,0\n2012-08-21 15:00:00,0\n2012-08-21 16:00:00,0\n2012-08-21 17:00:00,0\n2012-08-21 18:00:00,0\n2012-08-21 19:00:00,0\n2012-08-21 20:00:00,0\n2012-08-21 21:00:00,0\n2012-08-21 22:00:00,0\n2012-08-21 23:00:00,0\n2012-08-22 00:00:00,0\n2012-08-22 01:00:00,0\n2012-08-22 02:00:00,0\n2012-08-22 03:00:00,0\n2012-08-22 04:00:00,0\n2012-08-22 05:00:00,0\n2012-08-22 06:00:00,0\n2012-08-22 07:00:00,0\n2012-08-22 08:00:00,0\n2012-08-22 09:00:00,0\n2012-08-22 10:00:00,0\n2012-08-22 11:00:00,0\n2012-08-22 12:00:00,0\n2012-08-22 13:00:00,0\n2012-08-22 14:00:00,0\n2012-08-22 15:00:00,0\n2012-08-22 16:00:00,0\n2012-08-22 17:00:00,0\n2012-08-22 18:00:00,0\n2012-08-22 19:00:00,0\n2012-08-22 20:00:00,0\n2012-08-22 21:00:00,0\n2012-08-22 22:00:00,0\n2012-08-22 23:00:00,0\n2012-08-23 00:00:00,0\n2012-08-23 01:00:00,0\n2012-08-23 02:00:00,0\n2012-08-23 03:00:00,0\n2012-08-23 04:00:00,0\n2012-08-23 05:00:00,0\n2012-08-23 06:00:00,0\n2012-08-23 07:00:00,0\n2012-08-23 08:00:00,0\n2012-08-23 09:00:00,0\n2012-08-23 10:00:00,0\n2012-08-23 11:00:00,0\n2012-08-23 12:00:00,0\n2012-08-23 13:00:00,0\n2012-08-23 14:00:00,0\n2012-08-23 15:00:00,0\n2012-08-23 16:00:00,0\n2012-08-23 17:00:00,0\n2012-08-23 18:00:00,0\n2012-08-23 19:00:00,0\n2012-08-23 20:00:00,0\n2012-08-23 21:00:00,0\n2012-08-23 22:00:00,0\n2012-08-23 23:00:00,0\n2012-08-24 00:00:00,0\n2012-08-24 01:00:00,0\n2012-08-24 02:00:00,0\n2012-08-24 03:00:00,0\n2012-08-24 04:00:00,0\n2012-08-24 05:00:00,0\n2012-08-24 06:00:00,0\n2012-08-24 07:00:00,0\n2012-08-24 08:00:00,0\n2012-08-24 09:00:00,0\n2012-08-24 10:00:00,0\n2012-08-24 11:00:00,0\n2012-08-24 12:00:00,0\n2012-08-24 13:00:00,0\n2012-08-24 14:00:00,0\n2012-08-24 15:00:00,0\n2012-08-24 16:00:00,0\n2012-08-24 17:00:00,0\n2012-08-24 18:00:00,0\n2012-08-24 19:00:00,0\n2012-08-24 20:00:00,0\n2012-08-24 21:00:00,0\n2012-08-24 22:00:00,0\n2012-08-24 23:00:00,0\n2012-08-25 00:00:00,0\n2012-08-25 01:00:00,0\n2012-08-25 02:00:00,0\n2012-08-25 03:00:00,0\n2012-08-25 04:00:00,0\n2012-08-25 05:00:00,0\n2012-08-25 06:00:00,0\n2012-08-25 07:00:00,0\n2012-08-25 08:00:00,0\n2012-08-25 09:00:00,0\n2012-08-25 10:00:00,0\n2012-08-25 11:00:00,0\n2012-08-25 12:00:00,0\n2012-08-25 13:00:00,0\n2012-08-25 14:00:00,0\n2012-08-25 15:00:00,0\n2012-08-25 16:00:00,0\n2012-08-25 17:00:00,0\n2012-08-25 18:00:00,0\n2012-08-25 19:00:00,0\n2012-08-25 20:00:00,0\n2012-08-25 21:00:00,0\n2012-08-25 22:00:00,0\n2012-08-25 23:00:00,0\n2012-08-26 00:00:00,0\n2012-08-26 01:00:00,0\n2012-08-26 02:00:00,0\n2012-08-26 03:00:00,0\n2012-08-26 04:00:00,0\n2012-08-26 05:00:00,0\n2012-08-26 06:00:00,0\n2012-08-26 07:00:00,0\n2012-08-26 08:00:00,0\n2012-08-26 09:00:00,0\n2012-08-26 10:00:00,0\n2012-08-26 11:00:00,0\n2012-08-26 12:00:00,0\n2012-08-26 13:00:00,0\n2012-08-26 14:00:00,0\n2012-08-26 15:00:00,0\n2012-08-26 16:00:00,0\n2012-08-26 17:00:00,0\n2012-08-26 18:00:00,0\n2012-08-26 19:00:00,0\n2012-08-26 20:00:00,0\n2012-08-26 21:00:00,0\n2012-08-26 22:00:00,0\n2012-08-26 23:00:00,0\n2012-08-27 00:00:00,0\n2012-08-27 01:00:00,0\n2012-08-27 02:00:00,0\n2012-08-27 03:00:00,0\n2012-08-27 04:00:00,0\n2012-08-27 05:00:00,0\n2012-08-27 06:00:00,0\n2012-08-27 07:00:00,0\n2012-08-27 08:00:00,0\n2012-08-27 09:00:00,0\n2012-08-27 10:00:00,0\n2012-08-27 11:00:00,0\n2012-08-27 12:00:00,0\n2012-08-27 13:00:00,0\n2012-08-27 14:00:00,0\n2012-08-27 15:00:00,0\n2012-08-27 16:00:00,0\n2012-08-27 17:00:00,0\n2012-08-27 18:00:00,0\n2012-08-27 19:00:00,0\n2012-08-27 20:00:00,0\n2012-08-27 21:00:00,0\n2012-08-27 22:00:00,0\n2012-08-27 23:00:00,0\n2012-08-28 00:00:00,0\n2012-08-28 01:00:00,0\n2012-08-28 02:00:00,0\n2012-08-28 03:00:00,0\n2012-08-28 04:00:00,0\n2012-08-28 05:00:00,0\n2012-08-28 06:00:00,0\n2012-08-28 07:00:00,0\n2012-08-28 08:00:00,0\n2012-08-28 09:00:00,0\n2012-08-28 10:00:00,0\n2012-08-28 11:00:00,0\n2012-08-28 12:00:00,0\n2012-08-28 13:00:00,0\n2012-08-28 14:00:00,0\n2012-08-28 15:00:00,0\n2012-08-28 16:00:00,0\n2012-08-28 17:00:00,0\n2012-08-28 18:00:00,0\n2012-08-28 19:00:00,0\n2012-08-28 20:00:00,0\n2012-08-28 21:00:00,0\n2012-08-28 22:00:00,0\n2012-08-28 23:00:00,0\n2012-08-29 00:00:00,0\n2012-08-29 01:00:00,0\n2012-08-29 02:00:00,0\n2012-08-29 03:00:00,0\n2012-08-29 04:00:00,0\n2012-08-29 05:00:00,0\n2012-08-29 06:00:00,0\n2012-08-29 07:00:00,0\n2012-08-29 08:00:00,0\n2012-08-29 09:00:00,0\n2012-08-29 10:00:00,0\n2012-08-29 11:00:00,0\n2012-08-29 12:00:00,0\n2012-08-29 13:00:00,0\n2012-08-29 14:00:00,0\n2012-08-29 15:00:00,0\n2012-08-29 16:00:00,0\n2012-08-29 17:00:00,0\n2012-08-29 18:00:00,0\n2012-08-29 19:00:00,0\n2012-08-29 20:00:00,0\n2012-08-29 21:00:00,0\n2012-08-29 22:00:00,0\n2012-08-29 23:00:00,0\n2012-08-30 00:00:00,0\n2012-08-30 01:00:00,0\n2012-08-30 02:00:00,0\n2012-08-30 03:00:00,0\n2012-08-30 04:00:00,0\n2012-08-30 05:00:00,0\n2012-08-30 06:00:00,0\n2012-08-30 07:00:00,0\n2012-08-30 08:00:00,0\n2012-08-30 09:00:00,0\n2012-08-30 10:00:00,0\n2012-08-30 11:00:00,0\n2012-08-30 12:00:00,0\n2012-08-30 13:00:00,0\n2012-08-30 14:00:00,0\n2012-08-30 15:00:00,0\n2012-08-30 16:00:00,0\n2012-08-30 17:00:00,0\n2012-08-30 18:00:00,0\n2012-08-30 19:00:00,0\n2012-08-30 20:00:00,0\n2012-08-30 21:00:00,0\n2012-08-30 22:00:00,0\n2012-08-30 23:00:00,0\n2012-08-31 00:00:00,0\n2012-08-31 01:00:00,0\n2012-08-31 02:00:00,0\n2012-08-31 03:00:00,0\n2012-08-31 04:00:00,0\n2012-08-31 05:00:00,0\n2012-08-31 06:00:00,0\n2012-08-31 07:00:00,0\n2012-08-31 08:00:00,0\n2012-08-31 09:00:00,0\n2012-08-31 10:00:00,0\n2012-08-31 11:00:00,0\n2012-08-31 12:00:00,0\n2012-08-31 13:00:00,0\n2012-08-31 14:00:00,0\n2012-08-31 15:00:00,0\n2012-08-31 16:00:00,0\n2012-08-31 17:00:00,0\n2012-08-31 18:00:00,0\n2012-08-31 19:00:00,0\n2012-08-31 20:00:00,0\n2012-08-31 21:00:00,0\n2012-08-31 22:00:00,0\n2012-08-31 23:00:00,0\n2012-09-20 00:00:00,0\n2012-09-20 01:00:00,0\n2012-09-20 02:00:00,0\n2012-09-20 03:00:00,0\n2012-09-20 04:00:00,0\n2012-09-20 05:00:00,0\n2012-09-20 06:00:00,0\n2012-09-20 07:00:00,0\n2012-09-20 08:00:00,0\n2012-09-20 09:00:00,0\n2012-09-20 10:00:00,0\n2012-09-20 11:00:00,0\n2012-09-20 12:00:00,0\n2012-09-20 13:00:00,0\n2012-09-20 14:00:00,0\n2012-09-20 15:00:00,0\n2012-09-20 16:00:00,0\n2012-09-20 17:00:00,0\n2012-09-20 18:00:00,0\n2012-09-20 19:00:00,0\n2012-09-20 20:00:00,0\n2012-09-20 21:00:00,0\n2012-09-20 22:00:00,0\n2012-09-20 23:00:00,0\n2012-09-21 00:00:00,0\n2012-09-21 01:00:00,0\n2012-09-21 02:00:00,0\n2012-09-21 03:00:00,0\n2012-09-21 04:00:00,0\n2012-09-21 05:00:00,0\n2012-09-21 06:00:00,0\n2012-09-21 07:00:00,0\n2012-09-21 08:00:00,0\n2012-09-21 09:00:00,0\n2012-09-21 10:00:00,0\n2012-09-21 11:00:00,0\n2012-09-21 12:00:00,0\n2012-09-21 13:00:00,0\n2012-09-21 14:00:00,0\n2012-09-21 15:00:00,0\n2012-09-21 16:00:00,0\n2012-09-21 17:00:00,0\n2012-09-21 18:00:00,0\n2012-09-21 19:00:00,0\n2012-09-21 20:00:00,0\n2012-09-21 21:00:00,0\n2012-09-21 22:00:00,0\n2012-09-21 23:00:00,0\n2012-09-22 00:00:00,0\n2012-09-22 01:00:00,0\n2012-09-22 02:00:00,0\n2012-09-22 03:00:00,0\n2012-09-22 04:00:00,0\n2012-09-22 05:00:00,0\n2012-09-22 06:00:00,0\n2012-09-22 07:00:00,0\n2012-09-22 08:00:00,0\n2012-09-22 09:00:00,0\n2012-09-22 10:00:00,0\n2012-09-22 11:00:00,0\n2012-09-22 12:00:00,0\n2012-09-22 13:00:00,0\n2012-09-22 14:00:00,0\n2012-09-22 15:00:00,0\n2012-09-22 16:00:00,0\n2012-09-22 17:00:00,0\n2012-09-22 18:00:00,0\n2012-09-22 19:00:00,0\n2012-09-22 20:00:00,0\n2012-09-22 21:00:00,0\n2012-09-22 22:00:00,0\n2012-09-22 23:00:00,0\n2012-09-23 00:00:00,0\n2012-09-23 01:00:00,0\n2012-09-23 02:00:00,0\n2012-09-23 03:00:00,0\n2012-09-23 04:00:00,0\n2012-09-23 05:00:00,0\n2012-09-23 06:00:00,0\n2012-09-23 07:00:00,0\n2012-09-23 08:00:00,0\n2012-09-23 09:00:00,0\n2012-09-23 10:00:00,0\n2012-09-23 11:00:00,0\n2012-09-23 12:00:00,0\n2012-09-23 13:00:00,0\n2012-09-23 14:00:00,0\n2012-09-23 15:00:00,0\n2012-09-23 16:00:00,0\n2012-09-23 17:00:00,0\n2012-09-23 18:00:00,0\n2012-09-23 19:00:00,0\n2012-09-23 20:00:00,0\n2012-09-23 21:00:00,0\n2012-09-23 22:00:00,0\n2012-09-23 23:00:00,0\n2012-09-24 00:00:00,0\n2012-09-24 01:00:00,0\n2012-09-24 02:00:00,0\n2012-09-24 03:00:00,0\n2012-09-24 04:00:00,0\n2012-09-24 05:00:00,0\n2012-09-24 06:00:00,0\n2012-09-24 07:00:00,0\n2012-09-24 08:00:00,0\n2012-09-24 09:00:00,0\n2012-09-24 10:00:00,0\n2012-09-24 11:00:00,0\n2012-09-24 12:00:00,0\n2012-09-24 13:00:00,0\n2012-09-24 14:00:00,0\n2012-09-24 15:00:00,0\n2012-09-24 16:00:00,0\n2012-09-24 17:00:00,0\n2012-09-24 18:00:00,0\n2012-09-24 19:00:00,0\n2012-09-24 20:00:00,0\n2012-09-24 21:00:00,0\n2012-09-24 22:00:00,0\n2012-09-24 23:00:00,0\n2012-09-25 00:00:00,0\n2012-09-25 01:00:00,0\n2012-09-25 02:00:00,0\n2012-09-25 03:00:00,0\n2012-09-25 04:00:00,0\n2012-09-25 05:00:00,0\n2012-09-25 06:00:00,0\n2012-09-25 07:00:00,0\n2012-09-25 08:00:00,0\n2012-09-25 09:00:00,0\n2012-09-25 10:00:00,0\n2012-09-25 11:00:00,0\n2012-09-25 12:00:00,0\n2012-09-25 13:00:00,0\n2012-09-25 14:00:00,0\n2012-09-25 15:00:00,0\n2012-09-25 16:00:00,0\n2012-09-25 17:00:00,0\n2012-09-25 18:00:00,0\n2012-09-25 19:00:00,0\n2012-09-25 20:00:00,0\n2012-09-25 21:00:00,0\n2012-09-25 22:00:00,0\n2012-09-25 23:00:00,0\n2012-09-26 00:00:00,0\n2012-09-26 01:00:00,0\n2012-09-26 02:00:00,0\n2012-09-26 03:00:00,0\n2012-09-26 04:00:00,0\n2012-09-26 05:00:00,0\n2012-09-26 06:00:00,0\n2012-09-26 07:00:00,0\n2012-09-26 08:00:00,0\n2012-09-26 09:00:00,0\n2012-09-26 10:00:00,0\n2012-09-26 11:00:00,0\n2012-09-26 12:00:00,0\n2012-09-26 13:00:00,0\n2012-09-26 14:00:00,0\n2012-09-26 15:00:00,0\n2012-09-26 16:00:00,0\n2012-09-26 17:00:00,0\n2012-09-26 18:00:00,0\n2012-09-26 19:00:00,0\n2012-09-26 20:00:00,0\n2012-09-26 21:00:00,0\n2012-09-26 22:00:00,0\n2012-09-26 23:00:00,0\n2012-09-27 00:00:00,0\n2012-09-27 01:00:00,0\n2012-09-27 02:00:00,0\n2012-09-27 03:00:00,0\n2012-09-27 04:00:00,0\n2012-09-27 05:00:00,0\n2012-09-27 06:00:00,0\n2012-09-27 07:00:00,0\n2012-09-27 08:00:00,0\n2012-09-27 09:00:00,0\n2012-09-27 10:00:00,0\n2012-09-27 11:00:00,0\n2012-09-27 12:00:00,0\n2012-09-27 13:00:00,0\n2012-09-27 14:00:00,0\n2012-09-27 15:00:00,0\n2012-09-27 16:00:00,0\n2012-09-27 17:00:00,0\n2012-09-27 18:00:00,0\n2012-09-27 19:00:00,0\n2012-09-27 20:00:00,0\n2012-09-27 21:00:00,0\n2012-09-27 22:00:00,0\n2012-09-27 23:00:00,0\n2012-09-28 00:00:00,0\n2012-09-28 01:00:00,0\n2012-09-28 02:00:00,0\n2012-09-28 03:00:00,0\n2012-09-28 04:00:00,0\n2012-09-28 05:00:00,0\n2012-09-28 06:00:00,0\n2012-09-28 07:00:00,0\n2012-09-28 08:00:00,0\n2012-09-28 09:00:00,0\n2012-09-28 10:00:00,0\n2012-09-28 11:00:00,0\n2012-09-28 12:00:00,0\n2012-09-28 13:00:00,0\n2012-09-28 14:00:00,0\n2012-09-28 15:00:00,0\n2012-09-28 16:00:00,0\n2012-09-28 17:00:00,0\n2012-09-28 18:00:00,0\n2012-09-28 19:00:00,0\n2012-09-28 20:00:00,0\n2012-09-28 21:00:00,0\n2012-09-28 22:00:00,0\n2012-09-28 23:00:00,0\n2012-09-29 00:00:00,0\n2012-09-29 01:00:00,0\n2012-09-29 02:00:00,0\n2012-09-29 03:00:00,0\n2012-09-29 04:00:00,0\n2012-09-29 05:00:00,0\n2012-09-29 06:00:00,0\n2012-09-29 07:00:00,0\n2012-09-29 08:00:00,0\n2012-09-29 09:00:00,0\n2012-09-29 10:00:00,0\n2012-09-29 11:00:00,0\n2012-09-29 12:00:00,0\n2012-09-29 13:00:00,0\n2012-09-29 14:00:00,0\n2012-09-29 15:00:00,0\n2012-09-29 16:00:00,0\n2012-09-29 17:00:00,0\n2012-09-29 18:00:00,0\n2012-09-29 19:00:00,0\n2012-09-29 20:00:00,0\n2012-09-29 21:00:00,0\n2012-09-29 22:00:00,0\n2012-09-29 23:00:00,0\n2012-09-30 00:00:00,0\n2012-09-30 01:00:00,0\n2012-09-30 02:00:00,0\n2012-09-30 03:00:00,0\n2012-09-30 04:00:00,0\n2012-09-30 05:00:00,0\n2012-09-30 06:00:00,0\n2012-09-30 07:00:00,0\n2012-09-30 08:00:00,0\n2012-09-30 09:00:00,0\n2012-09-30 10:00:00,0\n2012-09-30 11:00:00,0\n2012-09-30 12:00:00,0\n2012-09-30 13:00:00,0\n2012-09-30 14:00:00,0\n2012-09-30 15:00:00,0\n2012-09-30 16:00:00,0\n2012-09-30 17:00:00,0\n2012-09-30 18:00:00,0\n2012-09-30 19:00:00,0\n2012-09-30 20:00:00,0\n2012-09-30 21:00:00,0\n2012-09-30 22:00:00,0\n2012-09-30 23:00:00,0\n2012-10-20 00:00:00,0\n2012-10-20 01:00:00,0\n2012-10-20 02:00:00,0\n2012-10-20 03:00:00,0\n2012-10-20 04:00:00,0\n2012-10-20 05:00:00,0\n2012-10-20 06:00:00,0\n2012-10-20 07:00:00,0\n2012-10-20 08:00:00,0\n2012-10-20 09:00:00,0\n2012-10-20 10:00:00,0\n2012-10-20 11:00:00,0\n2012-10-20 12:00:00,0\n2012-10-20 13:00:00,0\n2012-10-20 14:00:00,0\n2012-10-20 15:00:00,0\n2012-10-20 16:00:00,0\n2012-10-20 17:00:00,0\n2012-10-20 18:00:00,0\n2012-10-20 19:00:00,0\n2012-10-20 20:00:00,0\n2012-10-20 21:00:00,0\n2012-10-20 22:00:00,0\n2012-10-20 23:00:00,0\n2012-10-21 00:00:00,0\n2012-10-21 01:00:00,0\n2012-10-21 02:00:00,0\n2012-10-21 03:00:00,0\n2012-10-21 04:00:00,0\n2012-10-21 05:00:00,0\n2012-10-21 06:00:00,0\n2012-10-21 07:00:00,0\n2012-10-21 08:00:00,0\n2012-10-21 09:00:00,0\n2012-10-21 10:00:00,0\n2012-10-21 11:00:00,0\n2012-10-21 12:00:00,0\n2012-10-21 13:00:00,0\n2012-10-21 14:00:00,0\n2012-10-21 15:00:00,0\n2012-10-21 16:00:00,0\n2012-10-21 17:00:00,0\n2012-10-21 18:00:00,0\n2012-10-21 19:00:00,0\n2012-10-21 20:00:00,0\n2012-10-21 21:00:00,0\n2012-10-21 22:00:00,0\n2012-10-21 23:00:00,0\n2012-10-22 00:00:00,0\n2012-10-22 01:00:00,0\n2012-10-22 02:00:00,0\n2012-10-22 03:00:00,0\n2012-10-22 04:00:00,0\n2012-10-22 05:00:00,0\n2012-10-22 06:00:00,0\n2012-10-22 07:00:00,0\n2012-10-22 08:00:00,0\n2012-10-22 09:00:00,0\n2012-10-22 10:00:00,0\n2012-10-22 11:00:00,0\n2012-10-22 12:00:00,0\n2012-10-22 13:00:00,0\n2012-10-22 14:00:00,0\n2012-10-22 15:00:00,0\n2012-10-22 16:00:00,0\n2012-10-22 17:00:00,0\n2012-10-22 18:00:00,0\n2012-10-22 19:00:00,0\n2012-10-22 20:00:00,0\n2012-10-22 21:00:00,0\n2012-10-22 22:00:00,0\n2012-10-22 23:00:00,0\n2012-10-23 00:00:00,0\n2012-10-23 01:00:00,0\n2012-10-23 02:00:00,0\n2012-10-23 03:00:00,0\n2012-10-23 04:00:00,0\n2012-10-23 05:00:00,0\n2012-10-23 06:00:00,0\n2012-10-23 07:00:00,0\n2012-10-23 08:00:00,0\n2012-10-23 09:00:00,0\n2012-10-23 10:00:00,0\n2012-10-23 11:00:00,0\n2012-10-23 12:00:00,0\n2012-10-23 13:00:00,0\n2012-10-23 14:00:00,0\n2012-10-23 15:00:00,0\n2012-10-23 16:00:00,0\n2012-10-23 17:00:00,0\n2012-10-23 18:00:00,0\n2012-10-23 19:00:00,0\n2012-10-23 20:00:00,0\n2012-10-23 21:00:00,0\n2012-10-23 22:00:00,0\n2012-10-23 23:00:00,0\n2012-10-24 00:00:00,0\n2012-10-24 01:00:00,0\n2012-10-24 02:00:00,0\n2012-10-24 03:00:00,0\n2012-10-24 04:00:00,0\n2012-10-24 05:00:00,0\n2012-10-24 06:00:00,0\n2012-10-24 07:00:00,0\n2012-10-24 08:00:00,0\n2012-10-24 09:00:00,0\n2012-10-24 10:00:00,0\n2012-10-24 11:00:00,0\n2012-10-24 12:00:00,0\n2012-10-24 13:00:00,0\n2012-10-24 14:00:00,0\n2012-10-24 15:00:00,0\n2012-10-24 16:00:00,0\n2012-10-24 17:00:00,0\n2012-10-24 18:00:00,0\n2012-10-24 19:00:00,0\n2012-10-24 20:00:00,0\n2012-10-24 21:00:00,0\n2012-10-24 22:00:00,0\n2012-10-24 23:00:00,0\n2012-10-25 00:00:00,0\n2012-10-25 01:00:00,0\n2012-10-25 02:00:00,0\n2012-10-25 03:00:00,0\n2012-10-25 04:00:00,0\n2012-10-25 05:00:00,0\n2012-10-25 06:00:00,0\n2012-10-25 07:00:00,0\n2012-10-25 08:00:00,0\n2012-10-25 09:00:00,0\n2012-10-25 10:00:00,0\n2012-10-25 11:00:00,0\n2012-10-25 12:00:00,0\n2012-10-25 13:00:00,0\n2012-10-25 14:00:00,0\n2012-10-25 15:00:00,0\n2012-10-25 16:00:00,0\n2012-10-25 17:00:00,0\n2012-10-25 18:00:00,0\n2012-10-25 19:00:00,0\n2012-10-25 20:00:00,0\n2012-10-25 21:00:00,0\n2012-10-25 22:00:00,0\n2012-10-25 23:00:00,0\n2012-10-26 00:00:00,0\n2012-10-26 01:00:00,0\n2012-10-26 02:00:00,0\n2012-10-26 03:00:00,0\n2012-10-26 04:00:00,0\n2012-10-26 05:00:00,0\n2012-10-26 06:00:00,0\n2012-10-26 07:00:00,0\n2012-10-26 08:00:00,0\n2012-10-26 09:00:00,0\n2012-10-26 10:00:00,0\n2012-10-26 11:00:00,0\n2012-10-26 12:00:00,0\n2012-10-26 13:00:00,0\n2012-10-26 14:00:00,0\n2012-10-26 15:00:00,0\n2012-10-26 16:00:00,0\n2012-10-26 17:00:00,0\n2012-10-26 18:00:00,0\n2012-10-26 19:00:00,0\n2012-10-26 20:00:00,0\n2012-10-26 21:00:00,0\n2012-10-26 22:00:00,0\n2012-10-26 23:00:00,0\n2012-10-27 00:00:00,0\n2012-10-27 01:00:00,0\n2012-10-27 02:00:00,0\n2012-10-27 03:00:00,0\n2012-10-27 04:00:00,0\n2012-10-27 05:00:00,0\n2012-10-27 06:00:00,0\n2012-10-27 07:00:00,0\n2012-10-27 08:00:00,0\n2012-10-27 09:00:00,0\n2012-10-27 10:00:00,0\n2012-10-27 11:00:00,0\n2012-10-27 12:00:00,0\n2012-10-27 13:00:00,0\n2012-10-27 14:00:00,0\n2012-10-27 15:00:00,0\n2012-10-27 16:00:00,0\n2012-10-27 17:00:00,0\n2012-10-27 18:00:00,0\n2012-10-27 19:00:00,0\n2012-10-27 20:00:00,0\n2012-10-27 21:00:00,0\n2012-10-27 22:00:00,0\n2012-10-27 23:00:00,0\n2012-10-28 00:00:00,0\n2012-10-28 01:00:00,0\n2012-10-28 02:00:00,0\n2012-10-28 03:00:00,0\n2012-10-28 04:00:00,0\n2012-10-28 05:00:00,0\n2012-10-28 06:00:00,0\n2012-10-28 07:00:00,0\n2012-10-28 08:00:00,0\n2012-10-28 09:00:00,0\n2012-10-28 10:00:00,0\n2012-10-28 11:00:00,0\n2012-10-28 12:00:00,0\n2012-10-28 13:00:00,0\n2012-10-28 14:00:00,0\n2012-10-28 15:00:00,0\n2012-10-28 16:00:00,0\n2012-10-28 17:00:00,0\n2012-10-28 18:00:00,0\n2012-10-28 19:00:00,0\n2012-10-28 20:00:00,0\n2012-10-28 21:00:00,0\n2012-10-28 22:00:00,0\n2012-10-28 23:00:00,0\n2012-10-29 00:00:00,0\n2012-10-30 13:00:00,0\n2012-10-30 14:00:00,0\n2012-10-30 15:00:00,0\n2012-10-30 16:00:00,0\n2012-10-30 17:00:00,0\n2012-10-30 18:00:00,0\n2012-10-30 19:00:00,0\n2012-10-30 20:00:00,0\n2012-10-30 21:00:00,0\n2012-10-30 22:00:00,0\n2012-10-30 23:00:00,0\n2012-10-31 00:00:00,0\n2012-10-31 01:00:00,0\n2012-10-31 02:00:00,0\n2012-10-31 03:00:00,0\n2012-10-31 04:00:00,0\n2012-10-31 05:00:00,0\n2012-10-31 06:00:00,0\n2012-10-31 07:00:00,0\n2012-10-31 08:00:00,0\n2012-10-31 09:00:00,0\n2012-10-31 10:00:00,0\n2012-10-31 11:00:00,0\n2012-10-31 12:00:00,0\n2012-10-31 13:00:00,0\n2012-10-31 14:00:00,0\n2012-10-31 15:00:00,0\n2012-10-31 16:00:00,0\n2012-10-31 17:00:00,0\n2012-10-31 18:00:00,0\n2012-10-31 19:00:00,0\n2012-10-31 20:00:00,0\n2012-10-31 21:00:00,0\n2012-10-31 22:00:00,0\n2012-10-31 23:00:00,0\n2012-11-20 00:00:00,0\n2012-11-20 01:00:00,0\n2012-11-20 02:00:00,0\n2012-11-20 03:00:00,0\n2012-11-20 04:00:00,0\n2012-11-20 05:00:00,0\n2012-11-20 06:00:00,0\n2012-11-20 07:00:00,0\n2012-11-20 08:00:00,0\n2012-11-20 09:00:00,0\n2012-11-20 10:00:00,0\n2012-11-20 11:00:00,0\n2012-11-20 12:00:00,0\n2012-11-20 13:00:00,0\n2012-11-20 14:00:00,0\n2012-11-20 15:00:00,0\n2012-11-20 16:00:00,0\n2012-11-20 17:00:00,0\n2012-11-20 18:00:00,0\n2012-11-20 19:00:00,0\n2012-11-20 20:00:00,0\n2012-11-20 21:00:00,0\n2012-11-20 22:00:00,0\n2012-11-20 23:00:00,0\n2012-11-21 00:00:00,0\n2012-11-21 01:00:00,0\n2012-11-21 02:00:00,0\n2012-11-21 03:00:00,0\n2012-11-21 04:00:00,0\n2012-11-21 05:00:00,0\n2012-11-21 06:00:00,0\n2012-11-21 07:00:00,0\n2012-11-21 08:00:00,0\n2012-11-21 09:00:00,0\n2012-11-21 10:00:00,0\n2012-11-21 11:00:00,0\n2012-11-21 12:00:00,0\n2012-11-21 13:00:00,0\n2012-11-21 14:00:00,0\n2012-11-21 15:00:00,0\n2012-11-21 16:00:00,0\n2012-11-21 17:00:00,0\n2012-11-21 18:00:00,0\n2012-11-21 19:00:00,0\n2012-11-21 20:00:00,0\n2012-11-21 21:00:00,0\n2012-11-21 22:00:00,0\n2012-11-21 23:00:00,0\n2012-11-22 00:00:00,0\n2012-11-22 01:00:00,0\n2012-11-22 02:00:00,0\n2012-11-22 03:00:00,0\n2012-11-22 04:00:00,0\n2012-11-22 05:00:00,0\n2012-11-22 06:00:00,0\n2012-11-22 07:00:00,0\n2012-11-22 08:00:00,0\n2012-11-22 09:00:00,0\n2012-11-22 10:00:00,0\n2012-11-22 11:00:00,0\n2012-11-22 12:00:00,0\n2012-11-22 13:00:00,0\n2012-11-22 14:00:00,0\n2012-11-22 15:00:00,0\n2012-11-22 16:00:00,0\n2012-11-22 17:00:00,0\n2012-11-22 18:00:00,0\n2012-11-22 19:00:00,0\n2012-11-22 20:00:00,0\n2012-11-22 21:00:00,0\n2012-11-22 22:00:00,0\n2012-11-22 23:00:00,0\n2012-11-23 00:00:00,0\n2012-11-23 01:00:00,0\n2012-11-23 02:00:00,0\n2012-11-23 03:00:00,0\n2012-11-23 04:00:00,0\n2012-11-23 05:00:00,0\n2012-11-23 06:00:00,0\n2012-11-23 07:00:00,0\n2012-11-23 08:00:00,0\n2012-11-23 09:00:00,0\n2012-11-23 10:00:00,0\n2012-11-23 11:00:00,0\n2012-11-23 12:00:00,0\n2012-11-23 13:00:00,0\n2012-11-23 14:00:00,0\n2012-11-23 15:00:00,0\n2012-11-23 16:00:00,0\n2012-11-23 17:00:00,0\n2012-11-23 18:00:00,0\n2012-11-23 19:00:00,0\n2012-11-23 20:00:00,0\n2012-11-23 21:00:00,0\n2012-11-23 22:00:00,0\n2012-11-23 23:00:00,0\n2012-11-24 00:00:00,0\n2012-11-24 01:00:00,0\n2012-11-24 02:00:00,0\n2012-11-24 03:00:00,0\n2012-11-24 04:00:00,0\n2012-11-24 05:00:00,0\n2012-11-24 06:00:00,0\n2012-11-24 07:00:00,0\n2012-11-24 08:00:00,0\n2012-11-24 09:00:00,0\n2012-11-24 10:00:00,0\n2012-11-24 11:00:00,0\n2012-11-24 12:00:00,0\n2012-11-24 13:00:00,0\n2012-11-24 14:00:00,0\n2012-11-24 15:00:00,0\n2012-11-24 16:00:00,0\n2012-11-24 17:00:00,0\n2012-11-24 18:00:00,0\n2012-11-24 19:00:00,0\n2012-11-24 20:00:00,0\n2012-11-24 21:00:00,0\n2012-11-24 22:00:00,0\n2012-11-24 23:00:00,0\n2012-11-25 00:00:00,0\n2012-11-25 01:00:00,0\n2012-11-25 02:00:00,0\n2012-11-25 03:00:00,0\n2012-11-25 04:00:00,0\n2012-11-25 05:00:00,0\n2012-11-25 06:00:00,0\n2012-11-25 07:00:00,0\n2012-11-25 08:00:00,0\n2012-11-25 09:00:00,0\n2012-11-25 10:00:00,0\n2012-11-25 11:00:00,0\n2012-11-25 12:00:00,0\n2012-11-25 13:00:00,0\n2012-11-25 14:00:00,0\n2012-11-25 15:00:00,0\n2012-11-25 16:00:00,0\n2012-11-25 17:00:00,0\n2012-11-25 18:00:00,0\n2012-11-25 19:00:00,0\n2012-11-25 20:00:00,0\n2012-11-25 21:00:00,0\n2012-11-25 22:00:00,0\n2012-11-25 23:00:00,0\n2012-11-26 00:00:00,0\n2012-11-26 01:00:00,0\n2012-11-26 02:00:00,0\n2012-11-26 03:00:00,0\n2012-11-26 04:00:00,0\n2012-11-26 05:00:00,0\n2012-11-26 06:00:00,0\n2012-11-26 07:00:00,0\n2012-11-26 08:00:00,0\n2012-11-26 09:00:00,0\n2012-11-26 10:00:00,0\n2012-11-26 11:00:00,0\n2012-11-26 12:00:00,0\n2012-11-26 13:00:00,0\n2012-11-26 14:00:00,0\n2012-11-26 15:00:00,0\n2012-11-26 16:00:00,0\n2012-11-26 17:00:00,0\n2012-11-26 18:00:00,0\n2012-11-26 19:00:00,0\n2012-11-26 20:00:00,0\n2012-11-26 21:00:00,0\n2012-11-26 22:00:00,0\n2012-11-26 23:00:00,0\n2012-11-27 00:00:00,0\n2012-11-27 01:00:00,0\n2012-11-27 02:00:00,0\n2012-11-27 03:00:00,0\n2012-11-27 04:00:00,0\n2012-11-27 05:00:00,0\n2012-11-27 06:00:00,0\n2012-11-27 07:00:00,0\n2012-11-27 08:00:00,0\n2012-11-27 09:00:00,0\n2012-11-27 10:00:00,0\n2012-11-27 11:00:00,0\n2012-11-27 12:00:00,0\n2012-11-27 13:00:00,0\n2012-11-27 14:00:00,0\n2012-11-27 15:00:00,0\n2012-11-27 16:00:00,0\n2012-11-27 17:00:00,0\n2012-11-27 18:00:00,0\n2012-11-27 19:00:00,0\n2012-11-27 20:00:00,0\n2012-11-27 21:00:00,0\n2012-11-27 22:00:00,0\n2012-11-27 23:00:00,0\n2012-11-28 00:00:00,0\n2012-11-28 01:00:00,0\n2012-11-28 02:00:00,0\n2012-11-28 03:00:00,0\n2012-11-28 04:00:00,0\n2012-11-28 05:00:00,0\n2012-11-28 06:00:00,0\n2012-11-28 07:00:00,0\n2012-11-28 08:00:00,0\n2012-11-28 09:00:00,0\n2012-11-28 10:00:00,0\n2012-11-28 11:00:00,0\n2012-11-28 12:00:00,0\n2012-11-28 13:00:00,0\n2012-11-28 14:00:00,0\n2012-11-28 15:00:00,0\n2012-11-28 16:00:00,0\n2012-11-28 17:00:00,0\n2012-11-28 18:00:00,0\n2012-11-28 19:00:00,0\n2012-11-28 20:00:00,0\n2012-11-28 21:00:00,0\n2012-11-28 22:00:00,0\n2012-11-28 23:00:00,0\n2012-11-29 00:00:00,0\n2012-11-29 01:00:00,0\n2012-11-29 02:00:00,0\n2012-11-29 04:00:00,0\n2012-11-29 05:00:00,0\n2012-11-29 06:00:00,0\n2012-11-29 07:00:00,0\n2012-11-29 08:00:00,0\n2012-11-29 09:00:00,0\n2012-11-29 10:00:00,0\n2012-11-29 11:00:00,0\n2012-11-29 12:00:00,0\n2012-11-29 13:00:00,0\n2012-11-29 14:00:00,0\n2012-11-29 15:00:00,0\n2012-11-29 16:00:00,0\n2012-11-29 17:00:00,0\n2012-11-29 18:00:00,0\n2012-11-29 19:00:00,0\n2012-11-29 20:00:00,0\n2012-11-29 21:00:00,0\n2012-11-29 22:00:00,0\n2012-11-29 23:00:00,0\n2012-11-30 00:00:00,0\n2012-11-30 01:00:00,0\n2012-11-30 02:00:00,0\n2012-11-30 03:00:00,0\n2012-11-30 04:00:00,0\n2012-11-30 05:00:00,0\n2012-11-30 06:00:00,0\n2012-11-30 07:00:00,0\n2012-11-30 08:00:00,0\n2012-11-30 09:00:00,0\n2012-11-30 10:00:00,0\n2012-11-30 11:00:00,0\n2012-11-30 12:00:00,0\n2012-11-30 13:00:00,0\n2012-11-30 14:00:00,0\n2012-11-30 15:00:00,0\n2012-11-30 16:00:00,0\n2012-11-30 17:00:00,0\n2012-11-30 18:00:00,0\n2012-11-30 19:00:00,0\n2012-11-30 20:00:00,0\n2012-11-30 21:00:00,0\n2012-11-30 22:00:00,0\n2012-11-30 23:00:00,0\n2012-12-20 00:00:00,0\n2012-12-20 01:00:00,0\n2012-12-20 02:00:00,0\n2012-12-20 03:00:00,0\n2012-12-20 04:00:00,0\n2012-12-20 05:00:00,0\n2012-12-20 06:00:00,0\n2012-12-20 07:00:00,0\n2012-12-20 08:00:00,0\n2012-12-20 09:00:00,0\n2012-12-20 10:00:00,0\n2012-12-20 11:00:00,0\n2012-12-20 12:00:00,0\n2012-12-20 13:00:00,0\n2012-12-20 14:00:00,0\n2012-12-20 15:00:00,0\n2012-12-20 16:00:00,0\n2012-12-20 17:00:00,0\n2012-12-20 18:00:00,0\n2012-12-20 19:00:00,0\n2012-12-20 20:00:00,0\n2012-12-20 21:00:00,0\n2012-12-20 22:00:00,0\n2012-12-20 23:00:00,0\n2012-12-21 00:00:00,0\n2012-12-21 01:00:00,0\n2012-12-21 02:00:00,0\n2012-12-21 03:00:00,0\n2012-12-21 04:00:00,0\n2012-12-21 05:00:00,0\n2012-12-21 06:00:00,0\n2012-12-21 07:00:00,0\n2012-12-21 08:00:00,0\n2012-12-21 09:00:00,0\n2012-12-21 10:00:00,0\n2012-12-21 11:00:00,0\n2012-12-21 12:00:00,0\n2012-12-21 13:00:00,0\n2012-12-21 14:00:00,0\n2012-12-21 15:00:00,0\n2012-12-21 16:00:00,0\n2012-12-21 17:00:00,0\n2012-12-21 18:00:00,0\n2012-12-21 19:00:00,0\n2012-12-21 20:00:00,0\n2012-12-21 21:00:00,0\n2012-12-21 22:00:00,0\n2012-12-21 23:00:00,0\n2012-12-22 00:00:00,0\n2012-12-22 01:00:00,0\n2012-12-22 02:00:00,0\n2012-12-22 03:00:00,0\n2012-12-22 04:00:00,0\n2012-12-22 05:00:00,0\n2012-12-22 06:00:00,0\n2012-12-22 07:00:00,0\n2012-12-22 08:00:00,0\n2012-12-22 09:00:00,0\n2012-12-22 10:00:00,0\n2012-12-22 11:00:00,0\n2012-12-22 12:00:00,0\n2012-12-22 13:00:00,0\n2012-12-22 14:00:00,0\n2012-12-22 15:00:00,0\n2012-12-22 16:00:00,0\n2012-12-22 17:00:00,0\n2012-12-22 18:00:00,0\n2012-12-22 19:00:00,0\n2012-12-22 20:00:00,0\n2012-12-22 21:00:00,0\n2012-12-22 22:00:00,0\n2012-12-22 23:00:00,0\n2012-12-23 00:00:00,0\n2012-12-23 01:00:00,0\n2012-12-23 02:00:00,0\n2012-12-23 03:00:00,0\n2012-12-23 04:00:00,0\n2012-12-23 05:00:00,0\n2012-12-23 06:00:00,0\n2012-12-23 07:00:00,0\n2012-12-23 08:00:00,0\n2012-12-23 09:00:00,0\n2012-12-23 10:00:00,0\n2012-12-23 11:00:00,0\n2012-12-23 12:00:00,0\n2012-12-23 13:00:00,0\n2012-12-23 14:00:00,0\n2012-12-23 15:00:00,0\n2012-12-23 16:00:00,0\n2012-12-23 17:00:00,0\n2012-12-23 18:00:00,0\n2012-12-23 19:00:00,0\n2012-12-23 20:00:00,0\n2012-12-23 21:00:00,0\n2012-12-23 22:00:00,0\n2012-12-23 23:00:00,0\n2012-12-24 00:00:00,0\n2012-12-24 01:00:00,0\n2012-12-24 02:00:00,0\n2012-12-24 03:00:00,0\n2012-12-24 05:00:00,0\n2012-12-24 06:00:00,0\n2012-12-24 07:00:00,0\n2012-12-24 08:00:00,0\n2012-12-24 09:00:00,0\n2012-12-24 10:00:00,0\n2012-12-24 11:00:00,0\n2012-12-24 12:00:00,0\n2012-12-24 13:00:00,0\n2012-12-24 14:00:00,0\n2012-12-24 15:00:00,0\n2012-12-24 16:00:00,0\n2012-12-24 17:00:00,0\n2012-12-24 18:00:00,0\n2012-12-24 19:00:00,0\n2012-12-24 20:00:00,0\n2012-12-24 21:00:00,0\n2012-12-24 22:00:00,0\n2012-12-24 23:00:00,0\n2012-12-25 00:00:00,0\n2012-12-25 01:00:00,0\n2012-12-25 02:00:00,0\n2012-12-25 04:00:00,0\n2012-12-25 05:00:00,0\n2012-12-25 06:00:00,0\n2012-12-25 07:00:00,0\n2012-12-25 08:00:00,0\n2012-12-25 09:00:00,0\n2012-12-25 10:00:00,0\n2012-12-25 11:00:00,0\n2012-12-25 12:00:00,0\n2012-12-25 13:00:00,0\n2012-12-25 14:00:00,0\n2012-12-25 15:00:00,0\n2012-12-25 16:00:00,0\n2012-12-25 17:00:00,0\n2012-12-25 18:00:00,0\n2012-12-25 19:00:00,0\n2012-12-25 20:00:00,0\n2012-12-25 21:00:00,0\n2012-12-25 22:00:00,0\n2012-12-25 23:00:00,0\n2012-12-26 00:00:00,0\n2012-12-26 01:00:00,0\n2012-12-26 02:00:00,0\n2012-12-26 03:00:00,0\n2012-12-26 04:00:00,0\n2012-12-26 05:00:00,0\n2012-12-26 06:00:00,0\n2012-12-26 07:00:00,0\n2012-12-26 08:00:00,0\n2012-12-26 09:00:00,0\n2012-12-26 10:00:00,0\n2012-12-26 11:00:00,0\n2012-12-26 12:00:00,0\n2012-12-26 13:00:00,0\n2012-12-26 14:00:00,0\n2012-12-26 15:00:00,0\n2012-12-26 16:00:00,0\n2012-12-26 17:00:00,0\n2012-12-26 18:00:00,0\n2012-12-26 19:00:00,0\n2012-12-26 20:00:00,0\n2012-12-26 21:00:00,0\n2012-12-26 22:00:00,0\n2012-12-26 23:00:00,0\n2012-12-27 00:00:00,0\n2012-12-27 01:00:00,0\n2012-12-27 02:00:00,0\n2012-12-27 03:00:00,0\n2012-12-27 04:00:00,0\n2012-12-27 05:00:00,0\n2012-12-27 06:00:00,0\n2012-12-27 07:00:00,0\n2012-12-27 08:00:00,0\n2012-12-27 09:00:00,0\n2012-12-27 10:00:00,0\n2012-12-27 11:00:00,0\n2012-12-27 12:00:00,0\n2012-12-27 13:00:00,0\n2012-12-27 14:00:00,0\n2012-12-27 15:00:00,0\n2012-12-27 16:00:00,0\n2012-12-27 17:00:00,0\n2012-12-27 18:00:00,0\n2012-12-27 19:00:00,0\n2012-12-27 20:00:00,0\n2012-12-27 21:00:00,0\n2012-12-27 22:00:00,0\n2012-12-27 23:00:00,0\n2012-12-28 00:00:00,0\n2012-12-28 01:00:00,0\n2012-12-28 02:00:00,0\n2012-12-28 03:00:00,0\n2012-12-28 04:00:00,0\n2012-12-28 05:00:00,0\n2012-12-28 06:00:00,0\n2012-12-28 07:00:00,0\n2012-12-28 08:00:00,0\n2012-12-28 09:00:00,0\n2012-12-28 10:00:00,0\n2012-12-28 11:00:00,0\n2012-12-28 12:00:00,0\n2012-12-28 13:00:00,0\n2012-12-28 14:00:00,0\n2012-12-28 15:00:00,0\n2012-12-28 16:00:00,0\n2012-12-28 17:00:00,0\n2012-12-28 18:00:00,0\n2012-12-28 19:00:00,0\n2012-12-28 20:00:00,0\n2012-12-28 21:00:00,0\n2012-12-28 22:00:00,0\n2012-12-28 23:00:00,0\n2012-12-29 00:00:00,0\n2012-12-29 01:00:00,0\n2012-12-29 02:00:00,0\n2012-12-29 03:00:00,0\n2012-12-29 04:00:00,0\n2012-12-29 05:00:00,0\n2012-12-29 06:00:00,0\n2012-12-29 07:00:00,0\n2012-12-29 08:00:00,0\n2012-12-29 09:00:00,0\n2012-12-29 10:00:00,0\n2012-12-29 11:00:00,0\n2012-12-29 12:00:00,0\n2012-12-29 13:00:00,0\n2012-12-29 14:00:00,0\n2012-12-29 15:00:00,0\n2012-12-29 16:00:00,0\n2012-12-29 17:00:00,0\n2012-12-29 18:00:00,0\n2012-12-29 19:00:00,0\n2012-12-29 20:00:00,0\n2012-12-29 21:00:00,0\n2012-12-29 22:00:00,0\n2012-12-29 23:00:00,0\n2012-12-30 00:00:00,0\n2012-12-30 01:00:00,0\n2012-12-30 02:00:00,0\n2012-12-30 03:00:00,0\n2012-12-30 04:00:00,0\n2012-12-30 05:00:00,0\n2012-12-30 06:00:00,0\n2012-12-30 07:00:00,0\n2012-12-30 08:00:00,0\n2012-12-30 09:00:00,0\n2012-12-30 10:00:00,0\n2012-12-30 11:00:00,0\n2012-12-30 12:00:00,0\n2012-12-30 13:00:00,0\n2012-12-30 14:00:00,0\n2012-12-30 15:00:00,0\n2012-12-30 16:00:00,0\n2012-12-30 17:00:00,0\n2012-12-30 18:00:00,0\n2012-12-30 19:00:00,0\n2012-12-30 20:00:00,0\n2012-12-30 21:00:00,0\n2012-12-30 22:00:00,0\n2012-12-30 23:00:00,0\n2012-12-31 00:00:00,0\n2012-12-31 01:00:00,0\n2012-12-31 02:00:00,0\n2012-12-31 03:00:00,0\n2012-12-31 04:00:00,0\n2012-12-31 05:00:00,0\n2012-12-31 06:00:00,0\n2012-12-31 07:00:00,0\n2012-12-31 08:00:00,0\n2012-12-31 09:00:00,0\n2012-12-31 10:00:00,0\n2012-12-31 11:00:00,0\n2012-12-31 12:00:00,0\n2012-12-31 13:00:00,0\n2012-12-31 14:00:00,0\n2012-12-31 15:00:00,0\n2012-12-31 16:00:00,0\n2012-12-31 17:00:00,0\n2012-12-31 18:00:00,0\n2012-12-31 19:00:00,0\n2012-12-31 20:00:00,0\n2012-12-31 21:00:00,0\n2012-12-31 22:00:00,0\n2012-12-31 23:00:00,0"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html",
    "title": "02wk-005: 타이타닉, 첫 분석",
    "section": "",
    "text": "from autogluon.tabular import TabularDataset, TabularPredictor"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#a.-데이터",
    "title": "02wk-005: 타이타닉, 첫 분석",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"./titanic/train.csv\")\ntst = TabularDataset(\"./titanic/test.csv\")"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, 첫 분석",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_162629/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, 첫 분석",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr)\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_162629/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.27 GB / 490.57 GB (62.4%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    126262.11 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.16s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.33s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.49s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.58s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_162629/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fa18f4444f0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.193071  4.142821                0.000378           0.218161            2       True         14\n1         LightGBMLarge   0.832402       0.003180  0.422871                0.003180           0.422871            1       True         13\n2        NeuralNetTorch   0.832402       0.009243  1.488933                0.009243           1.488933            1       True         12\n3              CatBoost   0.826816       0.003432  0.526762                0.003432           0.526762            1       True          7\n4       NeuralNetFastAI   0.826816       0.007648  1.326021                0.007648           1.326021            1       True         10\n5              LightGBM   0.821229       0.003042  0.157547                0.003042           0.157547            1       True          4\n6            LightGBMXT   0.815642       0.003350  0.197612                0.003350           0.197612            1       True          3\n7        ExtraTreesGini   0.815642       0.050255  0.848478                0.050255           0.848478            1       True          8\n8      RandomForestEntr   0.815642       0.091255  0.525193                0.091255           0.525193            1       True          6\n9      RandomForestGini   0.815642       0.162890  0.340251                0.162890           0.340251            1       True          5\n10              XGBoost   0.810056       0.004424  0.141125                0.004424           0.141125            1       True         11\n11       ExtraTreesEntr   0.810056       0.060458  0.398653                0.060458           0.398653            1       True          9\n12       KNeighborsDist   0.653631       0.001958  0.007848                0.001958           0.007848            1       True          2\n13       KNeighborsUnif   0.653631       0.004457  0.087323                0.004457           0.087323            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.193071\n4.142821\n0.000378\n0.218161\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.003180\n0.422871\n0.003180\n0.422871\n1\nTrue\n13\n\n\n2\nNeuralNetTorch\n0.832402\n0.009243\n1.488933\n0.009243\n1.488933\n1\nTrue\n12\n\n\n3\nCatBoost\n0.826816\n0.003432\n0.526762\n0.003432\n0.526762\n1\nTrue\n7\n\n\n4\nNeuralNetFastAI\n0.826816\n0.007648\n1.326021\n0.007648\n1.326021\n1\nTrue\n10\n\n\n5\nLightGBM\n0.821229\n0.003042\n0.157547\n0.003042\n0.157547\n1\nTrue\n4\n\n\n6\nLightGBMXT\n0.815642\n0.003350\n0.197612\n0.003350\n0.197612\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.050255\n0.848478\n0.050255\n0.848478\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.091255\n0.525193\n0.091255\n0.525193\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.162890\n0.340251\n0.162890\n0.340251\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.004424\n0.141125\n0.004424\n0.141125\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.060458\n0.398653\n0.060458\n0.398653\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.001958\n0.007848\n0.001958\n0.007848\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.004457\n0.087323\n0.004457\n0.087323\n1\nTrue\n1\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, 첫 분석.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, 첫 분석",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9349046015712682\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\n#tst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01_invalidformat.csv\")\ntst.assign(Survived=predictr.predict(tst)).loc[:,['PassengerId','Survived']].to_csv(\"submission_01.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "",
    "text": "ref: https://www.jstatsoft.org/article/view/v012i08"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#ebayesthresh로-무엇을-할-수-있는가",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Ebayesthresh로 무엇을 할 수 있는가?",
    "text": "Ebayesthresh로 무엇을 할 수 있는가?\n아래와 같은 상황을 가정하자.\n\\[X_i = \\mu_i +\\epsilon_i.\\]\n여기에서 아래를 가정한다.\n\n\\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\)\neach \\(\\mu_i\\) is zero with probability \\((1−w)\\), while, with probability \\(w\\), \\(\\mu_i\\) is drawn from a symmetric heavy-tailed density \\(\\gamma\\).\n\n일반적으로 \\(w\\), 즉 \\(\\mu_i\\)가 0이 아닐 확률은 매우 작은값으로 설정된다. 따라서 위와 같은 구조로 \\(\\epsilon_i\\)와 \\(\\mu_i\\)를 생성하면 아래와 같이 된다.\n\n\\(\\epsilon_i\\): 절대값이 작은 신호들이 dense하게 있음.\n\\(\\mu_i\\): 절대값이 큰 신호들이 sparse하게 있음. (sparse한 이유는 \\(w\\)가 작으므로)\n\n따라서 \\(X_i\\)의 모양은 아래의 그림의 왼쪽과 같다.\n\n이 논문의 목표는 왼쪽의 그림 \\(X_i= \\mu_i +\\epsilon_i\\)로부터 오른쪽의 그림 \\(\\hat{\\mu}_i\\)을 구하는 것이다. 즉 작은 절대값의 노이즈 \\(\\epsilon_i\\)에서 큰 절대값의 신호 \\(\\mu_i\\)를 골라내는 일을 목표로 한다. 저자들은 이러한 작업을 “건초더미에서 바늘찾기”라는 말로 비유하였다. 이러한 “건초더미에서 바늘찾기”는 여러 분야에 응용될 수 있다. 구체적으로는 천문학, 이미지프로세싱, 데이터마이닝, 모형선택등에 사용될 수 있다고 한다. 언급한 분야에 대한 자세한 discussion은 Johnstone and Silverman (2004)에서 찾을 수 있다. 또한 “건초더미에서 바늘찾기”는 위에서 언급한 분야 이외에 퓨리에, 웨이블릿 혹은 다른 dictionaries에 의한 함수추정문제를 해결할 수 있다. 이는 퓨리에나 웨이블릿변환과 같은 multiscale trasnform이 원래 신호를 sparese한 구조로 바꾸기 때문이다. 즉 퓨리에변환 웨이블릿변환으로 underlying function을 추정할 수 있다는 의미이다. 우리는 이러한 접근법에 좀 더 초점을 맞추도록 하겠다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#간단한-사용법",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "간단한 사용법",
    "text": "간단한 사용법\nR을 이용하여 Ebayesthresh를 사용하는 간단한 방법을 살펴보도록 하자. 논문에 표현된 그림1을 재현하여 보자.\n\nlibrary(EbayesThresh)\n\n\nset.seed(1)\nx &lt;- rnorm(1000) + sample(c( runif(25,-7,7), rep(0,975)))\nplot(x,type='l',lwd=0.2)\n\n\n\n\n위와 같은 자료 \\(X_i\\)를 관측하였다고 가정하자. 이 신호에는 “건초(\\(\\epsilon_i\\))”더미에 25개의 “바늘(\\(\\mu_i\\))”이 섞여있다. 여기에서 “바늘”만 골라내는 코드는 아래와 같이 작성할 수 있다.\n\nmuhat &lt;- ebayesthresh(x, sdev=1)\n\n결과를 시각화하면 아래와 같다.\n\nplot(x,type='l',lwd=0.2)\nlines(muhat,col=2,lwd=2)"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#arguments",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "arguments",
    "text": "arguments\n일반적으로 ebayesthresh 함수를 사용하는 방법은 아래와 같다.\n\nmuhat &lt;- ebayesthresh(\n    x,\n    prior = \"laplace\", \n    a = 0.5, \n    bayesfac = FALSE, \n    sdev = NA, \n    verbose = FALSE, \n    threshrule = \"median\"\n)\n\nprior, a: \\(\\mu_i\\)의 density. 보통 \\(\\frac{1}{2}a \\exp(-a|u|)\\)라고 가정한다. parameter \\(a\\)는 Section 2.1에서 자시해 나옴.\nbayesfac, threshrule: Section 2.2, 2.3에 자세히 나온다.\nsdev: \\(\\epsilon_i\\)의 sd를 의미한다. 이 값을 알고 있다면 설정하면 되지만 보통은 이 값을 모른다고 가정한다. \\(\\epsilon_i\\)의 sd를 모르는 경우는 observed data로 부터 추정하는데 보통 \\({\\tt median}(|X_i|)\\)로 추정한다.\n\\(\\epsilon_i\\)의 sd를 \\({\\tt median}(|X_i|)\\)로 추정하는 motivation을 이해하는 것이 중요하다. 이는 sparse assumption of \\(\\mu_i\\)에서 시작한다. 신호 \\(\\mu_i\\)가 합리적인 수준에서 sparse하다면 median absolute value of \\(X_i\\)는 \\(\\mu_i\\)의 값들과 상관이 없을 것이다. 하지만 당연히 신호가 sparse하지 않다면 이러한 방식으로 sdev를 추정하는 것은 매우 조심스럽게 수행되어야 할 것이다.\n\nn &lt;- 1000\nx &lt;- rnorm(n) + sample(c(runif(25,-7,7), rep(0,n-25)))\nprint(sd(x))\nprint(median(abs(x)))\n\n[1] 1.117016\n[1] 0.6787613\n\n\n\n실제로는 잘 추론하지 못하는 것 같다?"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#원리",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "원리",
    "text": "원리\n어떻게 \\(\\hat{\\mu}_i\\)를 추정할 수 있을까? 가장 간단한 방법은 thresholding이다.\n많은 실제예제에서 \\(\\mu_i\\)는 어떤 의미에서 (in some sense) sparse하다고 여길 수 있다. EbayesThresh 패키지는 이처럼 \\(\\mu_i\\)가 sparse하다는 구조 (혹은 가정)을 이용하여 \\(\\mu_i\\)를 적절하게 추정한다.\nSparsity를 이용하는 자연스러운 방법은 threshoding이다: 여기에서 threshold의 값 \\(t\\)를 너무 크게 잡으면 신호를 잡음으로 잘못 판단할 것이고 \\(t\\)의 값이 너무 작다면 잡음을 신호로 잘못 판단할 수 있다. 따라서 \\(t\\)의 선택은 이 양쪽 기준사이의 tradeoff가 있는데 EbayesThresh는 이러한 tradeoff를 자동으로 조정하는 효과가 있다.\n\n\\(\\mu_i\\)는 \\(w\\)의 확률로 0 이며 \\((1-w)\\)의 확률로 0이 아니다. \\(\\mu_i\\)가 0이 아닐경우에는 symmetric heavy-tailed density \\(\\gamma\\)에서 추출된다고 가정한다. 여기에서 prior에 대한 key parameter인 \\(w\\)는 데이터로부터 자동으로 추정된다. (marginal maximum likelihood 를 이용한다) 그리고 추정된 \\(w\\)는 Bayesian model로 다시 대입된다.\n\\(w\\)가 추정되면 Bayesian model은 thresholding procedure를 수행할 수 있다. 왜냐하면 \\(w\\)를 추정하면 \\(t(w)\\)를 선택한다는 말과 같은말이기 때문이다.\n\nargument"
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#the-bayesian-model",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "The Bayesian model",
    "text": "The Bayesian model\n\\[X_i \\sim N(\\mu_i,1)\\]\n\\(f_{\\text{prior}}(\\mu)=(1-w)\\delta_0(\\mu)+w \\gamma_a(\\mu), \\quad \\gamma_a(\\mu)=\\frac{1}{2}a\\exp(-a|\\mu|)\\)\n여기에서 \\(\\gamma_a(\\mu)\\)는 하나의 예시일 뿐이다. Ebayesthresh에 디폴트로 설정된 prior=\"laplace\"를 셋팅하면 \\(\\gamma_a(\\mu)\\)가 사용된다. \\(\\gamma\\)의 선택은 tail이 polynomial rates로 줄어드는 어떠한 분포를 사용해도 무방하다. 저자들은 quasi-Cauchy분포를 제안하였는데 이는 Johnstone and Sliverman이 만든 theoretical assumption을 만족하는 분포중 가장 꼬리가 두꺼운 분포이다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#thresholding-rules",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Thresholding rules",
    "text": "Thresholding rules\n모수 \\(\\mu\\)는 사전분포(prior distribution)를 가진다고 가정하고 \\(X \\sim N(\\mu,1)\\)이라고 가정하자. 이 경우 \\(X=x\\)가 given되었을 경우 \\(\\mu\\)의 사후분포(posterior distribution)를 구할 수 있다. (자세한 내용은 Section 6을 참고해야함) 사후분포의 중앙값을 \\(\\hat{\\mu}(x;w)\\)라고 하자. (사후분포의 중앙값이 \\(w\\)에 영향받는 이유는 사전분포가 \\(w\\)에 depend하기 때문이다. 여기에서 \\(w\\)는 marginal MLE로 적절히 추론한다고 가정한다)\n\\(X_i\\)는 독립이라고 가정한다. 여기에서 \\(X_i\\)가 독립이 아니라면 약간의 정보손실이 있을 수 있다. 하지만 \\(X_i\\) 사이에 너무 많은 dependency가 존재하는 경우가 아니라면 Ebayesthresh는 어느정도 합리적인 결과를 제공한다.\n만약에 bayesfac=TRUE를 사용하면 \\(\\mu\\)의 사후분포의 중앙값 대신에 Bayes factor threshold 를 쓸 수도 있다."
  },
  {
    "objectID": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "href": "posts/2_Studies/Reviews/2022-12-23-Ebayesthresh.html#choosing-the-threshold",
    "title": "[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding",
    "section": "Choosing the threshold",
    "text": "Choosing the threshold\n\\(X_i\\)의 marginal density는\n\\((1-w)\\phi(x) +w(\\gamma \\star \\phi)(x)\\)\n\\(l(w) = \\sum_{i=1}^{n}\\log \\big\\{(1-w)\\phi(X_i)+wg(X_i) \\big\\}\\)\n와 같이 정의가능하다. 단, 여기에서 \\(g:= \\gamma\\star \\phi\\) 이다.\n이제 우리는 아래의 식을 풀면된다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]\n여기에서 \\(\\sqrt{2\\log n}\\)은 흔히 말하는 universal threshold 이다.\n만약에 \\(w\\)이외에 \\(a\\)도 추정해야 한다면 아래와 같이 추정할 수 있다.\n\\[\\underset{w}{\\operatorname{argmax}} l(w)\\quad\\quad \\text{subject to}\\quad t(w) \\leq \\sqrt{2\\log n}\\]"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html",
    "href": "posts/2_Studies/PyG/ls2.html",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "",
    "text": "import torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#정보",
    "href": "posts/2_Studies/PyG/ls2.html#정보",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "정보",
    "text": "정보\n- 기본정보: ENZYMES dataset\n\n(ChatGPT) ENZYMES는 그래프 분류를 위한 벤치마크 데이터셋 중 하나입니다. 이 데이터셋은 600개의 그래프로 구성되어 있으며, 6개의 클래스로 분류됩니다. 각 그래프는 효소(enzyme) 분자의 구조를 나타내며, 그래프의 노드는 원자(atom)를 나타내고, 엣지(edge)는 원자 간의 연결을 나타냅니다. ENZYMES 데이터셋은 화학 및 생물 정보학 분야에서 그래프 분류 알고리즘의 성능을 평가하기 위해 사용될 수 있습니다. 그래프 분류 알고리즘은 주어진 그래프를 특정 클래스 레이블로 분류하는 작업을 수행하는데 사용됩니다. 예를 들어, ENZYMES 데이터셋의 그래프는 특정 효소 종류를 나타내며, 그래프 분류 알고리즘은 주어진 효소 그래프가 어떤 종류의 효소인지 예측할 수 있습니다. PyG를 사용하여 ENZYMES 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 필요한 전처리를 자동으로 수행할 수 있습니다. 그래프 데이터를 다루는 머신 러닝 모델을 구축하고 훈련시키기 위해 ENZYMES 데이터셋을 사용할 수 있습니다.\n\n\ndataset # 데이터셋 이름\n\nENZYMES(600)\n\n\n\nlen(dataset) # 이 데이터셋에는 600개의 그래프가 있음\n\n600\n\n\n\ndataset.num_classes # 6개의 클래스\n\n6\n\n\n\ndataset.num_node_features # 각 노드에는 3개의 피처가 있음\n\n3\n\n\n- 600개의 그래프중 첫번째 그래프에 접근\n\ndataset[0]\n\nData(edge_index=[2, 168], x=[37, 3], y=[1])\n\n\n\nx=[37, 3]: \\(|{\\cal V}|=37\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 168]: \\(|{\\cal E}|=168\\)"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#traintest-분리",
    "href": "posts/2_Studies/PyG/ls2.html#traintest-분리",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "Train/Test 분리",
    "text": "Train/Test 분리\n- 600개의 그래프중 540를 train으로, 60개를 test로\n\ntrain_dataset = dataset[:540]\ntest_dataset = dataset[540:]"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#정보-1",
    "href": "posts/2_Studies/PyG/ls2.html#정보-1",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "정보",
    "text": "정보\n\nChatGPT: Cora는 그래프 분류를 위한 벤치마크 데이터셋 중 하나로, PyG에서도 사용할 수 있습니다. 이 데이터셋은 기계 학습 및 정보 검색 분야에서 널리 사용되는 학술 논문들의 인용 네트워크를 나타냅니다. Cora 데이터셋은 컴퓨터 과학 분야의 논문을 대상으로 합니다. 각 논문은 그래프의 노드로 표현되며, 노드는 논문을 나타냅니다. 노드 간의 엣지는 논문들 사이의 인용 관계를 나타냅니다. 따라서 Cora 데이터셋은 논문의 텍스트 기반 정보와 인용 관계에 대한 그래프 구조를 제공합니다. Cora 데이터셋은 7개의 클래스로 분류되며, 각 논문은 특성 벡터(feature vector)로 표현됩니다. 이 특성 벡터에는 논문의 단어 등 다양한 정보가 포함될 수 있습니다. PyG를 사용하여 Cora 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 전처리를 자동으로 수행할 수 있습니다. 이를 통해 머신 러닝 모델을 훈련시켜 Cora 데이터셋의 논문을 분류하거나 다양한 작업을 수행할 수 있습니다.\n\n- 기본정보\n\nlen(dataset) # 하나의 그래프가 있음\n\n1\n\n\n\ndataset.num_classes # 7개의 클래스가 있음\n\n7\n\n\n\ndataset.num_node_features # 각 노드는 1433개의 특징이 있음. (논문에 포함된 단어등 다양한 특성이 담겨있을 수 있음) \n\n1433\n\n\n- 그래프에 접근\n\ndataset[0] # 기본정보\n\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n\n\n\nx=[2708, 1433]: 2708개의 논문이 있고, 각 논문은 1433개의 특징벡터들로 이루어져 있음.\nedge_index=[2, 10556]: 논문간의 인용은 약 10556.\ny=[2708]:\n\n\ndataset[0].x.shape # 2708개의 논문이 있고 1433개의 특징벡터를 가짐\n\ntorch.Size([2708, 1433])\n\n\n\ndataset[0].y.unique() # 논문이 7개의 카테고리로 분류되는듯\n\ntensor([0, 1, 2, 3, 4, 5, 6])"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls2.html#traintest-이미-분리되어-있음",
    "href": "posts/2_Studies/PyG/ls2.html#traintest-이미-분리되어-있음",
    "title": "[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)",
    "section": "Train/Test (이미 분리되어 있음)",
    "text": "Train/Test (이미 분리되어 있음)\n\ndataset[0].train_mask \n# dataset[0].train_mask 는 True, False로 이루어져 있는 길이가 2708(=노드수=논문수)인 벡터\n# 여기에서 True인 노드만 훈련함\n\ntensor([ True,  True,  True,  ..., False, False, False])\n\n\n\ndataset[0].train_mask.sum() # 140개의 노드만 훈련함? \n\ntensor(140)\n\n\n\ndataset[0].val_mask.sum() # val은 500개의 노드?\n\ntensor(500)\n\n\n\ndataset[0].test_mask.sum() # test set은 1000?\n\ntensor(1000)"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls6.html",
    "href": "posts/2_Studies/PyG/ls6.html",
    "title": "[PyG] lesson6: GCN",
    "section": "",
    "text": "Kipf and Welling (2016): https://arxiv.org/abs/1609.02907\n\nimport torch\nimport torch_geometric"
  },
  {
    "objectID": "posts/2_Studies/PyG/ls6.html#예제",
    "href": "posts/2_Studies/PyG/ls6.html#예제",
    "title": "[PyG] lesson6: GCN",
    "section": "예제",
    "text": "예제\n- data\n\nx = torch.tensor([[20],\n                  [21],\n                  [19],\n                  [1],\n                  [2],\n                  [1]], dtype=torch.float)\n\nedge_index = torch.tensor([[0, 1, 2, 0, 1, 2, 3, 4, 3, 5, 4, 5],\n                           [1, 0, 0, 2, 2, 1, 4, 3, 5, 3, 5, 4]], dtype=torch.long)\n\ny = torch.tensor([1,1,1,0,0,0], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index, y=y) \n#data.train_mask = torch.tensor([True,False,True,True,False,True])\n#data.test_mask = torch.tensor([False,True,False,False,True,False])\ndata\n\nData(x=[6, 1], edge_index=[2, 12], y=[6])\n\n\n- GCNConv\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n        [ 11.6402, -15.0337, -13.0234, -15.7613],\n        [ 11.6402, -15.0337, -13.0234, -15.7613],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n        [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.5820],\n         [-0.7517],\n         [-0.6512],\n         [-0.7881]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[ 0.5820],\n        [-0.7517],\n        [-0.6512],\n        [-0.7881]], requires_grad=True)\n\n\n\nA = torch.tensor([[0., 1., 1., 0., 0., 0.],\n                  [1., 0., 1., 0., 0., 0.],\n                  [1., 1., 0., 0., 0., 0.],\n                  [0., 0., 0., 0., 1., 1.],\n                  [0., 0., 0., 1., 0., 1.],\n                  [0., 0., 0., 1., 1., 0.]])\nAtilde = A+torch.eye(6)\nAtilde\n\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.]])\n\n\n\nAtilde@data.x@W.T/3, gconv(data.x,data.edge_index)\n\n(tensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;DivBackward0&gt;),\n tensor([[ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [ 11.6402, -15.0337, -13.0234, -15.7613],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508],\n         [  0.7760,  -1.0022,  -0.8682,  -1.0508]], grad_fn=&lt;AddBackward0&gt;))\n\n\n- 즉 아래의 수식에서\n\\[H^{(l+1)} = \\sigma\\big(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H^{(l)}W^{(l)}\\big).\\]\n\\(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}H^{(l)}W^{(l)}\\)를 계산하는 Layer가 torch_geometric.nn.GCNConv() 으로 구현되어있음."
  },
  {
    "objectID": "posts/2_Studies/PyG/ls3.html",
    "href": "posts/2_Studies/PyG/ls3.html",
    "title": "[PyG] lesson3: 미니배치",
    "section": "",
    "text": "Download notebook\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/2_Studies/PyG/ls3.ipynb\n\n\n미니배치\n\nimport torch_geometric\n\n\ndataset = torch_geometric.datasets.TUDataset(\n    root='/tmp/ENZYMES', \n    name='ENZYMES'\n)\n\n\n(ChatGPT) ENZYMES는 그래프 분류를 위한 벤치마크 데이터셋 중 하나입니다. 이 데이터셋은 600개의 그래프로 구성되어 있으며, 6개의 클래스로 분류됩니다. 각 그래프는 효소(enzyme) 분자의 구조를 나타내며, 그래프의 노드는 원자(atom)를 나타내고, 엣지(edge)는 원자 간의 연결을 나타냅니다. ENZYMES 데이터셋은 화학 및 생물 정보학 분야에서 그래프 분류 알고리즘의 성능을 평가하기 위해 사용될 수 있습니다. 그래프 분류 알고리즘은 주어진 그래프를 특정 클래스 레이블로 분류하는 작업을 수행하는데 사용됩니다. 예를 들어, ENZYMES 데이터셋의 그래프는 특정 효소 종류를 나타내며, 그래프 분류 알고리즘은 주어진 효소 그래프가 어떤 종류의 효소인지 예측할 수 있습니다. PyG를 사용하여 ENZYMES 데이터셋을 초기화하면 해당 데이터셋을 다운로드하고 필요한 전처리를 자동으로 수행할 수 있습니다. 그래프 데이터를 다루는 머신 러닝 모델을 구축하고 훈련시키기 위해 ENZYMES 데이터셋을 사용할 수 있습니다.\n\n\nlen(dataset) # 이 데이터셋에는 600개의 그래프가 있음\n\n600\n\n\n\ndataset.num_classes # 6개의 클래스\n\n6\n\n\n\ndataset.num_node_features # 각 노드에는 3개의 피처가 있음\n\n3\n\n\n- 600개의 그래프중 첫번째 그래프에 접근\n\ndataset[0]\n\nData(edge_index=[2, 168], x=[37, 3], y=[1])\n\n\n\nx=[37, 3]: \\(|{\\cal V}|=37\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 168]: \\(|{\\cal E}|=168\\)\n\n- 600개중에서 두번째 그래프에 접근\n\ndataset[1]\n\nData(edge_index=[2, 102], x=[23, 3], y=[1])\n\n\n\nx=[23, 3]: \\(|{\\cal V}|=23\\), \\(f \\in \\mathbb{R}^3\\)\nedge_index=[2, 102]: \\(|{\\cal E}|=102\\)\n\n- dataset \\(\\to\\) loader\n\nloader = torch_geometric.loader.DataLoader(dataset, batch_size=2, shuffle=False)\n\n\nfor i,batch in enumerate(loader):\n    if i&lt;5:\n        print(i,batch)\n\n0 DataBatch(edge_index=[2, 270], x=[60, 3], y=[2], batch=[60], ptr=[3])\n1 DataBatch(edge_index=[2, 182], x=[49, 3], y=[2], batch=[49], ptr=[3])\n2 DataBatch(edge_index=[2, 182], x=[47, 3], y=[2], batch=[47], ptr=[3])\n3 DataBatch(edge_index=[2, 384], x=[114, 3], y=[2], batch=[114], ptr=[3])\n4 DataBatch(edge_index=[2, 184], x=[55, 3], y=[2], batch=[55], ptr=[3])\n\n\n\n600개 그래프를 2개씩 묶어서 배치를 만듬\n\n\ndataset[0], dataset[1]\n\n(Data(edge_index=[2, 168], x=[37, 3], y=[1]),\n Data(edge_index=[2, 102], x=[23, 3], y=[1]))\n\n\n\n이게 합쳐져서 0 DataBatch(edge_index=[2, 270], x=[60, 3], y=[2], batch=[60], ptr=[3])\n\n\ndataset[2], dataset[3]\n\n(Data(edge_index=[2, 92], x=[25, 3], y=[1]),\n Data(edge_index=[2, 90], x=[24, 3], y=[1]))\n\n\n\n이게 합쳐져서 1 DataBatch(edge_index=[2, 182], x=[49, 3], y=[2], batch=[49], ptr=[3])"
  },
  {
    "objectID": "posts/1_Essays/2019-04-26-퓨리에변환.html",
    "href": "posts/1_Essays/2019-04-26-퓨리에변환.html",
    "title": "[Essays] 퓨리에 변환",
    "section": "",
    "text": "About this doc\n- 이번에는 퓨리에 표현들을 정리하도록 하겠다. 내생각엔 퓨리에 표현들도 벡터의 미분만큼 복잡한 것 같다. 정의가 너무 많고 그게 그거 같아서 그렇다. 이번기회에 깔끔하게 정리하도록 하자. 참고한 문헌은 아래와 같다.\n\nHaykin, S., & Van Veen, B. (2007). Signals and systems. John Wiley & Sons.\n\n\n\n들어가며\n- 우선 신호와 하나의 신호값을 구분하는 notation을 생각하자. 우리가 다루는 신호 즉 데이터는 값들의 집합이다. 우리가 시계열자료를 다룬다면 데이터는 아래와 같이 표현한다.\n\n\\(\\{x_i: i \\in \\mathbb{Z}\\}\\)\n\n이와 유시하게 우리가 다루는 자료가 \\(t \\in \\mathbb{R}\\)인 연속신호라면 아래와 같이 표현한다.\n\n\\(\\{x(t): t \\in \\mathbb{R}\\}\\)\n\n우리가 모든 \\(i \\in \\mathbb{Z}\\) 혹은 모든 \\(t \\in \\mathbb{R}\\)에서 신호를 다룰 생각이 없다면 아래와 같은 표현도 얼마든지 가능하다.\n\n\\(\\{x_i: i=0,1,\\dots, \\xi-1 \\}\\)\n\\(\\{x(t):t \\in (0,\\zeta) \\}\\)\n\n- 위와 같이 집합의 표현 없이 단독으로 \\(x_i\\), \\(x(t)\\)와 같이 쓰면 하나의 고정된 값 \\(i,t\\)에 대한 \\(x_i\\), \\(x(t)\\)로 이해하자. 솔직히 이렇게 꼭 신호를 엄밀하게 집합으로 정의하는게 유별나 보일수도 있다. 일반적으로 사람들은 \\(\\{x(t): ~t \\in \\mathbb{R} \\}\\) 대신에 보통 \\(x(t)\\)로 간단하게 줄여서 쓰곤한다.1 하지만 이 포스팅에 한정하여 위와 같이 집합의 형태로 엄밀하게 구분해 쓰도록 하자. 처음에는 익숙하지 않지만 나중에는 편리하다.\n1 나도 그렇다.\n\n퓨리에표현들\n- 지금부터 우리가 고려하는 모든 신호들은 기본적으로 (1) infinity range에서 정의된 신호라고 가정한다. 즉 연속신호이면 \\(\\mathbb{R}\\)에서 정의된다고 가정하고 이산신호면 \\(\\mathbb{Z}\\)에서 정의된다고 가정한다. 또한 우리가 분석하고자 하는 신호는 (2) integrable 하다고 가정한다. 이건 퓨리에표현들이 적분 혹은 무한합의 형태로 표현된다는 것을 상기하면 타당하여 보인다.\n- 즉 우리가 고려하는 신호는 인피니티-레인지에서 정의되며 인피니티-레인지에서 적분값이 유한한 연속신호 혹은 이산신호 임을 알 수 있다. 이러한 신호는 구체적으로는 아래와 같이 쓸 수 있다.\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\}\\)\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\}\\)\n\n- 그런데 integrable 한 함수들만을 고려하다 보면 우리가 다룰 수 있는 신호의 범위가 확 줄어들게 된다. 가령 예를 들어서 아래와 같은 신호는 적분을 하면 무한대가 나오기 때문에 intergrable 하지 않다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in \\mathbb{R} \\right\\}\\)\n\n이것은 좀 불합리해 보이는데 위의 신호는 주기신호라서 한 주기의 패턴만 분석하면 될것 같이 보이기 때문이다. 위의 신호는 intergrable 하지않지만 아래의 신호는 intergrable 하다.\n\n\\(\\left\\{x(t): x(t)=\\sin(t)+1 ,~ t \\in (0,2\\pi) \\right\\}\\)\n\n우리는 이런신호까지 분석하기로 한다. 이런신호를 분석할 수 있는 이유는 해석학 교재를 참고하면 된다.2\n2 사실 나도 잘 모름 (뭐 quotient group이런거 알아야 하는데 공부하려면 꽤 걸릴듯)- 아무튼 우리는 (1) 인피니티-레인지에서 정의되는 가지는 신호 (2) 인피니티-레인지에서 적분값이 잘 정의되는 신호, 혹은 한 주기만 적분해 보았을때 그 값이 잘 정의되는 주기신호 를 타겟팅해 분석한다. 즉 분석하는 신호는 구체적으로 아래의 4가지이다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt &lt;\\infty, ~ , x(t)=x(t+\\zeta), ~ t \\in \\mathbb{R}, \\right\\}.\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| &lt;\\infty,~ i, x_i=x_{i+\\xi},~ i \\in \\mathbb{Z} \\right\\}.\\)\n\n- 표현들을 정리하기에 앞서서 몇 가지 알아두어야 할 사항이 있다. (1) 시간축에서 연속인 신호는 주파수측에서는 비주기신호가 나온다. (2) 시간축에서 디스크릿한 신호는 주파수측에서는 주기신호이다. (3) 시간축에서 주기인 신호는 주파수에서는 디스크릿하다. (4) 시간축에서 비주기신호는 주파수에서 연속이다. 이 사실들을 종합하면 각각의 경우에 해당하는 퓨리에 표현들은 아래와 같은 특징을 가지고 있음을 알 수 있다.\ncase 1: 연속-비주기\n\n\\(\\left\\{x(t): \\int_{-\\infty}^{\\infty} |x(t)| dt &lt;\\infty,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{-\\infty}^{\\infty} |\\hat x(\\omega)| d\\omega &lt;\\infty,~ \\omega \\in \\mathbb{R} \\right\\}\\)\n\ncase 2: 연속-주기\n\n\\(\\left\\{x(t): \\int_{0}^{\\zeta} |x(t)| dt &lt;\\infty,~ x(t)=x(t+\\zeta),~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=-\\infty}^{\\infty} |\\hat x_k| &lt;\\infty,~ k \\in \\mathbb{Z} \\right\\}\\)\n\ncase 3: 이산-비주기\n\n\\(\\left\\{x_i: \\sum_{i=-\\infty}^{\\infty} |x_i| &lt;\\infty,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\int_{0}^{2\\pi} |\\hat x(\\omega)| d\\omega &lt;\\infty,~ \\hat x(\\omega)=\\hat x(\\omega+2\\pi),~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4: 이산-주기\n\n\\(\\left\\{x_i: \\sum_{i=0}^{\\xi-1} |x_i| &lt;\\infty,~ x_i=x_{i+\\xi},~ i \\in \\mathbb{R} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\sum_{k=0}^{\\xi-1} |\\hat x_k| &lt;\\infty,~\\hat x_k = \\hat x_{k+\\xi} ,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 여기에서 \\(\\zeta\\)는 (시간축에서) 연속신호의 주기라고 정의하고 \\(\\xi\\)는 (시간축에서) 이산신호의 주기라고 약속하자. 주파수영역이 디스크릿하게 나오면 FS라고 부르고 주파수영역이 컨티뉴어스하게 나오면 FT라고 부른다. 특이한점은 비주기-이산신호에 대한 FS \\(\\hat x(\\omega)\\)는 주파수 영역에서 주기가 \\(2\\pi\\)임을 파악할 수 있다. 이유는 궁금해하지말자. (내생각에 그냥 \\(\\omega\\)를 적당히 스케일링하여 주기를 \\(2\\pi\\)로 맞췄을 거다.)\n- 이제 짜증나는 적분가능조건따위는 버리도록 하자. 대신에 각 경우에 퓨리에변환(혹은series)과 그 역이 어떻게 정의되는지 알아보자. 그리고 외우자. 각 신호가 어떠한 도메인에서 정의되는지만 잘 파악하면 의외로 외우기 쉽다.\ncase 1. 연속-비주기\n\n\\(\\left\\{x(t): x(t)=\\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\int_{-\\infty}^{\\infty} x(t)e^{-j\\omega t} dt,~ \\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\sum_{k=-\\infty}^{\\infty} \\hat x_k e^{j \\frac{2\\pi}{\\zeta} t} ,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt,~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 3. 이산-비주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega, ~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): \\hat x(\\omega)=\\sum_{i=-\\infty}^{\\infty}x_ie^{-j\\omega i}, ~\\omega \\in \\mathbb{R} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\sum_{k=0}^{\\xi-1} \\hat x_k e^{-j\\frac{2\\pi k}{\\xi}i},~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFS}{\\Longleftrightarrow} \\left\\{\\hat x_k: \\hat x_k= \\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i},~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주기함수는 (그것이 이산이든 연속이든) 주파수영역에서의 값이 디스크릿하다. 즉 위에서 case2와 case4인 경우는 주파수영역에서 값이 디스크릿하다. 이것을 연속함수인것처럼 바꿔보면 아래와 같이 쓸 수 있다.\ncase 2. 연속-주기\n\n\\(\\left\\{x(t): x(t)= \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat x(\\omega)e^{j\\omega t} d\\omega,~ t \\in \\mathbb{R} \\right\\} \\\\ \\overset{FT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\zeta}\\int_{0}^{\\zeta}x(t)e^{-j \\frac{2\\pi k}{\\zeta}t}dt\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\zeta}\\right), ~ k \\in \\mathbb{Z} \\right\\}.\\)\n\ncase 4. 이산-주기\n\n\\(\\left\\{x_i: x_i=\\frac{1}{2\\pi}\\int_{0}^{2\\pi}\\hat x(\\omega)e^{j \\omega i}d\\omega,,~ i \\in \\mathbb{Z} \\right\\} \\\\ \\overset{DTFT}{\\Longleftrightarrow} \\left\\{\\hat x(\\omega): 2\\pi\\sum_{k=-\\infty}^{\\infty}\\left[\\frac{1}{\\xi}\\sum_{i=0}^{\\xi-1} x_i e^{-j\\frac{2\\pi k}{\\xi}i}\\right]\\delta\\left(\\omega-\\frac{2\\pi k}{\\xi}\\right),~ k \\in \\mathbb{Z} \\right\\}.\\)\n\n- 주목할것은 주기가 \\(\\zeta\\) 혹은 \\(\\xi\\) 인 함수의 주파수 응답은 오로지\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\zeta}, k \\in \\mathbb{Z}\\right\\}\\)\n\n혹은\n\n\\(\\omega \\in \\left\\{\\frac{2 \\pi k}{\\xi}, k \\in \\mathbb{Z}\\right\\}\\)\n\n에서만 존재한다는 점이다. 또한 이산신호의 경우 \\(x_i\\)의 주기가 \\(\\xi\\) 이면 \\(\\hat{x}(\\omega)\\)의 주기역시 \\(\\xi\\) 라는점 역시 주목할만한 부분이다.\n- 주파수영역에서 디스크릿한 함수를 연속인것처럼 표현했듯이 시간영역에서 디스크릿한 함수 역시 연속인것처럼 표현할 수 있다. 예를들면 \\(\\{x_i: x_i=x(iT),~i \\in \\mathbb{Z}\\}\\) 와 같은 관계가 있는 경우 아래와 같이 표현 가능하다.\n\\[x_{\\delta}(t)=\\sum_{i=-\\infty}^{\\infty}x_i\\delta(t-iT).\\]\n이거 엄청 중요하다."
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html",
    "title": "[Essays] ggplot3",
    "section": "",
    "text": "ggplot2 공부하다가 포기함\n그냥 ggplot2를 랩핑하여 내 스타일에 맞는 코드를 새로 만듬 (가칭 ggplot3)\n\nsource('ggplot3.R')\n\n소스코드 다운로드: https://github.com/miruetoto/yechan3/blob/main/posts/1_Essays/ggplot3.R\n\n계속 업데이트할 예정"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#ggplot-line-point",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#ggplot-line-point",
    "title": "[Essays] ggplot3",
    "section": "ggplot + line + point",
    "text": "ggplot + line + point\n\ny = rnorm(100)\n\n- 예시1\n\nggplot() + line(1:100, y, lty=2, col='gray60')\n\n\n\n\n- 예시2\n\nggplot() + point(y,pch=2,col=2,cex=5)\n\n\n\n\n- 예시3\n\nggplot() + point(y,pch=2,col=2,cex=5) + line(25:50, y[25:50], col=2, lwd=2)"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#ggplot2와-호환",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#ggplot2와-호환",
    "title": "[Essays] ggplot3",
    "section": "ggplot2와 호환",
    "text": "ggplot2와 호환\n\ndf = data.frame(x=1:100,y=rnorm(100))\nhead(df)\n\n\n\n\n\n\nx\ny\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1.3039551\n\n\n2\n2\n0.6599091\n\n\n3\n3\n-1.0945483\n\n\n4\n4\n-0.8733234\n\n\n5\n5\n-1.0432541\n\n\n6\n6\n-1.9712522\n\n\n\nA data.frame: 6 × 2\n\n\nggplot(data=df) + \ngeom_point(aes(x=x,y=y)) + \nline(1:50,df$y[1:50],col=2,lty=2)\n\n\n\n\n\n완벽하게 호환가능"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#subplotting",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#subplotting",
    "title": "[Essays] ggplot3",
    "section": "subplotting",
    "text": "subplotting\n\ny = rnorm(100)\np1 = ggplot() + line(y,lty=2,col='gray60')\np2 = ggplot() + point(y,pch=2,col=2,cex=5)\np3 = ggplot() + point(y,pch=2,col=2,cex=5) + line(25:50, y[25:50], col=2, lwd=2)\np4 = ggplot() + point(y) + line(1:50,y[1:50],col='orange')\n\n- 예시1: 좌우로 나란히\n\nfigsize(10,3)\np1|p2|p3|p4\nfigsize()\n\n\n\n\n- 예시2: 위아래로\n\nfigsize(10,5)\np1/p2/p3/p4\nfigsize()\n\n\n\n\n- 예시3: 그리드로!\n\nfigsize(10,5)\n(p1|p2)/(p3|p4)\nfigsize()\n\n\n\n\n- 예시4: 좀 더 난해한 모양으로 (1)\n\nfigsize(10,5)\n(p1|p2|p3)/p4\nfigsize()\n\n\n\n\n- 예시5: 좀 더 난해한 모양으로 (2)\n\nfigsize(10,5)\n(p1|p2|p3)/(p4|p1)\nfigsize()\n\n\n\n\n- 예시6: 좀 더 난해한 모양으로 (3)\n\nfigsize(10,5)\np1|(p2/p3/p4)\nfigsize()"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#ggtitle",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#ggtitle",
    "title": "[Essays] ggplot3",
    "section": "ggtitle",
    "text": "ggtitle\n\ny = rnorm(100)\nggplot() + line(y,lty=2,col='gray60') + \nggtitle(\"(a) MY TITLE\")"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#xlab-ylab",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#xlab-ylab",
    "title": "[Essays] ggplot3",
    "section": "xlab, ylab",
    "text": "xlab, ylab\n\ny = rnorm(100)\nggplot() + line(y,lty=2,col='gray60') + \nxlab(\"asdfasdfasdf\") + ylab(\"asdfasdfasdf\")"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#legends",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#legends",
    "title": "[Essays] ggplot3",
    "section": "legends",
    "text": "legends\n\ny1 = rnorm(10)\ny2 = rnorm(10)\n\n\nggplot()+\nline(y1,label=\"y1\",lty=2)+point(y1,label=\"y1\",cex=3)+\nline(y2,label=\"y2\",lty=2)+point(y2,label=\"y2\",cex=3)"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#wide_y",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#wide_y",
    "title": "[Essays] ggplot3",
    "section": "wide_y",
    "text": "wide_y\n- 예시1\n\ny1 = rnorm(10)\ny2 = rnorm(10)\n\n\nfigsize(6,4)\nggplot()+line(cbind(y1,y2),lty=2)+point(cbind(y1,y2),cex=3)\nfigsize()\n\n\n\n\n- 예시2\n\nx = 11:15\ny1 = rnorm(5)\ny2 = rnorm(5)\n\n\nfigsize(6,4)\nggplot()+line(x,cbind(y1,y2),lty=2)+point(x,cbind(y1,y2),cex=3)\nfigsize()"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#smooth",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#smooth",
    "title": "[Essays] ggplot3",
    "section": "smooth",
    "text": "smooth\n\nx = 1:100/100\ny1 = 2*x + rnorm(100)*0.3\ny2 = -3*x + rnorm(100)*0.3 +3\n\n- 예시1\n\nggplot()+point(x,y1,alpha=0.5,col=\"gray60\") + smooth(x,y1,col=2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n- 예시2\n\nggplot()+point(x,y1,alpha=0.5,col=\"gray60\") +\nsmooth(x[1:60],y1[1:60],col=2) + \nsmooth(x[40:100],y1[40:100],col=4) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n- 예시3\n\nggplot()+point(x,cbind(y1,y2),alpha=0.5) +\nsmooth(x,cbind(y1,y2))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#area",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#area",
    "title": "[Essays] ggplot3",
    "section": "area",
    "text": "area\n\ny1 = c(10,20,10,50)\ny2 = c(15,20,5,30)\n\n- 예시1\n\nggplot() + area(y1,fill=2)\n\n\n\n\n- 예시2\n\nggplot() + area(cbind(y1,y2))"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#step",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#step",
    "title": "[Essays] ggplot3",
    "section": "step",
    "text": "step\n\ny1 = cumsum(rnorm(100))\ny2 = cumsum(rnorm(100))\n\n- 예시1\n\nggplot() + step(y1)\n\n\n\n\n- 예시2\n\nggplot() + step(cbind(y1,y2))"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#jitter",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#jitter",
    "title": "[Essays] ggplot3",
    "section": "jitter",
    "text": "jitter\n\nx = sample(c(20,30,40,50),size=100,replace = TRUE)\ny = rnorm(100)\n\n\np1 = ggplot()+point(x,y)+ggtitle(\"geom_point\")\np2 = ggplot()+jitter(x,y,width = 1) + ggtitle(\"geom_jitter\")\nfigsize(10,5)\np1|p2\nfigsize()"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#histogram",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#histogram",
    "title": "[Essays] ggplot3",
    "section": "histogram",
    "text": "histogram\n\ny1 = rnorm(1000)\ny2 = rnorm(1000)*0.5 + 3 \n\n- 예시1\n\nggplot()+histogram(c(y1,y2),fill=\"gray60\")\n\nWarning message:\n“`stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.”\n\n\n\n\n\n- 예시2\n\nggplot()+histogram(y1,label=\"A\")+histogram(y2,label=\"B\")\n\n\n\n\n- 예시3\n\nggplot()+histogram(cbind(y1,y2))"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#density",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#density",
    "title": "[Essays] ggplot3",
    "section": "density",
    "text": "density\n\ny1 = rnorm(1000)\ny2 = rnorm(1000)*0.5 + 3 \n\n- 예시1\n\nggplot()+density(c(y1,y2))\n\n\n\n\n- 예시2\n\nggplot()+density(y1,label=\"A\")+density(y2,label=\"B\")\n\n\n\n\n- 예시3\n\nggplot()+density(cbind(y1,y2))\n\n\n\n\n- 예시4\n\nggplot()+histogram(cbind(y1,y2))+density(cbind(y1,y2))"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#qq",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#qq",
    "title": "[Essays] ggplot3",
    "section": "qq",
    "text": "qq\n\ny1 = rnorm(100)\ny2 = rchisq(100,df=5)\n\n\nggplot()+\nqq(y1,label='y1')+qq_line(y1,label='y1')+\nqq(y2,label='y2')+qq_line(y2,label='y2')"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#col",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#col",
    "title": "[Essays] ggplot3",
    "section": "col",
    "text": "col\n\ny1=c(1,2,3,5)\ny2=c(1,4,2,1)\n\n\nggplot()+col(y1,fill=2)\n\n\n\n\n\nggplot()+col(cbind(y1,y2))"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#boxplot",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#boxplot",
    "title": "[Essays] ggplot3",
    "section": "boxplot",
    "text": "boxplot\n\ny1=rnorm(100)\ny2=rnorm(100)+3\n\n- 예시1\n\nggplot()+boxplot(y1)\n\n\n\n\n- 예시2\n\nggplot()+boxplot(cbind(y1,y2))\n\n\n\n\n- 예시3\n\nggplot()+boxplot(x='A',y1,label='A')+boxplot(x='B',y2,label='B')"
  },
  {
    "objectID": "posts/1_Essays/2099-01-01-ggplot3.html#violin",
    "href": "posts/1_Essays/2099-01-01-ggplot3.html#violin",
    "title": "[Essays] ggplot3",
    "section": "violin",
    "text": "violin\n\ny1=rnorm(100)\ny2=rnorm(100)+3\n\n- 예시1\n\nggplot()+violin(y1)\n\n\n\n\n- 예시2\n\nggplot()+violin(cbind(y1,y2), color = \"transparent\")\n\n\n\n\n- 예시3\n\nggplot()+violin(x='A',y1,label='A')+violin(x='B',y2,label='B')"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html",
    "href": "posts/1_Essays/2023-01-20-추정.html",
    "title": "[Essays] 추정",
    "section": "",
    "text": "using Distributions, Plots"
  },
  {
    "objectID": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "href": "posts/1_Essays/2023-01-20-추정.html#mle의-일치성에-대한-구체적인-논의",
    "title": "[Essays] 추정",
    "section": "MLE의 일치성에 대한 구체적인 논의",
    "text": "MLE의 일치성에 대한 구체적인 논의\n\\(X_1,\\dots,X_{10} \\overset{i.i.d.}{\\sim} Ber(\\theta)\\) 이라고 하자.\n\nx = rand(Bernoulli(0.3),10)\nx\n\n10-element Vector{Bool}:\n 0\n 0\n 1\n 1\n 1\n 1\n 0\n 0\n 0\n 0\n\n\n여기에서 \\(\\theta\\)는 추정해야할 미지의 모수이지만 우리는 시뮬레이션의 편의상 \\(\\theta\\)의 참값을 \\(\\theta_0=\\frac{1}{3}\\)로 알고 있다고 하자. MLE를 논의함에 있어 핵심적인 역할을 하는 것은 \\(Y_1=\\log f(X_1;\\theta)\\)이다. 아래는 \\(Y_1\\)에 대한 몇가지 코멘트이다.\n(1) \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n우선 \\(X_1\\)의 함수이므로 \\(Y_1\\)역시 확률변수이다. 따라서 \\(Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\\(Y_1\\)은 \\(\\theta\\)에 대한 함수이므로 \\(\\theta\\)에 대하여 미분할 수 있다.\n\n(베르누이 예제)\n우리의 베르누이 예제에서 \\(Y_1\\)은 아래와 같이 계산된다.\n\\[Y_1 = \\log f(X_1;\\theta)= X_1 \\log \\theta + (1-X_1)\\log(1-\\theta)\\]\n보는 것 처럼 \\(Y_1\\)은 \\(X_1\\)와 \\(\\theta\\)의 함수임\n(2) \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta\\) 만의 함수이다. 적당한 조건4이 만족된다면 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta_0\\) 에서 최대화 된다.\n4 identifiable & common support(베르누이 예제)\n\\(\\mathbb{E}_{\\theta_0}(Y_1) = \\mathbb{E}_{\\theta_0}(X_1)\\log\\theta + (1-\\mathbb{E}_{\\theta_0}(X_1))\\log(1-\\theta) = \\frac{1}{3} \\log\\theta + (1-\\frac{1}{3})\\log(1-\\theta)\\)\n\n일반적인 상황에서는 참모수를 모르지만 우리는 시뮬레이션을 \\(\\theta=1/3\\)에서 하였으므로 참모수 \\(\\theta_0=\\mathbb{E}_{\\theta_0}(X_1)=\\frac{1}{3}\\)을 알고 있다고 가정한다.\n\n\nplot(θ -&gt; (1/3)*log(θ) + (1-1/3)*log(1-θ)) \n\n\n\n\n보는것처럼 이 함수 \\(\\mathbb{E}_{\\theta_0}(Y_1)\\)은 \\(\\theta=\\theta_0=\\frac{1}{3}\\) 에서 최대값을 가진다.\n(3) \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 \\(X_1\\)와 \\(\\theta\\)의 함수이다.\n\n따라서 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\) 역시 확률변수이고 \\(\\frac{\\partial}{\\partial \\theta}Y_1\\)에 대하여 평균등을 취할 수 있으며 LLN을 쓸 수 있다.\n\n(베르누이 예제)\n\\(\\frac{\\partial}{\\partial\\theta}Y_1 = X_1\\frac{1}{\\theta} + (1-X_1)\\frac{-1}{1-\\theta}\\)\n(4) \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial \\theta}Y_1]=0\\) 이다.\n(베르누이 예제)\n\\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1] = \\theta\\frac{1}{\\theta} + (1-\\theta)\\frac{-1}{1-\\theta}=0\\)\n(5) \\(\\mathbb{V}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=\\mathbb{E}_{\\theta}[-\\frac{\\partial^2}{\\partial \\theta^2}Y_1]=I(\\theta)\\)\n(베르누이 예제)\n\\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]=\\mathbb{E}_{\\theta}\\big[(\\frac{\\partial}{\\partial\\theta}Y_1)^2\\big]=\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]=\\frac{1}{\\theta(1-\\theta)}\\)\n\n두번째 등호는 \\(\\mathbb{E}_{\\theta}[\\frac{\\partial}{\\partial\\theta}Y_1]=0\\)을 이용하여 증명가능하다.\n언뜻 보면 \\(\\mathbb{V}_{\\theta}\\big[\\frac{\\partial}{\\partial\\theta}Y_1\\big]\\)를 계산하는 것이 \\(\\mathbb{E}_{\\theta}\\big[-\\frac{\\partial^2}{\\partial\\theta^2}Y_1\\big]\\)를 계산하는것보다 훨씬 쉬워보인다. 그런데 \\(X_1\\)와 \\(1-X_1\\)이 독립이 아니라서 \\(\\mathbb{V}(X+Y)=\\mathbb{V}(X)+\\mathbb{V}(V)+2\\text{Cov}(X,Y)\\)와 같이 공분산 term을 계산해야 하므로 계산이 까다롭다.\n\n\n베르누이에 대한 피셔정보량은 https://en.wikipedia.org/wiki/Fisher_information 에서 확인할 수 있음"
  },
  {
    "objectID": "posts/1_Essays/2023-08-07-루나랜더 v1.html",
    "href": "posts/1_Essays/2023-08-07-루나랜더 v1.html",
    "title": "[Essays] 루나랜더 v1",
    "section": "",
    "text": "import gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch \nimport collections\nimport IPython\nimport random\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\n\n\nclass QNetwork(torch.nn.Module):\n    \"\"\"Actor (Policy) Model.\"\"\"\n\n    def __init__(self, state_size, action_size, seed):\n        \"\"\"Initialize parameters and build model.\n        Params\n        ======\n            state_size (int): Dimension of each state\n            action_size (int): Dimension of each action\n            seed (int): Random seed\n        \"\"\"\n        super(QNetwork, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.layer1 = torch.nn.Linear(state_size, 128)\n        self.layer2 = torch.nn.Linear(128, 64)\n        self.layer3 = torch.nn.Linear(64, 32)\n        self.layer4 = torch.nn.Linear(32, action_size)\n        \n    def forward(self, state):\n        \"\"\"Build a network that maps state -&gt; action values.\"\"\"\n        x = torch.nn.functional.relu(self.layer1(state))        \n        x = torch.nn.functional.relu(self.layer2(x))\n        x = torch.nn.functional.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n\n\nBUFFER_SIZE = 50000   # replay buffer size\nBATCH_SIZE = 128      # minibatch size\nGAMMA = 0.99          # discount factor\nTAU = 0.001           # for soft update of target parameters\nLR = 0.0001           # learning rate \nUPDATE_EVERY = 1      # how often to update the network\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda', index=0)\n\n\n\nclass ReplayBuffer:\n    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n\n    def __init__(self, action_size, buffer_size, batch_size, seed):\n        \"\"\"Initialize a ReplayBuffer object.\n\n        Params\n        ======\n            action_size (int): dimension of each action\n            buffer_size (int): maximum size of buffer\n            batch_size (int): size of each training batch\n            seed (int): random seed\n        \"\"\"\n        self.action_size = action_size\n        self.memory = collections.deque(maxlen=buffer_size)  \n        self.batch_size = batch_size\n        self.experience = collections.namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n        self.seed = random.seed(seed)\n    \n    def add(self, state, action, reward, next_state, done):\n        \"\"\"Add a new experience to memory.\"\"\"\n        e = self.experience(state, action, reward, next_state, done)\n        self.memory.append(e)\n    \n    def sample(self):\n        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n        experiences = random.sample(self.memory, k=self.batch_size)\n        \n        # Convert to torch tensors\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        \n        # Convert done from boolean to int\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)        \n        \n        return (states, actions, rewards, next_states, dones)\n\n    def __len__(self):\n        \"\"\"Return the current size of internal memory.\"\"\"\n        return len(self.memory)\n\n\nclass Agent():\n    \"\"\"Interacts with and learns from the environment.\"\"\"\n\n    def __init__(self, state_size, action_size, seed):\n        \"\"\"Initialize an Agent object.\n        \n        Params\n        ======\n            state_size (int): dimension of each state\n            action_size (int): dimension of each action\n            seed (int): random seed\n        \"\"\"\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Q-Network\n        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n        self.optimizer = torch.optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n\n        # Replay memory\n        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n        # Initialize time step (for updating every UPDATE_EVERY steps)\n        self.t_step = 0\n    \n    def step(self, state, action, reward, next_state, done):\n        # Save experience in replay memory\n        self.memory.add(state, action, reward, next_state, done)\n        \n        # Learn every UPDATE_EVERY time steps.\n        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n        if self.t_step == 0:\n            # If enough samples are available in memory, get random subset and learn\n            if len(self.memory) &gt; BATCH_SIZE:\n                experiences = self.memory.sample()\n                self.learn(experiences, GAMMA)\n\n    def act(self, state, eps=0.):\n        \"\"\"Returns actions for given state as per current policy.\n        \n        Params\n        ======\n            state (array_like): current state\n            eps (float): epsilon, for epsilon-greedy action selection\n        \"\"\"\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        \n        \n        self.qnetwork_local.eval()\n        with torch.no_grad():\n            action_values = self.qnetwork_local(state)            \n        self.qnetwork_local.train()\n\n        if random.random() &gt; eps:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return random.choice(np.arange(self.action_size))\n        \n\n    def learn(self, experiences, gamma):\n        \"\"\"Update value parameters using given batch of experience tuples.\n\n        Params\n        ======\n            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n            gamma (float): discount factor\n        \"\"\"\n        # Obtain random minibatch of tuples from D\n        states, actions, rewards, next_states, dones = experiences\n\n        ## Compute and minimize the loss\n        ### Extract next maximum estimated value from target network\n        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n\n        ### Calculate expected value from local network\n        q_expected = self.qnetwork_local(states).gather(1, actions)\n        \n        loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # ------------------- update target network ------------------- #\n        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n\n    def soft_update(self, local_model, target_model, tau):\n        \"\"\"Soft update model parameters.\n        θ_target = τ*θ_local + (1 - τ)*θ_target\n\n        Params\n        ======\n            local_model (PyTorch model): weights will be copied from\n            target_model (PyTorch model): weights will be copied to\n            tau (float): interpolation parameter \n        \"\"\"\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n\n\ndef dqn(n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n    \"\"\"Deep Q-Learning.\n    \n    Params\n    ======\n        n_episodes (int): maximum number of training episodes\n        max_t (int): maximum number of timesteps per episode\n        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n        eps_end (float): minimum value of epsilon\n        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n    \"\"\"\n    playtimes=[] \n    scores = []                        # list containing scores from each episode\n    scores_window = collections.deque(maxlen=100)  # last 100 scores\n    eps = eps_start                    # initialize epsilon\n    for i_episode in range(1, n_episodes+1):\n        state, _  = env.reset()\n        score = 0\n        for t in range(max_t):\n            ## STEP1: \n            action = agent.act(state, eps)\n            \n            ## STEP2: \n            next_state, reward, done, _, _ = env.step(action)\n            \n            ## STEP3: \n            agent.step(state, action, reward, next_state, done)\n            \n            ## STEP4: \n            state = next_state\n            \n            ## STEP5: \n            score += reward\n            \n            if done:\n                playtimes.append(t)\n                break \n        scores_window.append(score)       # save most recent score\n        scores.append(score)              # save most recent score\n        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)), end=\"\")\n        if i_episode % 100 == 0:\n            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)))\n            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n        if np.mean(scores_window)&gt;=200.0:\n            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n            break\n    return scores\n\n\nenv.reset()\n\n(array([-0.00494423,  1.4084052 , -0.50080436, -0.11177538,  0.00573585,\n         0.11343976,  0.        ,  0.        ], dtype=float32),\n {})\n\n\n\nagent = Agent(state_size=8, action_size=4, seed=0)\nscores = dqn()\n\nEpisode 100 Average Score: -149.80  Playtime: 116.55\nEpisode 200 Average Score: -22.61   Playtime: 143.535\nEpisode 300 Average Score: 50.80    Playtime: 144.625\nEpisode 400 Average Score: 85.70    Playtime: 148.13\nEpisode 500 Average Score: 96.33    Playtime: 169.16\nEpisode 600 Average Score: 186.44   Playtime: 222.15\nEpisode 628 Average Score: 200.36   Playtime: 234.24\nEnvironment solved in 628 episodes! Average Score: 200.36\n\n\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nplt.plot(np.arange(len(scores)), scores)\nplt.ylabel('Score')\nplt.xlabel('Episode #')\nplt.show()\n\n\n\n\n\nframes = []\nagent = Agent(state_size=8, action_size=4, seed=0)\nagent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\nstate,_ = env.reset()\ndone = False\nwhile not done:\n    frames.append(env.render())\n    action = agent.act(state)\n    state, reward, done, _,_ = env.step(action)\nenv.close()\n\n\nfig, ax = plt.subplots()\nani = FuncAnimation(fig,lambda i: ax.imshow(frames[::10][i]),frames=len(frames[::10]))\n\n\n\n\n\nIPython.display.HTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html",
    "title": "[Essays] 다중척도방법",
    "section": "",
    "text": "\\({\\boldsymbol y}={\\boldsymbol X}^\\top\\)\n\n이 논문에서는 몇 가지 멀티스케일 방법을 리뷰하고자 한다.\n멀티스케일 방법이란 ~ + 왜 멀티스케일 방법을 써야하는가.\n우리는 그중에서 시각화와 분해에 관심이 있다. + 다른 접근법들도 소개\n시각화가 사용되는 사례, 어떻게 이용할 수 있는가?\n분해가 이용되는 사례, 어떻게 이용할 수 있는가?\n많은 멀티스케일 방법 중 우리는 TPT, EPT, EBT 를 중점적으로 소개하는데 초점을 맞출 것이다.\n각각의 transform은 어떠한 공통점이 있는가?\n왜 이렇게 많은 트랜스폼이 있어야하는가?\n미래에는 어떠한 트랜스폼을 만드는 것이 좋을까?"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html#motivation-of-emd",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html#motivation-of-emd",
    "title": "[Essays] 다중척도방법",
    "section": "Motivation of EMD",
    "text": "Motivation of EMD\n- A necessary condition to define a meaningful instantaneous frequency is that signals are symmetric with respect to the local zero mean.\n- EMD is developed to decompose signals so that HT works well.\n- Suppose we have a signal \\(x(t) = 0.5 t + \\sin(\\pi t) + \\sin(2 \\pi t) + \\sin(6 \\pi t)\\).\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/1_Essays/2023-07-07-다중척도방법.html#sifting-process-for-extracting-imfs",
    "href": "posts/1_Essays/2023-07-07-다중척도방법.html#sifting-process-for-extracting-imfs",
    "title": "[Essays] 다중척도방법",
    "section": "Sifting process for extracting IMF’s",
    "text": "Sifting process for extracting IMF’s\n\nIdentify local extrema.\nObtain two functions interpolated by local maxima and minima.\nTake an average of envelopes.\nAn oscillated signal is extracted by subtracting the envelope mean from the original signal \\(x(t)\\).\n\n\n\n\nimage.png\n\n\n- One iteration of the above procedure does not guarantee that the signal is an IMF. The same procedure is applied to the signal until the properties of IMF are satisfied\n- This iterative algorithm is called sifting process.\n- Sifting makes the remaining signal more symmetric with respect to the local zero mean.\n- Finally, we have \\(L\\) empirical modes and a residue, \\[\nx(t) = \\sum_{\\ell=1}^{L} \\text{imf}_\\ell(t) + r(t).\n\\]\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/4_Notes/2000-01-09-docker.html",
    "href": "posts/4_Notes/2000-01-09-docker.html",
    "title": "[Note] docker",
    "section": "",
    "text": "- 우분투에서 설치\nref: https://docs.docker.com/engine/install/ubuntu/\n- 기본명령들\ndocker run penguin\ndocker stop penguin\ndocker image pull penguin \ndocker container start penguin\ndocker container run penguin\n- Docker에서 다운로드한 이미지와 실행 중인 컨테이너를 확인하기 위한 명령어를 소개하겠습니다:\n\n이미지 관련 명령어:\n\ndocker images: 현재 시스템에 다운로드된 모든 도커 이미지를 확인할 수 있습니다.\ndocker image ls: “docker images”와 동일한 결과를 출력합니다.\ndocker image ls &lt;이미지_이름&gt;: 특정 이미지의 버전을 명시하여 해당 이미지의 정보를 확인할 수 있습니다.\ndocker image history &lt;이미지_이름&gt;: 특정 이미지의 레이어별 변경 내역을 확인할 수 있습니다.\n\n컨테이너 관련 명령어:\n\ndocker ps: 현재 실행 중인 컨테이너의 목록을 확인합니다.\ndocker ps -a: 모든 컨테이너의 목록을 확인합니다 (실행 중인 것과 중지된 것 모두 포함).\ndocker container ls: “docker ps”와 동일한 결과를 출력합니다.\ndocker container ls -a: “docker ps -a”와 동일한 결과를 출력합니다.\ndocker container inspect &lt;컨테이너_ID 또는 이름&gt;: 특정 컨테이너의 자세한 정보를 확인할 수 있습니다.\ndocker logs &lt;컨테이너_ID 또는 이름&gt;: 컨테이너의 로그를 확인할 수 있습니다.\ndocker container stats: 실행 중인 컨테이너의 실시간 리소스 사용량을 모니터링할 수 있습니다.\n\n\n이 명령어들을 사용하여 이미지와 컨테이너에 대한 정보를 확인하고 관리할 수 있습니다.\n- docker run -ti --rm r-base 명령어에서 사용된 옵션들은 다음과 같은 의미를 갖습니다:\n\n-ti: 이 옵션은 두 개의 플래그인 -t와 -i를 결합한 것입니다.\n\n-t (또는 --tty): 이 플래그는 유사 터미널을 할당하고 터미널 상호작용을 가능하게 합니다. 컨테이너 내부에서 터미널 환경을 사용할 수 있게 해줍니다.\n-i (또는 --interactive): 이 플래그는 컨테이너와 상호작용할 수 있도록 표준 입력(STDIN)을 열어줍니다. 컨테이너 내부에서 입력을 주고 받을 수 있게 해줍니다.\n\n–rm: 이 옵션은 컨테이너가 실행을 마칠 때 자동으로 컨테이너를 삭제합니다. 컨테이너가 종료되면 그 내용과 상태는 모두 삭제됩니다. 주로 임시 작업을 수행하는데 사용됩니다.\n\n따라서 docker run -ti --rm r-base 명령어는 R 언어 기반의 컨테이너를 실행하는 명령어로, 터미널 환경을 제공하며 컨테이너가 종료될 때 컨테이너를 자동으로 삭제합니다. 이를 통해 R 언어를 실행하고 테스트하거나 실험할 수 있는 환경을 구성할 수 있습니다.\n- docker run -ti --rm -v \"$PWD\":/home/docker -w /home/docker -u docker r-base R CMD check 명령어에서 사용된 각 옵션에 대한 설명은 다음과 같습니다:\n\n-ti: 이 옵션은 터미널 상호작용을 위한 옵션입니다. -t는 유사 터미널 할당을 의미하고, -i는 터미널 입력 상호작용을 허용하는 것을 의미합니다.\n–rm: 컨테이너가 실행을 마칠 때 자동으로 컨테이너를 삭제합니다.\n-v “$PWD”:/home/docker: 이 옵션은 호스트 시스템의 현재 작업 디렉토리를 컨테이너 내의 /home/docker 디렉토리와 공유 마운트합니다. 이렇게 함으로써 호스트 시스템의 파일을 컨테이너 내부에서 사용할 수 있게 됩니다.\n-w /home/docker: 컨테이너가 실행될 작업 디렉토리를 /home/docker로 설정합니다. 즉, 컨테이너가 실행될 때 기본 디렉토리가 /home/docker가 됩니다.\n-u docker: 이 옵션은 컨테이너 안에서 사용자를 변경하여 실행하는 것을 의미합니다. docker라는 사용자로 컨테이너를 실행하게 됩니다.\nr-base: Docker 이미지 이름입니다. 여기서는 R 언어를 기반으로한 이미지를 사용합니다.\nR CMD check: 실제로 실행되는 컨테이너 내부의 명령어입니다. 이 경우 R 패키지를 체크하는 R 명령어 R CMD check가 실행됩니다.\n\n이 명령어는 호스트 시스템의 현재 작업 디렉토리에 있는 R 패키지를 컨테이너 내부에서 R CMD check 명령으로 체크하고자 할 때 사용될 수 있습니다.\n- “docker container start penguin”과 “docker container run penguin” 두 명령어의 차이점은 다음과 같습니다:\n\ndocker container start penguin:\n\n이 명령은 이미 생성된 (stopped 또는 exited 상태의) 컨테이너를 시작시킬 때 사용됩니다.\n컨테이너의 상태를 변경하지 않고, 중단된 상태에서 다시 실행합니다.\n이미지에서 컨테이너를 생성한 후, 실행 중인 컨테이너를 중단하고 다시 시작할 때 유용합니다.\n\ndocker container run penguin:\n\n이 명령은 도커 이미지를 기반으로 새로운 컨테이너를 생성하고 실행시킬 때 사용됩니다.\n이미지에서 새로운 컨테이너를 생성하고, 컨테이너를 시작합니다.\n이미 실행 중인 컨테이너가 아닌, 새로운 컨테이너를 생성하고 실행할 때 사용됩니다.\n\n\n요약하면, “docker container start penguin”은 이미 생성된 컨테이너를 다시 시작하고, “docker container run penguin”은 새로운 컨테이너를 생성하고 시작하는 데 사용됩니다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "href": "posts/4_Notes/2000-01-07-줄리아 설치 및 실행.html",
    "title": "[Note] 줄리아 설치 및 실행",
    "section": "",
    "text": "설치\n- 여기에 접속한다. 스크롤링하여 ’Generic Linux Binaries for x86 / 64-bit(GPG)’를 찾는다. 그리고 ’64-bit’를 클릭해서 다운받는다. (참고로 왼쪽에 ’help’를 누르면 설치페이지 설명서가 나온다.) 그러면 아래와 같은 파일이 나온다.\njulia-1.3.1-linux-x86_64.tar.gz\n이 파일을 더블클릭해서 압축을 풀어준다. 압축을 풀면 julia-1.3.1라는 폴더가 생긴다. 이 폴더를 원하는 위치로 (줄리아가 설치되기를 원하는 위치) 이동시킨다. 나는 home에 이동시켰다.\n- 아래를 실행하면 줄리아가 실행된다. (둘중 아무거나)\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n\n\n주피터와 연결\n- 아래중 하나를 실행하여 줄리아를 킨다.\n/home/cgb/julia-1.3.1/bin/julia\n~/julia-1.3.1/bin/julia\n- 줄리아를 실행한뒤에 아래를 입력하면 주피터노트북에 연결된다.\nusing Pkg\nPkg.add(\"IJulia\")\n- 한 가지 의문점이 있다. 나같은 경우는 ’(base)’에서 줄리아를 실행하고 연결하였다. 그런데 혹시 몰라서 (py38r40)에서도 줄리아를 실행해봤는데 잘 실행되었다. 줄리아를 실행시키고 위의 명령 Pkg.add(\"IJulia\")를 다시쳤는데, 이미 연결되어서 더이상 변화시킨게 없다는 메시지가 떴다. 이러면 (base)에 설치된 줄리아가 (py38r40)에서도 실행된 줄리아와 동일하다는 의미일까? \\(\\Longrightarrow\\) 그렇다. 왜냐하면 줄리아는 anaconda내의 폴더에 설치한 것이 아니기 때문에. home에 보통 설치하니깐.\n\n\n환경변수 조정\n- 참고로 어디서든 줄리아를 실행시키고 싶다면 환경변수를 조작하면 된다. 아래를 실행해서 나노에디터를 킨다.\nsudo nano /etc/environment\n맨끝에 다음과 같이 되어있을 것이다.\n~~ usr/local/games\"\n마지막에 /home/cgb/julia-1.3.1/bin/julia를 추가한다. 즉 아래와 같이 만든다.\n~~ usr/local/games:/home/cgb/julia-1.3.1/bin/julia\"\n세이브하고 나온다. (그런데 이 과정을 안거쳐도 되는것 같음.) 이제 커맨드에서 아래를 실행한다.\nexport PATH=$PATH:/home/cgb/julia-1.3.1/bin\n이렇게하면 이제 단순히 julia라고만 쳐도 julia가 실행된다.\n\n\n플루토에서 강의영상 넣는 방법\n- 아래를 삽입\nhtml\"\"\"\n&lt;div style=\"display: flex; justify-content: center;\"&gt;\n&lt;div  notthestyle=\"position: relative; right: 0; top: 0; z-index: 300;\"&gt;\n&lt;iframe src=\n\"\nhttps://www.youtube.com/embed/\n\"\nwidth=600 height=375  frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n\"\"\"\n\n\n플루토를 이용한 홈페이지 만드는 방법\n- 단계1: https://github.com/JuliaPluto/static-export-template 에 가서 Clone\n- 단계2: Setting -&gt; GitHub Pages -&gt; Source -&gt; gh-pages / root\n\n\n플루토 키는 방법\nimport Pluto\nPluto.run(host=\"0.0.0.0\",port=1234,launch_browser=false,require_secret_for_open_links=false,require_secret_for_access=false,threads=\"8\")"
  },
  {
    "objectID": "posts/4_Notes/2000-01-06-깃 익히기.html",
    "href": "posts/4_Notes/2000-01-06-깃 익히기.html",
    "title": "[Note] 깃 익히기",
    "section": "",
    "text": "clone\ngithub repository \\(\\to\\) code \\(\\to\\) clone tab, ssh를 복사 (git@github.com:miruetoto/yechan.git처럼 생김)\n터미널에서 아래를 입력\ngit clone git@github.com:miruetoto/yechan.git 01_yechan\n\n\npull\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력한다.\ngit pull\n\n\nbranch\n- 서버에 이미 guebin이라는 브랜치가 있다면 아래와 같이 동기화 시킨다.\ngit chechout guebin\ngit push -u origin guebin\n여기에서 git push -u origin guebin을 안해도 동기화가 잘될때도 있는데 아닐때도 있다.\n\n\nremote\n- 깃이 설치된 가장 상위폴더(Documents/Github/miruetoto.github.io)에 가서 아래를 입력하면 깃허브의 url 주소를 확인할 수 있다.\n(base) lgcgb2@lgcgb2:~/Documents/GitHub/miruetoto.github.io$ git remote -v\norigin https://github.com/miruetoto/miruetoto.github.io.git (fetch)\norigin https://github.com/miruetoto/miruetoto.github.io.git (push)\nupstream https://github.com/daattali/beautiful-jekyll.git (fetch)\nupstream https://github.com/daattali/beautiful-jekyll.git (push)\n\n\nconfig\n- 설정보기\ngit config —list \n- 설정삭제\ngit config --unset user.name\ngit config --unser user.email\n- 전역설정삭제\ngit config --unset --global user.name\ngit config --unset --global user.email\n- 중복값 설정삭제\ngit config --unset-all user.name\ngit config --unset-all user.email\n- 중복값 전역으로 설정삭제\ngit config --unset-all --global user.name\ngit config --unset-all --global user.email\n- 비번안치고 푸쉬하는법?\ngit config credential.helper store\n입력이후에 git push\n\n\nGit token\nhttps://github.com/settings/tokens 에서 확인가능\n\nAppendix\n\n리눅스에서 github desktop 설치\n\n여기로 간다.\n한 챕터의 (2.3.1 Linux RC1 와 같이 되어있음) 아래쪽에 보면 ▶ Assets 라고 되어있는데 이걸 클릭하면 다운받을 수 있는 파일들이 나온다. 확장자가 .deb로 끝나는걸 골라서 다운받은뒤에 실행한다."
  },
  {
    "objectID": "posts/4_Notes/2000-01-08-주피터랩- 설정 및 몇가지 팁.html",
    "href": "posts/4_Notes/2000-01-08-주피터랩- 설정 및 몇가지 팁.html",
    "title": "[Note] 주피터랩: 설정 및 몇가지 팁",
    "section": "",
    "text": "주피터에 R커널을 연결할 경우 그림크기 조정\noptions(repr.plot.width=10, repr.plot.height=3,repr.plot.res=300)\n\n\n깃허브에서 *.py파일 불러오기\nimport requests\nexec(requests.get('http://miruetoto.github.io/my_code/datahandling.py').text)\n\n\nrpy2 magic\nimport rpy2\n%load_ext rpy2.ipython\n\n\n깃허브에서 *.R파일 불러오기\nimport rpy2\n%load_ext rpy2.ipython\n%R library(devtools)\n%R source_url(\"http://miruetoto.github.io/my_code/datahandling.r\")\n\n\nmatplotlib 그림크기조정\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \nIpython_default=plt.rcParams.copy() # save initial value \nfrom matplotlib import cycler\nplt.rc('figure',dpi=150) # default value 4 figure.dpi is 72.0 \n# plt.rcParams.update(Ipython_default) # load initial value \n\n\n깃랩관련 (회사아니면 필요없음)\n- load *.py from gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\n\n# (1) load RF.py, RF_withGIT.py, RF_withR.py\nRF_py = project.files.get(file_path='modeling/RF.py', ref='fridge').decode()\nRF_GIT_py = project.files.get(file_path='utils/RF_withGIT.py', ref='fridge').decode()\nRF_R_py = project.files.get(file_path='utils/RF_withR.py', ref='fridge').decode()\nexec(str(RF_py, 'utf-8'))\nexec(str(RF_GIT_py, 'utf-8'))\nexec(str(RF_R_py, 'utf-8'))\n- load *.R in gitlab\nimport gitlab\ngl = gitlab.Gitlab('http://10.178.145.54:9000', private_token='RkZz465zdyyEChamLKy8')\ngl.auth()\nproject = gl.projects.get(2)\nRF_R_rcode = project.files.get(file_path='utils/RF_Rfunctions.r', ref='fridge').decode()\n# tricks for source('Rfunctions.r')\nfile1 = open(\"RF_Rfunctions.r\",\"w\") \nfile1.write(str(RF_R_rcode, 'utf-8'))\nfile1.close() \nro.r(\"source('RF_Rfunctions.r')\")\nimport os\nos.remove('RF_Rfunctions.r')\n\n\n& 옵션으로 주피터 실행\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n아래와 같이 끝에 &을 붙이면 된다.\nconda activate py20190129\njupyter lab &\n실행하고 난뒤에는 엔터를 쳐서 빠져나온다. 이렇게 하면 서버자체에 모니터를 연결하고 커널창을 띄운것과 같은 효과를 준다. 즉 서버에 접속한 컴퓨터를 끄는것과 상관없이 서버에서는 항상 주피터가 열려 있게 된다.\n\n\n& 옵션으로 실행한 주피터프로세스 죽이기\n- 서버에 접속한다.\nssh lgcgb@10.178.144.65\n실행된 프로세스를 찾기위해 아래를 실행한다.\nps aux | grep jupyter-lab\n결과는 아래와 같이 나온다.\nlgcgb    26888  0.2  0.1 326760 86724 ?        Sl   10:14   0:12 /home/lgcgb/anaconda3/envs/py20190129/bin/python3.7 /home/lgcgb/anaconda3/envs/py20190129/bin/jupyter-lab\nlgcgb    27146  0.0  0.0  15720  1008 pts/3    S+   11:56   0:00 grep --color=auto jupyter-lab\n26888에 해당하는 것이 주피터를 띄운 커널이다. 이 번호를 기억했다가 프로세스를 아래와 같은 명령으로 죽인다.\nkill 26888\n\n\n패스워드 없이 주피터 실행\n- 아래와 같이 하면 외부에서 접속할때 패스워드를 입력하지 않음.\njupyter notebook --ServerApp.token='' --ServerApp.password=''"
  },
  {
    "objectID": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-08-진행사항.html",
    "href": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-08-진행사항.html",
    "title": "[EPT-DISSIM] 진행사항",
    "section": "",
    "text": "진행사항\nEPT_DISSIM: visualization (23.08.07)\n\nPRCP_KOR 자료에 대한 시각화\n\nPRCP_KOR2.csv\n\ndownload: https://github.com/miruetoto/yechan3/blob/main/posts/3_Researches/YECHAN/EPT_DISSIM/PRCP_KOR2.csv\n결측치를 0으로 채움"
  },
  {
    "objectID": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-07-PRCP_KOR 데이터정리.html",
    "href": "posts/3_Researches/YECHAN/EPT_DISSIM/2023-08-07-PRCP_KOR 데이터정리.html",
    "title": "[EPT-DISSIM] PRCP_KOR 데이터정리",
    "section": "",
    "text": "import pandas as pd\n\n1. load raw-data\n\ndf = pd.read_csv(\"OBS_ASOS_DD_20230807193646.csv\", encoding='EUC-KR')\ndf = df.rename({'지점':'Station_ID','지점명':'Station_Name','일시':'Date','일강수량(mm)':'Precipitation'},axis=1)\ndf\n\n\n\n\n\n\n\n\nStation_ID\nStation_Name\nDate\nPrecipitation\n\n\n\n\n0\n90\n속초\n2013-08-09\n15.5\n\n\n1\n90\n속초\n2013-08-10\n9.5\n\n\n2\n90\n속초\n2013-08-16\n0.1\n\n\n3\n90\n속초\n2013-08-18\n0.0\n\n\n4\n90\n속초\n2013-08-19\n9.0\n\n\n...\n...\n...\n...\n...\n\n\n127200\n295\n남해\n2023-07-22\n4.3\n\n\n127201\n295\n남해\n2023-07-23\n13.4\n\n\n127202\n295\n남해\n2023-07-24\n17.0\n\n\n127203\n295\n남해\n2023-07-25\n4.7\n\n\n127204\n295\n남해\n2023-07-26\n42.5\n\n\n\n\n127205 rows × 4 columns\n\n\n\n2. save dataframe as PRCP_KOR.csv\n\ndf.to_csv('PRCP_KOR.csv',index=False)"
  },
  {
    "objectID": "posts/3_Researches/YEON/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "href": "posts/3_Researches/YEON/2023-03-01-수통문제-Type1 Err, Type2 Err.html",
    "title": "[SEOYEONC] type1 err, type2 err",
    "section": "",
    "text": "using Distributions, Plots\n\n(문제) \\(X_1,X_2\\)가 평균이 \\(\\theta\\)인 지수분포에서 추출한 랜덤표본이라고 하자. 가설 \\(H_0: \\theta=2\\) vs \\(H_1:\\theta=1\\) 에 대하여, \\(H_0\\)에 대한 기각영역을\n\\[\\frac{f(x_1;\\theta=2)f(x_2;\\theta=2)}{f(x_1;\\theta=1)f(x_2;\\theta=1)}&lt;\\frac{1}{2}\\]\n와 같이 설정하자. 이와 같은 검정법에 대한 \\(\\alpha\\)와 \\(\\beta\\)를 구하라.\n(풀이)\n문제요약\n\n\\(f(x) = \\frac{1}{\\theta} \\exp(-\\frac{x}{\\theta})\\) -&gt; 평균이 \\(\\theta\\) 인 지수분포\n검정통계량: \\(T=\\frac{f(x_1;2)f(x_2;2)}{f(x_1;1)f(x_2;1)}\\)\n\\(\\alpha = P(\\text{Reject $H_0$|$H_0$ is true}) = P(T&lt;\\frac{1}{2} | \\text{$H_0$ is true})\\)\n\\(\\beta = P(\\text{Accept $H_0$|$H_1$ is true}) = P(T&gt;\\frac{1}{2} | \\text{$H_1$ is true})\\)\n\n풀이시작\n\nT= x -&gt; 0.5*exp(-0.5*x[1]) * 0.5*exp(-0.5*x[2])  / (exp(-x[1])*exp(-x[2]))\n\n#1 (generic function with 1 method)\n\n\n\\(\\alpha\\)를 구해보자. (시뮬)\n\nθ=2 \nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.4932782791687658\n 3.904496314340747\n\n\n\nT(x)\n\n3.715796051759978\n\n\n\nTs = [rand(Exponential(θ),2) |&gt; T for i in 1:1400000]\nmean(Ts .&lt; 1/2)\n\n0.1535007142857143\n\n\n\\(\\beta\\)를 구해보자. (시뮬)\n\nθ=1\nx = rand(Exponential(θ),2)\n\n2-element Vector{Float64}:\n 1.0915718974295616\n 3.322182470278192\n\n\n\nTs = [rand(Exponential(θ),2) |&gt; T for i in 1:1400000]\nmean(Ts .&gt; 1/2)\n\n0.5967985714285714\n\n\n\\(\\alpha\\)를 구해보자. (이론)\n\\(T(X_1,X_2) = \\frac{0.25\\exp(-0.5X_1 -0.5X_2)}{\\exp(-X_1-X_2)}=0.25\\exp(0.5X_1+0.5X_2)\\)\n$T(X_1,X_2)&lt; (0.5X_1+0.5X_2) &lt; 2 X_1+X_2&lt; 2 $\n그런데 \\(X_1+X_2 \\sim \\chi^2(4)\\) under \\(H_0\\)\n\\(P(X_1+X_2 &lt; 2\\ln2) = \\int_0^{2\\ln2} \\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_0^{\\ln2} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_0^{\\ln2}\\)\n\nt = log(2) \nu = t*(-exp(-t)) - exp(-t)\nt = 0\nl = t*(-exp(-t)) - exp(-t)\n\n-1.0\n\n\n\nu-l\n\n0.1534264097200273\n\n\n\\(\\beta\\)를 구해보자. (이론)\n$T(X_1,X_2)&gt; (0.5X_1+0.5X_2) &gt; 2 (X_1+X_2)&gt; 4 $\n그런데 \\(2(X_1+X_2) \\sim \\chi^2(4)\\) under \\(H_1\\)\n\\(P(2(X_1+X_2) &gt; 4\\ln2) = \\int_{4\\ln2}^{\\infty}\\frac{1}{4\\Gamma(2)}x e^{-x/2}dx=\\int_{2\\ln2}^{\\infty} t e^{-t}dt=\\big[t(-e^{-t})-e^{-t}\\big]_{2\\ln2}^{\\infty}\\)\n\nu = 0\nt = 2*log(2)\nl = t*(-exp(-t)) - exp(-t)\nu-l\n\n0.5965735902799727"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-03-17-ITSTGCN-Tutorial.html",
    "title": "[IT-STGCN] ITSTGCN-Tutorial",
    "section": "",
    "text": "edit\n\n\nimport\n\nimport itstgcn \nimport torch\n\n\n\n예제1: vanilla STGCN\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n\n\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \n\n\ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset,dataset_name='five_nodes')\n\n/home/csy/Dropbox/blog/posts/GCN/itstgcn/learners.py:84: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180588308/work/torch/csrc/utils/tensor_new.cpp:201.)\n  self.lags = torch.tensor(train_dataset.features).shape[-1]\n\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) \nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제2: padding missing values\n- 데이터\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 임의로 결측치 발생\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex=mindex,mtype='rand')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n- 적절한 method로 결측치를 채움 (default 는 linear)\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n다른 method로 결측치를 채울수도 있음. 사용할 수 있는 방법들은 아래에 정리되어 있음\n\nref: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='nearest')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='quadratic')\n\n\nfig = itstgcn.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss,interpolation_method='cubic')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nitstgcn.utils.plot_add(fig,torch.tensor(train_dataset_padded.targets),'--x',color='C1',alpha=0.5)\n\n\n\n\n- 블락으로 결측치 발생\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\n\n\nfig = itstgcn.utils.plot(torch.tensor(train_dataset_miss.targets),'o')\nfig \n\n\n\n\n\n\n예제3: vanilla STGCN with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.learners.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제4: vanilla STGCN with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.StgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=5,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제5: threshold example (random)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.utils.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.utils.plot(f_miss,'o')\nitstgcn.utils.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제6: threshold example (block)\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n- 결측치 발생 및 패딩\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss)\n\n\nf_miss,_ = itstgcn.convert_train_dataset(train_dataset_miss)\nf_padded,_ = itstgcn.convert_train_dataset(train_dataset_padded)\n\n\nfig = itstgcn.plot(f_miss,'o')\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\n\n\n\n\n- update by frequency thresholding\n\nfig = itstgcn.plot(f_miss,'o',alpha=0.5)\nitstgcn.plot_add(fig,f_padded,'--x',alpha=0.5)\nf_updated = itstgcn.update_from_freq_domain(f_padded,train_dataset_padded.mindex)\nitstgcn.plot_add(fig,f_updated,'-')\n\n\n\n\n\n\n예제7: iterative thresholded STGCN (IT-STGCN) with random missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제8: iterative thresholded STGCN (IT-STGCN) with block missing\n- data\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex = [list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.ITStgcnLearner(train_dataset_padded)\n\n\nlrnr.learn(filters=4,epoch=5)\n\n5/5\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)['yhat'].shape\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제9: GNAR (random missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=itstgcn.rand_mindex(train_dataset,mrate=0.5)\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='rand')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig\n\n\n\n\n\n\n예제10: GNAR (block missing)\n\n_data = itstgcn.load_data('./data/fivenodes.pkl')\n_edges = torch.tensor(_data['edges']).nonzero().tolist()\n_FX = _data['f'].tolist()\n_node_ids = {'node1':0, 'node2':1, 'node3':2, 'node4':3, 'node5':4} \ndata_dict = {'edges':_edges, 'node_ids':_node_ids, 'FX':_FX}\n\n\nloader = itstgcn.DatasetLoader(data_dict)\n\n\nloader = itstgcn.DatasetLoader(data_dict)\ndataset = loader.get_dataset(lags=2)\ntrain_dataset, test_dataset = itstgcn.temporal_signal_split(dataset, train_ratio=0.8)\n\n\nmindex=[list(range(10,100)),[],list(range(50,80)),[],[]]\ntrain_dataset_miss = itstgcn.miss(train_dataset,mindex,mtype='block')\ntrain_dataset_padded = itstgcn.padding(train_dataset_miss) # padding(train_dataset_miss,method='linear'와 같음)\n\n- 학습\n\nlrnr = itstgcn.GNARLearner(train_dataset_padded)\n\n\nlrnr.learn()\n\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n- 적합값\n\n#lrnr(train_dataset_padded) \n#lrnr(test_dataset)\n\n\n실행하면 X,y,yhat 출력\n\n- 모형 평가 및 시각화\n\nevtor = itstgcn.Evaluator(lrnr,train_dataset_padded,test_dataset)\n\nWARNING: diagonal entries present in original matrix, these will be removed\nWARNING: diagonal entries present in original matrix, these will be removed\n\n\n\nfig = evtor.plot('--.',h=5,max_node=3,label='complete data',alpha=0.5) # max_nodes 는 1보다 커야함\nfig.set_figwidth(12)\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html",
    "title": "[IT-STGCN] Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nT = 100\nt = np.arange(T)/T * 10 \n\nx = 0.3*np.sin(2*t)+0.1*np.sin(4*t)+0.1*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=4)\n\n\nmindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\ndataset_miss05031.edge_index\n\narray([[0],\n       [1]])\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n\ndataset_padded_cubic05031.edge_index\n\narray([[0],\n       [1]])\n\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nlags4/0.3/filter16\nlags4/0.38/filter8\nlags8/0.38/filter8\n\n\nlrnr05031.learn(filters=16,epoch=50)\n\n50/50\n\n\n- 모형 평가 및 시각화\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nevtor05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2,figsize=(30,20))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'.',color='C0')\nax2.plot(df1['y'][:],'.',color='C0')\nax3.plot(df1['xer'][:],'.',color='C1')\nax4.plot(df1['yer'][:],'.',color='C1')\nax5.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax6.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax7.plot(df1['x'][:],'.',color='C0')\nax7.plot(evtor05031.fhat_tr[:,0],color='C3')\nax8.plot(df1['y'][:],'.',color='C0')\nax8.plot(evtor05031.fhat_tr[:,1],color='C3')\nax9.plot(df1['x'][:],'.',color='C0')\nax9.plot(df1['xer'][:],'.',color='C1')\nax9.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax9.plot(evtor05031.fhat_tr[:,0],color='C3')\nax10.plot(df1['y'][:],'.',color='C0')\nax10.plot(df1['yer'][:],'.',color='C1')\nax10.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax10.plot(evtor05031.fhat_tr[:,1],color='C3')\n\nfor ax in fig.get_axes():\n    ax.label_outer()"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html#import",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-04-27-toy_example_figure2.html#import",
    "title": "[IT-STGCN] Toy Example Figure(Intro)",
    "section": "",
    "text": "import itstgcn \nimport torch\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\n\n\nclass Eval_csy:\n    def __init__(self,learner,train_dataset):\n        self.learner = learner\n        # self.learner.model.eval()\n        try:self.learner.model.eval()\n        except:pass\n        self.train_dataset = train_dataset\n        self.lags = self.learner.lags\n        rslt_tr = self.learner(self.train_dataset) \n        self.X_tr = rslt_tr['X']\n        self.y_tr = rslt_tr['y']\n        self.f_tr = torch.concat([self.train_dataset[0].x.T,self.y_tr],axis=0).float()\n        self.yhat_tr = rslt_tr['yhat']\n        self.fhat_tr = torch.concat([self.train_dataset[0].x.T,self.yhat_tr],axis=0).float()\n\n\nimport pickle\nimport pandas as pd\n\n\ndef load_data(fname):\n    with open(fname, 'rb') as outfile:\n        data_dict = pickle.load(outfile)\n    return data_dict\n\ndef save_data(data_dict,fname):\n    with open(fname,'wb') as outfile:\n        pickle.dump(data_dict,outfile)\n\n\nT = 100\nt = np.arange(T)/T * 10 \n\nx = 0.3*np.sin(2*t)+0.1*np.sin(4*t)+0.1*np.sin(8*t)\neps_x  = np.random.normal(size=T)*0\ny = x.copy()\nfor i in range(2,T):\n    y[i] = 0.35*x[i-1] - 0.15*x[i-2] + 0.5*np.cos(0.5*t[i]) \neps_y  = np.random.normal(size=T)*0\nx = x\ny = y\nplt.plot(t,x,color='C0',lw=5)\nplt.plot(t,x+eps_x,alpha=0.5,color='C0')\nplt.plot(t,y,color='C1',lw=5)\nplt.plot(t,y+eps_y,alpha=0.5,color='C1')\n_node_ids = {'node1':0, 'node2':1}\n\n# _FX1 = np.stack([x,y],axis=1).tolist()\n_FX1 = np.stack([x+eps_x,y+eps_y],axis=1).tolist()\n\n_edges1 = torch.tensor([[0,1]]).tolist()\n\ndata_dict1 = {'edges':_edges1, 'node_ids':_node_ids, 'FX':_FX1}\n#data_dict = itstgcn.load_data('./data/fivenodes.pkl')\n\nsave_data(data_dict1, './data/toy_example1.pkl')\n\ndata1 = pd.DataFrame({'x':x,'y':y,'xer':x,'yer':y})\n\nsave_data(data1, './data/toy_example_true1.csv')\n\n\n\n\n\ndata_dict1 = itstgcn.load_data('./data/toy_example1.pkl')\nloader1 = itstgcn.DatasetLoader(data_dict1)\n\n\ndataset05031 = loader1.get_dataset(lags=4)\n\n\nmindex05031 = itstgcn.rand_mindex(dataset05031,mrate=0)\ndataset_miss05031 = itstgcn.miss(dataset05031,mindex05031,mtype='rand')\n\n\ndataset_miss05031.edge_index\n\narray([[0],\n       [1]])\n\n\n\ndataset_padded_cubic05031 = itstgcn.padding(dataset_miss05031,imputation_method='cubic')\n\n\ndataset_padded_cubic05031.edge_index\n\narray([[0],\n       [1]])\n\n\n- 학습\n\nlrnr05031 = itstgcn.StgcnLearner(dataset_padded_cubic05031)\n\n\nlrnr05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nlags4/0.3/filter16\nlags4/0.38/filter8\nlags8/0.38/filter8\n\n\nlrnr05031.learn(filters=16,epoch=50)\n\n50/50\n\n\n- 모형 평가 및 시각화\n\ndf1 = itstgcn.load_data('./data/toy_example_true1.csv')\n\n\nevtor05031 = Eval_csy(lrnr05031,dataset_padded_cubic05031)\n\n\nevtor05031.train_dataset.edge_index\n\narray([[0],\n       [1]])\n\n\n\nfig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2,figsize=(30,20))\n# fig.suptitle('Figure 1')\nax1.plot(df1['x'][:],'.',color='C0')\nax2.plot(df1['y'][:],'.',color='C0')\nax3.plot(df1['xer'][:],'.',color='C1')\nax4.plot(df1['yer'][:],'.',color='C1')\nax5.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax6.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax7.plot(df1['x'][:],'.',color='C0')\nax7.plot(evtor05031.fhat_tr[:,0],color='C3')\nax8.plot(df1['y'][:],'.',color='C0')\nax8.plot(evtor05031.fhat_tr[:,1],color='C3')\nax9.plot(df1['x'][:],'.',color='C0')\nax9.plot(df1['xer'][:],'.',color='C1')\nax9.plot(evtor05031.f_tr[:,0],'.',color='C2')\nax9.plot(evtor05031.fhat_tr[:,0],color='C3')\nax10.plot(df1['y'][:],'.',color='C0')\nax10.plot(df1['yer'][:],'.',color='C1')\nax10.plot(evtor05031.f_tr[:,1],'.',color='C2')\nax10.plot(evtor05031.fhat_tr[:,1],color='C3')\n\nfor ax in fig.get_axes():\n    ax.label_outer()"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pyg-의-data-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyG 의 Data 자료형",
    "text": "PyG 의 Data 자료형\n\nref: https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs\n\n- 자료는 PyG의 Data 오브젝트를 기반으로 한다.\n(예제) 아래와 같은 그래프자료를 고려하자.\n\n이러한 자료형은 아래와 같은 형식으로 저장한다.\n\nedge_index = torch.tensor([[0, 1, 1, 2],\n                           [1, 0, 2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\ndata = Data(x=x, edge_index=edge_index) # Data는 그래프자료형을 만드는 클래스\n\n\ntype(data)\n\ntorch_geometric.data.data.Data\n\n\n\ndata.x\n\ntensor([[-1.],\n        [ 0.],\n        [ 1.]])\n\n\n\ndata.edge_index\n\ntensor([[0, 1, 1, 2],\n        [1, 0, 2, 1]])"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#pytorch-geometric-temporal-의-자료형",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "PyTorch Geometric Temporal 의 자료형",
    "text": "PyTorch Geometric Temporal 의 자료형\n\nref: PyTorch Geometric Temporal Signal\n\n아래의 클래스들중 하나를 이용하여 만든다.\n## Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n## Heterogeneous Temporal Signal Iterators\ntorch_geometric_temporal.signal.StaticHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicHeteroGraphStaticSignal\n이중 “Heterogeneous Temporal Signal” 은 우리가 관심이 있는 신호가 아니므로 사실상 아래의 3개만 고려하면 된다.\n\ntorch_geometric_temporal.signal.StaticGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphTemporalSignal\ntorch_geometric_temporal.signal.DynamicGraphStaticSignal\n\n여기에서 StaticGraphTemporalSignal 는 시간에 따라서 그래프 구조가 일정한 경우, 즉 \\({\\cal G}_t=\\{{\\cal V},{\\cal E}\\}\\)와 같은 구조를 의미한다.\n(예제1) StaticGraphTemporalSignal 를 이용하여 데이터 셋 만들기\n- json data \\(\\to\\) dict\n\nimport json\nimport urllib\n\n\nurl = \"https://raw.githubusercontent.com/benedekrozemberczki/pytorch_geometric_temporal/master/dataset/chickenpox.json\"\ndata_dict = json.loads(urllib.request.urlopen(url).read())\n# data_dict 출력이 김\n\n\ndata_dict.keys()\n\ndict_keys(['edges', 'node_ids', 'FX'])\n\n\n- 살펴보기\n\nnp.array(data_dict['edges']).T\n\narray([[ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,\n         3,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n         6,  6,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12,\n        12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n        15, 15, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18,\n        18, 18, 19, 19, 19, 19],\n       [10,  6, 13,  1,  0,  5, 16,  0, 16,  1, 14, 10,  8,  2,  5,  8,\n        15, 12,  9, 10,  3,  4, 13,  0, 10,  2,  5,  0, 16,  6, 14, 13,\n        11, 18,  7, 17, 11, 18,  3,  2, 15,  8, 10,  9, 13,  3, 12, 10,\n         5,  9,  8,  3, 10,  2, 13,  0,  6, 11,  7, 13, 18,  3,  9, 13,\n        12, 13,  9,  6,  4, 12,  0, 11, 10, 18, 19,  1, 14,  6, 16,  3,\n        15,  8, 16, 14,  1,  0,  6,  7, 19, 17, 18, 14, 18, 17,  7,  6,\n        19, 11, 18, 14, 19, 17]])\n\n\n\n\\({\\cal E} = \\{(0,10),(0,6), \\dots, (19,17)\\}\\)\n혹은 \\({\\cal E} = \\{(\\tt{BACS},\\tt{JASZ}), ({\\tt BACS},{\\tt FEJER}), \\dots, (\\tt{ZALA},\\tt{VAS})\\}\\)\n\n\ndata_dict['node_ids']\n\n{'BACS': 0,\n 'BARANYA': 1,\n 'BEKES': 2,\n 'BORSOD': 3,\n 'BUDAPEST': 4,\n 'CSONGRAD': 5,\n 'FEJER': 6,\n 'GYOR': 7,\n 'HAJDU': 8,\n 'HEVES': 9,\n 'JASZ': 10,\n 'KOMAROM': 11,\n 'NOGRAD': 12,\n 'PEST': 13,\n 'SOMOGY': 14,\n 'SZABOLCS': 15,\n 'TOLNA': 16,\n 'VAS': 17,\n 'VESZPREM': 18,\n 'ZALA': 19}\n\n\n\n\\({\\cal V}=\\{\\tt{BACS},\\tt{BARANYA} \\dots, \\tt{ZALA}\\}\\)\n\n\nnp.array(data_dict['FX']), np.array(data_dict['FX']).shape\n\n(array([[-1.08135724e-03, -7.11136085e-01, -3.22808515e+00, ...,\n          1.09445310e+00, -7.08747750e-01, -1.82280792e+00],\n        [ 2.85705967e-02, -5.98430173e-01, -2.29097341e-01, ...,\n         -1.59220988e+00, -2.24597623e-01,  7.86330575e-01],\n        [ 3.54742090e-01,  1.90511208e-01,  1.61028185e+00, ...,\n          1.38183225e-01, -7.08747750e-01, -5.61724314e-01],\n        ...,\n        [-4.75512620e-01, -1.19952837e+00, -3.89043358e-01, ...,\n         -1.00023329e+00, -1.71429032e+00,  4.70746677e-02],\n        [-2.08645035e-01,  6.03766218e-01,  1.08216835e-02, ...,\n          4.71099041e-02,  2.45684924e+00, -3.44296107e-01],\n        [ 1.21464875e+00,  7.16472130e-01,  1.29038982e+00, ...,\n          4.56939849e-01,  7.43702632e-01,  1.00375878e+00]]),\n (521, 20))\n\n\n\n\\({\\bf f}=\\begin{bmatrix} {\\bf f}_1\\\\ {\\bf f}_2\\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}=\\begin{bmatrix} f(t=1,v=\\tt{BACS}) & \\dots & f(t=1,v=\\tt{ZALA}) \\\\ f(t=2,v=\\tt{BACS}) & \\dots & f(t=2,v=\\tt{ZALA}) \\\\ \\dots & \\dots & \\dots \\\\ f(t=521,v=\\tt{BACS}) & \\dots & f(t=521,v=\\tt{ZALA}) \\end{bmatrix}\\)\n\n즉 data_dict는 아래와 같이 구성되어 있음\n\n\n\n\n\n\n\n\n\n\n수학 기호\n코드에 저장된 변수\n자료형\n차원\n설명\n\n\n\n\n\\({\\cal V}\\)\ndata_dict['node_ids']\ndict\n20\n20개의 노드에 대한 설명이 있음\n\n\n\\({\\cal E}\\)\ndata_dict['edges']\nlist (double list)\n(102,2)\n노드들에 대한 102개의 연결을 정의함\n\n\n\\({\\bf f}\\)\ndata_dict['node_ids']\ndict\n(521,20)\n\\(f(t,v)\\) for \\(v \\in {\\cal V}\\) and \\(t = 1,\\dots, T\\)\n\n\n\n- 주어진 자료를 정리하여 그래프신호 \\(\\big(\\{{\\cal V},{\\cal E},{\\bf W}\\},{\\bf f}\\big)\\)를 만들면 아래와 같다.\n\nedges = np.array(data_dict[\"edges\"]).T\nedge_weight = np.ones(edges.shape[1])\nf = np.array(data_dict[\"FX\"])\n\n\n여기에서 edges는 \\({\\cal E}\\)에 대한 정보를\nedges_weight는 \\({\\bf W}\\)에 대한 정보를\nf는 \\({\\bf f}\\)에 대한 정보를 저장한다.\n\n\nNote: 이때 \\({\\bf W}={\\bf E}\\) 로 정의한다. (하지만 꼭 이래야 하는건 아니야)\n\n- data_dict \\(\\to\\) dl\n\nlags = 4\nfeatures = [f[i : i + lags, :].T for i in range(f.shape[0] - lags)]\ntargets = [f[i + lags, :].T for i in range(f.shape[0] - lags)]\n\n\nnp.array(features).shape, np.array(targets).shape\n\n((517, 20, 4), (517, 20))\n\n\n\n\n\n\n\n\n\n설명변수\n반응변수\n\n\n\n\n\\({\\bf X} = {\\tt features} = \\begin{bmatrix} {\\bf f}_1 & {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 \\\\ {\\bf f}_2 & {\\bf f}_3 & {\\bf f}_4 & {\\bf f}_5 \\\\ \\dots & \\dots & \\dots & \\dots \\\\ {\\bf f}_{517} & {\\bf f}_{518} & {\\bf f}_{519} & {\\bf f}_{520} \\end{bmatrix}\\)\n\\({\\bf y}= {\\tt targets} = \\begin{bmatrix} {\\bf f}_5 \\\\ {\\bf f}_6 \\\\ \\dots \\\\ {\\bf f}_{521} \\end{bmatrix}\\)\n\n\n\n\nAR 느낌으로 표현하면 AR(4) 임\n\n\ndataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(\n    edge_index= edges,\n    edge_weight = edge_weight,\n    features = features,\n    targets = targets\n)\n\n\ndataset\n\n&lt;torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal at 0x7faad2716a60&gt;\n\n\n- 그런데 이 과정을 아래와 같이 할 수도 있음\n# PyTorch Geometric Temporal 공식홈페이지에 소개된 코드\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset=loader.get_dataset(lags=4)\n- dataset은 dataset[0], \\(\\dots\\) , dataset[516]과 같은 방식으로 각 시점별 자료에 접근가능\n\ndataset[0]\n\nData(x=[20, 4], edge_index=[2, 102], edge_attr=[102], y=[20])\n\n\n각 시점에 대한 자료형은 아까 살펴보았던 PyG의 Data 자료형과 같음\n\ntype(dataset[0])\n\ntorch_geometric.data.data.Data\n\n\n\ndataset[0].x \n\ntensor([[-1.0814e-03,  2.8571e-02,  3.5474e-01,  2.9544e-01],\n        [-7.1114e-01, -5.9843e-01,  1.9051e-01,  1.0922e+00],\n        [-3.2281e+00, -2.2910e-01,  1.6103e+00, -1.5487e+00],\n        [ 6.4750e-01, -2.2117e+00, -9.6858e-01,  1.1862e+00],\n        [-1.7302e-01, -9.4717e-01,  1.0347e+00, -6.3751e-01],\n        [ 3.6345e-01, -7.5468e-01,  2.9768e-01, -1.6273e-01],\n        [-3.4174e+00,  1.7031e+00, -1.6434e+00,  1.7434e+00],\n        [-1.9641e+00,  5.5208e-01,  1.1811e+00,  6.7002e-01],\n        [-2.2133e+00,  3.0492e+00, -2.3839e+00,  1.8545e+00],\n        [-3.3141e-01,  9.5218e-01, -3.7281e-01, -8.2971e-02],\n        [-1.8380e+00, -5.8728e-01, -3.5514e-02, -7.2298e-02],\n        [-3.4669e-01, -1.9827e-01,  3.9540e-01, -2.4774e-01],\n        [ 1.4219e+00, -1.3266e+00,  5.2338e-01, -1.6374e-01],\n        [-7.7044e-01,  3.2872e-01, -1.0400e+00,  3.4945e-01],\n        [-7.8061e-01, -6.5022e-01,  1.4361e+00, -1.2864e-01],\n        [-1.0993e+00,  1.2732e-01,  5.3621e-01,  1.9023e-01],\n        [ 2.4583e+00, -1.7811e+00,  5.0732e-02, -9.4371e-01],\n        [ 1.0945e+00, -1.5922e+00,  1.3818e-01,  1.1855e+00],\n        [-7.0875e-01, -2.2460e-01, -7.0875e-01,  1.5630e+00],\n        [-1.8228e+00,  7.8633e-01, -5.6172e-01,  1.2647e+00]])\n\n\n\n이 값들은 features[0]의 값들과 같음. 즉 \\([{\\bf f}_1~ {\\bf f}_2~ {\\bf f}_3~ {\\bf f}_4]\\)를 의미함\n\n\ndataset[0].y\n\ntensor([ 0.7106, -0.0725,  2.6099,  1.7870,  0.8024, -0.2614, -0.8370,  1.9674,\n        -0.4212,  0.1655,  1.2519,  2.3743,  0.7877,  0.4531, -0.1721, -0.0614,\n         1.0452,  0.3203, -1.3791,  0.0036])\n\n\n\n이 값들은 targets[0]의 값들과 같음. 즉 \\({\\bf f}_5\\)를 의미함"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#summary-of-data",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "summary of data",
    "text": "summary of data\n\n\\(T\\) = 519\n\\(N\\) = 20 # number of nodes\n\\(|{\\cal E}|\\) = 102 # edges\n\\(f(t,v)\\)의 차원? (1,)\n시간에 따라서 Number of nodes가 변하는지? False\n시간에 따라서 Number of nodes가 변하는지? False\n\\({\\bf X}\\): (20,4)\n\\({\\bf y}\\): (20,)\n예제코드적용가능여부: Yes\n\n- Nodes : 20\n\nvertices are counties\n\n-Edges : 102\n\nedges are neighbourhoods\n\n- Time : 519\n\nbetween 2004 and 2014\nper weeks\n\n\nloader = torch_geometric_temporal.dataset.ChickenpoxDatasetLoader()\ndataset = loader.get_dataset(lags=4)\ntrain_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#learn",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "learn",
    "text": "learn\n\nmodel = RecurrentGCN(node_features=4, filters=32)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel.train()\n\nfor epoch in tqdm(range(50)):\n    for t, snapshot in enumerate(train_dataset):\n        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n        cost = torch.mean((yt_hat-snapshot.y)**2)\n        cost.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n100%|██████████| 50/50 [01:16&lt;00:00,  1.52s/it]\n\n\n\ndataset.features[0].shape\n\n(20, 4)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "href": "posts/3_Researches/YEON/ITSTGCN/2022-12-29-STGCN-tutorial.html#visualization",
    "title": "[IT-STGCN] STGCN 튜토리얼",
    "section": "visualization",
    "text": "visualization\n\nmodel.eval()\n\nRecurrentGCN(\n  (recurrent): GConvGRU(\n    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n  )\n  (linear): Linear(in_features=32, out_features=1, bias=True)\n)\n\n\n\nyhat_train = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in train_dataset]).detach().numpy()\nyhat_test = torch.stack([model(snapshot.x,snapshot.edge_index, snapshot.edge_attr) for snapshot in test_dataset]).detach().numpy()\n\n\nV = list(data_dict['node_ids'].keys())\n\n\nfig,ax = plt.subplots(20,1,figsize=(10,50))\nfor k in range(20):\n    ax[k].plot(f[:,k],'--',alpha=0.5,label='observed')\n    ax[k].set_title('node: {}'.format(V[k]))\n    ax[k].plot(yhat_train[:,k],label='predicted (tr)')\n    ax[k].plot(range(yhat_train.shape[0],yhat_train.shape[0]+yhat_test.shape[0]),yhat_test[:,k],label='predicted (test)')\n    ax[k].legend()\nfig.tight_layout()"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 비편향추정량(UB)란 \\(\\theta\\)의 추정량 중\n\\[\\forall \\theta\\in \\Theta:~ E(\\hat{\\theta})=\\theta\\]\n를 만족하는 추정량 \\(\\hat{\\theta}\\)을 의미한다.\n- (예시) 아래와 같은 상황을 가정하자.\n\\[X_n \\overset{iid}{\\sim} N(\\theta,1)\\]\n여기에서\n\n\\(\\hat{\\theta}_1=0\\) 은 \\(\\theta=0\\) 일 경우에는 \\(E(\\hat{\\theta})=\\theta\\) 를 만족하지만 그 외의 경우에는 \\(E(\\hat{\\theta})\\neq\\theta\\) 이므로 UB가 아니다.\n\\(\\hat{\\theta}_2=X_1\\) 은 UB이다.\n\\(\\hat{\\theta}_3=\\frac{X_1+X_2}{2}\\) 역시 UB이다.\n\\(\\hat{\\theta}_4=X_1+X_2-X_3\\) 역시 UB이다.\n\\(\\hat{\\theta}_5=-99X_1+100X_2\\) 역시 UB이다.\n\\(\\hat{\\theta}_6=\\frac{X_1+0}{2}\\) 은 1과 동일한 이유로 UB가 아니다.\n\\(\\hat{\\theta}_7=\\bar{X}\\)는 UB이다.\n\\(\\hat{\\theta}_8=w_1X_1+\\dots+w_nX_n\\) ,where \\(\\sum_{i=1}^{n}w_i=1\\) 형태의 estimator는 모두 UB이다.\n\n- 최소분산비편향추정량(MVUE)란 \\(\\theta\\)에 대한 비편향추정량을 모아놓은 집합 \\(\\hat{\\Theta}_{UB}\\) 에서 최소분산을 가지는 추정량을 의미한다. MVUE를 구하는 방법은 아래와 같다.\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n예를들어 위의 예제에서 \\(V(\\hat{\\theta}_2)=1\\) 이고 \\(V(\\hat{\\theta}_3)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}\\) 이므로 \\(\\hat{\\theta}_3\\) 이 더 좋은 추정량이라 볼 수 있다.\n- (의문) 왜 비편향추정량만 모아서 그중에서 최소분산을 구할까?\n\n\\(\\hat{\\theta}_1\\)와 같은 추정량은 \\(V(\\hat{\\theta}_1)=0\\) 이므로 그냥 최소분산을 만족한다. 따라서 이러한 추정량은 제외해야지 게임이 성립함.\n\n- 불만: 아래의 방법으로 구하는건 거의 불가능하지 않나?\n\n\\(\\theta\\)에 대한 모든 비편향추정량을 구한다. 즉 집합 \\(\\hat{\\Theta}_{UB}\\)를 구한다. 그리고 \\(\\forall \\hat{\\theta} \\in \\hat{\\Theta}_{UB}\\) 에 대하여 \\(V(\\hat{\\theta})\\) 를 구한 뒤 \\(V(\\hat{\\theta})\\)가 가장 작은 \\(\\hat{\\theta}\\)를 선택한다.\n\n- 이론: 크래머라오 하한값(편의상 \\(L^\\star\\)이라고 하자)이라고 있는데, 이는 \\({\\Theta}_{UB}\\)에 존재하는 모든 추정량에 대한 분산의 하한값을 제공한다.1 즉 아래가 성립한다.\n1 (사실 \\({\\Theta}_{UB}\\)가 아닌 집합에 대해서도 하한값을 제공함, 그런데 교재에서는 \\({\\Theta}_{UB}\\)에 대한 하한값만 다루는듯\n\\(L^\\star\\) is Cramer-Rao lower bound \\(\\Rightarrow\\) \\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L^\\star\\)\n\n역은 성립하지 않음을 주의하자. 즉 아래를 만족하는 \\(L\\)이 존재할 수 있다.\n\n\\(V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) for some \\(\\hat{\\theta} \\in \\Theta_{UB}\\)\n\n- 위의 이론을 이용하면 아래의 논리전개를 펼 수 있다.\n\n\\(L^\\star\\)를 구한다.\n왠지 MVUE가 될 것 같은 \\(\\hat{\\theta}\\)을 하나 찍고 그것의 분산 \\(V(\\hat{\\theta})\\)를 구한다.\n만약에 \\(V(\\hat{\\theta})=L^\\star\\)를 만족하면 그 \\(\\hat{\\theta}\\)이 MVUE라고 주장할 수 있다.\n\n- 위의 논리전개에 대한 불만 [p.212]\n\n\\(V(\\hat{\\theta})=L^\\star\\) 이길 기도해야함.\n\\(\\forall \\hat{\\theta} \\in {\\Theta}_{UB}:~ V(\\hat{\\theta}) \\geq L &gt; L^\\star\\) 와 같은 \\(L\\)이 존재하는 경우는 쓸 수 없음.\n\n- 또 다른 방법: 완비충분통계량을 이용함\n\n\n아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)\n\n\n\n\n- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다.\n\n\n\n- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#충분통계량",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#충분통계량",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "아래와 같은 상황을 가정하자.\n\\[ X_1,\\dots,X_n \\overset{iid}{\\sim} P_{\\theta}\\]\n- 충분통계량(SS)의 느낌: “이 값만 기억하면 \\(\\theta\\)를 추정하는데 무난할듯”\n- 예시1: \\(X_1 \\sim N(\\theta,1)\\)\n\n\\(X_1\\)은 \\(\\theta_1\\) 의 SS. (하나밖에 없으니 그거라도 기억해야지)\n즉 \\(\\hat{\\theta}=X_1\\)은 \\(\\theta\\)의 SS\n\n- 예시2: \\(X_1,X_2 \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)는 \\(\\theta\\)의 SS (둘다 기억하면 당연히 \\(\\theta\\)를 추정함에 있어서 충분함)\n그렇지만 좀 더 생각해보면 굳이 값 두개를 기억하기보다 \\(\\frac{1}{2}(X_1+X_2)\\)의 값만 기억해도 왠지 충분할것 같음. 따라서 \\(\\hat{\\theta} = \\frac{1}{2}(X_1+X_2)\\) 역시 \\(\\theta\\)의 SS 일듯\n그런데 좀 더 생각해보니까 \\(X_1+X_2\\)의 값만 기억해도 \\(\\frac{1}{2}(X_1+X_2)\\)를 나중에 만들 수 있음 (1/2만 곱하면 되니까) 따라서 \\(X_1+X_2\\)만 기억해도 왠지 충분할 것 같음. 따라서 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- 예시3: \\(X_1,\\dots,X_n \\sim N(\\theta,1)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2,\\dots,X_n)\\)은 \\(\\theta\\)의 SS.\n하지만 \\(n\\)개의 숫자를 기억할 필요 없이 \\(\\sum_{i=1}^{n} X_i\\) 하나의 숫자만 기억해도 왠지 충분할듯. 그래서 \\(\\hat{\\theta} = \\sum_{i=1}^{n} X_i\\) 역시 \\(\\theta\\)의 SS 일듯\n\n- SS에 대한 직관1\n\n기억할 숫자가 적을수록 유리 -&gt; MSS의 개념\n충분통계량의 1:1은 충분통계량 (\\(\\frac{1}{2}(X_1+X_2)\\)을 기억하면 충분한 상황이라면, \\(X_1+X_2\\)를 기억해도 충분하니까..)\n\n- 예시4: \\(X_1,X_2 \\sim {\\cal B}er(\\theta)\\)\n\n당연히 \\(\\hat{\\boldsymbol \\theta}=(X_1,X_2)\\)은 \\(\\theta\\)의 SS.\n그리고 \\(\\hat{\\theta}=X_1+X_2\\) 역시 \\(\\theta\\) SS 일듯.\n두개보다 한개가 유리하니까 둘다 SS이면 \\((X_1,X_2)\\)보다 \\(X_1+X_2\\)가 더 좋은 SS.\n\\(X_1\\)은 SS가 아닐듯. \\(p\\)를 추정함에 있어서 \\(X_1\\)만 가지고서는 충분하지 않아보임\n\\(X_2\\)도 SS가 아닐듯.\n\n왠지 충분할 것 같은 느낌의 정의\n아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n- 일반적으로\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=\\theta(1-\\theta)\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=(1-\\theta)^2\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=\\theta^2\\)\n\n와 같은 확률들은 \\(\\theta\\)가 unknown일 때 하나의 숫자로 정할 수 없다. 예를들어 \\(\\theta=0\\) 이라면 아래와 같을 것이고\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=0\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=0\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=0\\)\n\n\\(\\theta=1/2\\) 이라면 아래와 같을 것이다.\n\n\\(P((X_1,X_2)=(0,0))=P(X_1=0,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(0,1))=P(X_1=0,X_2=1)=1/4\\)\n\\(P((X_1,X_2)=(1,0))=P(X_1=1,X_2=0)=1/4\\)\n\\(P((X_1,X_2)=(1,1))=P(X_1=1,X_2=1)=1/4\\)\n\n즉 \\(X_1,X_2\\)의 결합확률분포는 \\(\\theta\\)가 변함에 따라 같이 변화한다. 이를 이용해 우리는 \\(X_1,X_2\\)의 결합확률분포에서 관찰한 샘플들을 이용하여 \\(\\theta\\)의 값을 역으로 추론한다.\n- 만약에 어떠한 “특수한 정보를 알고 있을 경우” \\(X_1,X_2\\)의 결합확률분포를 완벽하게 기술할 수 있을 때를 가정해보자.\n- 경우1: \\(\\theta\\)를 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(\\theta=1/2\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=0,X_2=1 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=0 | \\theta=1/2)=1/4\\)\n\\(P(X_1=1,X_2=1 | \\theta=1/2)=1/4\\)\n\n- 경우2: \\(X_1,X_2\\)의 realization을 알고 있을 경우. \\((X_1,X_2)\\)의 조인트를 완벽하게 기술할 수 있다. 예를들어 \\(X_1=0,X_2=1\\)일 경우는 아래와 같다.\n\n\\(P(X_1=0,X_2=0 | X_1=0,X_2=0)=0\\)\n\\(P(X_1=0,X_2=1| X_1=0,X_2=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1,X_2=0)=1\\)\n\\(P(X_1=1,X_2=1| X_1=1,X_2=1)=0\\)\n\n- 경우3: \\((X_1+X_2)(\\omega)\\)의 realization을 알고 있을 경우. 이때도 매우 특이하게 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 있다.\ncase1: \\(X_1+X_2=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=0)=1\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=0)=0\\)\n\ncase2: \\(X_1+X_2=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=1)=1/2\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=1)=0\\)\n\ncase3: \\(X_1+X_2=2\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=0,X_2=1| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=0| X_1+X_2=2)=0\\)\n\\(P(X_1=1,X_2=1| X_1+X_2=2)=1\\)\n\n- 경우4: \\(X_1\\)의 realization만 알고 있을 경우. 이때는 \\((X_1,X_2)\\) 의 조인트를 완벽하게 기술할 수 없다.\ncase1: \\(X_1=0\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=0)=1-\\theta\\)\n\\(P(X_1=0,X_2=1| X_1=0)=\\theta\\)\n\\(P(X_1=1,X_2=0| X_1=0)=0\\)\n\\(P(X_1=1,X_2=1| X_1=0)=0\\)\n\ncase2: \\(X_1=1\\)일 경우\n\n\\(P(X_1=0,X_2=0| X_1=1)=0\\)\n\\(P(X_1=0,X_2=1| X_1=1)=0\\)\n\\(P(X_1=1,X_2=0| X_1=1)=1-\\theta\\)\n\\(P(X_1=1,X_2=1| X_1=1)=\\theta\\)\n\n- 종합해보면 경우1,경우2,경우3은 경우4와 구분되는 어떠한 공통점을 가지고 있다 볼 수 있다. 특징은 결합확률분포가 \\(\\theta\\)에 대한 함수로 표현되지 않는다는 것이다. 하나씩 살펴보면\n\n경우1: 당연히 \\(\\theta\\)를 줬으니까 \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우2: \\(X_1,X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n경우3: \\(X_1+X_2\\)를 줬음. \\((X_1,X_2)\\)의 조인트는 \\(\\theta\\)에 의존하지 않음.\n\n이렇게보면 경우1과 경우2,3은 또 다시 구분된다. 경우1은 \\(\\theta\\)에 대한 완전한 정보를 준 상황이므로 당연히 조인트는 \\(\\theta\\)에 의존하지 않는다. 경우2-3은 \\(\\theta\\)를 주지 않았음에도 조인트가 \\(\\theta\\)에 의존하지 않는 매우 특별해보이는 상황이다. 따라서 이를 통해서 유추하면\n\n경우2에서는 \\((X_1,X_2)\\) 가 경우3에서는 \\(X_1+X_2\\)가 \\(\\theta\\)에 대한 완전한 정보를 대신하고 있는것 아닐까?\n\n라는 생각이 든다. 정리하면\n\n경우2: \\((X_1,X_2)\\)을 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n경우3: \\(X_1+X_2\\)를 주는 것은 \\(\\theta\\)의 값을 그냥 알려주는 것과 대등한 효과\n\n라고 해석할 수 있는데 이를 수식화 하면 아래와 같다.\n- 대충정의: 어떠한 통계량 \\(S\\)의 값을 줬을때, \\((X_1,X_2\\dots,X_n)\\)의 조인트가 \\(\\theta\\)에 의존하지 않으면 그 통계량 \\(S\\)를 \\(\\theta\\)의 충분통계량이라고 한다.\n- 충분통계량 구하는 방법\n\n지수족일때 구하는 방식이 있음! &lt;– 외우세여\n분해정리를 쓰는 경우. &lt;– 거의 안쓰는거같은데..\n1-2로도 잘 모르겠으면 충분통계량일듯한 애를 잡아와서 정의에 넣고 노가다로 때려맞춤. (문제가 디스크릿할때만 쓸것)"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#최소충분통계량",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#최소충분통계량",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량에 대한 realization을 알려주면 \\(\\theta\\)의 값을 그냥 알려주는 효과임. 그래서 충분통계량은 좋은 것임\n- 그런데 충분통계량에도 급이 있음. 아래와 같은 상황을 가정하자.\n\\[X_1,X_2 \\sim {\\cal B}er(\\theta)\\]\n이 경우\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n이지만 1은 두개의 숫자를 기억해야하고 2는 하나의 숫자만 기억하면 되니까 2가 더 좋음\n- 예비개념: 상태1과 상태2가 있다고 하자. 상태1에서 상태2로 가는 변화는 쉽지만, 상태2에서 상태1로 가는 변화는 어렵다고 할때, 상태1이 더 좋은 상태이다.\n\n두가지 상태 “500원을 가지고 있음”, “1000원을 가지고 있음” 을 고려하자. 1000원을 500원을 만드는 것은 쉽지만 500원을 1000원으로 만들기는 어렵다. 따라서 1000원이 더 좋은 상태이다.\n\n- 충분통계량의 급을 어떻게 구분할까? 아래의 상황에서\n\n\\((X_1,X_2)\\)는 SS\n\\(X_1+X_2\\)는 SS\n\n1을 이용하면 2를 만들 수 있지만, 2를 이용해서 1을 만들 수는 없음. 즉 \\(1\\to 2\\) 인 변환(=함수)는 가능하지만 \\(2\\to 1\\)로 만드는 변환(=함수)는 가능하지 않음. 예비개념을 잘 이해했다면 2가 더 좋은 상태라고 볼 수 있다.\n- 이를 확장하자. 어떠한 충분 통계량 \\(S^\\star\\)가 있다고 가정하자. 다른 모든 충분통계량 \\(S_1,S_2,S_3 \\dots\\)에서 \\(S^\\star\\)로 만드는 변환은 존재하는데 (함수는 존재하는데) 그 반대는 \\(S^\\star\\)의 전단사인 충분통계량만 가능하다고 하자. 그렇다면 \\(S^\\star\\)는 가장 좋은 충분통계량이라고 하며, 가장 적은 숫자만 기억하면 되는 충분통계량이라 볼 수 있다. 이러한 충분통계량을 MSS 라고 하자."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#라오블랙웰",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#라오블랙웰",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#레만쉐페정리",
    "href": "posts/3_Researches/PINKOCTO/2023-05-13-추정 for JY.html#레만쉐페정리",
    "title": "[PINKOCTO] 추정 for JY",
    "section": "",
    "text": "- 충분통계량 \\(S\\)을 알려주면 (기븐하면) \\(\\theta\\)에 대한 완벽한 정보를 알려주는 셈이다. 따라서 \\(\\theta\\)를 추정하는 어떠한 추정량 \\(\\hat{\\theta}\\)도 충분통계량의 정보 \\(S\\)가 있다면 \\(\\hat{\\theta}\\)를 업그레이드할 수 있다고 볼 수 있다. (뭐 수틀리면 정보야 안쓰면 그만이니까 나빠질것은 없다)\n- 충분통계량을 줬을때 \\(\\hat{\\theta}\\)의 업그레이드 방법은\n\\[\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\]\n와 같이 할 수 있는데 이는 충분통계량 \\(S\\)의 정보를 받아서 어떠한 방식으로 업데이트된 \\(\\hat{\\theta}\\) 이므로 \\(S\\)의 함수라 해석할 수 있다.\n- 이론 (라오블랙웰, Thm, 4.5): \\(\\hat{\\theta}\\)가 UB일때 충분통계량의 값을 알려주면 \\(\\hat{\\theta}^{new}:=E(\\hat{\\theta}|S)\\)와 같이 \\(\\hat{\\theta}\\)를 업그레이드 할 수 있다."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/notes.html",
    "href": "posts/3_Researches/PINKOCTO/notes.html",
    "title": "[PINKOCTO] 면담",
    "section": "",
    "text": "23-07-17\n- 실험해볼것 (모형, 필터/lag, 에폭 변화하여서) 50번정도..\n\n에폭이 적을수록 우리가 잘맞았음.\n필터나 lag도 단순할수록 괜찮았음\nlag은 24를 넘길 필요는 없음.\n필터는 64이상은 필요없는듯\n\n- quarto 팁\nquarto preview --host 210.117.173.182 --no-browser\n- 파라메트릭 vs 넌파 vs 세미\n- 비트시그널\n- 발표자료 + 페이퍼작업\n- 레퍼런스정리\n\nGNN -&gt; GCN / RGNN -&gt; STGCN, ST-GCN\n\n23-07-20\n- 라마: https://www.youtube.com/watch?v=OZbarkziC14\n- 저널 vs 학회:\n\n원래개념: SCIE (~10) &gt;&gt;&gt;&gt;&gt;&gt; SCIE (10~50) &gt;&gt; SCIE(50~) &gt;&gt;&gt;&gt;&gt;&gt; KCI &gt;&gt; 컨퍼런스\nAI분야 한정: SCIE (~10), 일부top컨퍼런스 (NIPS, AAAI, ICML, ICLR) &gt;&gt;&gt;&gt;&gt;&gt; SCIE (10~50) &gt;&gt; SCIE(50~) &gt;&gt;&gt;&gt;&gt;&gt; KCI &gt;&gt; 컨퍼런스\n\n- 오타발견시알려주세요\n\nhttps://guebin.github.io/PP2023SUM/\n\n23-08-14\n논문쓰는 방법\n1. introduction = motivating ex (원래 반반)\n\nmotivating ex 로 시작할수도 있고 –&gt; 방법론에 강점이 있을때\nsolar enery에 대한 개론으로 시작 –&gt; 데이터가 자신있을 경우\n담백하게 (1) goal (2) method (3) contribution –&gt; 이론적인 측면에 자신이 있을때 꼭 들어가야 하는 것\n구성 // Graphical abstract\n기존방법과 차이점 (*)\n\n\n\n\nGraphical abstract 예시 Li et al. (2022)\nLi, Dan, Fuxin Jiang, Min Chen, and Tao Qian. 2022. “Multi-Step-Ahead Wind Speed Forecasting Based on a Hybrid Decomposition Method and Temporal Convolutional Networks.” Energy 238: 121981.\n\n\n\n2. review existing methods (원래 지윤학생, 수틀리면 내가)\n\nRGCN 까지 이어지는 연구흐름을 간략하게 소개\n\n3. proposed method (원래 지윤학생이 해야하는데, 안되면 내가)\n\n우리가 제안하는 방법이 기존과 무엇이 다르냐?\n수식, 알고리즘, 이론적 백그라운드\n\n4. simulation study (연구의 꽃) - beat signal - priodic signal * low-freq amplitude signal 꼴의 신호의 분해 - 시뮬레이션 데이터를 고안 + 결과를 입증\n5. real-data analysis (지윤학생이 거의다 해야함) - 일사량자료 분석 결과 - 우리가 제인한방법 + 기존의 방법들 -&gt; 퍼포먼스 비교 - 데이터설명 - 데이터는 어디에서 얻었는지, 언제부터 언제까지인지, 데이터의 특징 (주기성을 가진다. 0인 값이 많다) - 그림을 보여줘야함\n6. conclusion & future works & limitations & 어디에 활용\n\n결론: 좋은 분석이었다..\n한계: 어떠한 점에 한계가 있다. -&gt; 향후연구: 향후에는 어떤연구로 발전할 것이다.. (다른변수를 고려해서 할 예정이다)\n활용: 주기성을 가진 어떠한 자료도 우리의 방법을 사용하면 잘 예측할수있어요.."
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-03-일사량자료정리.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-04-03-일사량자료정리.html",
    "title": "[SOLAR] 일사량자료정리",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport itertools\n\n\ndf0 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data0.csv', encoding='cp949') # 2021-01-01 ~ 2021-12-31\ndf1 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/OBS_ASOS_TIM_data1.csv') # 2022-01-01 ~ 2023-12-31\ndf2 = pd.read_csv('https://raw.githubusercontent.com/pinkocto/noteda/main/posts/data/test_raw.csv', encoding='cp949') # 2023-01-01 ~ 2023-01-15\n\n- df_raw\n\ndf_raw = pd.concat([df0, df1])\ndf_raw\n\n\n\n\n\n\n\n\n지점\n지점명\n일시\n일사(MJ/m2)\n\n\n\n\n0\n93\n북춘천\n2021-01-01 08:00\n0.00\n\n\n1\n93\n북춘천\n2021-01-01 09:00\n0.37\n\n\n2\n93\n북춘천\n2021-01-01 10:00\n0.96\n\n\n3\n93\n북춘천\n2021-01-01 11:00\n1.40\n\n\n4\n93\n북춘천\n2021-01-01 12:00\n1.72\n\n\n...\n...\n...\n...\n...\n\n\n229672\n283\n경주시\n2022-12-31 14:00:00\n1.82\n\n\n229673\n283\n경주시\n2022-12-31 15:00:00\n1.52\n\n\n229674\n283\n경주시\n2022-12-31 16:00:00\n0.96\n\n\n229675\n283\n경주시\n2022-12-31 17:00:00\n0.35\n\n\n229676\n283\n경주시\n2022-12-31 18:00:00\n0.01\n\n\n\n\n444720 rows × 4 columns\n\n\n\n- 지점칼럼 삭제 // 일시 \\(\\to\\) 날짜,시간 으로 분리\n\ndf_temp = df_raw.assign(날짜= list(map(lambda x: x[:10],df_raw['일시'])))\\\n.assign(시간= list(map(lambda x: x[11:16],df_raw['일시'])))\\\n.drop(['일시','지점'],axis=1).rename({'일사(MJ/m2)':'일사'},axis=1).reset_index(drop=True)\ndf_temp\n\n\n\n\n\n\n\n\n지점명\n일사\n날짜\n시간\n\n\n\n\n0\n북춘천\n0.00\n2021-01-01\n08:00\n\n\n1\n북춘천\n0.37\n2021-01-01\n09:00\n\n\n2\n북춘천\n0.96\n2021-01-01\n10:00\n\n\n3\n북춘천\n1.40\n2021-01-01\n11:00\n\n\n4\n북춘천\n1.72\n2021-01-01\n12:00\n\n\n...\n...\n...\n...\n...\n\n\n444715\n경주시\n1.82\n2022-12-31\n14:00\n\n\n444716\n경주시\n1.52\n2022-12-31\n15:00\n\n\n444717\n경주시\n0.96\n2022-12-31\n16:00\n\n\n444718\n경주시\n0.35\n2022-12-31\n17:00\n\n\n444719\n경주시\n0.01\n2022-12-31\n18:00\n\n\n\n\n444720 rows × 4 columns\n\n\n\n- 파주,상주,동두천,충주,제천은 삭제\n\ndf_temp = df_temp.query(\"지점명 not in ['파주','상주','동두천','충주','제천']\").reset_index(drop=True)\ndf_temp\n\n\n\n\n\n\n\n\n지점명\n일사\n날짜\n시간\n\n\n\n\n0\n북춘천\n0.00\n2021-01-01\n08:00\n\n\n1\n북춘천\n0.37\n2021-01-01\n09:00\n\n\n2\n북춘천\n0.96\n2021-01-01\n10:00\n\n\n3\n북춘천\n1.40\n2021-01-01\n11:00\n\n\n4\n북춘천\n1.72\n2021-01-01\n12:00\n\n\n...\n...\n...\n...\n...\n\n\n420955\n경주시\n1.82\n2022-12-31\n14:00\n\n\n420956\n경주시\n1.52\n2022-12-31\n15:00\n\n\n420957\n경주시\n0.96\n2022-12-31\n16:00\n\n\n420958\n경주시\n0.35\n2022-12-31\n17:00\n\n\n420959\n경주시\n0.01\n2022-12-31\n18:00\n\n\n\n\n420960 rows × 4 columns\n\n\n\n- 시간이 비어있지 않도록..\n\nreg = df_temp['지점명'].unique().tolist() \nday = df_temp['날짜'].unique().tolist() \ntime = list(df_temp['시간'].unique())\ntime = ['0{}:00'.format(i) for i in range(0,8)] + time\n\n\ndf_temp2 = pd.DataFrame(itertools.product(reg,day,time)).rename({0:'지점명',1:'날짜',2:'시간'},axis=1).merge(df_temp,how='left').fillna(0)\ndf_temp2\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.0\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.0\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.0\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.0\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.0\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.0\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.0\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.0\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.0\n\n\n\n\n803000 rows × 4 columns\n\n\n\n\ndf_temp2[:20]\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.00\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.00\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.00\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.00\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.00\n\n\n5\n북춘천\n2021-01-01\n05:00\n0.00\n\n\n6\n북춘천\n2021-01-01\n06:00\n0.00\n\n\n7\n북춘천\n2021-01-01\n07:00\n0.00\n\n\n8\n북춘천\n2021-01-01\n08:00\n0.00\n\n\n9\n북춘천\n2021-01-01\n09:00\n0.37\n\n\n10\n북춘천\n2021-01-01\n10:00\n0.96\n\n\n11\n북춘천\n2021-01-01\n11:00\n1.40\n\n\n12\n북춘천\n2021-01-01\n12:00\n1.72\n\n\n13\n북춘천\n2021-01-01\n13:00\n1.84\n\n\n14\n북춘천\n2021-01-01\n14:00\n1.74\n\n\n15\n북춘천\n2021-01-01\n15:00\n1.30\n\n\n16\n북춘천\n2021-01-01\n16:00\n0.93\n\n\n17\n북춘천\n2021-01-01\n17:00\n0.29\n\n\n18\n북춘천\n2021-01-01\n18:00\n0.01\n\n\n19\n북춘천\n2021-01-01\n19:00\n0.00\n\n\n\n\n\n\n\n\ndf_temp2[-20:]\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n802980\n경주시\n2022-12-31\n05:00\n0.00\n\n\n802981\n경주시\n2022-12-31\n06:00\n0.00\n\n\n802982\n경주시\n2022-12-31\n07:00\n0.00\n\n\n802983\n경주시\n2022-12-31\n08:00\n0.02\n\n\n802984\n경주시\n2022-12-31\n09:00\n0.41\n\n\n802985\n경주시\n2022-12-31\n10:00\n1.05\n\n\n802986\n경주시\n2022-12-31\n11:00\n1.52\n\n\n802987\n경주시\n2022-12-31\n12:00\n1.86\n\n\n802988\n경주시\n2022-12-31\n13:00\n1.93\n\n\n802989\n경주시\n2022-12-31\n14:00\n1.82\n\n\n802990\n경주시\n2022-12-31\n15:00\n1.52\n\n\n802991\n경주시\n2022-12-31\n16:00\n0.96\n\n\n802992\n경주시\n2022-12-31\n17:00\n0.35\n\n\n802993\n경주시\n2022-12-31\n18:00\n0.01\n\n\n802994\n경주시\n2022-12-31\n19:00\n0.00\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.00\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.00\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.00\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.00\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.00\n\n\n\n\n\n\n\n\ndf_temp2\n\n\n\n\n\n\n\n\n지점명\n날짜\n시간\n일사\n\n\n\n\n0\n북춘천\n2021-01-01\n00:00\n0.0\n\n\n1\n북춘천\n2021-01-01\n01:00\n0.0\n\n\n2\n북춘천\n2021-01-01\n02:00\n0.0\n\n\n3\n북춘천\n2021-01-01\n03:00\n0.0\n\n\n4\n북춘천\n2021-01-01\n04:00\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n802995\n경주시\n2022-12-31\n07:00\n0.0\n\n\n802996\n경주시\n2022-12-31\n20:00\n0.0\n\n\n802997\n경주시\n2022-12-31\n06:00\n0.0\n\n\n802998\n경주시\n2022-12-31\n21:00\n0.0\n\n\n802999\n경주시\n2022-12-31\n05:00\n0.0\n\n\n\n\n803000 rows × 4 columns\n\n\n\n- 시간,날짜 \\(\\to\\) 일시\n\ndf_temp3=df_temp2.assign(일시 = list(map(lambda x,y: x+'-'+y,df_temp2['날짜'],df_temp2['시간'])))\\\n.drop(['날짜','시간'],axis=1)\ndf_temp3\n\n\n\n\n\n\n\n\n지점명\n일사\n일시\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n- 저장\n\ndf_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\n\n\n\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n\ndf = df_temp3.rename({'지점명':'region','일사':'solar_radiation','일시':'date'},axis=1)\ndf.to_csv(\"solar_radiation.csv\",index=False)\n!git add .\n!git commit -m .\n!git push \n\n[main 299d058] .\n 3 files changed, 806273 insertions(+)\n create mode 100644 \"posts/3_Researches/SOLAR/.ipynb_checkpoints/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211-checkpoint.ipynb\"\n create mode 100644 \"posts/3_Researches/SOLAR/2023-04-03-\\354\\235\\274\\354\\202\\254\\353\\237\\211.ipynb\"\n create mode 100644 posts/3_Researches/SOLAR/solar_radiation.csv\nEnumerating objects: 10, done.\nCounting objects: 100% (10/10), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (7/7), done.\nWriting objects: 100% (7/7), 8.74 KiB | 8.74 MiB/s, done.\nTotal 7 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo https://github.com/miruetoto/yechan3.git\n   495d9ce..299d058  main -&gt; main\n\n\n- 불러오기\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv\")\ndf\n\n\n\n\n\n\n\n\nregion\nsolar_radiation\ndate\n\n\n\n\n0\n북춘천\n0.0\n2021-01-01-00:00\n\n\n1\n북춘천\n0.0\n2021-01-01-01:00\n\n\n2\n북춘천\n0.0\n2021-01-01-02:00\n\n\n3\n북춘천\n0.0\n2021-01-01-03:00\n\n\n4\n북춘천\n0.0\n2021-01-01-04:00\n\n\n...\n...\n...\n...\n\n\n802995\n경주시\n0.0\n2022-12-31-07:00\n\n\n802996\n경주시\n0.0\n2022-12-31-20:00\n\n\n802997\n경주시\n0.0\n2022-12-31-06:00\n\n\n802998\n경주시\n0.0\n2022-12-31-21:00\n\n\n802999\n경주시\n0.0\n2022-12-31-05:00\n\n\n\n\n803000 rows × 3 columns\n\n\n\n- 다운로드\n!wget https://raw.githubusercontent.com/miruetoto/yechan3/main/posts/3_Researches/SOLAR/solar_radiation.csv"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-20-시뮬레이션.html",
    "href": "posts/3_Researches/PINKOCTO/SOLAR/2023-07-20-시뮬레이션.html",
    "title": "[SOLAR] EPT + RGCN (시뮬레이션)",
    "section": "",
    "text": "%run 0807.py\n\n\nimport os\n\n\ns=Simulator()\n\n\nclass GConv_GRU(torch.nn.Module):\n    def __init__(self, node_features, filters):\n        super(GConv_GRU, self).__init__()\n        self.recurrent = GConvGRU(node_features, filters, 2)\n        self.linear = torch.nn.Linear(filters, 1)\n\n    def forward(self, x, edge_index, edge_weight):\n        h = self.recurrent(x, edge_index, edge_weight)\n        h = F.relu(h)\n        h = self.linear(h)\n        return h\n        \ns.simulate(\n    n_iteration = 1,\n    lags = [1],\n    filters = [4,6],\n    epoch = [1],\n    Model = GConv_GRU\n)\n\nmethod:classic lag:1 filters:4 epoch:1 is done\nmethod:classic lag:1 filters:6 epoch:1 is done\nmethod:proposed lag:1 filters:4 epoch:1 is done\nmethod:proposed lag:1 filters:6 epoch:1 is done\nsave results\n\n\n\nfnames = ['./results/'+l for l in os.listdir('./results/') if l[0] != '.']\nfnames \n\n['./results/SOLAR_2023-08-07 16:12:29.037623.csv',\n './results/SOLAR_2023-08-07 15:48:50.109743.csv',\n './results/SOLAR_2023-08-07 16:08:41.687843.csv',\n './results/SOLAR_2023-08-07 15:48:25.728993.csv',\n './results/SOLAR_2023-08-07 16:28:55.929269.csv']\n\n\n\npd.concat([pd.read_csv(fname) for fname in fnames]).reset_index(drop=True)\n\n\n\n\n\n\n\n\nindex_time\nmse_mean\nmodel\nmethod\nlags\nnof_filters\nepoch\ncalculation_time\nnode\nmse\n\n\n\n\n0\n2023-08-07 16:11:11.072667\n0.419854\nRecurrentGCN\nclassic\n1\n4\n1\n15.519606\nBukchoncheon\n0.491867\n\n\n1\n2023-08-07 16:11:11.072667\n0.419854\nRecurrentGCN\nclassic\n1\n4\n1\n15.519606\nCheorwon\n0.474034\n\n\n2\n2023-08-07 16:11:11.072667\n0.419854\nRecurrentGCN\nclassic\n1\n4\n1\n15.519606\nDaegwallyeong\n0.468020\n\n\n3\n2023-08-07 16:11:11.072667\n0.419854\nRecurrentGCN\nclassic\n1\n4\n1\n15.519606\nChuncheon\n0.433953\n\n\n4\n2023-08-07 16:11:11.072667\n0.419854\nRecurrentGCN\nclassic\n1\n4\n1\n15.519606\nBaengnyeongdo\n0.620410\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n875\n2023-08-07 16:28:55.924567\n0.139105\nGConvGRU\nproposed\n1\n6\n1\n30.982189\nUiryeong-gun\n0.140765\n\n\n876\n2023-08-07 16:28:55.924567\n0.139105\nGConvGRU\nproposed\n1\n6\n1\n30.982189\nHamyang-gun\n0.122213\n\n\n877\n2023-08-07 16:28:55.924567\n0.139105\nGConvGRU\nproposed\n1\n6\n1\n30.982189\nGwangyang-si\n0.144140\n\n\n878\n2023-08-07 16:28:55.924567\n0.139105\nGConvGRU\nproposed\n1\n6\n1\n30.982189\nCheongsong-gun\n0.143174\n\n\n879\n2023-08-07 16:28:55.924567\n0.139105\nGConvGRU\nproposed\n1\n6\n1\n30.982189\nGyeongju-si\n0.109372\n\n\n\n\n880 rows × 10 columns"
  },
  {
    "objectID": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "href": "posts/3_Researches/PINKOCTO/2023-06-23-퓨리에변환4jy.html",
    "title": "[PINKOCTO] 퓨리에변환4jy",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\n회귀모형 (1)\n\nx = np.linspace(-10,10,1000)\nx0 = x*0+1\nx1 = x \nbeta0 = 3 \nbeta1 = 2\ny = x0*beta0+x1*beta1+np.random.randn(1000)\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\n\n회귀모형 (2)\n- 관측한자료\n\nN=1000\nx=np.linspace(0,1,N)\neps = np.random.randn(N)\nX0 = np.sin(x*0*np.pi)\nX1 = np.sin(x*2*np.pi)\nX2 = np.sin(x*4*np.pi)\nX3 = np.sin(x*6*np.pi)\n\ny=2*X1+1*X2+3*X3+eps\n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n\nobserved signal\n\n- 위의 자료를 해석하는 방법\n\ndef spec(y):\n    N= len(y)\n    return abs(np.fft.fft(y)/N)*2 \n\n\ny=2*X1+1*X2+3*X3+eps\nyfft =spec(y) \ny1=2*X1\ny2=1*X2\ny3=3*X3\nyfft1=spec(y1)\nyfft2=spec(y2)\nyfft3=spec(y3)\nepsfft=spec(eps)\n\n\nplt.plot(yfft[:20],'o',alpha=0.5)\nplt.plot(yfft1[:20],'x',alpha=1,)\nplt.plot(yfft2[:20],'x',alpha=1)\nplt.plot(yfft3[:20],'x',alpha=1)\nplt.plot(epsfft[:20],'x',alpha=1)\n\n\n\n\n- 퓨리에변환 -&gt; threshold -&gt; 역퓨리에변환을 이용한 스킬\n\nyfft=np.fft.fft(y)\n\n\nplt.plot(abs(yfft[1:50]),'o')\n\n\n\n\n\nyfft[abs(yfft)&lt;100] = 0\n\n\nplt.plot(y,'o',alpha=0.1)\nyhat=np.fft.ifft(yfft)\nplt.plot(yhat,'--')\nplt.plot(y-eps,'-')\n\n\n\n\n\nplt.plot(spec2(y)[:50],'o')\nplt.plot(spec2(yhat)[:50],'x')\n\n\n\n\n\n\n삼성전자 주가자료를 스무딩해보기\n- 삼성전자 자료\n\nimport yfinance as yf\n\n\nstart_date = \"2023-01-01\"\nend_date = \"2023-05-02\"\ny = yf.download(\"005930.KS\", start=start_date, end=end_date)['Adj Close'].to_numpy()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\nplt.plot(y)\n\n\n\n\n- 스펙트럼\n\nyfft = np.fft.fft(y)\n\n\nplt.plot(abs(yfft))\n\n\n\n\n- 처음 50개정도만 관찰\n\nplt.plot(abs(yfft[:50]),'o')\n\n\n\n\n\n첫값이 너무커서 나머지는 잘안보임\n\n- 2번째부터 50번째까지만 관찰\n\nplt.plot(abs(yfft)[2:50],'o')\nplt.axhline(y=22500, color='r', linestyle='--')\n\n&lt;matplotlib.lines.Line2D at 0x7f6250d5bfd0&gt;\n\n\n\n\n\n\n대충 이정도 짜르면 될것같음\n\n- thresholded value\n\ntresh_value = 22500\n\n\nyfft[abs(yfft)&lt;tresh_value] =0 \n\n- 퓨리에역변환\n\nyhat = np.fft.ifft(yfft)\nyhat[:5]\n\narray([59664.72193044+8.87311904e-14j, 58572.98839934+8.87311904e-14j,\n       58066.07369126+3.39894326e-14j, 58169.18671667-6.87747670e-14j,\n       58706.41986821-1.14383435e-13j])\n\n\n실수화\n\nyhat = np.real(yhat)\nyhat[:5]\n\narray([59664.72193044, 58572.98839934, 58066.07369126, 58169.18671667,\n       58706.41986821])\n\n\n- 적합결과 시각화\n\nplt.plot(y)\nplt.plot(yhat,'--')\n\n\n\n\n- 숙제: treshold value를 관찰하며 시각화해볼것\n\n\nminor topics\n- y의 FFT 결과는 항상 y와 같은길이임\n\nlen(y)\n\n82\n\n\n\nlen(np.fft.fft(y))\n\n82\n\n\n- 에일리어싱: number of observation은 얼마나 세밀한 주파수까지 측정가능하냐를 결정함\n예시1: 에일리어싱\n\nx = np.linspace(-3.14,3.14,10)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.        , -0.99975131],\n       [-0.99975131,  1.        ]])\n\n\n\nplt.plot(x1,label='x1')\nplt.plot(x2,label='x2')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f6252112ee0&gt;\n\n\n\n\n\n\n실제로는 x2가 더 고주파인데, 같은 주파수처럼 보임\n\n예시2: 에일리어싱이 없는 경우\n\nx = np.linspace(-3.14,3.14,100000)\n\n\nx1 = np.sin(8*x)\nx2 = np.sin(10*x)\n\n\nnp.corrcoef([x1,x2])\n\narray([[ 1.00000000e+00, -6.45767105e-08],\n       [-6.45767105e-08,  1.00000000e+00]])\n\n\n\nplt.plot(x1)\nplt.plot(x2)\n\n\n\n\n\n주파수 왜곡떄문에 실제로는 corr ceof = 0 일지라도 관측되는건 corr coef &gt;0 일 수 있음"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\n# from ctgan import CTGAN\n# from ctgan import load_demo\n\n\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html#데이터-종류",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-03-CTGAN_신용카드.html#데이터-종류",
    "title": "[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.columns\n\nIndex(['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt',\n       'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat',\n       'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat',\n       'merch_long', 'is_fraud'],\n      dtype='object')\n\n\n\nfraudTrain['is_fraud']\n\n0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n1048570    0\n1048571    0\n1048572    0\n1048573    0\n1048574    0\nName: is_fraud, Length: 1048575, dtype: int64\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\nNameError: name 'sklearn' is not defined\n\n\n\ndf50\n\nNameError: name 'df50' is not defined\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#imports",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#imports",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n/home/cgb2/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"fraudTrain.csv\")\n_df1=  df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42)\n_df2 = df[df[\"is_fraud\"] == 1]\ndf = pd.concat([_df1,_df2])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#embedding",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#embedding",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#learn",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#learn",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#evaluate",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#evaluate",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data-1",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#read-and-define-data-1",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "read and define data",
    "text": "read and define data\n\nlrnr2, _,_,_ = anal(our_sampling1(df),n_estimators=100)\n\nComputing transition probabilities: 100%|██████████| 1289/1289 [00:49&lt;00:00, 26.00it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:10&lt;00:00,  1.10s/it]"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#네트워크-토폴로지",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#네트워크-토폴로지",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =&gt;\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#커뮤니티-감지",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#커뮤니티-감지",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96&gt;&gt;113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n&lt;Axes: ylabel='Frequency'&gt;\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n&lt;Axes: ylabel='Frequency'&gt;"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-05-06-start.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "[FRAUD] 신용카드 거래 사기탐지 Start",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\ndf.shape\n\n(318777, 23)\n\n\n\ndf_minority.shape\n\n(6006, 23)\n\n\n\ndf_majority.shape\n\n(312771, 23)\n\n\n\n6006 / 312771 \n\n0.019202547550763976\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib&lt;2.0.0,&gt;=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim&lt;5.0.0,&gt;=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm&lt;5.0.0,&gt;=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx&lt;3.0,&gt;=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy&gt;=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim&lt;5.0.0,&gt;=4.1.2-&gt;node2vec) (1.10.1)\nCollecting smart-open&gt;=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.45it/s]\n\n\n\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(yhat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7236842105263158\nRecall: 0.1407849829351536\nF1-Score: 0.2357142857142857\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 0])\n\n\n\nyhat = rf.predict_proba(test_embeddings)\n\n\nyhat\n\narray([[0.457, 0.543],\n       [0.634, 0.366],\n       [0.609, 0.391],\n       ...,\n       [0.577, 0.423],\n       [0.59 , 0.41 ],\n       [0.557, 0.443]])\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-19-그래프자료로 데이터정리.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-19-그래프자료로 데이터정리.html",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport pandas as pd\nimport torch \nimport torch_geometric\n\n- 모든엣지를 고려\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\ndiff = fraudTrain.trans_date_trans_time[10]-fraudTrain.trans_date_trans_time[0]\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9965337989703691\n\n\n\n!git add .\n!git commit -m. \n!git push \n!quarto publish --no-browser --no-prompt"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-07-02-CTGAN-TOY.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-07-02-CTGAN-TOY.html",
    "title": "[CTGAN] CTGAN ToyEX",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ctgan import CTGAN\nfrom ctgan import load_demo\n\n\n\ndata\n\nx1 = np.random.randn(1000).tolist()\ny1 = ['A']*1000\nx2 = (np.random.randn(100)+5).tolist()\ny2 = ['B']*100\n\n\ndf = pd.DataFrame({'x':x1+x2, 'y':y1+y2})\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n-0.905509\nA\n\n\n1\n0.844968\nA\n\n\n2\n-0.374190\nA\n\n\n3\n-1.311042\nA\n\n\n4\n-1.657453\nA\n\n\n...\n...\n...\n\n\n1095\n4.890136\nB\n\n\n1096\n6.158887\nB\n\n\n1097\n4.367205\nB\n\n\n1098\n4.704626\nB\n\n\n1099\n4.428676\nB\n\n\n\n\n1100 rows × 2 columns\n\n\n\n\nplt.hist(df.x,bins=50)\n\n(array([ 3.,  0.,  1.,  3.,  7.,  9., 20., 26., 43., 39., 44., 46., 72.,\n        76., 78., 69., 80., 89., 59., 49., 59., 48., 27., 22., 12.,  5.,\n         8.,  3.,  2.,  1.,  1.,  4.,  1.,  4.,  3.,  8.,  9.,  7., 11.,\n         6.,  6.,  6.,  8.,  8.,  6.,  4.,  5.,  2.,  0.,  1.]),\n array([-3.13891042, -2.93493634, -2.73096226, -2.52698818, -2.3230141 ,\n        -2.11904002, -1.91506594, -1.71109186, -1.50711778, -1.3031437 ,\n        -1.09916962, -0.89519554, -0.69122146, -0.48724738, -0.2832733 ,\n        -0.07929922,  0.12467486,  0.32864894,  0.53262302,  0.7365971 ,\n         0.94057118,  1.14454526,  1.34851934,  1.55249342,  1.7564675 ,\n         1.96044158,  2.16441566,  2.36838974,  2.57236382,  2.7763379 ,\n         2.98031198,  3.18428606,  3.38826014,  3.59223422,  3.7962083 ,\n         4.00018238,  4.20415646,  4.40813053,  4.61210461,  4.81607869,\n         5.02005277,  5.22402685,  5.42800093,  5.63197501,  5.83594909,\n         6.03992317,  6.24389725,  6.44787133,  6.65184541,  6.85581949,\n         7.05979357]),\n &lt;BarContainer object of 50 artists&gt;)\n\n\n\n\n\n\n\nCTGAN\n\n# Names of the columns that are discrete\ndiscrete_columns = ['y']\nctgan = CTGAN(epochs=500) # 겁나많이해야하네?\nctgan.fit(df, discrete_columns)\n\n# Create synthetic data\ndf2 = ctgan.sample(1000)\n\n/home/cgb2/anaconda3/envs/ctgan/lib/python3.10/site-packages/rdt/transformers/base.py:132: FutureWarning: Future versions of RDT will not support the 'model_missing_values' parameter. Please switch to using the 'missing_value_generation' parameter to select your strategy.\n  warnings.warn(\n\n\n\ndf2.groupby('y').count()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\nA\n364\n\n\nB\n636\n\n\n\n\n\n\n\n\nplt.hist(df.x,bins=50,alpha=0.5,label='real')\nplt.hist(df2.x,bins=50,alpha=0.5,label='syn')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fc14a3222c0&gt;\n\n\n\n\n\n\ndf2[df2.y=='A'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)\n\n\n\n\n\n\ndf2[df2.y=='B'].hist()\n\narray([[&lt;Axes: title={'center': 'x'}&gt;]], dtype=object)"
  },
  {
    "objectID": "posts/3_Researches/BORAM/FRAUD/2023-08-22 데이터(8, df02)커널죽음.out.html",
    "href": "posts/3_Researches/BORAM/FRAUD/2023-08-22 데이터(8, df02)커널죽음.out.html",
    "title": "[FRAUD] 데이터정리 시도(8.25 df02 커널죽음)",
    "section": "",
    "text": "김보람\n2023-08-25\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection # split함수이용\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n- df02\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02 = df02.reset_index()\n\n- df_toy\n\ndf_toy=df02[:5].copy()\ndf_toy.cc_num = pd.Series([1,1,1,2,2])\ndf_toy\n\n\n\n\n\n\n\n\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n669418\n2019-10-12 18:21:00\n1\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n1\n32567\n2019-01-20 13:06:00\n1\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n2\n156587\n2019-03-24 18:09:00\n1\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n3\n1020243\n2020-02-25 15:12:00\n2\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n4\n116272\n2019-03-06 23:19:00\n2\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n- df_toy 에서 time_difference 구함\n고객1\n\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[1].trans_date_trans_time.value\n\n22914900000000000\n\n\n\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n17453520000000000\n\n\n\ndf_toy.iloc[1].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n-5461380000000000\n\n\n고객2\n\ndf_toy.iloc[3].trans_date_trans_time.value - df_toy.iloc[4].trans_date_trans_time.value\n\n30729180000000000\n\n\n고객1,2\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\ngroups = df_toy.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nedge_index_list_plus_nparr\n\narray([[                0,                 0,                 0],\n       [                0,                 1, 22914900000000000],\n       [                0,                 2, 17453520000000000],\n       [                1,                 0, 22914900000000000],\n       [                1,                 1,                 0],\n       [                1,                 2,  5461380000000000],\n       [                2,                 0, 17453520000000000],\n       [                2,                 1,  5461380000000000],\n       [                2,                 2,                 0],\n       [                3,                 3,                 0],\n       [                3,                 4, 30729180000000000],\n       [                4,                 3, 30729180000000000],\n       [                4,                 4,                 0]])\n\n\n- df02에서 time_difference 구함\n\nt1 = time.time()\ngroups = df02.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nnp.save('edge_index_list_plus02.npy', edge_index_list_plus_nparr)\nt2 = time.time()\nt2-t1\n\n15537.573440551758\n\n\n\n(t2-t1)/3600\n\n4.3159926223754885"
  },
  {
    "objectID": "3_yeon.html",
    "href": "3_yeon.html",
    "title": "SEOYEONC",
    "section": "",
    "text": "https://github.com/seoyeonc\nhttps://seoyeonc.github.io/sy_hub/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 14, 2023\n\n\n[IT-STGCN] 논문리비전\n\n\n최서연 \n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연 \n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 1, 2023\n\n\n[SEOYEONC] type1 err, type2 err\n\n\n신록예찬 \n\n\n\n\nJan 12, 2023\n\n\n[SEOYEONC] 지수분포 평균검정\n\n\n신록예찬 \n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬 \n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬 \n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_yechan.html",
    "href": "3_yechan.html",
    "title": "YECHAN",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 18, 2023\n\n\n[EPT-DISSIM] PRCP_KOR2 데이터정리\n\n\n신록예찬 \n\n\n\n\nAug 8, 2023\n\n\n[EPT-DISSIM] 진행사항\n\n\n신록예찬 \n\n\n\n\nAug 7, 2023\n\n\n[EPT-DISSIM] PRCP_KOR 데이터정리\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "2_pyg.html",
    "href": "2_pyg.html",
    "title": "PyG",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 19, 2023\n\n\n[PyG] lesson6: GCN\n\n\n신록예찬 \n\n\n\n\nJul 14, 2023\n\n\n[PyG] lesson5: Learning Methods on Graphs\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson4: Data Transform???\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson3: 미니배치\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)\n\n\n신록예찬 \n\n\n\n\nJul 2, 2023\n\n\n[PyG] lesson1: 자료형\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "3_pinkocto.html",
    "href": "3_pinkocto.html",
    "title": "PINKOCTO",
    "section": "",
    "text": "https://github.com/pinkocto\nhttps://pinkocto.github.io/noteda/\nhttps://pinkocto.github.io/Quarto-Blog/\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 14, 2023\n\n\n[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++\n\n\nJiyunLim \n\n\n\n\nAug 7, 2023\n\n\n[SOLAR] EPT + RGCN (시뮬레이션)\n\n\n신록예찬 \n\n\n\n\nJul 17, 2023\n\n\n[SOLAR] EPT + RGCN\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬 \n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\nJan 1, 2023\n\n\n[PINKOCTO] 면담\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "신록예찬's Blog",
    "section": "",
    "text": "About this blog\nThis blog was created for my personal research, study and lecture preparation. Therefore, the contents of the blog can be thought of as my practice notes. As a result, sometimes the content of a post may be left unstructured or unfinished. The blog is named after my favorite essay ‘신록예찬’, which is also a nickname I use informally. You can check the written article in the sidebar on the left. It is a great honor for me if these posts can help others to learn and research.\n\nSome links that help me\nfirst url bunch\n\nmatplotlib: https://matplotlib.org/stable/gallery/index.html\njupyterlab: https://jupyterlab.readthedocs.io/en/stable/index.html\npandas: https://pandas.pydata.org/docs/user_guide/index.html#user-guide\njulia: https://docs.julialang.org/en/v1/\nkeras: https://keras.io/examples/\njulia plots: https://docs.juliaplots.org/stable/\npytorch lightning: https://www.pytorchlightning.ai/\nplotly: https://plotly.com/graphing-libraries/\nquarto: https://quarto.org/\n\nsecond url bunch\n\nlatex: https://editor.codecogs.com/\ntable: https://www.tablesgenerator.com/\npytorch lightning (codes in book): https://github.com/PacktPublishing/Deep-Learning-with-PyTorch-Lightning\nPyG: https://github.com/rusty1s/pytorch_geometric\nrayshader: https://www.rayshader.com/reference/plot_gg.html\nSouth Korea (map): https://github.com/southkorea\nregexp: https://zvon.org/comp/m/regexp.html\nrpy2: https://rpy2.github.io/doc/v3.1.x/html/index.html\npywave: https://pywavelets.readthedocs.io/en/latest/ref/wavelets.html\nPyGSP: https://pygsp.readthedocs.io/en/stable/#\nlatex (neural networks): https://tikz.net/neural_networks/\nplotly overview: https://plotly.com/python/plotly-express/\nfastai (official): https://docs.fast.ai/\nfastai (lecture): https://course.fast.ai/#\nfastai (github codes): https://github.com/fastai/fastai/tree/master/dev_nbs/course\nGML (codes in book): https://github.com/PacktPublishing/Graph-Machine-Learning\nGMLKOR (codes in book): https://github.com/AcornPublishing/graph-ml\nmathNET https://gtribello.github.io/mathNET/index.html\n\nlectures notes\n\nhttp://personal.psu.edu/drh20/asymp/fall2006/lectures/\nhttps://web.ma.utexas.edu/users/gordanz/lecture_notes_page.html\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n \n\n\n[FRAUD] 데이터정리 시도(8.25 df02 커널죽음)\n\n\n \n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nJan 1, 2099\n\n\n[Essays] ggplot3\n\n\n신록예찬 \n\n\n\n\nSep 14, 2023\n\n\n[IT-STGCN] 논문리비전\n\n\n최서연 \n\n\n\n\nSep 12, 2023\n\n\n02wk-003: 타이타닉, 첫 제출\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-004: 타이타닉, Alexis Cook의 코드\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-005: 타이타닉, Autogluon\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-006: 타이타닉, Autogluon (Fsize)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-007: 타이타닉, Autogluon (Fsize,Drop)\n\n\n최규빈 \n\n\n\n\nSep 12, 2023\n\n\n02wk-008: 타이타닉, Autogluon (best_quality)\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-001: 캐글소개\n\n\n최규빈 \n\n\n\n\nSep 5, 2023\n\n\n01wk-002: 타이타닉, 데이터의 이해\n\n\n최규빈 \n\n\n\n\nAug 30, 2023\n\n\n[Essays] 강화학습(3) – LunarLander\n\n\n신록예찬 \n\n\n\n\nAug 23, 2023\n\n\n[Essays] 강화학습(1) – bandit\n\n\n신록예찬 \n\n\n\n\nAug 23, 2023\n\n\n[Essays] 강화학습(2) – 4x4 grid\n\n\n신록예찬 \n\n\n\n\nAug 18, 2023\n\n\n[EPT-DISSIM] PRCP_KOR2 데이터정리\n\n\n신록예찬 \n\n\n\n\nAug 16, 2023\n\n\n[Essays] 루나랜더 v2 (간략화)\n\n\n신록예찬 \n\n\n\n\nAug 14, 2023\n\n\n[SOLAR] GConvLSTM +++++++ 이거!!! ++++++++++++\n\n\nJiyunLim \n\n\n\n\nAug 8, 2023\n\n\n[EPT-DISSIM] 진행사항\n\n\n신록예찬 \n\n\n\n\nAug 7, 2023\n\n\n[EPT-DISSIM] PRCP_KOR 데이터정리\n\n\n신록예찬 \n\n\n\n\nAug 7, 2023\n\n\n[Essays] 루나랜더 v1\n\n\n신록예찬 \n\n\n\n\nAug 7, 2023\n\n\n[SOLAR] EPT + RGCN (시뮬레이션)\n\n\n신록예찬 \n\n\n\n\nJul 30, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 CTGAN 이용 (하다가 말았음)\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[PyG] lesson6: GCN\n\n\n신록예찬 \n\n\n\n\nJul 19, 2023\n\n\n[Review] 사기탐지 + 그래프 관련연구\n\n\n신록예찬 \n\n\n\n\nJul 17, 2023\n\n\n[SOLAR] EPT + RGCN\n\n\n신록예찬 \n\n\n\n\nJul 14, 2023\n\n\n[PyG] lesson5: Learning Methods on Graphs\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson3: 미니배치\n\n\n신록예찬 \n\n\n\n\nJul 12, 2023\n\n\n[PyG] lesson4: Data Transform???\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[Essays] 다중척도방법\n\n\n신록예찬 \n\n\n\n\nJul 7, 2023\n\n\n[PyG] lesson2: 벤치마크 데이터셋 (train/test분리)\n\n\n신록예찬 \n\n\n\n\nJul 4, 2023\n\n\n[Essays] 토폴로지\n\n\n신록예찬 \n\n\n\n\nJul 2, 2023\n\n\n[PyG] lesson1: 자료형\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN\n\n\n신록예찬 \n\n\n\n\nJul 1, 2023\n\n\n[CTGAN] CTGAN ToyEX\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 커널리그레션\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환(detailed)\n\n\n신록예찬 \n\n\n\n\nJun 23, 2023\n\n\n[PINKOCTO] 퓨리에변환4jy\n\n\n신록예찬 \n\n\n\n\nJun 14, 2023\n\n\n[PINKOCTO] 추정 for JY\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2\n\n\n신록예찬 \n\n\n\n\nMay 19, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try2 변형\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1\n\n\n신록예찬 \n\n\n\n\nMay 12, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Try1 변형\n\n\n신록예찬 \n\n\n\n\nMay 6, 2023\n\n\n[FRAUD] 신용카드 거래 사기탐지 Start\n\n\n신록예찬 \n\n\n\n\nApr 27, 2023\n\n\n[IT-STGCN] Toy Example Figure(Intro)\n\n\n최서연 \n\n\n\n\nApr 4, 2023\n\n\n[SOLAR] EPT\n\n\n신록예찬 \n\n\n\n\nApr 3, 2023\n\n\n[SOLAR] 일사량자료정리\n\n\n임지윤, 신록예찬\n\n\n\n\nMar 18, 2023\n\n\n[IT-STGCN] SimualtionPlanner-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 17, 2023\n\n\n[IT-STGCN] ITSTGCN-Tutorial\n\n\nSEOYEON CHOI\n\n\n\n\nMar 1, 2023\n\n\n[SEOYEONC] type1 err, type2 err\n\n\n신록예찬 \n\n\n\n\nJan 20, 2023\n\n\n[Essays] 추정\n\n\n신록예찬 \n\n\n\n\nJan 15, 2023\n\n\n[CGSP] Chap 12.4: Node Subsampling for PSD Estimation\n\n\n신록예찬 \n\n\n\n\nJan 12, 2023\n\n\n[SEOYEONC] 지수분포 평균검정\n\n\n신록예찬 \n\n\n\n\nJan 1, 2023\n\n\n[PINKOCTO] 면담\n\n\n신록예찬 \n\n\n\n\nDec 30, 2022\n\n\n[IT-STGCN] STGCN: Toy Example\n\n\n신록예찬 \n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] STGCN 튜토리얼\n\n\n신록예찬, 최서연\n\n\n\n\nDec 29, 2022\n\n\n[IT-STGCN] Tables\n\n\n신록예찬 \n\n\n\n\nDec 27, 2022\n\n\n[CGSP] Chap 12.2 ~ 3: Power Spectral Density and its Estimators\n\n\n신록예찬 \n\n\n\n\nDec 26, 2022\n\n\n[CGSP] Chap 12.2: Weakly Stationary Graph Processes\n\n\n신록예찬 \n\n\n\n\nDec 24, 2022\n\n\n[CGSP] Chap 8.3: Discrete Fourier Transform\n\n\n신록예찬 \n\n\n\n\nDec 23, 2022\n\n\n[Review] EbayesThresh: R Programs for Empirical Bayes Thresholding\n\n\n신록예찬 \n\n\n\n\nSep 21, 2022\n\n\n[PL] Lesson1: 단순선형회귀\n\n\n신록예찬 \n\n\n\n\nApr 26, 2019\n\n\n[Essays] 퓨리에 변환\n\n\n신록예찬 \n\n\n\n\nJan 9, 2000\n\n\n[Note] docker\n\n\n신록예찬 \n\n\n\n\nJan 8, 2000\n\n\n[Note] 주피터랩: 설정 및 몇가지 팁\n\n\n신록예찬 \n\n\n\n\nJan 7, 2000\n\n\n[Note] 줄리아 설치 및 실행\n\n\n신록예찬 \n\n\n\n\nJan 6, 2000\n\n\n[Note] 깃 익히기\n\n\n신록예찬 \n\n\n\n\nJan 5, 2000\n\n\n[Note] vi 익히기\n\n\n신록예찬 \n\n\n\n\nJan 4, 2000\n\n\n[Note] 우분투 익히기\n\n\n신록예찬 \n\n\n\n\nJan 2, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅2\n\n\n신록예찬 \n\n\n\n\nJan 1, 2000\n\n\n[Note] 우분투 포맷 및 개발용 서버 셋팅 (old)\n\n\n신록예찬 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#a.-gender_submission.csv",
    "href": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#a.-gender_submission.csv",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "A. gender_submission.csv",
    "text": "A. gender_submission.csv\n- 왜 여자만 살까?\n\n\n\n\n\n\nGTP 답변\n\n\n\ngender_submission.csv파일은 단순히 모든 여성 승객이 생존한다고 가정한 예측을 담은 파일로, 실제 데이터에 기반한 예측이 아닙니다. 이 파일은 제출 파일의 형식과 구성을 보여주기 위한 예시입니다. 실제로는 성별이나 다른 특성에 따라 생존 여부가 결정되는 것이 아닌, 여러 다양한 변수들이 생존에 영향을 미치게 됩니다.\n실제로 타이타닉호에서는 여성들이 생존율이 높았습니다. 타이타닉호 침몰 사건 당시 여성과 아이들을 먼저 구조하고 구명보트에 태우는 등의 우선순위가 있었기 때문입니다. 따라서 “gender_submission.csv” 파일은 예시로서 제공되었고, 실제 예측 모델을 개발할 때에는 여러 다른 변수들도 고려하여 예측해야 합니다."
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#b.-embarked",
    "href": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#b.-embarked",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "B. embarked",
    "text": "B. embarked\n- 항구는 승객의 배경과 사회적 상황을 반영할 수 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n네, 항구별로 승객들의 배경과 사회적 상황이 다를 수 있습니다. 다양한 항구에서 탑승한 승객들은 지역적, 경제적, 문화적인 차이를 반영할 수 있습니다. 몇 가지 예시를 들어보겠습니다:\n\n경제적 차이: 서로 다른 항구에서 탑승한 승객들은 다른 경제적 배경을 가지고 있을 수 있습니다. 상류층 승객이나 부유한 승객들이 탑승한 항구일수록 1등급 티켓을 선택한 경우가 많을 것입니다.\n문화적 차이: 다른 항구에서 탑승한 승객들은 다른 문화적 배경을 가지고 있을 수 있습니다. 이로 인해 승객들의 습관, 언어, 관습 등이 다를 수 있습니다.\n지역적 차이: 항구의 위치와 지역적 특성에 따라 승객들의 배경이 달라질 수 있습니다. 해당 지역의 주요 산업이나 경제 활동에 따라 승객들의 직업이나 사회적 지위가 다를 수 있습니다.\n가족 관계: 특정 항구에서 가족들이 함께 탑승한 경우, 해당 지역의 가족 구조와 관련된 차이가 반영될 수 있습니다.\n\n이러한 다양한 차이를 분석하여 항구별로 생존율이나 다른 변수들과의 상관 관계를 파악할 수 있습니다. 이는 모델링 과정에서 추가적인 특성 엔지니어링이나 분석에 활용될 수 있습니다.\n\n\n- 항구는 국적에 대한 정보를 내포할 수도 있다.\n\n\n\n\n\n\nGPT 답변\n\n\n\n타이타닉호의 데이터에서는 승객들의 국적에 대한 직접적인 정보는 제공되지 않습니다. 따라서 승객들의 국적에 대한 정보를 확인하는 것은 어렵습니다. 그러나 항구별로 승객들의 출발지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수는 있습니다.\n예를 들어, Cherbourg 항구가 프랑스에 위치하며, Queenstown 항구가 아일랜드에 위치해 있습니다. 따라서 해당 항구에서 탑승한 승객들 중 일부는 해당 국가의 국적을 가질 가능성이 높을 것입니다. 그러나 국적은 항상 탑승 항구와 일치하지 않을 수 있으며, 탑승한 승객들의 다양한 출신지와 배경을 고려해야 합니다.\n요약하면, 타이타닉호 데이터에서는 직접적인 국적 정보는 제공되지 않지만, 항구별로 승객들의 출신지나 탑승 도시에 따라서 일부 국적적 특성이 반영될 수 있습니다."
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "href": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#c.-sibsp와-parch에-대한-피처엔지니어링-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어",
    "text": "C. sibsp와 parch에 대한 피처엔지니어링 아이디어\n- 두 변수 sibsp와 parch를 합쳐 family_size라는 하나의 변수를 만들고, 이를 활용하여 y를 예측할 수 있다.\n\n데이터를 봐야 하겠지만 가족단위로 보트를 줬을 확률도 큼\n가족단위로 보트를 실제로 주었을 경우 family_size라는 변수는 매우 의미있는 해석이 됨\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n가족 규모 특성 생성: ‘sibsp’와 ’parch’ 변수를 활용하여 승객의 가족 규모를 나타내는 새로운 변수를 만들 수 있습니다. 예를 들어, ‘sibsp’와 ’parch’ 값을 더한 후 1을 더해 ‘family_size’ 변수로 만들 수 있습니다. 이를 통해 가족 규모가 큰 승객이 생존 여부에 영향을 미칠 수 있는지 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "href": "posts/2_Studies/MP/01wk-002-타이타닉, 데이터의 이해.html#d.-embarked와-class에-대한-피처엔지니어링의-아이디어",
    "title": "01wk-002: 타이타닉, 데이터의 이해",
    "section": "D. embarked와 class에 대한 피처엔지니어링의 아이디어",
    "text": "D. embarked와 class에 대한 피처엔지니어링의 아이디어\n- 경우에 따라서 두가지 범주를 하나도 합쳐서 만든 새로운 변수가 매우 의미있을 수 있음\n\n경우에 따라서 특정 국적 (프랑스)의 1등급 손님이 구명보트를 많이 얻었을 수도 있음.\n이럴경우 embarked와 class를 독립적으로 생각하는것보다 둘을 합친 변수가 훨씬 유의미한 변수임\n\n\n\n\n\n\n\nGPT 답변\n\n\n\n물론, 자세한 설명을 드리겠습니다. “항구와 클래스의 조합 특성 생성”이라는 개념은 ‘embarked’ (탑승 항구)와 ‘pclass’ (티켓 클래스) 두 변수를 활용하여 새로운 특성을 만들어 분석하는 것을 의미합니다.\n예를 들어, “embarked” 변수에는 ‘C’, ‘Q’, ‘S’와 같은 항구 정보가 있으며, “pclass” 변수에는 ’1’, ‘2’, ’3’과 같은 티켓 클래스 정보가 있습니다. 이 두 변수를 조합하여 새로운 변수를 만들어보겠습니다.\n생성할 새로운 변수의 예시:\n“embarked_pclass”: ‘C_1’, ‘C_2’, ‘C_3’, ‘Q_1’, ‘Q_2’, ‘Q_3’, ‘S_1’, ‘S_2’, ‘S_3’ 위와 같이 “embarked”와 “pclass”를 조합하여 새로운 변수 값을 만듭니다. 예를 들어 ’embarked’가 ’C’이고 ’pclass’가 ’1’인 경우, “embarked_pclass” 변수 값은 ’C_1’이 됩니다. 이렇게 조합된 변수는 해당 승객이 어떤 항구에서 탑승했고 어떤 티켓 클래스를 사용했는지를 동시에 나타내는 특성입니다.\n이렇게 생성된 “embarked_pclass” 변수를 사용하여 특정 항구와 클래스의 조합이 생존 여부에 어떤 영향을 미치는지를 분석할 수 있습니다. 예를 들어, ‘C_1’ 조합이 생존율과 관련이 있는지, ‘Q_3’ 조합이 생존 여부에 영향을 미치는지 등을 확인하여 해당 특성이 모델링에 유용한 정보를 제공하는지 알아볼 수 있습니다.\n\n\n\n\n\n\n\n\nGPT 답변 2 – 그냥 “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것과 무슨차이냐는 질문에 대한 답변\n\n\n\n네, “embarked” 변수와 “pclass” 변수를 각각 넣고 분석하는 것도 가능합니다. 그러나 새로운 변수를 만드는 것은 데이터를 조금 더 유용하게 활용하는 방법 중 하나입니다. 여기에 몇 가지 이유가 있습니다:\n상호작용 효과 강조: 새로운 변수를 만들면 “embarked”와 “pclass”의 조합이 생존 여부에 미치는 영향을 강조할 수 있습니다. 단순히 각각의 변수를 개별적으로 사용하는 것보다 조합된 변수를 통해 서로 간의 상호작용 효과를 쉽게 확인할 수 있습니다.\n더 풍부한 패턴 탐색: 새로운 변수를 만들면 서로 다른 항구와 클래스 조합들 간의 생존률 차이를 더 세부적으로 분석할 수 있습니다. 이를 통해 특정 항구와 클래스의 승객들이 생존 여부에 미치는 영향을 더 자세히 이해할 수 있습니다.\n다양한 조합 고려: 조합된 변수를 만들면 “embarked”와 “pclass”의 가능한 모든 조합을 커버할 수 있습니다. 이는 분석의 전체적인 시각을 확장하고, 어떤 조합이 중요한지를 더 효과적으로 파악할 수 있도록 도와줍니다.\n모델 학습에 더 적합한 데이터 생성: 몇몇 머신 러닝 알고리즘은 상호작용 변수를 활용하여 모델의 성능을 향상시킬 수 있습니다. 이러한 알고리즘들은 새로운 변수를 활용하여 더 복잡한 패턴을 학습할 수 있습니다.\n새로운 변수를 만들어 분석하는 것은 분석의 창의성과 도메인 지식을 활용하여 데이터로부터 더 많은 의미를 도출하려는 노력의 일부입니다. 분석의 목적에 따라 각 변수를 개별적으로 사용하거나 조합하여 새로운 변수를 만들 수 있습니다."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#a.-데이터",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#b.-predictor-생성",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120554/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#c.-적합fit",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr,presets='best_quality') # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120554/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.94 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125495.28 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.1s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.16s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.6319   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.6364   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.835    = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    0.65s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8373   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8361   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8608   = Validation score   (accuracy)\n    1.85s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8294   = Validation score   (accuracy)\n    0.58s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8328   = Validation score   (accuracy)\n    0.3s     = Training   runtime\n    0.07s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8507   = Validation score   (accuracy)\n    2.85s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.09s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    4.25s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8406   = Validation score   (accuracy)\n    1.36s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8608   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 23.18s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120554/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46e583f8e0&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           CatBoost_BAG_L1   0.860831       0.026401  1.851676                0.026401           1.851676            1       True          7\n1       WeightedEnsemble_L2   0.860831       0.027539  2.251745                0.001138           0.400069            2       True         14\n2    NeuralNetFastAI_BAG_L1   0.850730       0.093112  2.852662                0.093112           2.852662            1       True         10\n3      LightGBMLarge_BAG_L1   0.840629       0.029128  1.360948                0.029128           1.360948            1       True         13\n4           LightGBM_BAG_L1   0.840629       0.031956  0.654864                0.031956           0.654864            1       True          4\n5            XGBoost_BAG_L1   0.840629       0.048379  1.094147                0.048379           1.094147            1       True         11\n6     NeuralNetTorch_BAG_L1   0.840629       0.102560  4.247738                0.102560           4.247738            1       True         12\n7   RandomForestGini_BAG_L1   0.837262       0.088506  0.341183                0.088506           0.341183            1       True          5\n8   RandomForestEntr_BAG_L1   0.836139       0.081470  0.341543                0.081470           0.341543            1       True          6\n9         LightGBMXT_BAG_L1   0.835017       0.027167  0.462362                0.027167           0.462362            1       True          3\n10    ExtraTreesEntr_BAG_L1   0.832772       0.073440  0.297061                0.073440           0.297061            1       True          9\n11    ExtraTreesGini_BAG_L1   0.829405       0.097789  0.580898                0.097789           0.580898            1       True          8\n12    KNeighborsDist_BAG_L1   0.636364       0.001740  0.003219                0.001740           0.003219            1       True          2\n13    KNeighborsUnif_BAG_L1   0.631874       0.001945  0.003195                0.001945           0.003195            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost_BAG_L1\n0.860831\n0.026401\n1.851676\n0.026401\n1.851676\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.860831\n0.027539\n2.251745\n0.001138\n0.400069\n2\nTrue\n14\n\n\n2\nNeuralNetFastAI_BAG_L1\n0.850730\n0.093112\n2.852662\n0.093112\n2.852662\n1\nTrue\n10\n\n\n3\nLightGBMLarge_BAG_L1\n0.840629\n0.029128\n1.360948\n0.029128\n1.360948\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.840629\n0.031956\n0.654864\n0.031956\n0.654864\n1\nTrue\n4\n\n\n5\nXGBoost_BAG_L1\n0.840629\n0.048379\n1.094147\n0.048379\n1.094147\n1\nTrue\n11\n\n\n6\nNeuralNetTorch_BAG_L1\n0.840629\n0.102560\n4.247738\n0.102560\n4.247738\n1\nTrue\n12\n\n\n7\nRandomForestGini_BAG_L1\n0.837262\n0.088506\n0.341183\n0.088506\n0.341183\n1\nTrue\n5\n\n\n8\nRandomForestEntr_BAG_L1\n0.836139\n0.081470\n0.341543\n0.081470\n0.341543\n1\nTrue\n6\n\n\n9\nLightGBMXT_BAG_L1\n0.835017\n0.027167\n0.462362\n0.027167\n0.462362\n1\nTrue\n3\n\n\n10\nExtraTreesEntr_BAG_L1\n0.832772\n0.073440\n0.297061\n0.073440\n0.297061\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.829405\n0.097789\n0.580898\n0.097789\n0.580898\n1\nTrue\n8\n\n\n12\nKNeighborsDist_BAG_L1\n0.636364\n0.001740\n0.003219\n0.001740\n0.003219\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.631874\n0.001945\n0.003195\n0.001945\n0.003195\n1\nTrue\n1"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-008-타이타닉, Autogluon (best_quality).html#d.-예측-predict",
    "title": "02wk-008: 타이타닉, Autogluon (best_quality)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.898989898989899\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(best_quality)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#a.-데이터",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\n- 피처엔지니어링\n\ntr.eval('Fsize = SibSp + Parch')\ntst.eval('Fsize = SibSp + Parch')\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nFsize\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n0\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n1\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n0\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n0\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n0\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n0\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n0\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n0\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n2\n\n\n\n\n418 rows × 12 columns"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#b.-predictor-생성",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_115814/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#c.-적합fit",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr.eval('Fsize = SibSp + Parch'))\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_115814/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   294.00 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 12\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    125936.3 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 5 | ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    12 features in original data used to generate 29 features in processed data.\n    Train Data (Processed) Memory Usage: 0.08 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.21s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.648    = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.648    = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.08s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8324   = Validation score   (accuracy)\n    1.32s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.38s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.24s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.68s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_115814/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f46f0228310&gt;\n\n\n- 리더보드확인 (모의고사채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.016608  2.934600                0.000400           0.236214            2       True         14\n1        NeuralNetTorch   0.837989       0.008671  1.382263                0.008671           1.382263            1       True         12\n2         LightGBMLarge   0.832402       0.003077  0.377827                0.003077           0.377827            1       True         13\n3       NeuralNetFastAI   0.832402       0.007537  1.316123                0.007537           1.316123            1       True         10\n4              CatBoost   0.826816       0.003649  0.528946                0.003649           0.528946            1       True          7\n5               XGBoost   0.826816       0.004545  0.150251                0.004545           0.150251            1       True         11\n6              LightGBM   0.821229       0.003294  0.180805                0.003294           0.180805            1       True          4\n7      RandomForestEntr   0.821229       0.194986  0.478185                0.194986           0.478185            1       True          6\n8            LightGBMXT   0.815642       0.003372  0.198229                0.003372           0.198229            1       True          3\n9      RandomForestGini   0.815642       0.097445  0.306731                0.097445           0.306731            1       True          5\n10       ExtraTreesEntr   0.810056       0.076015  0.512563                0.076015           0.512563            1       True          9\n11       ExtraTreesGini   0.804469       0.118582  0.881634                0.118582           0.881634            1       True          8\n12       KNeighborsDist   0.648045       0.001869  0.007722                0.001869           0.007722            1       True          2\n13       KNeighborsUnif   0.648045       0.004271  0.092099                0.004271           0.092099            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.016608\n2.934600\n0.000400\n0.236214\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008671\n1.382263\n0.008671\n1.382263\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.832402\n0.003077\n0.377827\n0.003077\n0.377827\n1\nTrue\n13\n\n\n3\nNeuralNetFastAI\n0.832402\n0.007537\n1.316123\n0.007537\n1.316123\n1\nTrue\n10\n\n\n4\nCatBoost\n0.826816\n0.003649\n0.528946\n0.003649\n0.528946\n1\nTrue\n7\n\n\n5\nXGBoost\n0.826816\n0.004545\n0.150251\n0.004545\n0.150251\n1\nTrue\n11\n\n\n6\nLightGBM\n0.821229\n0.003294\n0.180805\n0.003294\n0.180805\n1\nTrue\n4\n\n\n7\nRandomForestEntr\n0.821229\n0.194986\n0.478185\n0.194986\n0.478185\n1\nTrue\n6\n\n\n8\nLightGBMXT\n0.815642\n0.003372\n0.198229\n0.003372\n0.198229\n1\nTrue\n3\n\n\n9\nRandomForestGini\n0.815642\n0.097445\n0.306731\n0.097445\n0.306731\n1\nTrue\n5\n\n\n10\nExtraTreesEntr\n0.810056\n0.076015\n0.512563\n0.076015\n0.512563\n1\nTrue\n9\n\n\n11\nExtraTreesGini\n0.804469\n0.118582\n0.881634\n0.118582\n0.881634\n1\nTrue\n8\n\n\n12\nKNeighborsDist\n0.648045\n0.001869\n0.007722\n0.001869\n0.007722\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.648045\n0.004271\n0.092099\n0.004271\n0.092099\n1\nTrue\n1"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-006-타이타닉, Autogluon (Fsize).html#d.-예측-predict",
    "title": "02wk-006: 타이타닉, Autogluon (Fsize)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr.eval('Fsize = SibSp + Parch'))).mean()\n\n0.9169472502805837\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst.eval('Fsize = SibSp + Parch'))).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/MP/01wk-001-캐글소개.html",
    "href": "posts/2_Studies/MP/01wk-001-캐글소개.html",
    "title": "01wk-001: 캐글소개",
    "section": "",
    "text": "1. 강의영상\n\n\n\n2. 타이타닉 경진대회 찾기\n- 방법1: 검색을 통하여 접속하기\n\n\n\n그림1: 타이타닉 경진대회 검색\n\n\n- 방법2: 직접주소를 입력 https://www.kaggle.com/competitions/titanic\n\n\n3. 데이터구경\n- 데이터의 설명을 빠르게 파악하는 방법\n\n방법1: 변수위주로 Kaggle 홈페이지에서 파악\n방법2: 구글번역기 활용\n방법3: \bChatGPT이용\n\n- 제 생각\n\nChatGPT가 옳지 않은 소리를 할 떄도 있음. 하지만 처음에 데이터에 대한 개념을 빠르게 정리하고자 할 때 도움이 된다.\n변수이름이 약어로 된 경우가 많은데 이럴경우 GPT가 유용하다.\n\n\n\n4. 메뉴소개\n- 캐글 웹사이트의 메뉴탭 소개\n\nOverview (개요):\n\n\n경진대회 주최자가 경진대회의 배경, 목표, 데이터셋 설명 등을 설명\n\n\nData (데이터):\n\n\n경진대회에 사용되는 데이터셋에 관한 정보를 찾을 수 있음.\n데이터의 구성, 변수 설명, 예시 데이터 등이 제공되며, 데이터를 이해하고 분석할 수 있는데 필요한 정보들이 여기에 포함됨.\n\n\nCode (코드):\n\n\n경진대회 참가자들이 코드를 공유하고 토론하는 공간.\n주로 주어진 문제에 대한 데이터 분석 및 모델링 코드, 데이터 전처리 방법, 모델 학습 등에 관련된 내용이 이곳에 포함됨.\n\n\nDiscussion (토론):\n\n\n참가자들이 서로 의견을 교환하고 질문을 주고받을 수 있는 공간.\n데이터 분석 방법, 모델 구축 전략, 문제 해결 과정 등에 대한 토론이 이루어짐.\n\n\nLeaderboard (리더보드):\n\n\n경진대회 참가자들의 성능 평가 지표와 순위가 나열.\n참가자들의 모델 성능을 비교하고 경쟁 상황을 실시간으로 확인할 수 있음.\n\n\nRules (규칙):\n\n\n참가자들이 따라야 할 규칙, 데이터 사용 방법, 평가 지표 등이 명시되어 있음.\n\n- 체크하면 좋은 것들\n\nOveview: 평가지표(metric), 상(prize), 시간(timeline)\nRules: 평가지표(metric), 외부데이터 사용여부, 하루 최대 제출 수, 최종선택가능한 솔루션 수 (limit)\n\n- 대회의 유형 (ref: https://github.com/PacktPublishing/The-Kaggle-Book)\n\nGetting Started: 상을 제공하지 않음. 튜토리얼용. 매우 협력적인 환경.\nFeatured: 가장 일반적인 유형. 스폰서 회사의 비지니스 관련문제. 상금이 후함. 솔루션을 소개하는 자세한 리포트를 준비해야하고 발표할 것을 요구받을 수 있음.\nAnalytics: 질적 평가. 참가자의 PPT를 제출로 받음."
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#a.-데이터",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")\n\nLoaded data from: ./titanic/train.csv | Columns = 12 / 12 | Rows = 891 -&gt; 891\nLoaded data from: ./titanic/test.csv | Columns = 11 / 11 | Rows = 418 -&gt; 418\n\n\n- 피처엔지니어링\n\n_tr = tr.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)\n_tst = tst.eval('Fsize = SibSp + Parch').drop(['SibSp','Parch'],axis=1)"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#b.-predictor-생성",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230912_120900/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#c.-적합fit",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(_tr) # 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230912_120900/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   293.85 GB / 490.57 GB (59.9%)\nTrain Data Rows:    891\nTrain Data Columns: 10\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124991.67 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 3 | ['PassengerId', 'Pclass', 'Fsize']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    10 features in original data used to generate 27 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8101   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8268   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8212   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.66s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8045   = Validation score   (accuracy)\n    0.52s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.7989   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: NeuralNetFastAI ...\n    0.8268   = Validation score   (accuracy)\n    0.46s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8212   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.838    = Validation score   (accuracy)\n    1.55s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8268   = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 6.75s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230912_120900/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f463532b730&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.210158  3.510778                0.000527           0.246271            2       True         14\n1        NeuralNetTorch   0.837989       0.008443  1.547239                0.008443           1.547239            1       True         12\n2         LightGBMLarge   0.826816       0.003162  0.443558                0.003162           0.443558            1       True         13\n3              CatBoost   0.826816       0.003559  0.661513                0.003559           0.661513            1       True          7\n4              LightGBM   0.826816       0.003659  0.283715                0.003659           0.283715            1       True          4\n5       NeuralNetFastAI   0.826816       0.007515  0.462052                0.007515           0.462052            1       True         10\n6               XGBoost   0.821229       0.004702  0.148207                0.004702           0.148207            1       True         11\n7      RandomForestEntr   0.821229       0.214481  0.381199                0.214481           0.381199            1       True          6\n8      RandomForestGini   0.815642       0.091963  0.328555                0.091963           0.328555            1       True          5\n9            LightGBMXT   0.810056       0.003035  0.204797                0.003035           0.204797            1       True          3\n10       ExtraTreesGini   0.804469       0.184527  0.518099                0.184527           0.518099            1       True          8\n11       ExtraTreesEntr   0.798883       0.179535  0.353106                0.179535           0.353106            1       True          9\n12       KNeighborsUnif   0.653631       0.002189  0.009097                0.002189           0.009097            1       True          1\n13       KNeighborsDist   0.653631       0.002325  0.009845                0.002325           0.009845            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.210158\n3.510778\n0.000527\n0.246271\n2\nTrue\n14\n\n\n1\nNeuralNetTorch\n0.837989\n0.008443\n1.547239\n0.008443\n1.547239\n1\nTrue\n12\n\n\n2\nLightGBMLarge\n0.826816\n0.003162\n0.443558\n0.003162\n0.443558\n1\nTrue\n13\n\n\n3\nCatBoost\n0.826816\n0.003559\n0.661513\n0.003559\n0.661513\n1\nTrue\n7\n\n\n4\nLightGBM\n0.826816\n0.003659\n0.283715\n0.003659\n0.283715\n1\nTrue\n4\n\n\n5\nNeuralNetFastAI\n0.826816\n0.007515\n0.462052\n0.007515\n0.462052\n1\nTrue\n10\n\n\n6\nXGBoost\n0.821229\n0.004702\n0.148207\n0.004702\n0.148207\n1\nTrue\n11\n\n\n7\nRandomForestEntr\n0.821229\n0.214481\n0.381199\n0.214481\n0.381199\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.815642\n0.091963\n0.328555\n0.091963\n0.328555\n1\nTrue\n5\n\n\n9\nLightGBMXT\n0.810056\n0.003035\n0.204797\n0.003035\n0.204797\n1\nTrue\n3\n\n\n10\nExtraTreesGini\n0.804469\n0.184527\n0.518099\n0.184527\n0.518099\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.798883\n0.179535\n0.353106\n0.179535\n0.353106\n1\nTrue\n9\n\n\n12\nKNeighborsUnif\n0.653631\n0.002189\n0.009097\n0.002189\n0.009097\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.653631\n0.002325\n0.009845\n0.002325\n0.009845\n1\nTrue\n2\n\n\n\n\n\n\n\n- validation set의 의미:"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-007-타이타닉, Autogluon (Fsize,Drop).html#d.-예측-predict",
    "title": "02wk-007: 타이타닉, Autogluon (Fsize,Drop)",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(_tr)).mean()\n\n0.9539842873176206\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(_tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon(Fsize,Drop)_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#a.-데이터",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#a.-데이터",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "A. 데이터",
    "text": "A. 데이터\n- 비유: 문제를 받아오는 과정으로 비유할 수 있다.\n\ntr = TabularDataset(\"/kaggle/input/titanic/train.csv\")\ntst = TabularDataset(\"/kaggle/input/titanic/test.csv\")"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#b.-predictor-생성",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#b.-predictor-생성",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "B. Predictor 생성",
    "text": "B. Predictor 생성\n- 비유: 문제를 풀 학생을 생성하는 과정으로 비유할 수 있다.\n\npredictr = TabularPredictor(\"Survived\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230910_162629/\""
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#c.-적합fit",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#c.-적합fit",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "C. 적합(fit)",
    "text": "C. 적합(fit)\n- 비유: 학생이 공부를 하는 과정으로 비유할 수 있다.\n- 학습\n\npredictr.fit(tr) \n# 학생(predictr)에게 문제(tr)를 줘서 학습을 시킴(predictr.fit())\n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230910_162629/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   306.27 GB / 490.57 GB (62.4%)\nTrain Data Rows:    891\nTrain Data Columns: 11\nLabel Column: Survived\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [0, 1]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    126262.11 MB\n    Train Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n/home/cgb2/anaconda3/envs/mp/lib/python3.10/site-packages/autogluon/common/features/infer_types.py:118: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  result = pd.to_datetime(X, errors=\"coerce\")\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 1 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting CategoryFeatureGenerator...\n            Fitting CategoryMemoryMinimizeFeatureGenerator...\n        Fitting TextSpecialFeatureGenerator...\n            Fitting BinnedFeatureGenerator...\n            Fitting DropDuplicatesFeatureGenerator...\n        Fitting TextNgramFeatureGenerator...\n            Fitting CountVectorizer for text features: ['Name']\n            CountVectorizer fit with vocabulary size = 8\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])        : 2 | ['Age', 'Fare']\n        ('int', [])          : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('object', [])       : 4 | ['Sex', 'Ticket', 'Cabin', 'Embarked']\n        ('object', ['text']) : 1 | ['Name']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('category', [])                    : 3 | ['Ticket', 'Cabin', 'Embarked']\n        ('float', [])                       : 2 | ['Age', 'Fare']\n        ('int', [])                         : 4 | ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n        ('int', ['binned', 'text_special']) : 9 | ['Name.char_count', 'Name.word_count', 'Name.capital_ratio', 'Name.lower_ratio', 'Name.special_ratio', ...]\n        ('int', ['bool'])                   : 1 | ['Sex']\n        ('int', ['text_ngram'])             : 9 | ['__nlp__.henry', '__nlp__.john', '__nlp__.master', '__nlp__.miss', '__nlp__.mr', ...]\n    0.2s = Fit runtime\n    11 features in original data used to generate 28 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 712, Val Rows: 179\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.6536   = Validation score   (accuracy)\n    0.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    0.6536   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8156   = Validation score   (accuracy)\n    0.2s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8212   = Validation score   (accuracy)\n    0.16s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8156   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.16s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8156   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: CatBoost ...\n    0.8268   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8156   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.05s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8101   = Validation score   (accuracy)\n    0.4s     = Training   runtime\n    0.06s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 9: early stopping\n    0.8268   = Validation score   (accuracy)\n    1.33s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8101   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8324   = Validation score   (accuracy)\n    1.49s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8324   = Validation score   (accuracy)\n    0.42s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8603   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.58s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230910_162629/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fa18f4444f0&gt;\n\n\n- 리더보드확인 (모의고사 채점)\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.860335       0.193071  4.142821                0.000378           0.218161            2       True         14\n1         LightGBMLarge   0.832402       0.003180  0.422871                0.003180           0.422871            1       True         13\n2        NeuralNetTorch   0.832402       0.009243  1.488933                0.009243           1.488933            1       True         12\n3              CatBoost   0.826816       0.003432  0.526762                0.003432           0.526762            1       True          7\n4       NeuralNetFastAI   0.826816       0.007648  1.326021                0.007648           1.326021            1       True         10\n5              LightGBM   0.821229       0.003042  0.157547                0.003042           0.157547            1       True          4\n6            LightGBMXT   0.815642       0.003350  0.197612                0.003350           0.197612            1       True          3\n7        ExtraTreesGini   0.815642       0.050255  0.848478                0.050255           0.848478            1       True          8\n8      RandomForestEntr   0.815642       0.091255  0.525193                0.091255           0.525193            1       True          6\n9      RandomForestGini   0.815642       0.162890  0.340251                0.162890           0.340251            1       True          5\n10              XGBoost   0.810056       0.004424  0.141125                0.004424           0.141125            1       True         11\n11       ExtraTreesEntr   0.810056       0.060458  0.398653                0.060458           0.398653            1       True          9\n12       KNeighborsDist   0.653631       0.001958  0.007848                0.001958           0.007848            1       True          2\n13       KNeighborsUnif   0.653631       0.004457  0.087323                0.004457           0.087323            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.860335\n0.193071\n4.142821\n0.000378\n0.218161\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.832402\n0.003180\n0.422871\n0.003180\n0.422871\n1\nTrue\n13\n\n\n2\nNeuralNetTorch\n0.832402\n0.009243\n1.488933\n0.009243\n1.488933\n1\nTrue\n12\n\n\n3\nCatBoost\n0.826816\n0.003432\n0.526762\n0.003432\n0.526762\n1\nTrue\n7\n\n\n4\nNeuralNetFastAI\n0.826816\n0.007648\n1.326021\n0.007648\n1.326021\n1\nTrue\n10\n\n\n5\nLightGBM\n0.821229\n0.003042\n0.157547\n0.003042\n0.157547\n1\nTrue\n4\n\n\n6\nLightGBMXT\n0.815642\n0.003350\n0.197612\n0.003350\n0.197612\n1\nTrue\n3\n\n\n7\nExtraTreesGini\n0.815642\n0.050255\n0.848478\n0.050255\n0.848478\n1\nTrue\n8\n\n\n8\nRandomForestEntr\n0.815642\n0.091255\n0.525193\n0.091255\n0.525193\n1\nTrue\n6\n\n\n9\nRandomForestGini\n0.815642\n0.162890\n0.340251\n0.162890\n0.340251\n1\nTrue\n5\n\n\n10\nXGBoost\n0.810056\n0.004424\n0.141125\n0.004424\n0.141125\n1\nTrue\n11\n\n\n11\nExtraTreesEntr\n0.810056\n0.060458\n0.398653\n0.060458\n0.398653\n1\nTrue\n9\n\n\n12\nKNeighborsDist\n0.653631\n0.001958\n0.007848\n0.001958\n0.007848\n1\nTrue\n2\n\n\n13\nKNeighborsUnif\n0.653631\n0.004457\n0.087323\n0.004457\n0.087323\n1\nTrue\n1"
  },
  {
    "objectID": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#d.-예측-predict",
    "href": "posts/2_Studies/MP/02wk-005-타이타닉, Autogluon.html#d.-예측-predict",
    "title": "02wk-005: 타이타닉, Autogluon",
    "section": "D. 예측 (predict)",
    "text": "D. 예측 (predict)\n- 비유: 학습이후에 문제를 푸는 과정으로 비유할 수 있다.\n- training set 을 풀어봄 (predict) \\(\\to\\) 점수 확인\n\n(tr.Survived == predictr.predict(tr)).mean()\n\n0.9349046015712682\n\n\n\n(tr.Survived == (tr.Sex == \"female\")).mean() # 예전점수와 비교\n\n0.7867564534231201\n\n\n- test set 을 풀어봄 (predict) \\(\\to\\) 점수 확인 하러 캐글에 결과제출\n\ntst.assign(Survived = predictr.predict(tst)).loc[:,['PassengerId','Survived']]\\\n.to_csv(\"autogluon_submission.csv\",index=False)"
  },
  {
    "objectID": "posts/3_Researches/YEON/ITSTGCN/2023-09-14-논문리비전.html",
    "href": "posts/3_Researches/YEON/ITSTGCN/2023-09-14-논문리비전.html",
    "title": "[IT-STGCN] 논문리비전",
    "section": "",
    "text": "1. intro\n\n\n\nimage.png\n\n\n- 초록색은 나쁘지 않음. 하지만 아래의 내용을 보완하는게 좋음.\n\n분야의 예시로 신경과학, 환경데이터, 교통자료가 있는데 우리가 실제로 분석한 자료들이 사용된 논문을 찾아보며 예시를 들것 (Chickenpox, …) 사용하지 않더라도 예시를 들것.\n이러한 자료를 분석하는것이 왜 어려운지 설명할 것. 즉 단순히 시계열로 해석하거나 공간자료로 해석하면 어떠한 문제가 있는지 간단히 서술할 것. (1~2문장) 레퍼런스 찾을것. (torch_geometric_temporal 의 도입부분 활용)\n\n- 붉은부분\n\n의도는 좋으나 sparse data 는 올바르지 않은 표현임. missing, irregulary observed data 등으로 설명할 것.\n이러한 자료가 왜 발생하는지 설명할 것. (이부분은 레퍼런스 필요) 이러한 자료를 처리하는 것이 어려운 이유를 설명할 것.1\n우리의 아이디어는 “호모지니우스하지 않은 그래프 -&gt; 호모지니우스화 시킴” 인데 이러한 방식은 이상한방식이 아님. Yu et al. (2017) and Guo et al. (2019) Bai et al. (2020), Li et al. (2019), Zhao et al. (2019) 이 우리와 비슷한 연구를 했음.\n\n1 보통 결측없이 모두 관측한상태에서는 모형이 잘 동작함, 대부분의 spatio temporal data는 각각의 스냅샷마다 동일한 그래프구조를 가진다는 가정을 사용함. 스냅샷마다 그래프구조가 다른 경우를 가정하는 모형도 있음. 그러한 모형의 예시는 A,B,C,…. 등이 있음. 하지만 이러한 연구는 애초에 데이터가 스냅샷마다 non-호모지니우스하게 생겼으면 효율적일 수 있으나, 실제true model은 스냅샷마다 그래프구조가 동일하다고 여겨지지만 결측치로 인하여 스냅샷마다 호모지니우스가 깨지는 경우는 효율적이지 않을 수 있음. 우리는 이 부분에 초점을 맞추었음. 우리의 아이디어는 호모지니우스 하지 않은 그래프를 A,B,C, 등을 이용하여 그대로 처리하는것 보다 missing을 처리하여 호모지니우스하게 강제로 만들고 그 자료를 분석하자는 아이디어임.- 아래식은 틀렸음. 이건 회귀모형이 아님.. GNAR의 notation을 사용하여 모형을 다시표현해볼것..\n\n\n\nimage.png\n\n\n\n이부분이 아주 클리어 해야함\n사용하는 대부분의 Notations들이 정리되어야함.\nintro에 쓰는 것이 부담스러우면 제외해도 무방\n뒤에 self consistence estimator에 사용할 Notation을 함께 고려\n\n- 빨간부분 삭제후 다시 작성 (혹은 공부할 것)\n\n\n\nimage.png\n\n\n- 초록색부분은 나쁘지 않음\n\n\n\nimage.png\n\n\n\n\n2. Related works\n- 2.1과 2.2를 왜 리뷰하는지 설명이 필요함\n- 2.1에서 왜 Convolution Operator에 집중하는지 설명이 필요\n- 2.2에서 왜 Dynamic graphs에 집중하는지 설명이 필요\n- 전체적으로 이름은 related works인데 뭐가 related 되어있길래 이런것들을 소개하는지 클리어하지 않음. (솔직히 저도 저 방법들이 우리랑 뭔 관련있는지 잘모르겠어요)\n\n\n3. Backgrounds\n- 좋아요\n- 자잘한건 제가 수정하면 될 듯합니다.\n\n\n4.\n- 내용을 좀 더 팬시하게 쓸 필요가 있어보임\n- 아래부분을 정리하여 알고리즘화 해야함.\n\n\n\nimage.png\n\n\n\n\n5. Experiments\n- 아직 덜 읽어봄\n- 데이터 설명은 Appendix에, 실험결과와 Fig는 본문에 있는게 좋음"
  }
]