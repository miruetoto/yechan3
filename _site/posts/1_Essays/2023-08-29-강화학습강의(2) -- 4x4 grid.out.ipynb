{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **\\[Essays\\]** 강화학습(2) – 4x4 grid\n",
        "\n",
        "신록예찬  \n",
        "2023-08-23\n",
        "\n",
        "## Game2: 4 $\\times$ 4 그리드\n",
        "\n",
        "## imports"
      ],
      "id": "80bc5444-78f0-4b9e-b9f7-3246c803eca1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import torch\n",
        "import collections\n",
        "import IPython"
      ],
      "id": "a0aa2ea8-f975-4718-b614-4bcca0497a4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 예비학습: 시각화"
      ],
      "id": "19392877-1a86-4e57-b545-8bfdbd8207c7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    def update(t):\n",
        "        sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "664dcc82-0c4d-41fb-84eb-ae85685a8d16"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [[0,0],[0,1],[1,1],[1,2],[1,3]]"
      ],
      "id": "abb8a1e0-da86-4c51-aafa-20db584f384c"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "b4bdefc6-4c48-4af2-a32d-2932c2fff21f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env 클래스 구현\n",
        "\n",
        "`-` GridWorld: 강화학습에서 많이 사용되는 기본적인 시뮬레이션 환경\n",
        "\n",
        "1.  **State**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **Action**: 에이전트는 상태에서 다른 상태로 이동하기 위해 상, 하,\n",
        "    좌, 우로 이동하는 행동을 할 수 있음.\n",
        "3.  **Reward**: 에이전트가 특정 행동을 취할 때 환경에서 보상이 주어짐.\n",
        "4.  **Terminal State**: 일반적으로 하나 또는 그 이상의 종료 상태가\n",
        "    있으며, 에이전트가 이 상태에 도달하면 에피소드가 종료됨.\n",
        "\n",
        "`-` 환경과 에이전트\n",
        "\n",
        "-   env: $(S_t,A_t) \\to (S_{t+1}, R_t)$\n",
        "\n",
        "`-` 수학기호들\n",
        "\n",
        "-   `state_space`: ${\\cal S}=\\{1,\\dots,16\\}=\\{(0,0)\\dots,(3,3)\\}$\n",
        "-   `action_space`:\n",
        "    ${\\cal A} = \\{0,1,2,3\\} = \\{\\text{right}, \\text{up}, \\text{left}, \\text{down}\\}$\n",
        "-   `current_state`: $S_t \\in {\\cal S}$\n",
        "-   `next_state`: $S_{t+1} \\in {\\cal S}$\n",
        "-   `action`: $A_t \\in {\\cal A}$\n",
        "-   `reward`: $R_t \\in {\\cal R}=\\{-1,-10,100\\}$"
      ],
      "id": "a4e69414-2f42-48eb-881c-725d53696b5e"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]), # x+ \n",
        "            1: np.array([0, 1]), # y+\n",
        "            2: np.array([-1, 0]), # x-  \n",
        "            3: np.array([0, -1]), # y-\n",
        "        }\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4, 4])\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "    def reset(self):\n",
        "        self.agent_action = None\n",
        "        self.agent_state = np.array([0, 0])\n",
        "        return self.agent_state\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.agent_state = self.agent_state + direction\n",
        "        # 목표지점에 도달 \n",
        "        if np.array_equal(np.array([3,3]), self.agent_state):\n",
        "            reward = 100\n",
        "            terminated = True\n",
        "        else:\n",
        "            reward = -1\n",
        "            terminated = False\n",
        "        # 4*4밖에 있을 경우 \n",
        "        if self.agent_state not in self.state_space:\n",
        "            reward = -10\n",
        "            terminated = True\n",
        "            self.agent_state = self.agent_state - 1/2*direction\n",
        "        return self.agent_state, reward, terminated"
      ],
      "id": "3a5fea73-af23-4dc9-be0c-d083122175cc"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "env.reset()\n",
        "states = []\n",
        "rewards = [] \n",
        "terminations = [] \n",
        "for t in range(500):\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminated = env.step(action)\n",
        "    states.append(state)\n",
        "    rewards.append(reward)\n",
        "    terminations.append(terminated)\n",
        "    if terminated: \n",
        "        break "
      ],
      "id": "0c9492ff-1979-4b2d-8a04-f656bc030df3"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "[np.array([0,0])]+states"
      ],
      "id": "2c11a0aa-9518-4013-b8ec-b7287a22ab7a"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "show([np.array([0,0])]+states)"
      ],
      "id": "e78cc49b-1f29-4ac4-aa4c-3345cc839777"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent1 클래스 구현 + Run\n",
        "\n",
        "`-` 첫번째 시도"
      ],
      "id": "90120be4-922e-4eca-ae46-e8e2b66e2491"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent1:\n",
        "    def __init__(self,env):\n",
        "        self.action_space = env.action_space\n",
        "        self.state_space = env.state_space\n",
        "        self.n_experiences = 0\n",
        "        self.n_episode = 0  \n",
        "        \n",
        "        ## episode-wise info \n",
        "        self.scores = [] \n",
        "        self.playtimes = [] \n",
        "\n",
        "        ## time-wise info\n",
        "        self.current_state = None \n",
        "        self.action = None\n",
        "        self.reward = None        \n",
        "        self.next_state = None \n",
        "        self.socre = 0\n",
        "        \n",
        "        ## ReplayBuffer\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.current_states = []\n",
        "        self.next_states = [] \n",
        "        self.terminations = []\n",
        "        \n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "        \n",
        "    def save_experience(self): \n",
        "        self.actions.append(self.action)\n",
        "        self.current_states.append(self.current_state)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.terminations.append(self.terminated)\n",
        "        self.n_experiences += 1\n",
        "        self.score += self.reward\n",
        "\n",
        "    def learn(self):\n",
        "        pass"
      ],
      "id": "8fce5e60-18f1-4e4b-9ebb-0ec2f047514b"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1   Score: -11  Playtime: 2\n",
            "Episode 2   Score: -10  Playtime: 1\n",
            "Episode 3   Score: -10  Playtime: 1\n",
            "Episode 4   Score: -12  Playtime: 3\n",
            "Episode 5   Score: -11  Playtime: 2\n",
            "Episode 6   Score: -20  Playtime: 11\n",
            "Episode 7   Score: -16  Playtime: 7\n",
            "Episode 8   Score: -26  Playtime: 17\n",
            "Episode 9   Score: -10  Playtime: 1\n",
            "Episode 10  Score: -10  Playtime: 1\n",
            "Episode 11  Score: -12  Playtime: 3\n",
            "Episode 12  Score: -10  Playtime: 1\n",
            "Episode 13  Score: -12  Playtime: 3\n",
            "Episode 14  Score: -11  Playtime: 2\n",
            "Episode 15  Score: -11  Playtime: 2\n",
            "Episode 16  Score: -10  Playtime: 1\n",
            "Episode 17  Score: -10  Playtime: 1\n",
            "Episode 18  Score: -10  Playtime: 1\n",
            "Episode 19  Score: -13  Playtime: 4\n",
            "Episode 20  Score: -10  Playtime: 1"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent1(env)\n",
        "for _ in range(20):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 학습\n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: 종료조건 체크\n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    ## 2. 비본질적 코드\n",
        "    print(\n",
        "        f'Episode {agent.n_episode}\\t'\n",
        "        f'Score: {agent.scores[-1]}\\t'\n",
        "        f'Playtime: {agent.playtimes[-1]}'\n",
        "    )"
      ],
      "id": "c2577e9d-b45a-4e04-bb4f-f8df81cc7b9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 어떻게 학습을 할까? 즉 어떻게 “환경의 이해 $\\to$ 행동의 결정” 의\n",
        "과정을 수행할까?\n",
        "\n",
        "1.  어떠한 상태에서, 어떠한 행동을 했을때, 어떠한 보상과 어떠한\n",
        "    다음상태를 받았는지 기록하자.\n",
        "2.  1을 바탕으로 다음행동을 어떻게 할지 판단하자."
      ],
      "id": "bd86a89d-ff09-410d-be16-fe0d887ed70b"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])]+agent.next_states[-5:-1]\n",
        "states"
      ],
      "id": "8847e529-e74d-4c62-93c8-bbe948752fa2"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "bc8c510d-a248-43b6-870a-a3fda09e15b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 환경의 이해 (1차원적 이해)\n",
        "\n",
        "`-` 무작위로 10000판을 진행하여 보자."
      ],
      "id": "51fcb889-23f5-4f3c-8d6b-9b5e7ae274c3"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent1(env)\n",
        "for _ in range(10000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: agent << env\n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 학습\n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: 종료조건 체크\n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 "
      ],
      "id": "80e04276-c30d-44de-a3e3-6985a4832589"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "cff1062b-f5de-4897-bec9-0d7b7b636c44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "9959a896-61ef-4d2b-8d63-8eb22483191b"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[0], agent.actions[0], agent.rewards[0]"
      ],
      "id": "9ee8fdd8-a045-4e0f-a898-9ffee4609551"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[1], agent.actions[1], agent.rewards[1]"
      ],
      "id": "085336fb-f8c5-4d9a-aa93-c22cdfae20d0"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[2], agent.actions[2], agent.rewards[2]"
      ],
      "id": "d2d9ffdf-6eb6-4aba-8109-b62e924d6a36"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[3], agent.actions[3], agent.rewards[3]"
      ],
      "id": "6253a16c-a396-4837-b643-d74aaa414297"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "a71f5bcb-b477-4e1e-9139-7beffd4ce0ff"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q[x,y,a] = q[x,y,a] + agent.rewards[i]\n",
        "    count[x,y,a] = count[x,y,a] + 1 "
      ],
      "id": "237d6e30-0c45-432b-90db-a92eec386b19"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "count[count==0] = 0.1\n",
        "count"
      ],
      "id": "06270762-0bd6-4e5b-878e-48b044310761"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = (q/count).round(2)\n",
        "q"
      ],
      "id": "127edf98-279a-4bc4-9c77-b6c3dd74c834"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(f\"action = {i}\\n\"\n",
        "          f\"action-value function =\\n {q[:,:,i]}\\n\"\n",
        "          )"
      ],
      "id": "b467c815-99c7-411e-8153-d36904abd880"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2) – 이렇게하면 count를 따로 기록할 필요\n",
        "없음"
      ],
      "id": "c006035d-e823-445e-a962-6901c4de7c13"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a] # 풀이한 답\n",
        "    q_observed = agent.rewards[i] # 실제 답\n",
        "    diff = q_observed - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값이라고 하자\n",
        "    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백 * 학습률"
      ],
      "id": "3e5dddde-9d7f-4973-bc94-ce426c636188"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ -1.   -1.   -1.   -1. ]\n",
            " [ -1.   -1.   -1.   -1. ]\n",
            " [ -1.   -1.   -1.   99.2]\n",
            " [-10.  -10.   -9.9   0. ]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ -1.   -1.   -1.  -10. ]\n",
            " [ -1.   -1.   -1.  -10. ]\n",
            " [ -1.   -1.   -1.   -9.9]\n",
            " [ -1.   -1.   99.    0. ]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.   -1.   -1.   -1. ]\n",
            " [-10.   -1.   -1.   -1. ]\n",
            " [-10.   -1.   -1.   -1. ]\n",
            " [ -9.9  -1.   -1.    0. ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(\n",
        "        f\"action = {i}\\n\"\n",
        "        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n",
        "    )"
      ],
      "id": "24909698-cb58-4520-b80b-82f0c6070003"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 환경의 깊은 이해 (좀 더 고차원적인 이해)\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "42a37aea-120b-4cb0-afa9-c7133ae14897"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[:,:,1]"
      ],
      "id": "2f86c9f7-41e0-4865-aecd-08ea1bcda5ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "2259112f-06d2-4350-9366-c9d5957408b4"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,1] "
      ],
      "id": "f939b243-6bab-4143-a14b-c47102c2e93c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게 되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 -\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "55282404-26b0-4140-8871-6dc756dfdfde"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,1,1] "
      ],
      "id": "74d617aa-45e6-4d88-9c7c-d8e0d6961737"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동1을 하게 되면 -1의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 -\\> 합리적일까?\n",
        "\n",
        "`-` 비판: 분석2는 합리적인듯 하지만 data를 분석한뒤는 그다지 합리적이지\n",
        "못함\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 or 1)\n",
        "-   0을 쓸 때와 1을 쓸 때의 보상은 다름\n",
        "-   그런데 무수히 많은 데이터를 분석한 결과 0을 쓰면 0원을 보상으로\n",
        "    주고, 1을 쓰면 10만원을 보상으로 준다는 것을 “알게 되었음”\n",
        "-   빈 종이의 가치는 5만원인가? 아니면 10만원인가? –\\> 10만원 아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 `q[3,1,1]`에서는 실제보상(-1)과 잠재적보상(100)을\n",
        "동시에 고려해야하는게 합리적인듯"
      ],
      "id": "0a3ee0fb-11a3-4639-b868-3b70d1e2499f"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,1,1] = (-1) + 0.99 * (100) "
      ],
      "id": "bdcb9da2-8ac5-4972-8567-a9bc11d1d206"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[:,:,1]"
      ],
      "id": "d9dad524-5cb1-4a23-ab7c-b81ce8c6abea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99 는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이 =\n",
        "    십만원 으로 생각한다는 의미)\n",
        "\n",
        "`-` 수식화: `q[3,1,1] = (-1) + 0.99 * (100)`를 수식화하면 아래와 같다.\n",
        "\n",
        "$$q(s,a) = r(s,a) + 0.99\\times \\max_{a} q(s',a)$$\n",
        "\n",
        "좀 더 정확하게는 아래와 같이 볼 수 있다.\n",
        "\n",
        "$$q(s,a)= \\begin{cases} r(s,a) & \\text{terminated}  \\\\ r(s,a)+ 0.99\\times \\max_{a} q(s',a) & \\text{not terminated} \\end{cases} $$"
      ],
      "id": "2eb42b3b-db52-4ee4-95d8-9b45f83a2695"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    xx,yy = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a]\n",
        "    if agent.terminations[i]:\n",
        "        q_observed = agent.rewards[i] \n",
        "    else:\n",
        "        q_observed = agent.rewards[i] + 0.99*(q[xx,yy,:].max()) # 이걸 관측했다고 치는거임\n",
        "    diff = q_observed - q_estimated \n",
        "    q[x,y,a] = q_estimated + 0.1 * diff"
      ],
      "id": "c0055150-d5de-4560-b0a2-458b663457ce"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ 90.2  92.1  94.   95.8]\n",
            " [ 92.1  94.   96.   98. ]\n",
            " [ 94.   96.   98.  100. ]\n",
            " [-10.  -10.  -10.    0. ]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ 90.2  92.1  93.8 -10. ]\n",
            " [ 92.1  94.   96.  -10. ]\n",
            " [ 94.   96.   98.  -10. ]\n",
            " [ 96.   98.  100.    0. ]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10.  -10.  -10.  -10. ]\n",
            " [ 88.3  90.2  92.1  93.7]\n",
            " [ 90.2  92.1  94.   95.8]\n",
            " [ 92.1  94.   95.9   0. ]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.   88.3  90.2  92. ]\n",
            " [-10.   90.2  92.1  94. ]\n",
            " [-10.   92.1  94.   95.9]\n",
            " [-10.   93.9  95.8   0. ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(\n",
        "        f\"action = {i}\\n\"\n",
        "        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n",
        "    )"
      ],
      "id": "0a86c9c0-43b9-495f-968f-876f3220c356"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "44c42d9a-f579-4914-929e-06d2936e15e4"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:]"
      ],
      "id": "887300ae-49de-4b85-b3e5-85c5fa552c4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 1을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "7ca33ca9-fec3-4646-b6ea-091a5c40c3eb"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:]"
      ],
      "id": "b2c26efc-0fab-4fa6-a589-7b0527485aa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "1f8e42ca-8313-4e5a-85e8-cfad1d390e82"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:] "
      ],
      "id": "9e2123b6-1be2-4c67-83ec-46d1659fe720"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 1을 하는게 유리함.\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "416a2fdd-af2f-41c6-bc8f-bcc8060feb75"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:].argmax()"
      ],
      "id": "dbcbb679-b8d8-4f28-899a-c5b0e3cb246e"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:].argmax()"
      ],
      "id": "fcdbb861-1c06-4d03-b6bf-5ceee5c737a9"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:].argmax()"
      ],
      "id": "47e2694b-b9ae-4a82-a63d-db4c17a10ac3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "82d928c3-9df1-4f2f-b2a7-0948a56e5a56"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "1a696d2e-8202-499d-8aeb-e1d529a230b8"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "directions = {0: 'down', 1: 'right', 2:'up', 3:'left'}"
      ],
      "id": "6adc009d-79b3-45e4-9495-679a10b086fa"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        policy[i,j] = directions[q[i,j,:].argmax()]\n",
        "policy"
      ],
      "id": "ce7065e8-5fca-4335-84c2-b2a1631c72ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 요약: 값이 큰 쪽으로 이동"
      ],
      "id": "b578ce18-6649-41d3-b6fa-11c5bb00505a"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "q.max(axis=-1)"
      ],
      "id": "1c3beff3-7523-46ed-abd8-39af5e6c1783"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent2 클래스 구현 + Run"
      ],
      "id": "42cd0ae3-e0ec-45e8-ae96-b38cffb08820"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent2(Agent1):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q = np.zeros([4,4,4])\n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            x,y = self.current_state\n",
        "            self.action = self.q[x,y,:].argmax()\n",
        "    def learn(self): # make q\n",
        "        x,y = agent.current_state\n",
        "        xx,yy = agent.next_state\n",
        "        a = agent.action\n",
        "        q_estimated = self.q[x,y,a]\n",
        "        if agent.terminated:\n",
        "            q_observed = agent.reward\n",
        "        else:\n",
        "            q_observed = agent.reward + 0.99*(q[xx,yy,:].max()) \n",
        "        # q_observed 와 q_estimated를 점점 비슷하게 만들어주는 역할\n",
        "        diff = q_observed - q_estimated\n",
        "        self.q[x,y,a] = q_estimated + 0.1 * diff "
      ],
      "id": "fd0d1b87-11c0-4f7e-bdea-fee402f05c6d"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 Score:  0.00    Playtime:  3.54 n_experiences: 0\n",
            "Episode 200 Score:  0.00    Playtime:  3.22 n_experiences: 0\n",
            "Episode 300 Score:  0.00    Playtime:  3.44 n_experiences: 0\n",
            "Episode 400 Score:  0.00    Playtime:  2.93 n_experiences: 0\n",
            "Episode 500 Score:  0.00    Playtime:  2.74 n_experiences: 0\n",
            "Episode 600 Score:  0.00    Playtime:  3.08 n_experiences: 0\n",
            "Episode 700 Score:  0.00    Playtime:  3.25 n_experiences: 0\n",
            "Episode 800 Score:  0.00    Playtime:  3.38 n_experiences: 0\n",
            "Episode 900 Score:  0.00    Playtime:  2.53 n_experiences: 0\n",
            "Episode 1000    Score:  0.00    Playtime:  2.74 n_experiences: 0\n",
            "Episode 1100    Score:  0.00    Playtime:  3.29 n_experiences: 0\n",
            "Episode 1200    Score:  0.00    Playtime:  3.66 n_experiences: 0\n",
            "Episode 1300    Score:  0.00    Playtime:  2.75 n_experiences: 0\n",
            "Episode 1400    Score:  0.00    Playtime:  2.76 n_experiences: 0\n",
            "Episode 1500    Score:  0.00    Playtime:  3.56 n_experiences: 0\n",
            "Episode 1600    Score:  0.00    Playtime:  2.80 n_experiences: 0\n",
            "Episode 1700    Score:  0.00    Playtime:  3.89 n_experiences: 0\n",
            "Episode 1800    Score:  0.00    Playtime:  3.89 n_experiences: 0\n",
            "Episode 1900    Score:  0.00    Playtime:  3.35 n_experiences: 0\n",
            "Episode 2000    Score:  0.00    Playtime:  3.14 n_experiences: 0"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent2(env)\n",
        "for _ in range(2000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        # step3: 데이터저장 및 학습\n",
        "        agent.learn()        \n",
        "        # step4: 다음 iteration 준비 + 종료조건체크\n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 100) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_experiences: {agent.n_experiences}'\n",
        "        )"
      ],
      "id": "9b9c36b8-3040-415d-93cb-890199998bba"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "a70344ac-72be-449a-8563-656133189570"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent3 클래스 구현 + Run"
      ],
      "id": "5d51e24d-91f7-4c2e-b820-174f59fd9f89"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent3(Agent2):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 1\n",
        "    def act(self):\n",
        "        if self.eps > np.random.rand():\n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            x,y = self.current_state\n",
        "            self.action = self.q[x,y,:].argmax()"
      ],
      "id": "2dd12733-9b32-4bea-9f4c-ba510c4fe72a"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000    Score:  29.81   Playtime:  6.29 n_experiences: 0.3676954247709635\n",
            "Episode 2000    Score:  77.69   Playtime:  6.81 n_experiences: 0.1351999253974994\n",
            "Episode 3000    Score:  88.16   Playtime:  6.24 n_experiences: 0.04971239399803625\n",
            "Episode 4000    Score:  93.35   Playtime:  6.64 n_experiences: 0.018279019827489446\n",
            "Episode 5000    Score:  92.88   Playtime:  5.92 n_experiences: 0.006721111959865607"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent3(env)\n",
        "for _ in range(5000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 데이터저장 및 학습\n",
        "        agent.learn()        \n",
        "        # step4: 다음 iteration 준비 + 종료조건체크\n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    agent.eps = agent.eps * 0.999\n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 1000) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_experiences: {agent.eps}'\n",
        "        )"
      ],
      "id": "fb5ecb97-d847-41a8-aeb2-df73d7731893"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "d6b1ead8-f25a-458f-bf81-b8c54edb9ddd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 최종 Agent 클래스 구현 + Run"
      ],
      "id": "0388f07b-b253-4713-baec-e0a39b466b79"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent(Agent3):\n",
        "    # agent >> env \n",
        "    def __rshift__(self,env):\n",
        "        self.act()\n",
        "        env.agent_action = self.action\n",
        "    # agent << env \n",
        "    def __lshift__(self,env): \n",
        "        self.next_state, self.reward, self.terminated = env.step(env.agent_action)\n",
        "        self.save_experience()"
      ],
      "id": "15fd18f8-6201-4cd1-8cf4-73a7be0f9915"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000    Score:  30.69   Playtime:  7.61 n_eps: 0.3676954247709635\n",
            "Episode 2000    Score:  79.25   Playtime:  6.35 n_eps: 0.1351999253974994\n",
            "Episode 3000    Score:  90.43   Playtime:  6.17 n_eps: 0.04971239399803625\n",
            "Episode 4000    Score:  91.61   Playtime:  6.09 n_eps: 0.018279019827489446\n",
            "Episode 5000    Score:  95.00   Playtime:  6.00 n_eps: 0.006721111959865607"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent4(env)\n",
        "for _ in range(5000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        ## step1 \n",
        "        agent >> env \n",
        "        ## step2 \n",
        "        agent << env \n",
        "        ## step3 \n",
        "        agent.learn()    \n",
        "        ## step4 \n",
        "        agent.current_state = agent.next_state \n",
        "        ## step5 \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    agent.eps = agent.eps*0.999\n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 1000) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_eps: {agent.eps}'\n",
        "        )"
      ],
      "id": "59f43204-71eb-4470-a4bc-ba61dce65514"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "00cd65e7-f9a2-4bb8-836d-c1a593eb24eb"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  }
}