{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# imports"
      ],
      "id": "038077ab-9b6c-4412-b685-3dda32db0d76"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import torch \n",
        "import collections\n",
        "import IPython"
      ],
      "id": "df6688cb-b243-4e39-80c3-3886d1481186"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Game1\n",
        "\n",
        "`-` 문제설명: 두개의 버튼이 있다. 버튼1을 누르면 1의 보상을, 버튼2를\n",
        "누르면 100의 보상을 준다고 가정"
      ],
      "id": "579c7d68-0022-44a7-91b9-33f4eec9e529"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = np.random.choice(['button1','button2'])\n",
        "action"
      ],
      "id": "d771a51a-0733-42df-bf20-b06444164b35"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "if action == 'button1': \n",
        "    reward = 1 \n",
        "else:\n",
        "    reward = 100"
      ],
      "id": "97ab8e9f-5d74-4600-bbc9-64ed47fec6d9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "button1 1\n",
            "button2 100\n",
            "button2 100\n",
            "button2 100\n",
            "button2 100\n",
            "button1 1\n",
            "button2 100\n",
            "button1 1\n",
            "button1 1\n",
            "button1 1"
          ]
        }
      ],
      "source": [
        "for _ in range(10):\n",
        "    action = np.random.choice(['button1','button2'])\n",
        "    if action == 'button1': \n",
        "        reward = 1 \n",
        "    else:\n",
        "        reward = 100\n",
        "    print(action,reward)"
      ],
      "id": "afb693ba-a1f0-4886-88f2-093c8c632cbb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 게임을 푸는 방법? 버튼2를 누른다.\n",
        "\n",
        "`-` 용어 정리 - Agent = 버튼을 누르는 사람 - Action = 에이전트가 할 수\n",
        "있는 행동 (현재는 2개의 action이 가능) - Env = Agent의 action을 보고\n",
        "reward를 주는 존재 - 게임의 종료 = 버튼을 누르면 게임이 종료 - 게임을\n",
        "푸는 방법 = reward를 최대화하는 action을 선택\n",
        "\n",
        "## Game2\n",
        "\n",
        "`-` 문제설명: 에이전트는 현재 0의 위치에 있다. 에이전트는 (1) 정지 (2)\n",
        "왼쪽으로 이동 (3) 오른쪽으로 이동 하는 3개의 행동을 할 수 있다.\n",
        "에이전트가 +2의 위치에 도달하면 100의 보상을 얻고 게임이 종료된다.\n",
        "에이전트가 -2의 위치에 도달하면 보상없이 게임이 종료된다.\n",
        "\n",
        "`-` 에이전트와 환경의 상호작용 구현1"
      ],
      "id": "a35a8812-1390-4690-83f5-987bb9016f71"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = 0"
      ],
      "id": "d753194b-c29d-408d-bbb4-6e82d6455be7"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "action = np.random.choice([-1,0,1])"
      ],
      "id": "49be0f5a-2337-454b-ae5b-973628356a35"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1"
          ]
        }
      ],
      "source": [
        "print(state,action)"
      ],
      "id": "13527401-e8e7-4ded-be35-f6b25c83bc41"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n",
            "1 1\n",
            "2\n",
            "2의 위치에 도달, 보상 200점 획득"
          ]
        }
      ],
      "source": [
        "for _ in range(9): \n",
        "    if state == 2:\n",
        "        print(state)\n",
        "        reward = 200         \n",
        "        print(\"2의 위치에 도달, 보상 {}점 획득\".format(reward))\n",
        "        break \n",
        "    elif state == -2:\n",
        "        print(state)\n",
        "        reward = 0 \n",
        "        print(\"-2의 위치에 도달, 보상 {}점 획득\".format(reward))\n",
        "        break\n",
        "    else:\n",
        "        print(state,action)\n",
        "        state = state + action \n",
        "        action = np.random.choice([-1,0,1])"
      ],
      "id": "1e6c4ab7-15e9-491e-8b44-392491213600"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 에이전트와 환경의 상호작용 구현2"
      ],
      "id": "8925dece-1fdb-49fb-a172-c76621243682"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "observation_space = gym.spaces.Discrete(5,start=-2)"
      ],
      "id": "982dc58a-b969-4086-8f3a-994467a0899e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "observation_space.sample()"
      ],
      "id": "33b84075-ad4f-4d8f-8757-9332ffc45b30"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "observation_space.shape"
      ],
      "id": "9addcb23-27b9-4fb5-82ac-8386337054cf"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "observation_space.contains(-3),observation_space.contains(-2)"
      ],
      "id": "fc7b0d94-3fff-44dc-be2a-6cdded371de0"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space = gym.spaces.Discrete(3,start=-1)"
      ],
      "id": "44a3bacd-7fe7-4988-9edc-ac37d5e59dd4"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "action_space"
      ],
      "id": "d03dd2d8-470d-4099-aad5-76bb3c4a208b"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Game2(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = gym.spaces.Discrete(3,start=-1) # Acition = {-1,0,1}  \n",
        "        self.observation_space = gym.spaces.Discrete(5,start=-2) # State = {-2,-1,0,1,2} \n",
        "        self.state = 0 \n",
        "        self.t = 0\n",
        "    def step(self,action):\n",
        "        self.state = self.state + action\n",
        "        self.t = self.t + 1 \n",
        "        if self.state == 2:\n",
        "            reward = 100\n",
        "        else:\n",
        "            reward = -1\n",
        "        info = {}\n",
        "        if self.state == -2 or self.state==2: \n",
        "            done = True\n",
        "        else: \n",
        "            done = False\n",
        "        return self.state, reward, terminated, truncated, info\n",
        "    def render(self):\n",
        "        print('state: {}'.format(self.state))\n",
        "    def reset(self):\n",
        "        self.state = 0 \n",
        "        return self.state"
      ],
      "id": "943478ae-efb4-461e-9ec8-47b48c1050bf"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "env=Game2()"
      ],
      "id": "fa6c4fa4-f2bc-416d-9b2d-e635cfcff6ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Game3"
      ],
      "id": "aa76c48e-bcce-4c46-9188-efe9193b0f4e"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None, size=5):\n",
        "        self.size = size  # The size of the square grid\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "                \"target\": spaces.Box(0, size - 1, shape=(2,), dtype=int),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\"\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),\n",
        "            1: np.array([0, 1]),\n",
        "            2: np.array([-1, 0]),\n",
        "            3: np.array([0, -1]),\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "    def _get_obs(self):\n",
        "        return {\"agent\": self._agent_location, \"target\": self._target_location}        \n",
        "    def _get_info(self):\n",
        "        return {\n",
        "            \"distance\": np.linalg.norm(\n",
        "                self._agent_location - self._target_location, ord=1\n",
        "            )\n",
        "        }\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "    \n",
        "        # Choose the agent's location uniformly at random\n",
        "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
        "    \n",
        "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "        self._target_location = self._agent_location\n",
        "        while np.array_equal(self._target_location, self._agent_location):\n",
        "            self._target_location = self.np_random.integers(\n",
        "                0, self.size, size=2, dtype=int\n",
        "            )\n",
        "    \n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "    \n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "    \n",
        "        return observation, info\n",
        "    def step(self, action):\n",
        "        # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "        direction = self._action_to_direction[action]\n",
        "        # We use `np.clip` to make sure we don't leave the grid\n",
        "        self._agent_location = np.clip(\n",
        "            self._agent_location + direction, 0, self.size - 1\n",
        "        )\n",
        "        # An episode is done iff the agent has reached the target\n",
        "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
        "        reward = 1 if terminated else 0  # Binary sparse rewards\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "    \n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "    \n",
        "        return observation, reward, terminated, False, info\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "    \n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode(\n",
        "                (self.window_size, self.window_size)\n",
        "            )\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "    \n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "            self.window_size / self.size\n",
        "        )  # The size of a single grid square in pixels\n",
        "    \n",
        "        # First we draw the target\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (255, 0, 0),\n",
        "            pygame.Rect(\n",
        "                pix_square_size * self._target_location,\n",
        "                (pix_square_size, pix_square_size),\n",
        "            ),\n",
        "        )\n",
        "        # Now we draw the agent\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            (0, 0, 255),\n",
        "            (self._agent_location + 0.5) * pix_square_size,\n",
        "            pix_square_size / 3,\n",
        "        )\n",
        "    \n",
        "        # Finally, add some gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, pix_square_size * x),\n",
        "                (self.window_size, pix_square_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, 0),\n",
        "                (pix_square_size * x, self.window_size),\n",
        "                width=3,\n",
        "            )\n",
        "    \n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "    \n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()            "
      ],
      "id": "a94554fe-b0f9-4bc0-b7a5-dba900312a36"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorldEnv()"
      ],
      "id": "ab3800a3-6937-497c-9f5a-62cc76d265b6"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.observation_space"
      ],
      "id": "851fe51b-44aa-41bb-a2d4-94fd2eb67423"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.action_space"
      ],
      "id": "63c56269-14b1-406d-bf37-27f4773ac2ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Game4: LunarLander\n",
        "\n",
        "### 환경만들기\n",
        "\n",
        "`-` 환경을 만드는 방법은 아래와 같다."
      ],
      "id": "03aabef6-d570-4034-801d-2fee4763f375"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2',render_mode='rgb_array')"
      ],
      "id": "3e085d50-9d39-4e75-9151-843deb6ee4e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경에 대한 기본 정보를 조사하여 보자."
      ],
      "id": "3fe4edd3-5424-4a06-9f4a-fb2e8c9fb1b9"
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.observation_space"
      ],
      "id": "49e5d1b8-e68d-41da-9c08-26530b8fdacd"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.action_space"
      ],
      "id": "cfde2681-4846-4aa0-87ce-66333cd7485a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 환경관찰\n",
        "\n",
        "`-` 환경관찰"
      ],
      "id": "09b707b2-cf17-4e3b-bab0-0b4ee0104449"
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()"
      ],
      "id": "35312d3e-1276-4895-b94a-224a0053089b"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.step(0) # observation, reward, terminated, truncated, info"
      ],
      "id": "9fa82115-7a62-4916-b62c-76548737af76"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` action"
      ],
      "id": "1f917f29-896f-43b3-be29-4ffa17eaadf5"
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.action_space.sample()"
      ],
      "id": "37dafca5-cbbc-4b88-96a7-a180f77d9023"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   int형으로 전달\n",
        "\n",
        "`-` action -\\> nextstate, reward, done"
      ],
      "id": "ffcd95fa-04dd-479a-8922-092f5d43a25d"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.step(env.action_space.sample())"
      ],
      "id": "23f3049d-0c1d-438f-8fcc-3854ee723c10"
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "frames = []\n",
        "env.reset()\n",
        "for _ in range(300):\n",
        "    frames.append(env.render())\n",
        "    env.step(env.action_space.sample())\n",
        "env.close()"
      ],
      "id": "3cb75d3b-016b-42ce-8497-93120341a230"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\nACyHgAIAACyHgAIAACyHgAIAACzn/wMf58/7Q/TYGQAAAABJRU5ErkJggg==\n"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ani = FuncAnimation(fig,lambda i: ax.imshow(frames[::10][i]),frames=len(frames[::10]))"
      ],
      "id": "9ceab5f9-1e5a-457b-8464-20c9db84d769"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !conda install -c conda-forge ffmpeg"
      ],
      "id": "bd4071f6-8541-46c2-8120-4e6fe196bf09"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "IPython.display.HTML(ani.to_jshtml())"
      ],
      "id": "6fad79d6-2d16-49d1-b425-d2f7ad74896d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay Buffer\n",
        "\n",
        "`-` 랜덤액션을 연속적으로 생성하고 그 결과를 기록해보자."
      ],
      "id": "d346f551-7087-47ff-9dd3-2f51baeb06ce"
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = []\n",
        "actions = []\n",
        "rewards = []\n",
        "next_states = []\n",
        "terminateds = []"
      ],
      "id": "ae71b487-9754-4f06-ba45-2acb2bdefda4"
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {},
      "outputs": [],
      "source": [
        "_state1, _ = env.reset()\n",
        "for t in range(1500):\n",
        "    _action = env.action_space.sample() \n",
        "    _state2, _reward, _terminated, _truncated, _info = env.step(_action)\n",
        "    ## save code \n",
        "    states.append(_state1) \n",
        "    actions.append(_action)\n",
        "    rewards.append(_reward)\n",
        "    next_states.append(_state2)\n",
        "    terminateds.append(_terminated)\n",
        "    ## save code end \n",
        "    _state1 = _state2 \n",
        "    if _terminated:\n",
        "        break"
      ],
      "id": "6b950bc8-0ae9-461e-ba22-0e9e6139c4db"
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.reset()"
      ],
      "id": "58710491-4c49-4740-9823-818daf83d927"
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.step(_action)"
      ],
      "id": "01bdd075-1288-480b-aa1e-8204da75bf53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 모인 히스토리를 확인해보자."
      ],
      "id": "24ca7c8b-ed4f-4ac1-9696-2767e3a8b36e"
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(states), len(actions), len(next_states), len(rewards), len(terminateds)"
      ],
      "id": "58524369-1817-4ee5-84ae-b05dfb323b81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Qnetwork 설계\n",
        "\n",
        "`-` 네트워크의 목적: 내가 여기서 뭘 해야하는지 알려줘! = 내가 이\n",
        "상태에서, 어떠한 액션을 해야하는지 알려줘 $\\to$ 8개의 상태를 입력으로\n",
        "받으면 4개의 액션에 대한 좋은 정도를 숫자로 표현하는 어떠한 함수를\n",
        "만들자.\n",
        "\n",
        "`-` net 설계"
      ],
      "id": "78af1e1b-74ca-4d91-8f75-2b8dc6ac825c"
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=8, out_features=128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=128, out_features=64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=64, out_features=32),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=32, out_features=4)\n",
        ")\n",
        "net"
      ],
      "id": "54860fb4-9cbd-4aab-9ce7-d716d07e0e7d"
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(torch.tensor(states))"
      ],
      "id": "ab62e365-d629-42c6-91e0-f492ff3a99d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Policy 설계\n",
        "\n",
        "`-` 네트워크의 의미"
      ],
      "id": "fc207a9e-ea4c-433d-8848-0e87b3698179"
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {},
      "outputs": [],
      "source": [
        "states[0],states[1]"
      ],
      "id": "78306592-ca58-4621-8e21-1acd4941278a"
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(torch.tensor(states[0:2]))"
      ],
      "id": "4ba397f6-33b6-429c-b718-321f69c1f765"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태0에서는 액션0이, 상태1에서도 액션0이 가장 좋다는 의미 (왜?\n",
        "    q-value가 젤 높으니까..)\n",
        "\n",
        "`-` 따라서 Agent는 아래와 같이 행동해야 한다. (네트워크가 잘\n",
        "학습되었다는 전제가 필요함) - state\\[0\\] -\\> action = 0 - state\\[1\\] -\\>\n",
        "action = 0"
      ],
      "id": "f06d5438-f991-4c94-9bd8-9a0541bb9d0d"
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(torch.tensor(states[0:2])).max(axis=1)"
      ],
      "id": "5b630b0e-bf3b-48a3-bce5-7fd135237ef5"
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [],
      "source": [
        "net(torch.tensor(states[0:2])).max(axis=1)[1]"
      ],
      "id": "4cbfbbec-8253-46de-8a60-de5ae1bb8e19"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 네트워크가 있으므로 이제 어떠한 state에 대해서도 뭘 해야할지 (=어떤\n",
        "액션을 해야할지) 알 수 있다."
      ],
      "id": "4591b6e3-5f48-45aa-886d-185a16537a89"
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {},
      "outputs": [],
      "source": [
        "_state1 # 어떤 state에 대해서도.. "
      ],
      "id": "5f604754-d57a-41b4-a1cb-387cd21a4311"
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {},
      "outputs": [],
      "source": [
        "int(torch.argmax(net(torch.tensor(_state1)))) # 그래서 다음에 우리가 어떤행동을 해야할 지 알 수 있음"
      ],
      "id": "7beb3de0-1ead-4e15-b15d-4ca6be971d75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 학습\n",
        "\n",
        "`-` 네트워크를 학습시키자."
      ],
      "id": "963badae-d99e-406a-b274-08ad25189108"
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {},
      "outputs": [],
      "source": [
        "net"
      ],
      "id": "2c71eee5-de1f-41c9-ab1b-23a8ba923a18"
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {},
      "outputs": [],
      "source": [
        "scores=[]\n",
        "playtimes=[] \n",
        "eps = 0\n",
        "opt = torch.optim.Adam(net.parameters(),lr=0.01)"
      ],
      "id": "9e0e9cba-2f10-4f55-bc0a-d20a6177725c"
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 Score: -387.24  Playtime: 80.000\n",
            "Episode 200 Score: -290.72  Playtime: 61.000\n",
            "Episode 300 Score: -290.32  Playtime: 82.0000\n",
            "Episode 400 Score: -321.50  Playtime: 158.00\n",
            "Episode 500 Score: -345.35  Playtime: 78.0000\n",
            "Episode 600 Score: -347.79  Playtime: 56.0000\n",
            "Episode 700 Score: -187.64  Playtime: 61.000\n",
            "Episode 800 Score: -206.72  Playtime: 51.000\n",
            "Episode 900 Score: -242.74  Playtime: 82.0000\n",
            "Episode 1000    Score: -270.24  Playtime: 172.00\n",
            "Episode 1100    Score: -239.00  Playtime: 88.0000\n",
            "Episode 1200    Score: -256.10  Playtime: 136.00\n",
            "Episode 1300    Score: -261.52  Playtime: 77.000\n",
            "Episode 1400    Score: -310.77  Playtime: 110.00\n",
            "Episode 1500    Score: -443.30  Playtime: 99.0000\n",
            "Episode 1600    Score: -553.26  Playtime: 68.0000\n",
            "Episode 1700    Score: -571.14  Playtime: 51.000\n",
            "Episode 1800    Score: -592.59  Playtime: 82.0000\n",
            "Episode 1900    Score: -606.76  Playtime: 66.0000\n",
            "Episode 2000    Score: -679.92  Playtime: 83.0000"
          ]
        }
      ],
      "source": [
        "for epsd in range(1,2001): # 게임 2000판 시켜줌.. \n",
        "    state1,_  = env.reset() # 환경리셋 + 초기화된 환경을 state라는 변수에 저장 \n",
        "    score = 0 \n",
        "    for t in range(1000): # 게임1판당 max 1000프레임만 할 수 있음\n",
        "        action = int(torch.argmax(net(torch.tensor(state1)))) # 네트워크가 알려주는 action을 뽑음 \n",
        "        \n",
        "        # (step2) Agent -> Env // Env -> Agent \n",
        "        state2, reward, terminated, truncated, info = env.step(action) # 액션을 환경에 전달 -> (next_state, reward, done) 을 받음 \n",
        "        \n",
        "        # (step3) Agnet: save data and learn \n",
        "        ## save data \n",
        "        states.append(state1)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        next_states.append(state2)\n",
        "        terminateds.append(terminated)\n",
        "    \n",
        "        ## 최근 500개의 자료만 준비함. \n",
        "        if len(states)>500:\n",
        "            _states = torch.tensor(states[-500:])\n",
        "            _actions = torch.tensor(actions[-500:]).reshape(-1,1)\n",
        "            _next_states = torch.tensor(next_states[-500:])\n",
        "            _rewards = torch.tensor(rewards[-500:]).reshape(-1,1)\n",
        "            _terminateds = torch.tensor(terminateds[-500:]).to(torch.float).reshape(-1,1) \n",
        "        else:\n",
        "            _states = torch.tensor(states)\n",
        "            _actions = torch.tensor(actions).reshape(-1,1)\n",
        "            _next_states = torch.tensor(next_states)\n",
        "            _rewards = torch.tensor(rewards).reshape(-1,1)\n",
        "            _terminateds = torch.tensor(terminateds).to(torch.float).reshape(-1,1)\n",
        "\n",
        "        ## 최근 50000개의 자료에서 8개를 임의로 추출함. \n",
        "        _n = len(_states)\n",
        "        _index = np.random.choice(_n,64) # 128 is batch_size \n",
        "        _states = _states[_index]\n",
        "        _actions = _actions[_index]\n",
        "        _next_states = _next_states[_index]\n",
        "        _rewards = _rewards[_index]\n",
        "        _terminateds = _terminateds[_index]\n",
        "        \n",
        "        ## leanrn with pytorch \n",
        "        yhat = net(_states).gather(1,_actions).squeeze() ## (s,a) -> q(s,a) // 내가 현재상태 state에서, 현재 action을 하여 얻을 것이라 예상하는 보상 (net가 알려주는) \n",
        "        y = _rewards.squeeze() #+ 0.99 * net(_next_states).detach().max(1)[0] * (1-_terminateds.squeeze()) ## 그런데 실제로는 이게 맞다고 봐야지~ \n",
        "        loss = torch.mean((y-yhat)**2)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # (step4) Agent: prepare next steps \n",
        "        state1 = state2  \n",
        "        #eps = max(0.05, 0.99*eps) \n",
        "        score = score + reward\n",
        "        \n",
        "        # terminate \n",
        "        if terminated:\n",
        "            scores.append(score)\n",
        "            playtimes.append(t)\n",
        "            break\n",
        "            \n",
        "    print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, scores[-1],playtimes[-1]), end=\"\")\n",
        "    if epsd % 100 == 0:\n",
        "        print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, np.mean(scores[-100:]),np.mean(playtimes[-100])))"
      ],
      "id": "5c18836f-283b-4455-828b-847dec289dfe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# for epsd in range(1,2001): # 게임 2000판 시켜줌.. \n",
        "#     state1,_  = env.reset() # 환경리셋 + 초기화된 환경을 state라는 변수에 저장 \n",
        "#     score = 0 \n",
        "#     for t in range(1000): # 게임1판당 max 1000프레임만 할 수 있음\n",
        "#         # (step1) Agent: action \n",
        "#         if np.random.rand() < eps: \n",
        "#             action = env.action_space.sample() # 랜덤액션을 뽑음 \n",
        "#         else:\n",
        "#             action = int(torch.argmax(net(torch.tensor(state1).to(\"cuda:0\")))) # 네트워크가 알려주는 action을 뽑음 \n",
        "        \n",
        "#         # (step2) Agent -> Env // Env -> Agent \n",
        "#         state2, reward, terminated, truncated, info = env.step(action) # 액션을 환경에 전달 -> (next_state, reward, done) 을 받음 \n",
        "        \n",
        "#         # (step3) Agnet: save data and learn \n",
        "#         ## save data \n",
        "#         states.append(state1)\n",
        "#         actions.append(action)\n",
        "#         rewards.append(reward)\n",
        "#         next_states.append(state2)\n",
        "#         terminateds.append(terminated)\n",
        "    \n",
        "#         ## 최근 50000개의 자료만 준비함. \n",
        "#         if len(states)>50000:\n",
        "#             _states = torch.tensor(states[-50000:])\n",
        "#             _actions = torch.tensor(actions[-50000:]).reshape(-1,1)\n",
        "#             _next_states = torch.tensor(next_states[-50000:])\n",
        "#             _rewards = torch.tensor(rewards[-50000:]).reshape(-1,1)\n",
        "#             _terminateds = torch.tensor(terminateds[-50000:]).to(torch.float).reshape(-1,1) \n",
        "#         else:\n",
        "#             _states = torch.tensor(states)\n",
        "#             _actions = torch.tensor(actions).reshape(-1,1)\n",
        "#             _next_states = torch.tensor(next_states)\n",
        "#             _rewards = torch.tensor(rewards).reshape(-1,1)\n",
        "#             _terminateds = torch.tensor(terminateds).to(torch.float).reshape(-1,1)\n",
        "\n",
        "#         ## 최근 50000개의 자료에서 128개를 임의로 추출함. \n",
        "#         _n = len(_states)\n",
        "#         _index = np.random.choice(_n,128) # 128 is batch_size \n",
        "#         _states = _states[_index]\n",
        "#         _actions = _actions[_index]\n",
        "#         _next_states = _next_states[_index]\n",
        "#         _rewards = _rewards[_index]\n",
        "#         _terminateds = _terminateds[_index]\n",
        "        \n",
        "#         ## GPU로 이동 \n",
        "#         _states = _states.to(\"cuda:0\")\n",
        "#         _actions = _actions.to(\"cuda:0\")\n",
        "#         _next_states = _next_states.to(\"cuda:0\")\n",
        "#         _rewards = _rewards.to(\"cuda:0\")\n",
        "#         _terminateds = _terminateds.to(\"cuda:0\")\n",
        "        \n",
        "#         ## leanrn with pytorch \n",
        "#         yhat = net(_states).gather(1,_actions) ## (s,a) -> q(s,a) // 내가 현재상태 state에서, 현재 action을 하여 얻을 것이라 예상하는 보상 (net가 알려주는) \n",
        "#         y = _rewards + 0.99 * net(_next_states).detach().max(1)[0].reshape(-1,1)*(1-_terminateds) ## 그런데 실제로는 이게 맞다고 봐야지~ \n",
        "#         loss = torch.mean((y-yhat)**2)\n",
        "#         loss.backward()\n",
        "        \n",
        "#         opt.step()\n",
        "#         opt.zero_grad()\n",
        "\n",
        "#         # (step4) Agent: prepare next steps \n",
        "#         state1 = state2  \n",
        "#         eps = max(0.05, 0.99*eps) \n",
        "#         score = score + reward\n",
        "        \n",
        "#         # terminate \n",
        "#         if terminated:\n",
        "#             scores.append(score)\n",
        "#             playtimes.append(t)\n",
        "#             break\n",
        "            \n",
        "#     print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, scores[-1],playtimes[-1]), end=\"\")\n",
        "#     if epsd % 100 == 0:\n",
        "#         print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, np.mean(scores[-100:]),np.mean(playtimes[-100])))"
      ],
      "id": "54ac5087-13fb-443b-8fe5-68e7a7fc774c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  }
}