{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **\\[Essays\\]** 강화학습(2) – 4x4 grid\n",
        "\n",
        "신록예찬  \n",
        "2023-08-23\n",
        "\n",
        "## Game2: 4 $\\times$ 4 그리드\n",
        "\n",
        "## imports"
      ],
      "id": "8b0a2c16-a44e-45a6-ab50-a09d371ba99f"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "a0aa2ea8-f975-4718-b614-4bcca0497a4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 예비학습: 시각화"
      ],
      "id": "e45b356c-404e-47a0-be17-7daf04b806de"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    def update(t):\n",
        "        sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "664dcc82-0c4d-41fb-84eb-ae85685a8d16"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [[0,0],[0,1],[1,1],[1,2],[1,3]]"
      ],
      "id": "abb8a1e0-da86-4c51-aafa-20db584f384c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "b4bdefc6-4c48-4af2-a32d-2932c2fff21f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env 클래스 구현\n",
        "\n",
        "`-` GridWorld: 강화학습에서 많이 사용되는 기본적인 시뮬레이션 환경\n",
        "\n",
        "1.  **State**: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중\n",
        "    하나에 있을 수 있음.\n",
        "2.  **Action**: 에이전트는 상태에서 다른 상태로 이동하기 위해 상, 하,\n",
        "    좌, 우로 이동하는 행동을 할 수 있음.\n",
        "3.  **Reward**: 에이전트가 특정 행동을 취할 때 환경에서 보상이 주어짐.\n",
        "4.  **Terminal State**: 일반적으로 하나 또는 그 이상의 종료 상태가\n",
        "    있으며, 에이전트가 이 상태에 도달하면 에피소드가 종료됨."
      ],
      "id": "2257113f-45e1-44b1-93f7-8e34c7914d7e"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]), # x+ \n",
        "            1: np.array([0, 1]), # y+\n",
        "            2: np.array([-1, 0]), # x-  \n",
        "            3: np.array([0, -1]), # y-\n",
        "        }\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4, 4])\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "    def reset(self):\n",
        "        self.agent_action = None\n",
        "        self.agent_state = np.array([0, 0])\n",
        "        return self.agent_state\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.agent_state = self.agent_state + direction\n",
        "        # 목표지점에 도달 \n",
        "        if np.array_equal(np.array([3,3]), self.agent_state):\n",
        "            reward = 100\n",
        "            terminated = True\n",
        "        else:\n",
        "            reward = -1\n",
        "            terminated = False\n",
        "        # 4*4밖에 있을 경우 \n",
        "        if self.agent_state not in self.state_space:\n",
        "            reward = -10\n",
        "            terminated = True\n",
        "            self.agent_state = self.agent_state - 1/2*direction\n",
        "        return self.agent_state, reward, terminated"
      ],
      "id": "3a5fea73-af23-4dc9-be0c-d083122175cc"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "env.reset()\n",
        "states = []\n",
        "rewards = [] \n",
        "terminations = [] \n",
        "for t in range(500):\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminated = env.step(action)\n",
        "    states.append(state)\n",
        "    rewards.append(reward)\n",
        "    terminations.append(terminated)\n",
        "    if terminated: \n",
        "        break "
      ],
      "id": "0c9492ff-1979-4b2d-8a04-f656bc030df3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "[np.array([0,0])]+states"
      ],
      "id": "2c11a0aa-9518-4013-b8ec-b7287a22ab7a"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "show([np.array([0,0])]+states)"
      ],
      "id": "e78cc49b-1f29-4ac4-aa4c-3345cc839777"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent1 클래스 구현 + Run\n",
        "\n",
        "`-` 첫번째 시도"
      ],
      "id": "f1c58c1f-909e-443a-8509-e7f3e8ef7359"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent1:\n",
        "    def __init__(self,env):\n",
        "        self.action_space = env.action_space\n",
        "        self.state_space = env.state_space\n",
        "        self.n_experiences = 0\n",
        "        self.n_episode = 0  \n",
        "        \n",
        "        ## episode-wise info \n",
        "        self.scores = [] \n",
        "        self.playtimes = [] \n",
        "\n",
        "        ## time-wise info\n",
        "        self.current_state = None \n",
        "        self.action = None\n",
        "        self.reward = None        \n",
        "        self.next_state = None \n",
        "        self.socre = 0\n",
        "        \n",
        "        ## ReplayBuffer\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.current_states = []\n",
        "        self.next_states = [] \n",
        "        self.terminations = []\n",
        "        \n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "        \n",
        "    def save_experience(self): \n",
        "        self.actions.append(self.action)\n",
        "        self.current_states.append(self.current_state)\n",
        "        self.next_states.append(self.next_state)\n",
        "        self.rewards.append(self.reward)\n",
        "        self.terminations.append(self.terminated)\n",
        "        self.n_experiences += 1\n",
        "        self.score += self.reward\n",
        "\n",
        "    def learn(self):\n",
        "        pass"
      ],
      "id": "8fce5e60-18f1-4e4b-9ebb-0ec2f047514b"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1   Score: -16  Playtime: 7\n",
            "Episode 2   Score: -12  Playtime: 3\n",
            "Episode 3   Score: -10  Playtime: 1\n",
            "Episode 4   Score: -11  Playtime: 2\n",
            "Episode 5   Score: -10  Playtime: 1\n",
            "Episode 6   Score: -11  Playtime: 2\n",
            "Episode 7   Score: -10  Playtime: 1\n",
            "Episode 8   Score: -19  Playtime: 10\n",
            "Episode 9   Score: -10  Playtime: 1\n",
            "Episode 10  Score: -12  Playtime: 3\n",
            "Episode 11  Score: -22  Playtime: 13\n",
            "Episode 12  Score: -10  Playtime: 1\n",
            "Episode 13  Score: -10  Playtime: 1\n",
            "Episode 14  Score: -12  Playtime: 3\n",
            "Episode 15  Score: -11  Playtime: 2\n",
            "Episode 16  Score: -10  Playtime: 1\n",
            "Episode 17  Score: -16  Playtime: 7\n",
            "Episode 18  Score: -10  Playtime: 1\n",
            "Episode 19  Score: -20  Playtime: 11\n",
            "Episode 20  Score: -10  Playtime: 1"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent1(env)\n",
        "for _ in range(20):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 학습\n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: 종료조건 체크\n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    ## 2. 비본질적 코드\n",
        "    print(\n",
        "        f'Episode {agent.n_episode}\\t'\n",
        "        f'Score: {agent.scores[-1]}\\t'\n",
        "        f'Playtime: {agent.playtimes[-1]}'\n",
        "    )"
      ],
      "id": "c2577e9d-b45a-4e04-bb4f-f8df81cc7b9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 어떻게 학습을 할까? 즉 어떻게 “환경의 이해 $\\to$ 행동의 결정” 의\n",
        "과정을 수행할까?\n",
        "\n",
        "1.  어떠한 상태에서, 어떠한 행동을 했을때, 어떠한 보상과 어떠한\n",
        "    다음상태를 받았는지 기록하자.\n",
        "2.  1을 바탕으로 다음행동을 어떻게 할지 판단하자."
      ],
      "id": "65552587-2b02-40c8-a7cc-5500c7fc1c84"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])]+agent.next_states[-5:-1]\n",
        "states"
      ],
      "id": "8847e529-e74d-4c62-93c8-bbe948752fa2"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "show(states)"
      ],
      "id": "bc8c510d-a248-43b6-870a-a3fda09e15b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 환경의 이해 (1차원적 이해)\n",
        "\n",
        "`-` 무작위로 10000판을 진행하여 보자."
      ],
      "id": "2f59c9c4-585a-4e87-b80f-e725a1d863f1"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent1(env)\n",
        "for _ in range(10000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: agent << env\n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 학습\n",
        "        agent.learn()\n",
        "        # step4: state update \n",
        "        agent.current_state = agent.next_state \n",
        "        # step5: 종료조건 체크\n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 "
      ],
      "id": "80e04276-c30d-44de-a3e3-6985a4832589"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "cff1062b-f5de-4897-bec9-0d7b7b636c44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "fd0fd6fa-3742-4904-a9aa-259f00dc7ab9"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[0], agent.actions[0], agent.rewards[0]"
      ],
      "id": "9ee8fdd8-a045-4e0f-a898-9ffee4609551"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[1], agent.actions[1], agent.rewards[1]"
      ],
      "id": "085336fb-f8c5-4d9a-aa93-c22cdfae20d0"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[2], agent.actions[2], agent.rewards[2]"
      ],
      "id": "d2d9ffdf-6eb6-4aba-8109-b62e924d6a36"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.current_states[3], agent.actions[3], agent.rewards[3]"
      ],
      "id": "6253a16c-a396-4837-b643-d74aaa414297"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "5bb9edc4-9e9d-481b-8096-984fa5a17b2c"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q[x,y,a] = q[x,y,a] + agent.rewards[i]\n",
        "    count[x,y,a] = count[x,y,a] + 1 "
      ],
      "id": "237d6e30-0c45-432b-90db-a92eec386b19"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "count[count==0] = 0.1\n",
        "count"
      ],
      "id": "06270762-0bd6-4e5b-878e-48b044310761"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = (q/count).round(2)\n",
        "q"
      ],
      "id": "127edf98-279a-4bc4-9c77-b6c3dd74c834"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(f\"action = {i}\\n\"\n",
        "          f\"action-value function =\\n {q[:,:,i]}\\n\"\n",
        "          )"
      ],
      "id": "b467c815-99c7-411e-8153-d36904abd880"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2) – 이렇게하면 count를 따로 기록할 필요\n",
        "없음"
      ],
      "id": "e5847633-63cd-4abe-977c-aceb15c9c134"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a] # 풀이한 답\n",
        "    q_realistic = agent.rewards[i] # 실제 답\n",
        "    diff = q_realistic - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값이라고 하자\n",
        "    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백 * 학습률"
      ],
      "id": "3e5dddde-9d7f-4973-bc94-ce426c636188"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ -1.   -1.   -1.   -1. ]\n",
            " [ -1.   -1.   -1.   -1. ]\n",
            " [ -1.   -1.   -1.   98.8]\n",
            " [-10.  -10.   -9.8   0. ]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  98.   0.]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(\n",
        "        f\"action = {i}\\n\"\n",
        "        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n",
        "    )"
      ],
      "id": "24909698-cb58-4520-b80b-82f0c6070003"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 환경의 깊은 이해 (좀 더 고차원적인 이해)\n",
        "\n",
        "`-` action=1 일때 각 state의 가치 (=기대보상)"
      ],
      "id": "f467bed8-8921-440d-ab21-15f02f4d58ea"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[:,:,1]"
      ],
      "id": "2f86c9f7-41e0-4865-aecd-08ea1bcda5ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 분석1"
      ],
      "id": "85d186a3-d1fa-4f57-9876-a45ef2a252a7"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2][1] "
      ],
      "id": "f939b243-6bab-4143-a14b-c47102c2e93c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동 1을 하게 되면 100의 보상을 얻으므로 기대보상값은\n",
        "    100근처 -\\> 합리적임\n",
        "\n",
        "`-` 분석2"
      ],
      "id": "644b6acb-55ad-4788-8cf7-83b8b8f60c9e"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,1][1] "
      ],
      "id": "74d617aa-45e6-4d88-9c7c-d8e0d6961737"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동1을 하게 되면 -1의 보상을 얻으므로 기대보상값은\n",
        "    -1 근처 -\\> 합리적일까?\n",
        "\n",
        "`-` 비판: 분석2는 합리적인듯 하지만 data를 분석한뒤는 그다지 합리적이지\n",
        "못함\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 or 1)\n",
        "-   0을 쓸 때와 1을 쓸 때의 보상은 다름\n",
        "-   그런데 무수히 많은 데이터를 분석한 결과 0을 쓰면 0원을 보상으로\n",
        "    주고, 1을 쓰면 10만원을 보상으로 준다는 것을 “알게 되었음”\n",
        "-   빈 종이의 가치는 5만원인가? 아니면 10만원인가? –\\> 10만원 아니야?\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$, $a=1$에서 추정된(esimated) 값은\n",
        "`q[3,1][1]`이지만[1], 현실적으로는 “실제보상(-1)과 잠재적보상(100)”을\n",
        "동시에 고려해야하는게 합리적인듯\n",
        "\n",
        "[1] 즉 잠재적 보상은 고려되어있지 않음"
      ],
      "id": "f5e36eef-d0f9-4a70-932c-f8f86ba169f7"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_esimated = q[3,1][1]\n",
        "q_esimated"
      ],
      "id": "dd3182be-720f-4fc3-bab6-56ab0233f41f"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "q_realistic = (-1) + 0.99 * (100) \n",
        "q_realistic"
      ],
      "id": "bdcb9da2-8ac5-4972-8567-a9bc11d1d206"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99 는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이 =\n",
        "    십만원 으로 생각한다는 의미)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s,a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99\\times \\max_{a} q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$ 타당하게 추정된 것이라 볼 수 있다. 좀 더\n",
        "정확하게는\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases} \\text{reward}(s,a) & \\text{terminated}  \\\\ \\text{reward}(s,a)+ 0.99\\times \\max_{a} q(s',a) & \\text{not terminated} \\end{cases} $$"
      ],
      "id": "dea592ea-cd20-4c3c-82ef-9d3b59d4f731"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "q = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    x,y = agent.current_states[i]\n",
        "    xx,yy = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    q_estimated = q[x,y,a]\n",
        "    if agent.terminations[i]:\n",
        "        q_realistic = agent.rewards[i] \n",
        "    else:\n",
        "        q_future = q[xx,yy,:].max()\n",
        "        q_realistic = agent.rewards[i] + 0.99*q_future # 이걸 관측했다고 치는거임\n",
        "    diff = q_realistic - q_estimated \n",
        "    q[x,y,a] = q_estimated + 0.1 * diff"
      ],
      "id": "c0055150-d5de-4560-b0a2-458b663457ce"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0\n",
            "action-value function =\n",
            " [[ 90.2  92.1  94.   95.8]\n",
            " [ 92.1  94.   96.   98. ]\n",
            " [ 93.8  95.9  98.  100. ]\n",
            " [-10.  -10.  -10.    0. ]]\n",
            "\n",
            "action = 1\n",
            "action-value function =\n",
            " [[ 90.2  92.1  93.8 -10. ]\n",
            " [ 92.1  94.   96.  -10. ]\n",
            " [ 94.   96.   98.  -10. ]\n",
            " [ 95.8  97.9 100.    0. ]]\n",
            "\n",
            "action = 2\n",
            "action-value function =\n",
            " [[-10.  -10.  -10.  -10. ]\n",
            " [ 88.3  90.2  92.1  93.2]\n",
            " [ 90.2  92.1  94.   95.4]\n",
            " [ 92.1  94.   96.    0. ]]\n",
            "\n",
            "action = 3\n",
            "action-value function =\n",
            " [[-10.   88.3  90.2  92.1]\n",
            " [-10.   90.2  92.1  94. ]\n",
            " [-10.   92.1  94.   95.9]\n",
            " [-10.   93.6  95.2   0. ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(\n",
        "        f\"action = {i}\\n\"\n",
        "        f\"action-value function =\\n {q[:,:,i].round(1)}\\n\"\n",
        "    )"
      ],
      "id": "0a86c9c0-43b9-495f-968f-876f3220c356"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "0b9df898-f60d-4401-804d-ba729b812cd4"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:]"
      ],
      "id": "887300ae-49de-4b85-b3e5-85c5fa552c4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 1을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "9340fa78-e66c-425a-8632-5636d38cb394"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:]"
      ],
      "id": "b2c26efc-0fab-4fa6-a589-7b0527485aa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "645a66f6-5922-4082-872e-254ce1f6060f"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:] "
      ],
      "id": "9e2123b6-1be2-4c67-83ec-46d1659fe720"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 1을 하는게 유리함.\n",
        "\n",
        "`-` 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "7c9abe2e-40f8-43b0-9ff6-8aba39552f43"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[0,0,:].argmax()"
      ],
      "id": "dbcbb679-b8d8-4f28-899a-c5b0e3cb246e"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[2,3,:].argmax()"
      ],
      "id": "fcdbb861-1c06-4d03-b6bf-5ceee5c737a9"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "q[3,2,:].argmax()"
      ],
      "id": "47e2694b-b9ae-4a82-a63d-db4c17a10ac3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자."
      ],
      "id": "a3853ab5-b456-4721-95fb-ce03d470b0f6"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = np.array(['?????']*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "1a696d2e-8202-499d-8aeb-e1d529a230b8"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "directions = {0: 'down', 1: 'right', 2:'up', 3:'left'}"
      ],
      "id": "6adc009d-79b3-45e4-9495-679a10b086fa"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        policy[i,j] = directions[q[i,j,:].argmax()]\n",
        "policy"
      ],
      "id": "ce7065e8-5fca-4335-84c2-b2a1631c72ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 요약: 값이 큰 쪽으로 이동"
      ],
      "id": "4e599ef2-fcfa-43cc-8c44-f000e04e183a"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "q.max(axis=-1)"
      ],
      "id": "1c3beff3-7523-46ed-abd8-39af5e6c1783"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent2 클래스 구현 + Run"
      ],
      "id": "6b0d0989-ae6d-4774-a31b-58f5abf09f2a"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent2(Agent1):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.q = np.zeros([4,4,4])\n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000: \n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            x,y = self.current_state\n",
        "            self.action = self.q[x,y,:].argmax()\n",
        "    def learn(self): # make q\n",
        "        x,y = self.current_state\n",
        "        xx,yy = self.next_state\n",
        "        a = self.action\n",
        "        q_estimated = self.q[x,y,a]\n",
        "        if agent.terminated:\n",
        "            q_realistic = self.reward\n",
        "        else:\n",
        "            q_future = q[xx,yy,:].max()\n",
        "            q_realistic = self.reward + 0.99*q_future\n",
        "        # q_observed 와 q_estimated를 점점 비슷하게 만들어주는 역할\n",
        "        diff = q_realistic - q_estimated\n",
        "        self.q[x,y,a] = q_estimated + 0.1 * diff "
      ],
      "id": "fd0d1b87-11c0-4f7e-bdea-fee402f05c6d"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100 Score: -8.30    Playtime:  3.70 n_experiences: 370\n",
            "Episode 200 Score: -10.66   Playtime:  2.76 n_experiences: 646\n",
            "Episode 300 Score: -12.63   Playtime:  3.63 n_experiences: 1009\n",
            "Episode 400 Score: -6.53    Playtime:  3.03 n_experiences: 1312\n",
            "Episode 500 Score: -10.20   Playtime:  3.40 n_experiences: 1652\n",
            "Episode 600 Score: -11.89   Playtime:  2.89 n_experiences: 1941\n",
            "Episode 700 Score: -12.92   Playtime:  3.92 n_experiences: 2333\n",
            "Episode 800 Score: -10.03   Playtime:  3.23 n_experiences: 2656\n",
            "Episode 900 Score: -10.12   Playtime:  3.32 n_experiences: 2988\n",
            "Episode 1000    Score: -48.86   Playtime:  48.59    n_experiences: 7847\n",
            "Episode 1100    Score: -50.00   Playtime:  50.00    n_experiences: 12847\n",
            "Episode 1200    Score: -50.00   Playtime:  50.00    n_experiences: 17847\n",
            "Episode 1300    Score: -50.00   Playtime:  50.00    n_experiences: 22847\n",
            "Episode 1400    Score: -50.00   Playtime:  50.00    n_experiences: 27847\n",
            "Episode 1500    Score: -50.00   Playtime:  50.00    n_experiences: 32847\n",
            "Episode 1600    Score: -50.00   Playtime:  50.00    n_experiences: 37847\n",
            "Episode 1700    Score: -50.00   Playtime:  50.00    n_experiences: 42847\n",
            "Episode 1800    Score: -50.00   Playtime:  50.00    n_experiences: 47847\n",
            "Episode 1900    Score: -50.00   Playtime:  50.00    n_experiences: 52847\n",
            "Episode 2000    Score: -50.00   Playtime:  50.00    n_experiences: 57847"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent2(env)\n",
        "for _ in range(2000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 데이터저장 및 학습\n",
        "        agent.learn()        \n",
        "        # step4: 다음 iteration 준비 + 종료조건체크\n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 100) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_experiences: {agent.n_experiences}'\n",
        "        )"
      ],
      "id": "9b9c36b8-3040-415d-93cb-890199998bba"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "a70344ac-72be-449a-8563-656133189570"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent3 클래스 구현 + Run"
      ],
      "id": "70402564-3d5d-4e99-9484-a9f843fef3d8"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent3(Agent2):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0\n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            x,y = self.current_state\n",
        "            self.action = self.q[x,y,:].argmax()"
      ],
      "id": "2dd12733-9b32-4bea-9f4c-ba510c4fe72a"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000    Score:  35.77   Playtime:  5.83 n_experiences: 0.3676954247709635\n",
            "Episode 2000    Score:  78.15   Playtime:  6.35 n_experiences: 0.1351999253974994\n",
            "Episode 3000    Score:  85.34   Playtime:  5.76 n_experiences: 0.04971239399803625\n",
            "Episode 4000    Score:  92.77   Playtime:  6.03 n_experiences: 0.018279019827489446\n",
            "Episode 5000    Score:  91.76   Playtime:  5.94 n_experiences: 0.006721111959865607"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent3(env)\n",
        "agent.eps = 1 \n",
        "for _ in range(5000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        # step1: agent >> env \n",
        "        agent.act()\n",
        "        env.agent_action = agent.action\n",
        "        # step2: env << agent \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n",
        "        agent.save_experience()\n",
        "        # step3: 데이터저장 및 학습\n",
        "        agent.learn()        \n",
        "        # step4: 다음 iteration 준비 + 종료조건체크\n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    agent.eps = agent.eps * 0.999\n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 1000) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_experiences: {agent.eps}'\n",
        "        )"
      ],
      "id": "fb5ecb97-d847-41a8-aeb2-df73d7731893"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "d6b1ead8-f25a-458f-bf81-b8c54edb9ddd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 최종 Agent 클래스 구현 + Run"
      ],
      "id": "5be38a1c-b596-4f94-b113-ffc0ebb52c78"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# learn 추가\n",
        "class Agent(Agent3):\n",
        "    # agent >> env \n",
        "    def __rshift__(self,env):\n",
        "        self.act()\n",
        "        env.agent_action = self.action\n",
        "    # agent << env \n",
        "    def __lshift__(self,env): \n",
        "        self.next_state, self.reward, self.terminated = env.step(env.agent_action)\n",
        "        self.save_experience()"
      ],
      "id": "15fd18f8-6201-4cd1-8cf4-73a7be0f9915"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000    Score:  39.50   Playtime:  6.50 n_eps: 0.3676954247709635\n",
            "Episode 2000    Score:  71.99   Playtime:  5.91 n_eps: 0.1351999253974994\n",
            "Episode 3000    Score:  92.76   Playtime:  6.04 n_eps: 0.04971239399803625\n",
            "Episode 4000    Score:  92.82   Playtime:  5.98 n_eps: 0.018279019827489446\n",
            "Episode 5000    Score:  93.86   Playtime:  6.04 n_eps: 0.006721111959865607"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = Agent(env)\n",
        "agent.eps = 1 \n",
        "for _ in range(5000):\n",
        "    ### 1. 본질적인 코드\n",
        "    agent.current_state = env.reset() \n",
        "    agent.terminated = False\n",
        "    agent.score = 0 \n",
        "    for t in range(50):\n",
        "        ## step1 \n",
        "        agent >> env \n",
        "        ## step2 \n",
        "        agent << env \n",
        "        ## step3 \n",
        "        agent.learn()    \n",
        "        ## step4 \n",
        "        agent.current_state = agent.next_state \n",
        "        ## step5 \n",
        "        if agent.terminated: break \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t+1)\n",
        "    agent.n_episode = agent.n_episode + 1 \n",
        "    agent.eps = agent.eps*0.999\n",
        "    ## 2. 비본질적 코드\n",
        "    if (agent.n_episode % 1000) == 0:\n",
        "        print(\n",
        "            f'Episode {agent.n_episode}\\t'\n",
        "            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n",
        "            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n",
        "            f'n_eps: {agent.eps}'\n",
        "        )"
      ],
      "id": "59f43204-71eb-4470-a4bc-ba61dce65514"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "00cd65e7-f9a2-4bb8-836d-c1a593eb24eb"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  }
}