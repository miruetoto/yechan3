{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (강의) 감성분류\n",
        "\n",
        "신록예찬  \n",
        "2024-09-08\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "# 2. Install"
      ],
      "id": "63fe169f-c938-4880-a301-12fcfa647503"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install transformers datasets evaluate accelerate"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이 코드는 뭔가를 설치하는 코드이다.\n",
        "-   `pip`: 여기에서 `pip` 은 앱스토어 이름이라고 생각하자. (구글스토어,\n",
        "    T-store)\n",
        "-   `install`: 이것은 “설치버튼” 이라고 생각하자.\n",
        "-   `transformer`, `datasets`, `evaluate`, `accelerate`: 설치하고 싶은\n",
        "    프로그램 이름\n",
        "\n",
        "> 이거 코랩에서 실행해도 여러분의 컴퓨터에 뭐가 깔리는건 아닙니다.\n",
        "> 마음편하게 실행하세요.\n",
        "\n",
        "# 3. 코딩패턴\n",
        "\n",
        "`-` 파이썬에서 보통 지도학습은 적당히 아래와 같은 흐름으로 코드가\n",
        "진행된다.\n",
        "\n",
        "``` python\n",
        "## Step1: 데이터 정리 \n",
        "데이터 = 데이터읽기()\n",
        "전처리된데이터 = 데이터전처리하기(데이터)\n",
        "train_data, test_data = 데이터분리하기(전처리된데이터)\n",
        "\n",
        "## Step2: 인공지능 생성\n",
        "인공지능 = 인공지능생생하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(train_data)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(train_data) # train_data 풀기 \n",
        "인공지능.예측하기(val_data) # val_data 풀기 \n",
        "인공지능.예측하기(test_data) # test_data 풀기 \n",
        "인공지능.예측하기(기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "-   특징: 오브젝트 중심적\n",
        "-   R의 경우는 위의 코드가 함수중심적으로 사용됨\n",
        "\n",
        "`-` 이전시간에 언급한 내용\n",
        "\n",
        "-   전략: (1) 블로그의 코드를 돌려본다. (2) 블로그의 코드를 이해한다.\n",
        "    (이론을 이해하는게 아님. 코드를 이해하는 것임) (3) 블로그의 코드에서\n",
        "    데이터 부분만 내가 사용할 데이터로 바꿔친다. (4) 돌려서 결과를\n",
        "    제출한다.\n",
        "-   마음가짐: 나는 어떠한 코드도 짤 수 있는 사람이다. (X) // 나는 어떠한\n",
        "    코드도 이해할 수 있는 사람이다. (X) // 나는 어떠한 코드도 베낄 수\n",
        "    있는 (활용할 수 있는) 사람이다. (O)\n",
        "\n",
        "`-` 코드의 세부적인 내용을 이해하지 못했지만 전체적인 흐름을 이해하지 한\n",
        "상황 = 블로그의 코드를 보고 `Step1` ~ `Step4` 에 대응하는 부분을 빠르게\n",
        "정리해야해요.\n",
        "\n",
        "-   실제로는 위의 코드 그대로 나오지 않고 변형된형태로 나옴.\n",
        "-   그 뼈대를 빠르게 이해해야 함.\n",
        "\n",
        "`-` 변형패턴1: train/test 를 먼저 분리하고 전처리할 수도 있음.\n",
        "\n",
        "``` python\n",
        "## Step1: 데이터 정리 \n",
        "텍스트데이터 = 데이터읽기()\n",
        "train_텍스트데이터, test_텍스트데이터 = 데이터분리하기(텍스트데이터)\n",
        "train_data = train_전처리된숫자데이터 = 데이터전처리하기(train_텍스트데이터)\n",
        "test_data = test_전처리된숫자데이터 = 데이터전처리하기(test_텍스트데이터)\n",
        "\n",
        "## Step2: 인공지능 생성\n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(train_)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(train_data) # train_data 풀기 \n",
        "인공지능.예측하기(val_data) # val_data 풀기 \n",
        "인공지능.예측하기(test_data) # test_data 풀기 \n",
        "인공지능.예측하기(기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "`-` 변형패턴2: 데이터에 변형함수가 내장된 경우도 있음.\n",
        "\n",
        "``` python\n",
        "## Step1: 데이터 정리 \n",
        "데이터 = 데이터읽기()\n",
        "\n",
        "데이터.변환하기(데이터전처리하기) # 변환하기()라는 함수의 입력으로 데이터전처리하기() 라는 함수 자체를 전달하는 구조!!\n",
        "train_data = train_전처리된숫자데이터 = 데이터전처리하기(train_텍스트데이터)\n",
        "test_data = test_전처리된숫자데이터 = 데이터전처리하기(test_텍스트데이터)\n",
        "\n",
        "## Step2: 인공지능 생성\n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(train_)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(train_data) # train_data 풀기 \n",
        "인공지능.예측하기(val_data) # val_data 풀기 \n",
        "인공지능.예측하기(test_data) # test_data 풀기 \n",
        "인공지능.예측하기(기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "# 4. Step1: 데이터의 정리"
      ],
      "id": "a78c1dd4-0110-432a-8f7a-65fc0a0e4664"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset('imdb')"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn("
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "id": "cell-21"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-22"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 50000/50000 [00:05<00:00, 9436.37 examples/s]"
          ]
        }
      ],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ],
      "id": "cell-23"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "id": "cell-24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step2: 인공지능 생성"
      ],
      "id": "4349dbc5-6e43-4a3c-a234-c1adfd92f969"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "cell-26"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "id": "cell-27"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\"\n",
        ")"
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1563/1563 01:34]\n",
              "    </div>\n",
              "    "
            ]
          }
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object `Trainer` not found."
          ]
        }
      ],
      "source": [
        "Trainer??"
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. 토큰화\n",
        "\n",
        "`-`"
      ],
      "id": "f2d839b6-4a1b-4379-9bcd-94b8464fc804"
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 토큰화할 문장\n",
        "text = \"Hugging Face is creating a tool that democratizes AI.\"\n",
        "\n",
        "# 텍스트를 토큰으로 변환\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n"
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Hugging Face is creating a tool that democratizes AI.\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Hugging Hugging Hugging\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Face Face Face face face face\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"is is is\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-39"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set(dir(tokenizer)) & {'__call__'}"
      ],
      "id": "cell-40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-41"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ],
      "id": "cell-42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "id": "cell-43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator"
      ],
      "id": "cell-44"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "id": "cell-45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "id": "cell-46"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "id": "cell-47"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "id": "cell-48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "cell-49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
      ],
      "id": "cell-50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n",
        "classifier(text)"
      ],
      "id": "cell-51"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ],
      "id": "cell-52"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ],
      "id": "cell-53"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.iㅌd2label[predicted_class_id]"
      ],
      "id": "cell-54"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "hf",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  }
}