{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (강의) 감성분류\n",
        "\n",
        "신록예찬  \n",
        "2024-09-08\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "# 2. Install"
      ],
      "id": "22ddbb65-d876-4534-8fda-82998293e823"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install transformers datasets evaluate accelerate"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   이 코드는 뭔가를 설치하는 코드이다.\n",
        "-   `pip`: 여기에서 `pip` 은 앱스토어 이름이라고 생각하자. (구글스토어,\n",
        "    T-store)\n",
        "-   `install`: 이것은 “설치버튼” 이라고 생각하자.\n",
        "-   `transformer`, `datasets`, `evaluate`, `accelerate`: 설치하고 싶은\n",
        "    프로그램 이름\n",
        "\n",
        "> 이거 코랩에서 실행해도 여러분의 컴퓨터에 뭐가 깔리는건 아닙니다.\n",
        "> 마음편하게 실행하세요.\n",
        "\n",
        "# 3. 코딩패턴\n",
        "\n",
        "`-` 파이썬에서 보통 지도학습은 적당히 아래와 같은 흐름으로 코드가\n",
        "진행된다.\n",
        "\n",
        "``` python\n",
        "## Step1: 데이터 정리 \n",
        "데이터 = 데이터읽기()\n",
        "전처리된데이터 = 데이터전처리하기(데이터)\n",
        "전처리된자료1, 전처리된자료2 = 데이터분리하기(전처리된데이터) \n",
        "\n",
        "## Step2: 인공지능 생성\n",
        "인공지능 = 인공지능생생하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(전처리된자료1)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(전처리된자료1) \n",
        "인공지능.예측하기(전처리된자료2) \n",
        "인공지능.예측하기(전처리된자료3) \n",
        "```\n",
        "\n",
        "-   특징: 오브젝트 중심적\n",
        "-   R의 경우는 위의 코드가 함수중심적으로 사용됨\n",
        "\n",
        "`-` 이전시간에 언급한 내용\n",
        "\n",
        "-   전략: (1) 블로그의 코드를 돌려본다. (2) 블로그의 코드를 이해한다.\n",
        "    (이론을 이해하는게 아님. 코드를 이해하는 것임) (3) 블로그의 코드에서\n",
        "    데이터 부분만 내가 사용할 데이터로 바꿔친다. (4) 돌려서 결과를\n",
        "    제출한다.\n",
        "-   마음가짐: 나는 어떠한 코드도 짤 수 있는 사람이다. (X) // 나는 어떠한\n",
        "    코드도 이해할 수 있는 사람이다. (X) // 나는 어떠한 코드도 베낄 수\n",
        "    있는 (활용할 수 있는) 사람이다. (O)\n",
        "\n",
        "`-` 코드의 세부적인 내용을 이해하지 못했지만 전체적인 흐름을 이해하지 한\n",
        "상황 = 블로그의 코드를 보고 `Step1` ~ `Step4` 에 대응하는 부분을 빠르게\n",
        "정리해야해요.\n",
        "\n",
        "-   실제로는 위의 코드 그대로 나오지 않고 변형된형태로 나옴.\n",
        "-   그 뼈대를 빠르게 이해해야 함.\n",
        "\n",
        "`-` 변형패턴1\n",
        "\n",
        "``` python\n",
        "## Step1 \n",
        "데이터 = 데이터불러오기()\n",
        "전처리된훈련자료 = 데이터전처리하기(데이터['train'])\n",
        "전처리된검증자료 = 데이터전처리하기(데이터['val'])\n",
        "\n",
        "\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(전처리된훈련자료)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(전처리된훈련자료) # train_data 풀기 \n",
        "인공지능.예측하기(전처리된검증자료) # val_data 풀기\n",
        "인공지능.예측하기(전처리된기습질문) # 기습질문 풀기 , test_data 로 생각해도 무방\n",
        "```\n",
        "\n",
        "`-` 변형패턴2: 데이터자체에 “변환하기” 관련된 method가 있는 경우\n",
        "\n",
        "``` python\n",
        "## Step1 \n",
        "데이터 = 데이터불러오기()\n",
        "전처리된데이터 = 데이터.변환하기(데이터전처리하기)\n",
        "전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['val']\n",
        "\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.학습하기(전처리된훈련자료)\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(전처리된훈련자료) # train_data 풀기 \n",
        "인공지능.예측하기(전처리된검증자료) # val_data 풀기 \n",
        "인공지능.예측하기(전처리된기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "`-` 변형패턴3: 트레이너가 존재하는 경우\n",
        "\n",
        "``` python\n",
        "## Step1 \n",
        "데이터 = 데이터불러오기()\n",
        "전처리된데이터 = 데이터.변환하기(데이터전처리하기)\n",
        "전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['val']\n",
        "\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "트레이너 = 트레이너생성하기(인공지능,전처리된훈련자료,전처리된검증자료)\n",
        "트레이너.train()\n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(전처리된훈련자료) # train_data 풀기 \n",
        "인공지능.예측하기(전처리된검증자료) # val_data 풀기 \n",
        "인공지능.예측하기(전처리된기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "`-` 변형패턴4: 강인공지능이 존재하는 경우\n",
        "\n",
        "``` python\n",
        "## Step1 \n",
        "데이터 = 데이터불러오기()\n",
        "전처리된데이터 = 데이터.변환하기(데이터전처리하기)\n",
        "전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['val']\n",
        "\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "트레이너 = 트레이너생성하기(인공지능,전처리된훈련자료,전처리된검증자료)\n",
        "트레이너.train()\n",
        "\n",
        "## Step4 \n",
        "강인공지능 = 강인공지능생성하기(인공지능) \n",
        "강인공지능.예측하기(전처리되지않은자료) \n",
        "```\n",
        "\n",
        "`-` 기타변형패턴들..\n",
        "\n",
        "``` python\n",
        "## Step1 \n",
        "데이터 = 데이터불러오기()\n",
        "전처리된데이터 = 데이터.변환하기(데이터전처리하기)\n",
        "훈련자료, 테스트자료 = 전처리된데이터['train'], 전처리된데이터['test']\n",
        "\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기()\n",
        "\n",
        "## Step3: 인공지능 학습 \n",
        "인공지능.훈련모드()\n",
        "인공지능.학습하기(전처리된훈련자료)\n",
        "인공지능.평가모드() \n",
        "\n",
        "## Step4: 예측\n",
        "인공지능.예측하기(전처리된훈련자료) # train_data 풀기 \n",
        "인공지능.예측하기(전처리된검증자료) # val_data 풀기 \n",
        "인공지능.예측하기(전처리된기습질문) # 기습질문 풀기 \n",
        "```\n",
        "\n",
        "# 4. 코드정리\n",
        "\n",
        "ref:\n",
        "<https://huggingface.co/docs/transformers/tasks/sequence_classification>\n",
        "\n",
        "## A. Step1 – 데이터의 정리\n",
        "\n",
        "`-` 데이터불러오기"
      ],
      "id": "bc19996f-5990-4b7e-bce4-c33548ada85e"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\") # 텍스트데이터 = 데이터불러오기()"
      ],
      "id": "cell-26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 토크나이저 생성"
      ],
      "id": "4ecc32a7-de1f-4d2c-8820-7da24f821e44"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "뭐하는거?"
      ],
      "id": "b9bd4998-56ba-4f02-8ab7-8213e8d7a38c"
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['SPECIAL_TOKENS_ATTRIBUTES',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_add_tokens',\n",
              " '_additional_special_tokens',\n",
              " '_auto_class',\n",
              " '_batch_encode_plus',\n",
              " '_bos_token',\n",
              " '_call_one',\n",
              " '_cls_token',\n",
              " '_compile_jinja_template',\n",
              " '_convert_encoding',\n",
              " '_convert_id_to_token',\n",
              " '_convert_token_to_id_with_added_voc',\n",
              " '_create_repo',\n",
              " '_decode',\n",
              " '_decode_use_source_tokenizer',\n",
              " '_encode_plus',\n",
              " '_eos_token',\n",
              " '_eventual_warn_about_too_long_sequence',\n",
              " '_eventually_correct_t5_max_length',\n",
              " '_from_pretrained',\n",
              " '_get_files_timestamps',\n",
              " '_get_padding_truncation_strategies',\n",
              " '_in_target_context_manager',\n",
              " '_mask_token',\n",
              " '_pad',\n",
              " '_pad_token',\n",
              " '_pad_token_type_id',\n",
              " '_processor_class',\n",
              " '_render_with_assistant_indices',\n",
              " '_save_pretrained',\n",
              " '_sep_token',\n",
              " '_set_processor_class',\n",
              " '_switch_to_input_mode',\n",
              " '_switch_to_target_mode',\n",
              " '_tokenizer',\n",
              " '_unk_token',\n",
              " '_upload_modified_files',\n",
              " 'add_special_tokens',\n",
              " 'add_tokens',\n",
              " 'added_tokens_decoder',\n",
              " 'added_tokens_encoder',\n",
              " 'additional_special_tokens',\n",
              " 'additional_special_tokens_ids',\n",
              " 'all_special_ids',\n",
              " 'all_special_tokens',\n",
              " 'all_special_tokens_extended',\n",
              " 'apply_chat_template',\n",
              " 'as_target_tokenizer',\n",
              " 'backend_tokenizer',\n",
              " 'batch_decode',\n",
              " 'batch_encode_plus',\n",
              " 'bos_token',\n",
              " 'bos_token_id',\n",
              " 'build_inputs_with_special_tokens',\n",
              " 'can_save_slow_tokenizer',\n",
              " 'chat_template',\n",
              " 'clean_up_tokenization',\n",
              " 'clean_up_tokenization_spaces',\n",
              " 'cls_token',\n",
              " 'cls_token_id',\n",
              " 'convert_added_tokens',\n",
              " 'convert_ids_to_tokens',\n",
              " 'convert_tokens_to_ids',\n",
              " 'convert_tokens_to_string',\n",
              " 'create_token_type_ids_from_sequences',\n",
              " 'decode',\n",
              " 'decoder',\n",
              " 'deprecation_warnings',\n",
              " 'do_lower_case',\n",
              " 'encode',\n",
              " 'encode_plus',\n",
              " 'eos_token',\n",
              " 'eos_token_id',\n",
              " 'from_pretrained',\n",
              " 'get_added_vocab',\n",
              " 'get_chat_template',\n",
              " 'get_special_tokens_mask',\n",
              " 'get_vocab',\n",
              " 'init_inputs',\n",
              " 'init_kwargs',\n",
              " 'is_fast',\n",
              " 'mask_token',\n",
              " 'mask_token_id',\n",
              " 'max_len_sentences_pair',\n",
              " 'max_len_single_sentence',\n",
              " 'model_input_names',\n",
              " 'model_max_length',\n",
              " 'name_or_path',\n",
              " 'num_special_tokens_to_add',\n",
              " 'pad',\n",
              " 'pad_token',\n",
              " 'pad_token_id',\n",
              " 'pad_token_type_id',\n",
              " 'padding_side',\n",
              " 'prepare_for_model',\n",
              " 'prepare_seq2seq_batch',\n",
              " 'pretrained_vocab_files_map',\n",
              " 'push_to_hub',\n",
              " 'register_for_auto_class',\n",
              " 'sanitize_special_tokens',\n",
              " 'save_pretrained',\n",
              " 'save_vocabulary',\n",
              " 'sep_token',\n",
              " 'sep_token_id',\n",
              " 'set_truncation_and_padding',\n",
              " 'slow_tokenizer_class',\n",
              " 'special_tokens_map',\n",
              " 'special_tokens_map_extended',\n",
              " 'split_special_tokens',\n",
              " 'tokenize',\n",
              " 'train_new_from_iterator',\n",
              " 'truncate_sequences',\n",
              " 'truncation_side',\n",
              " 'unk_token',\n",
              " 'unk_token_id',\n",
              " 'verbose',\n",
              " 'vocab',\n",
              " 'vocab_files_names',\n",
              " 'vocab_size']"
            ]
          }
        }
      ],
      "source": [
        "dir(tokenizer) "
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "tokenizer(\"Transformers are really useful for natural language processing.\") # 텍스트 -> 숫자들"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` preprocess_function 선언"
      ],
      "id": "151540cd-3248-4e3e-a2c2-67a5b5a0de1c"
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "뭐하는함수?"
      ],
      "id": "c8e0e0d1-8cca-441c-942e-fbfbff08fdba"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 19081, 2024, 2428, 6179, 2005, 3019, 2653, 6364, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "tokenizer(\"Transformers are really useful for natural language processing.\", truncation=True) # ?? 비슷한듯.."
      ],
      "id": "cell-35"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "examples = imdb['train'][0]\n",
        "tokenizer(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-36"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
            ]
          }
        }
      ],
      "source": [
        "examples = imdb['train'][:2]\n",
        "tokenizer(examples[\"text\"], truncation=True)"
      ],
      "id": "cell-37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` map 함수 사용"
      ],
      "id": "b0b59718-0ad0-4312-b6fc-ef2d846e9bfd"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 25000/25000 [00:03<00:00, 7777.72 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:03<00:00, 7018.03 examples/s]\n",
            "Map: 100%|██████████| 50000/50000 [00:06<00:00, 7553.70 examples/s]"
          ]
        }
      ],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ],
      "id": "cell-39"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "뭐한거지?"
      ],
      "id": "ba75845c-d2e8-4775-a11d-134888d8189f"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "({'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              "  'label': 0},\n",
              " {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              "  'label': 0,\n",
              "  'input_ids': [101,\n",
              "   1045,\n",
              "   12524,\n",
              "   1045,\n",
              "   2572,\n",
              "   8025,\n",
              "   1011,\n",
              "   3756,\n",
              "   2013,\n",
              "   2026,\n",
              "   2678,\n",
              "   3573,\n",
              "   2138,\n",
              "   1997,\n",
              "   2035,\n",
              "   1996,\n",
              "   6704,\n",
              "   2008,\n",
              "   5129,\n",
              "   2009,\n",
              "   2043,\n",
              "   2009,\n",
              "   2001,\n",
              "   2034,\n",
              "   2207,\n",
              "   1999,\n",
              "   3476,\n",
              "   1012,\n",
              "   1045,\n",
              "   2036,\n",
              "   2657,\n",
              "   2008,\n",
              "   2012,\n",
              "   2034,\n",
              "   2009,\n",
              "   2001,\n",
              "   8243,\n",
              "   2011,\n",
              "   1057,\n",
              "   1012,\n",
              "   1055,\n",
              "   1012,\n",
              "   8205,\n",
              "   2065,\n",
              "   2009,\n",
              "   2412,\n",
              "   2699,\n",
              "   2000,\n",
              "   4607,\n",
              "   2023,\n",
              "   2406,\n",
              "   1010,\n",
              "   3568,\n",
              "   2108,\n",
              "   1037,\n",
              "   5470,\n",
              "   1997,\n",
              "   3152,\n",
              "   2641,\n",
              "   1000,\n",
              "   6801,\n",
              "   1000,\n",
              "   1045,\n",
              "   2428,\n",
              "   2018,\n",
              "   2000,\n",
              "   2156,\n",
              "   2023,\n",
              "   2005,\n",
              "   2870,\n",
              "   1012,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   1996,\n",
              "   5436,\n",
              "   2003,\n",
              "   8857,\n",
              "   2105,\n",
              "   1037,\n",
              "   2402,\n",
              "   4467,\n",
              "   3689,\n",
              "   3076,\n",
              "   2315,\n",
              "   14229,\n",
              "   2040,\n",
              "   4122,\n",
              "   2000,\n",
              "   4553,\n",
              "   2673,\n",
              "   2016,\n",
              "   2064,\n",
              "   2055,\n",
              "   2166,\n",
              "   1012,\n",
              "   1999,\n",
              "   3327,\n",
              "   2016,\n",
              "   4122,\n",
              "   2000,\n",
              "   3579,\n",
              "   2014,\n",
              "   3086,\n",
              "   2015,\n",
              "   2000,\n",
              "   2437,\n",
              "   2070,\n",
              "   4066,\n",
              "   1997,\n",
              "   4516,\n",
              "   2006,\n",
              "   2054,\n",
              "   1996,\n",
              "   2779,\n",
              "   25430,\n",
              "   14728,\n",
              "   2245,\n",
              "   2055,\n",
              "   3056,\n",
              "   2576,\n",
              "   3314,\n",
              "   2107,\n",
              "   2004,\n",
              "   1996,\n",
              "   5148,\n",
              "   2162,\n",
              "   1998,\n",
              "   2679,\n",
              "   3314,\n",
              "   1999,\n",
              "   1996,\n",
              "   2142,\n",
              "   2163,\n",
              "   1012,\n",
              "   1999,\n",
              "   2090,\n",
              "   4851,\n",
              "   8801,\n",
              "   1998,\n",
              "   6623,\n",
              "   7939,\n",
              "   4697,\n",
              "   3619,\n",
              "   1997,\n",
              "   8947,\n",
              "   2055,\n",
              "   2037,\n",
              "   10740,\n",
              "   2006,\n",
              "   4331,\n",
              "   1010,\n",
              "   2016,\n",
              "   2038,\n",
              "   3348,\n",
              "   2007,\n",
              "   2014,\n",
              "   3689,\n",
              "   3836,\n",
              "   1010,\n",
              "   19846,\n",
              "   1010,\n",
              "   1998,\n",
              "   2496,\n",
              "   2273,\n",
              "   1012,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   2054,\n",
              "   8563,\n",
              "   2033,\n",
              "   2055,\n",
              "   1045,\n",
              "   2572,\n",
              "   8025,\n",
              "   1011,\n",
              "   3756,\n",
              "   2003,\n",
              "   2008,\n",
              "   2871,\n",
              "   2086,\n",
              "   3283,\n",
              "   1010,\n",
              "   2023,\n",
              "   2001,\n",
              "   2641,\n",
              "   26932,\n",
              "   1012,\n",
              "   2428,\n",
              "   1010,\n",
              "   1996,\n",
              "   3348,\n",
              "   1998,\n",
              "   16371,\n",
              "   25469,\n",
              "   5019,\n",
              "   2024,\n",
              "   2261,\n",
              "   1998,\n",
              "   2521,\n",
              "   2090,\n",
              "   1010,\n",
              "   2130,\n",
              "   2059,\n",
              "   2009,\n",
              "   1005,\n",
              "   1055,\n",
              "   2025,\n",
              "   2915,\n",
              "   2066,\n",
              "   2070,\n",
              "   10036,\n",
              "   2135,\n",
              "   2081,\n",
              "   22555,\n",
              "   2080,\n",
              "   1012,\n",
              "   2096,\n",
              "   2026,\n",
              "   2406,\n",
              "   3549,\n",
              "   2568,\n",
              "   2424,\n",
              "   2009,\n",
              "   16880,\n",
              "   1010,\n",
              "   1999,\n",
              "   4507,\n",
              "   3348,\n",
              "   1998,\n",
              "   16371,\n",
              "   25469,\n",
              "   2024,\n",
              "   1037,\n",
              "   2350,\n",
              "   18785,\n",
              "   1999,\n",
              "   4467,\n",
              "   5988,\n",
              "   1012,\n",
              "   2130,\n",
              "   13749,\n",
              "   7849,\n",
              "   24544,\n",
              "   1010,\n",
              "   15835,\n",
              "   2037,\n",
              "   3437,\n",
              "   2000,\n",
              "   2204,\n",
              "   2214,\n",
              "   2879,\n",
              "   2198,\n",
              "   4811,\n",
              "   1010,\n",
              "   2018,\n",
              "   3348,\n",
              "   5019,\n",
              "   1999,\n",
              "   2010,\n",
              "   3152,\n",
              "   1012,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   1026,\n",
              "   7987,\n",
              "   1013,\n",
              "   1028,\n",
              "   1045,\n",
              "   2079,\n",
              "   4012,\n",
              "   3549,\n",
              "   2094,\n",
              "   1996,\n",
              "   16587,\n",
              "   2005,\n",
              "   1996,\n",
              "   2755,\n",
              "   2008,\n",
              "   2151,\n",
              "   3348,\n",
              "   3491,\n",
              "   1999,\n",
              "   1996,\n",
              "   2143,\n",
              "   2003,\n",
              "   3491,\n",
              "   2005,\n",
              "   6018,\n",
              "   5682,\n",
              "   2738,\n",
              "   2084,\n",
              "   2074,\n",
              "   2000,\n",
              "   5213,\n",
              "   2111,\n",
              "   1998,\n",
              "   2191,\n",
              "   2769,\n",
              "   2000,\n",
              "   2022,\n",
              "   3491,\n",
              "   1999,\n",
              "   26932,\n",
              "   12370,\n",
              "   1999,\n",
              "   2637,\n",
              "   1012,\n",
              "   1045,\n",
              "   2572,\n",
              "   8025,\n",
              "   1011,\n",
              "   3756,\n",
              "   2003,\n",
              "   1037,\n",
              "   2204,\n",
              "   2143,\n",
              "   2005,\n",
              "   3087,\n",
              "   5782,\n",
              "   2000,\n",
              "   2817,\n",
              "   1996,\n",
              "   6240,\n",
              "   1998,\n",
              "   14629,\n",
              "   1006,\n",
              "   2053,\n",
              "   26136,\n",
              "   3832,\n",
              "   1007,\n",
              "   1997,\n",
              "   4467,\n",
              "   5988,\n",
              "   1012,\n",
              "   2021,\n",
              "   2428,\n",
              "   1010,\n",
              "   2023,\n",
              "   2143,\n",
              "   2987,\n",
              "   1005,\n",
              "   1056,\n",
              "   2031,\n",
              "   2172,\n",
              "   1997,\n",
              "   1037,\n",
              "   5436,\n",
              "   1012,\n",
              "   102],\n",
              "  'attention_mask': [1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1,\n",
              "   1]})"
            ]
          }
        }
      ],
      "source": [
        "imdb['train'][0], tokenized_imdb['train'][0]"
      ],
      "id": "cell-41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "뭐거 더 추가된 형태"
      ],
      "id": "3f4f05db-08f7-4e3a-be18-91ee082be319"
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(preprocess_function(imdb['train'][i]))"
      ],
      "id": "cell-43"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102]\n",
            "[101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102]\n",
            "[101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]\n",
            "[101, 2023, 2143, 2001, 2763, 4427, 2011, 2643, 4232, 1005, 1055, 16137, 10841, 4115, 1010, 10768, 25300, 2078, 1998, 1045, 9075, 2017, 2000, 2156, 2008, 2143, 2612, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2143, 2038, 2048, 2844, 3787, 1998, 2216, 2024, 1010, 1006, 1015, 1007, 1996, 12689, 3772, 1006, 1016, 1007, 1996, 8052, 1010, 6151, 6810, 2099, 7178, 2135, 2204, 1010, 6302, 1012, 4237, 2013, 2008, 1010, 2054, 9326, 2033, 2087, 2003, 1996, 10866, 5460, 1997, 9033, 21202, 7971, 1012, 14229, 6396, 2386, 2038, 2000, 2022, 2087, 15703, 3883, 1999, 1996, 2088, 1012, 2016, 4490, 2061, 5236, 1998, 2007, 2035, 1996, 16371, 25469, 1999, 2023, 2143, 1010, 1012, 1012, 1012, 2009, 1005, 1055, 14477, 4779, 26884, 1012, 13599, 2000, 2643, 4232, 1005, 1055, 2143, 1010, 7789, 3012, 2038, 2042, 2999, 2007, 28072, 1012, 2302, 2183, 2205, 2521, 2006, 2023, 3395, 1010, 1045, 2052, 2360, 2008, 4076, 2013, 1996, 4489, 1999, 15084, 2090, 1996, 2413, 1998, 1996, 4467, 2554, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1037, 3185, 1997, 2049, 2051, 1010, 1998, 2173, 1012, 1016, 1013, 2184, 1012, 102]\n",
            "[101, 2821, 1010, 2567, 1012, 1012, 1012, 2044, 4994, 2055, 2023, 9951, 2143, 2005, 8529, 13876, 12129, 2086, 2035, 1045, 2064, 2228, 1997, 2003, 2008, 2214, 14911, 3389, 2299, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 2003, 2008, 2035, 2045, 2003, 1029, 1029, 1000, 1012, 1012, 1012, 1045, 2001, 2074, 2019, 2220, 9458, 2043, 2023, 20482, 3869, 2718, 1996, 1057, 1012, 1055, 1012, 1045, 2001, 2205, 2402, 2000, 2131, 1999, 1996, 4258, 1006, 2348, 1045, 2106, 6133, 2000, 13583, 2046, 1000, 9119, 8912, 1000, 1007, 1012, 2059, 1037, 11326, 2012, 1037, 2334, 2143, 2688, 10272, 17799, 1011, 2633, 1045, 2071, 2156, 2023, 2143, 1010, 3272, 2085, 1045, 2001, 2004, 2214, 2004, 2026, 3008, 2020, 2043, 2027, 8040, 7317, 13699, 5669, 2000, 2156, 2009, 999, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 3114, 2023, 2143, 2001, 2025, 10033, 2000, 1996, 10812, 13457, 1997, 2051, 2001, 2138, 1997, 1996, 27885, 11020, 20693, 2553, 13977, 2011, 2049, 1057, 1012, 1055, 1012, 2713, 1012, 8817, 1997, 2111, 19311, 2098, 2000, 2023, 27136, 2121, 1010, 3241, 2027, 2020, 2183, 2000, 2156, 1037, 3348, 2143, 1012, 1012, 1012, 2612, 1010, 2027, 2288, 7167, 1997, 2485, 22264, 1997, 1043, 11802, 2135, 1010, 16360, 23004, 25430, 18352, 1010, 2006, 1011, 2395, 7636, 1999, 20857, 6023, 25943, 1010, 2004, 5498, 2063, 2576, 3653, 29048, 1012, 1012, 1012, 1998, 7408, 3468, 2040, 1011, 14977, 23599, 3348, 5019, 2007, 7842, 22772, 1010, 5122, 5889, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 3451, 12696, 1010, 4151, 24665, 12502, 1010, 3181, 20785, 1012, 1012, 3649, 2023, 2518, 2001, 1010, 14021, 5596, 2009, 1010, 6402, 2009, 1010, 2059, 4933, 1996, 11289, 1999, 1037, 2599, 3482, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7069, 9765, 27065, 2229, 2145, 26988, 2000, 2424, 3643, 1999, 2049, 11771, 18404, 6208, 2576, 11867, 7974, 8613, 1012, 1012, 2021, 2065, 2009, 4694, 1005, 1056, 2005, 1996, 15657, 9446, 1010, 2009, 2052, 2031, 2042, 6439, 1010, 2059, 6404, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2612, 1010, 1996, 1000, 1045, 2572, 8744, 1010, 8744, 1000, 1054, 10536, 16921, 7583, 2516, 2001, 5567, 10866, 2135, 2005, 2086, 2004, 1037, 14841, 26065, 3508, 2005, 22555, 2080, 3152, 1006, 1045, 2572, 8025, 1010, 20920, 1011, 2005, 5637, 3152, 1010, 1045, 2572, 8025, 1010, 2304, 1011, 2005, 1038, 2721, 2595, 24759, 28100, 3370, 3152, 1010, 4385, 1012, 1012, 1007, 1998, 2296, 2702, 2086, 2030, 2061, 1996, 2518, 9466, 2013, 1996, 2757, 1010, 2000, 2022, 7021, 2011, 1037, 2047, 4245, 1997, 26476, 2015, 2040, 2215, 2000, 2156, 2008, 1000, 20355, 3348, 2143, 1000, 2008, 1000, 4329, 3550, 1996, 2143, 3068, 1000, 1012, 1012, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 6300, 9953, 1010, 4468, 2066, 1996, 11629, 1012, 1012, 2030, 2065, 2017, 2442, 2156, 2009, 1011, 9278, 1996, 2678, 1998, 3435, 2830, 2000, 1996, 1000, 6530, 1000, 3033, 1010, 2074, 2000, 2131, 2009, 2058, 2007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]"
          ]
        }
      ],
      "source": [
        "print(tokenized_imdb['train'][0]['input_ids'])\n",
        "print(tokenized_imdb['train'][1]['input_ids'])\n",
        "print(tokenized_imdb['train'][2]['input_ids'])\n",
        "print(tokenized_imdb['train'][3]['input_ids'])\n",
        "print(tokenized_imdb['train'][4]['input_ids'])"
      ],
      "id": "cell-44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   `imdb.map(preprocess_function, batched=True)`는 구조상\n",
        "    `map(imdb,preprocess_function,batched=True)`와 같을 듯\n",
        "-   map은 리스트 비슷한 것의 각 원소에 함수를 적용하는 꼴이었음.\n",
        "-   `map`의 사용법을 통하여 유추하면, `imdb.map`은 `imdb[0],imdb[1],...`\n",
        "    와 같은 원소들에 함수 preprocess 를 처리하는 형태의 문법으로 유추할\n",
        "    수 있다. (유추 불가능하다고 절망할 필요 없음.)\n",
        "\n",
        "## B. Step2 – 인공지능 생성\n",
        "\n",
        "`-` 모델생성하기"
      ],
      "id": "f7994b87-b3ef-40fd-b2c3-9475a9132e65"
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\"\n",
        ")"
      ],
      "id": "cell-48"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "뭐한거지??\n",
        "\n",
        "-   `AutoModelForSequenceClassification.from_pretrained` 는 인공지능을\n",
        "    만드는 함수임\n",
        "-   인공지능은 보통 웹에 저장되어있음.\n",
        "-   “distilbert/distilbert-base-uncased”는 허깅페이스에서 어떠한 모델이\n",
        "    저장된 경로를 의미함."
      ],
      "id": "2faaa608-5ce7-441b-8a40-53a7e0bfae5a"
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          }
        }
      ],
      "source": [
        "model"
      ],
      "id": "cell-51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   파이토치를 많이 다뤄봤다면 이거보고 뭔지 알수도 있음.\n",
        "-   아니라면 일단 넘어가자.\n",
        "\n",
        "`-` 모델을 사용하는 방법 (고급편) // 유추불가능한 내용"
      ],
      "id": "dbcd4e58-c4f6-450f-9e88-6f333ffe2c00"
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [],
      "source": [
        "text0 = \"This movie was a huge disappointment.\"\n",
        "text1 = \"This was a masterpiece.\""
      ],
      "id": "cell-54"
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2023,  2001,  1037, 17743,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
            ]
          }
        }
      ],
      "source": [
        "model_input = tokenizer(text1, return_tensors='pt', truncation=True)\n",
        "model_input"
      ],
      "id": "cell-55"
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[-0.1256,  0.0509]], grad_fn=<AddmmBackward0>)"
            ]
          }
        }
      ],
      "source": [
        "model.eval() # 문제풀이모드로 전환\n",
        "model(**model_input).logits"
      ],
      "id": "cell-56"
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[0.4560, 0.5440]], grad_fn=<SoftmaxBackward0>)"
            ]
          }
        }
      ],
      "source": [
        "torch.softmax(model(**model_input).logits,dim=1) # 0일확률, 1일확률"
      ],
      "id": "cell-57"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 왜 잘 맞춤?\n",
        "\n",
        "-   이미 잘맞는애를 데려옴\n",
        "\n",
        "`-` 초기화하고 싶다면..?"
      ],
      "id": "93bf7da4-f185-4243-9b53-a59bed925d8f"
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.classifier = torch.nn.Linear(768,2)"
      ],
      "id": "cell-61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   끝의 가중치를 날려서 멍청이로 만듦"
      ],
      "id": "848b9280-4888-42f9-826f-a02f749786eb"
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2023,  3185,  2001,  1037,  4121, 10520,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          }
        }
      ],
      "source": [
        "model_input = tokenizer(text0, return_tensors='pt', truncation=True)\n",
        "model_input"
      ],
      "id": "cell-63"
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[-0.0302,  0.0913]], grad_fn=<AddmmBackward0>)"
            ]
          }
        }
      ],
      "source": [
        "model.eval()\n",
        "model(**model_input).logits"
      ],
      "id": "cell-64"
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[0.4697, 0.5303]], grad_fn=<SoftmaxBackward0>)"
            ]
          }
        }
      ],
      "source": [
        "torch.softmax(model(**model_input).logits,dim=1) # 0일확률, 1일확률"
      ],
      "id": "cell-65"
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          }
        }
      ],
      "source": [
        "model.train() # 학습모드로전환 "
      ],
      "id": "cell-66"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. Step3 – 학습하기\n",
        "\n",
        "`-` 트레이너 생성을 위한 재료"
      ],
      "id": "0c60e85f-e63a-48dc-a50c-7c93e224db25"
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "id": "cell-69"
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "id": "cell-70"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 트레이너 생성 + 학습"
      ],
      "id": "2081aad6-7e5a-4795-9696-e6e35b4c6e81"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<p>"
            ]
          }
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4689, training_loss=0.16536911699812049, metrics={'train_runtime': 1226.4332, 'train_samples_per_second': 61.153, 'train_steps_per_second': 3.823, 'total_flos': 9834539051060448.0, 'train_loss': 0.16536911699812049, 'epoch': 3.0})"
            ]
          }
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "cell-72"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. Step4 – 예측하기\n",
        "\n",
        "`-` 강인공지능 생성"
      ],
      "id": "4998caef-6dae-4e7b-aea3-9339ad5fe252"
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU."
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\")\n"
      ],
      "id": "cell-75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 강인공지능을 통한 예측"
      ],
      "id": "b31633f6-adaf-45fb-bcac-2d4a4e2c2a2a"
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.991926908493042}]"
            ]
          }
        }
      ],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
        "classifier(text)"
      ],
      "id": "cell-77"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. 코드정리2\n",
        "\n",
        "`-` 패키지"
      ],
      "id": "845f5cc3-2497-4d2d-aa36-a80ba8301e09"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "id": "cell-80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` Step1~4 준비하기"
      ],
      "id": "78d6f039-66c9-4ab3-a78a-35c3387b2822"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn("
          ]
        }
      ],
      "source": [
        "## Step1 \n",
        "데이터불러오기 = datasets.load_dataset\n",
        "데이터전처리하기1 = 토크나이저 = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") # str -> {input_ids:, attention_mask:}\n",
        "def 데이터전처리하기2(examples): # imdb['train'] ->  {input_ids:, attention_mask:}\n",
        "    return 데이터전처리하기1(examples[\"text\"], truncation=True)\n",
        "## Step2\n",
        "인공지능생성하기 = transformers.AutoModelForSequenceClassification.from_pretrained\n",
        "## Step3 \n",
        "트레이너세부지침생성기 = transformers.TrainingArguments\n",
        "트레이너생성기 = transformers.Trainer\n",
        "데이터콜렉터 = transformers.DataCollatorWithPadding(tokenizer=토크나이저) ## ???? \n",
        "accuracy계산도구모음 = evaluate.load(\"accuracy\")\n",
        "def 평가하기(eval_pred):\n",
        "    예측확률, 실제정답 = eval_pred\n",
        "    예측결과 = np.argmax(예측확률, axis=1)\n",
        "    return accuracy계산도구모음.compute(predictions=예측결과, references=실제정답)     \n",
        "## Step4 \n",
        "강인공지능생성하기 = transformers.pipeline "
      ],
      "id": "cell-82"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` Step1~4"
      ],
      "id": "bf4a7aef-35b5-48eb-82e4-3e0e7b7d5967"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference."
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "<p>"
            ]
          }
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1563, training_loss=0.2563062113824748, metrics={'train_runtime': 412.5941, 'train_samples_per_second': 60.592, 'train_steps_per_second': 3.788, 'total_flos': 3280166004732288.0, 'train_loss': 0.2563062113824748, 'epoch': 1.0})"
            ]
          }
        }
      ],
      "source": [
        "## Step1 \n",
        "데이터 = 데이터불러오기(\"imdb\")\n",
        "전처리된데이터 = 데이터.map(데이터전처리하기2, batched=True)\n",
        "훈련자료, 테스트자료 = 전처리된데이터['train'], 전처리된데이터['test']\n",
        "## Step2 \n",
        "인공지능 = 인공지능생성하기(\"distilbert/distilbert-base-uncased\")\n",
        "## Step3 \n",
        "트레이너_세부지침 = 트레이너세부지침생성기(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "트레이너 = 트레이너생성기( \n",
        "    model=인공지능,\n",
        "    args=트레이너_세부지침,\n",
        "    train_dataset=훈련자료,\n",
        "    eval_dataset=테스트자료,\n",
        "    tokenizer=토크나이저,\n",
        "    data_collator=데이터콜렉터,\n",
        "    compute_metrics=평가하기,\n",
        ")\n",
        "트레이너.train()"
      ],
      "id": "cell-84"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU."
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "[{'label': 'LABEL_1', 'score': 0.991926908493042}]"
            ]
          }
        }
      ],
      "source": [
        "## Step4 \n",
        "강인공지능 = 강인공지능생성하기(\"sentiment-analysis\", model=\"my_awesome_model/checkpoint-1563\") \n",
        "돌발질문 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
        "강인공지능(돌발질문)"
      ],
      "id": "cell-85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. 토크나이저"
      ],
      "id": "0d743c06-f654-49b0-aa0f-6788caa1ad89"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "토크나이저(\"Hugging Face is creating a tool that democratizes AI.\")"
      ],
      "id": "cell-87"
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'input_ids': [101, 17662, 2227, 2003, 4526, 1037, 6994, 2008, 7672, 10057, 9932, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          }
        }
      ],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Hugging Face is creating a tool that democratizes AI.\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Hugging Hugging Hugging\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"Face Face Face face face face\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-90"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 텍스트를 인풋 형태로 변환한 후, 인덱스 추출\n",
        "text = \"is is is\"\n",
        "inputs = tokenizer(text,truncation=True)\n",
        "inputs"
      ],
      "id": "cell-91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set(dir(tokenizer)) & {'__call__'}"
      ],
      "id": "cell-92"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ],
      "id": "cell-93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. 데이터콜렉터의 역할\n",
        "\n",
        "# 8. 인공지능 저장 & 불러오기\n",
        "\n",
        "# 9. 인공지능 $\\to$ 강인공지능\n",
        "\n",
        "# 10. 훈련자료,테스트자료 채점\n",
        "\n",
        "# 11. 더 적은 자료로 훈련"
      ],
      "id": "07397af8-299c-4a03-a7c8-e7095bb0517e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# "
      ],
      "id": "cell-100"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "id": "cell-101"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator"
      ],
      "id": "cell-102"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "id": "cell-103"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "id": "cell-104"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "id": "cell-105"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "id": "cell-106"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "cell-107"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
      ],
      "id": "cell-108"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n",
        "classifier(text)"
      ],
      "id": "cell-109"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ],
      "id": "cell-110"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits"
      ],
      "id": "cell-111"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "id": "cell-112"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "hf",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  }
}