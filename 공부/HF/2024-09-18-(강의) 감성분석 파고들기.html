<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.533">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="신록예찬">
<meta name="dcterms.date" content="2024-09-18">

<title>신록예찬’s Blog - (강의) 감성분석 파고들기</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">신록예찬’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/DL2024/"> 
<span class="menu-text">DL2024</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/AP2023/"> 
<span class="menu-text">AP2023</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/PP2024/"> 
<span class="menu-text">PP2024</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/DV2023/"> 
<span class="menu-text">DV2023</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/MP2023/"> 
<span class="menu-text">MP2023</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/SC2024/"> 
<span class="menu-text">SC2024</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/DS2022/"> 
<span class="menu-text">DS2022</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/SP2023/"> 
<span class="menu-text">SP2023</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://guebin.github.io/IR2021/"> 
<span class="menu-text">IR2021</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://miruetoto.github.io/yechan/"> 
<span class="menu-text">yechan</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://miruetoto.github.io/yechan2/"> 
<span class="menu-text">yechan2</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/miruetoto/yechan3"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">(강의) 감성분석 파고들기</li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../메모.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">메모</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../공부.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">공부</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../연구.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">연구</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../자료.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">자료</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#imports" id="toc-imports" class="nav-link active" data-scroll-target="#imports">1. Imports</a></li>
  <li><a href="#데이터셋" id="toc-데이터셋" class="nav-link" data-scroll-target="#데이터셋">2. 데이터셋</a></li>
  <li><a href="#토크나이저" id="toc-토크나이저" class="nav-link" data-scroll-target="#토크나이저">3. 토크나이저</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="2024-09-18-(강의) 감성분석 파고들기.out.ipynb" download="2024-09-18-(강의) 감성분석 파고들기.out.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">(강의) 감성분석 파고들기</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>신록예찬 </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 18, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="imports" class="level1">
<h1>1. Imports</h1>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> datasets</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> transformers</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> evaluate</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
</div>
</section>
<section id="데이터셋" class="level1">
<h1>2. 데이터셋</h1>
<div id="cell-4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co">## Step1 </span></span>
<span id="cb3-2"><a href="#cb3-2"></a>데이터불러오기 <span class="op">=</span> datasets.load_dataset</span>
<span id="cb3-3"><a href="#cb3-3"></a>데이터전처리하기<span class="dv">1</span> <span class="op">=</span> 토크나이저 <span class="op">=</span> transformers.AutoTokenizer.from_pretrained(<span class="st">"distilbert/distilbert-base-uncased"</span>) </span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="kw">def</span> 데이터전처리하기<span class="dv">2</span>(examples):</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="cf">return</span> 데이터전처리하기<span class="dv">1</span>(examples[<span class="st">"text"</span>], truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">## Step2 </span></span>
<span id="cb3-7"><a href="#cb3-7"></a>인공지능생성하기 <span class="op">=</span> transformers.AutoModelForSequenceClassification.from_pretrained</span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">## Step3 </span></span>
<span id="cb3-9"><a href="#cb3-9"></a>데이터콜렉터 <span class="op">=</span> transformers.DataCollatorWithPadding(tokenizer<span class="op">=</span>토크나이저)</span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="kw">def</span> 평가하기(eval_pred):</span>
<span id="cb3-11"><a href="#cb3-11"></a>    predictions, labels <span class="op">=</span> eval_pred</span>
<span id="cb3-12"><a href="#cb3-12"></a>    predictions <span class="op">=</span> np.argmax(predictions, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-13"><a href="#cb3-13"></a>    accuracy <span class="op">=</span> evaluate.load(<span class="st">"accuracy"</span>)</span>
<span id="cb3-14"><a href="#cb3-14"></a>    <span class="cf">return</span> accuracy.compute(predictions<span class="op">=</span>predictions, references<span class="op">=</span>labels)</span>
<span id="cb3-15"><a href="#cb3-15"></a>트레이너세부지침생성기 <span class="op">=</span> transformers.TrainingArguments</span>
<span id="cb3-16"><a href="#cb3-16"></a>트레이너생성기 <span class="op">=</span> transformers.Trainer</span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co">## Step4 </span></span>
<span id="cb3-18"><a href="#cb3-18"></a>강인공지능생성하기 <span class="op">=</span> transformers.pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/cgb3/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(</code></pre>
</div>
</div>
<div id="cell-5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># ## Step1 </span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># 데이터 = 데이터불러오기('imdb')</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># 전처리된데이터 = 데이터.map(데이터전처리하기2,batched=True)</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co"># 전처리된훈련자료, 전처리된검증자료 = 전처리된데이터['train'], 전처리된데이터['test']</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># ## Step2 </span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="co"># 인공지능 = 인공지능생성하기("distilbert/distilbert-base-uncased", num_labels=2)</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co"># ## Step3 </span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co"># 트레이너세부지침 = 트레이너세부지침생성기(</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">#     output_dir="my_awesome_model",</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">#     learning_rate=2e-5,</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co">#     per_device_train_batch_size=16,</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">#     per_device_eval_batch_size=16,</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co">#     num_train_epochs=2, # 전체문제세트를 2번 공부하라..</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co">#     weight_decay=0.01,</span></span>
<span id="cb5-15"><a href="#cb5-15"></a><span class="co">#     eval_strategy="epoch",</span></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co">#     save_strategy="epoch",</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">#     load_best_model_at_end=True,</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">#     push_to_hub=False,</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co"># )</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="co"># 트레이너 = 트레이너생성기(</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co">#     model=인공지능,</span></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="co">#     args=트레이너세부지침,</span></span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="co">#     train_dataset=전처리된훈련자료,</span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="co">#     eval_dataset=전처리된검증자료,</span></span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="co">#     tokenizer=토크나이저,</span></span>
<span id="cb5-26"><a href="#cb5-26"></a><span class="co">#     data_collator=데이터콜렉터,</span></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co">#     compute_metrics=평가하기,</span></span>
<span id="cb5-28"><a href="#cb5-28"></a><span class="co"># )</span></span>
<span id="cb5-29"><a href="#cb5-29"></a><span class="co"># 트레이너.train()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># ## Step4 </span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="co"># 강인공지능 = 강인공지능생성하기("sentiment-analysis", model="my_awesome_model")</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># print(강인공지능(text0))</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># print(강인공지능(text1))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="토크나이저" class="level1">
<h1>3. 토크나이저</h1>
<p><code>-</code> 기본적인 사용방법</p>
<div id="cell-9" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>토크나이저?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Signature:</span>     
토크나이저<span class="ansi-blue-fg">(</span>
    text<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    text_pair<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> NoneType<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    text_target<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    text_pair_target<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> List<span class="ansi-blue-fg">[</span>List<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">,</span> NoneType<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    add_special_tokens<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    padding<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>bool<span class="ansi-blue-fg">,</span> str<span class="ansi-blue-fg">,</span> transformers<span class="ansi-blue-fg">.</span>utils<span class="ansi-blue-fg">.</span>generic<span class="ansi-blue-fg">.</span>PaddingStrategy<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    truncation<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>bool<span class="ansi-blue-fg">,</span> str<span class="ansi-blue-fg">,</span> transformers<span class="ansi-blue-fg">.</span>tokenization_utils_base<span class="ansi-blue-fg">.</span>TruncationStrategy<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    max_length<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>int<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    stride<span class="ansi-blue-fg">:</span> int <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span>
    is_split_into_words<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    pad_to_multiple_of<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>int<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    return_tensors<span class="ansi-blue-fg">:</span> Union<span class="ansi-blue-fg">[</span>str<span class="ansi-blue-fg">,</span> transformers<span class="ansi-blue-fg">.</span>utils<span class="ansi-blue-fg">.</span>generic<span class="ansi-blue-fg">.</span>TensorType<span class="ansi-blue-fg">,</span> NoneType<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    return_token_type_ids<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>bool<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    return_attention_mask<span class="ansi-blue-fg">:</span> Optional<span class="ansi-blue-fg">[</span>bool<span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">,</span>
    return_overflowing_tokens<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    return_special_tokens_mask<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    return_offsets_mapping<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    return_length<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span><span class="ansi-blue-fg">,</span>
    verbose<span class="ansi-blue-fg">:</span> bool <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span>
    <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">,</span>
<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-&gt;</span> transformers<span class="ansi-blue-fg">.</span>tokenization_utils_base<span class="ansi-blue-fg">.</span>BatchEncoding
<span class="ansi-red-fg">Type:</span>           DistilBertTokenizerFast
<span class="ansi-red-fg">String form:</span>   
DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, mode &lt;...&gt; Token("[MASK]", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
           }
<span class="ansi-red-fg">Length:</span>         30522
<span class="ansi-red-fg">File:</span>           ~/anaconda3/envs/hf/lib/python3.12/site-packages/transformers/models/distilbert/tokenization_distilbert_fast.py
<span class="ansi-red-fg">Docstring:</span>     
Construct a "fast" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.
This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.
Args:
    vocab_file (`str`):
        File containing the vocabulary.
    do_lower_case (`bool`, *optional*, defaults to `True`):
        Whether or not to lowercase the input when tokenizing.
    unk_token (`str`, *optional*, defaults to `"[UNK]"`):
        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
        token instead.
    sep_token (`str`, *optional*, defaults to `"[SEP]"`):
        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
        sequence classification or for a text and a question for question answering. It is also used as the last
        token of a sequence built with special tokens.
    pad_token (`str`, *optional*, defaults to `"[PAD]"`):
        The token used for padding, for example when batching sequences of different lengths.
    cls_token (`str`, *optional*, defaults to `"[CLS]"`):
        The classifier token which is used when doing sequence classification (classification of the whole sequence
        instead of per-token classification). It is the first token of the sequence when built with special tokens.
    mask_token (`str`, *optional*, defaults to `"[MASK]"`):
        The token used for masking values. This is the token used when training this model with masked language
        modeling. This is the token which the model will try to predict.
    clean_text (`bool`, *optional*, defaults to `True`):
        Whether or not to clean the text before tokenization by removing any control characters and replacing all
        whitespaces by the classic one.
    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):
        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this
        issue](https://github.com/huggingface/transformers/issues/328)).
    strip_accents (`bool`, *optional*):
        Whether or not to strip all accents. If this option is not specified, then it will be determined by the
        value for `lowercase` (as in the original BERT).
    wordpieces_prefix (`str`, *optional*, defaults to `"##"`):
        The prefix for subwords.
<span class="ansi-red-fg">Call docstring:</span>
Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
sequences.
Args:
    text (`str`, `List[str]`, `List[List[str]]`, *optional*):
        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).
    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):
        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).
    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):
        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).
    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):
        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a
        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),
        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).
    add_special_tokens (`bool`, *optional*, defaults to `True`):
        Whether or not to add special tokens when encoding the sequences. This will use the underlying
        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are
        automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens
        automatically.
    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):
        Activates and controls padding. Accepts the following values:
        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
          sequence if provided).
        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
          acceptable input length for the model if that argument is not provided.
        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different
          lengths).
    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):
        Activates and controls truncation. Accepts the following values:
        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or
          to the maximum acceptable input length for the model if that argument is not provided. This will
          truncate token by token, removing a token from the longest sequence in the pair if a pair of
          sequences (or a batch of pairs) is provided.
        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the
          maximum acceptable input length for the model if that argument is not provided. This will only
          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the
          maximum acceptable input length for the model if that argument is not provided. This will only
          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.
        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths
          greater than the model maximum admissible input size).
    max_length (`int`, *optional*):
        Controls the maximum length to use by one of the truncation/padding parameters.
        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length
        is required by one of the truncation/padding parameters. If the model has no specific maximum input
        length (like XLNet) truncation/padding to a maximum length will be deactivated.
    stride (`int`, *optional*, defaults to 0):
        If set to a number along with `max_length`, the overflowing tokens returned when
        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence
        returned to provide some overlap between truncated and overflowing sequences. The value of this
        argument defines the number of overlapping tokens.
    is_split_into_words (`bool`, *optional*, defaults to `False`):
        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the
        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
        which it will tokenize. This is useful for NER or token classification.
    pad_to_multiple_of (`int`, *optional*):
        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.
        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
        `&gt;= 7.5` (Volta).
    return_tensors (`str` or [`~utils.TensorType`], *optional*):
        If set, will return tensors instead of list of python integers. Acceptable values are:
        - `'tf'`: Return TensorFlow `tf.constant` objects.
        - `'pt'`: Return PyTorch `torch.Tensor` objects.
        - `'np'`: Return Numpy `np.ndarray` objects.
    return_token_type_ids (`bool`, *optional*):
        Whether to return token type IDs. If left to the default, will return the token type IDs according to
        the specific tokenizer's default, defined by the `return_outputs` attribute.
        [What are token type IDs?](../glossary#token-type-ids)
    return_attention_mask (`bool`, *optional*):
        Whether to return the attention mask. If left to the default, will return the attention mask according
        to the specific tokenizer's default, defined by the `return_outputs` attribute.
        [What are attention masks?](../glossary#attention-mask)
    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):
        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch
        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead
        of returning overflowing tokens.
    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):
        Whether or not to return special tokens mask information.
    return_offsets_mapping (`bool`, *optional*, defaults to `False`):
        Whether or not to return `(char_start, char_end)` for each token.
        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using
        Python's tokenizer, this method will raise `NotImplementedError`.
    return_length  (`bool`, *optional*, defaults to `False`):
        Whether or not to return the lengths of the encoded inputs.
    verbose (`bool`, *optional*, defaults to `True`):
        Whether or not to print more information and warnings.
    **kwargs: passed to the `self.tokenize()` method
Return:
    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:
    - **input_ids** -- List of token ids to be fed to a model.
      [What are input IDs?](../glossary#input-ids)
    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or
      if *"token_type_ids"* is in `self.model_input_names`).
      [What are token type IDs?](../glossary#token-type-ids)
    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when
      `return_attention_mask=True` or if *"attention_mask"* is in `self.model_input_names`).
      [What are attention masks?](../glossary#attention-mask)
    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and
      `return_overflowing_tokens=True`).
    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and
      `return_overflowing_tokens=True`).
    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying
      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).
    - **length** -- The length of the inputs (when `return_length=True`)</pre>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<code>토크나이저</code> 사용법 (ref: ChatGPT)
</div>
</div>
<div class="callout-body-container callout-body">
<section id="주요-파라미터" class="level4">
<h4 class="anchored" data-anchor-id="주요-파라미터">주요 파라미터:</h4>
<ol type="1">
<li><strong>text</strong>:
<ul>
<li><code>Union[str, List[str], List[List[str]]]</code></li>
<li>주어진 텍스트를 토큰화합니다. 이 텍스트는 문자열일 수도 있고, 문자열의 리스트 또는 리스트 안의 리스트일 수도 있습니다.</li>
</ul></li>
<li><strong>text_pair</strong>:
<ul>
<li><code>Union[str, List[str], List[List[str]], NoneType]</code></li>
<li>두 개의 텍스트를 함께 모델에 입력할 때 사용됩니다. 예를 들어, 질문-답변 쌍 같은 경우 이 두 번째 텍스트를 넣습니다.</li>
</ul></li>
<li><strong>text_target</strong>:
<ul>
<li><code>Union[str, List[str], List[List[str]]]</code></li>
<li>토큰화를 할 때 목표(target) 텍스트에 해당하는 부분입니다. 주로 시퀀스 생성 모델에서 활용됩니다.</li>
</ul></li>
<li><strong>text_pair_target</strong>:
<ul>
<li><code>Union[str, List[str], List[List[str]], NoneType]</code></li>
<li>위의 <code>text_pair</code>와 유사하게 목표(target) 텍스트의 두 번째 텍스트를 나타냅니다.</li>
</ul></li>
<li><strong>add_special_tokens</strong>:
<ul>
<li><code>bool</code></li>
<li>문장의 시작, 끝, 구분자 같은 특별한 토큰을 추가할지 여부를 결정합니다. 기본값은 <code>True</code>입니다.</li>
</ul></li>
<li><strong>padding</strong>:
<ul>
<li><code>Union[bool, str, transformers.utils.generic.PaddingStrategy]</code></li>
<li>문장 길이가 다를 때 패딩을 넣어 문장의 길이를 동일하게 맞춥니다. 패딩 전략에는 <code>True</code>, <code>False</code>, <code>'longest'</code>, <code>'max_length'</code> 등이 있습니다.</li>
</ul></li>
<li><strong>truncation</strong>:
<ul>
<li><code>Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy]</code></li>
<li>문장이 너무 길 경우 지정된 최대 길이에 맞춰 잘라내는 옵션입니다. 전략에는 <code>True</code>, <code>False</code>, <code>'longest_first'</code>, <code>'only_first'</code>, <code>'only_second'</code> 등이 있습니다.</li>
</ul></li>
<li><strong>max_length</strong>:
<ul>
<li><code>Optional[int]</code></li>
<li>문장의 최대 길이를 설정합니다. <code>None</code>일 경우 기본 설정을 따릅니다.</li>
</ul></li>
<li><strong>stride</strong>:
<ul>
<li><code>int</code></li>
<li>텍스트를 자를 때 중첩을 만들기 위한 옵션입니다. 즉, 자른 부분과 다음 부분 사이의 겹치는 범위를 설정합니다.</li>
</ul></li>
<li><strong>is_split_into_words</strong>:
<ul>
<li><code>bool</code></li>
<li>텍스트가 이미 단어 단위로 분리되어 있는지 여부를 나타냅니다. 기본적으로는 <code>False</code>로, 텍스트가 단어 단위로 분리되지 않았다고 가정합니다.</li>
</ul></li>
<li><strong>return_tensors</strong>:
<ul>
<li><code>Union[str, transformers.utils.generic.TensorType, NoneType]</code></li>
<li>출력 형식으로 텐서를 반환할지 여부를 설정합니다. <code>'pt'</code>(PyTorch), <code>'tf'</code>(TensorFlow), <code>'np'</code>(NumPy) 등을 지정할 수 있습니다.</li>
</ul></li>
<li><strong>return_token_type_ids</strong>:
<ul>
<li><code>Optional[bool]</code></li>
<li>토큰 타입 ID를 반환할지 여부를 설정합니다. 주로 두 개의 문장을 함께 처리할 때 문장을 구분하기 위해 사용됩니다.</li>
</ul></li>
<li><strong>return_attention_mask</strong>:
<ul>
<li><code>Optional[bool]</code></li>
<li><code>attention_mask</code>를 반환할지 여부를 설정합니다. 패딩된 토큰이 모델의 어텐션에 영향을 주지 않도록 마스크를 설정합니다.</li>
</ul></li>
<li><strong>return_overflowing_tokens</strong>:
<ul>
<li><code>bool</code></li>
<li>텍스트가 최대 길이를 초과하는 경우, 잘린 토큰을 반환할지 여부를 결정합니다.</li>
</ul></li>
<li><strong>return_special_tokens_mask</strong>:
<ul>
<li><code>bool</code></li>
<li>특별한 토큰에 대한 마스크를 반환할지 여부를 설정합니다.</li>
</ul></li>
<li><strong>return_offsets_mapping</strong>:
<ul>
<li><code>bool</code></li>
<li>텍스트의 각 토큰이 원본 텍스트에서 어느 위치에 있는지 나타내는 오프셋 맵핑을 반환할지 여부를 설정합니다.</li>
</ul></li>
<li><strong>return_length</strong>:
<ul>
<li><code>bool</code></li>
<li>토큰화된 문장의 길이를 반환할지 여부를 설정합니다.</li>
</ul></li>
<li><strong>verbose</strong>:
<ul>
<li><code>bool</code></li>
<li>디버깅 메시지를 출력할지 여부를 설정합니다. 기본값은 <code>True</code>로 설정되어 있습니다.</li>
</ul></li>
</ol>
</section>
<section id="사용-예시" class="level4">
<h4 class="anchored" data-anchor-id="사용-예시">사용 예시:</h4>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># 토크나이저 불러오기</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb8-5"><a href="#cb8-5"></a></span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="co"># 텍스트 토큰화</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>encoding <span class="op">=</span> tokenizer(</span>
<span id="cb8-8"><a href="#cb8-8"></a>    text<span class="op">=</span><span class="st">"Hello, how are you?"</span>,</span>
<span id="cb8-9"><a href="#cb8-9"></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-10"><a href="#cb8-10"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-11"><a href="#cb8-11"></a>    max_length<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb8-12"><a href="#cb8-12"></a>    return_tensors<span class="op">=</span><span class="st">'pt'</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>)</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="bu">print</span>(encoding)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>이 코드에서는 “Hello, how are you?”라는 텍스트를 <code>bert-base-uncased</code> 토크나이저로 토큰화하고, 패딩과 트렁케이션을 적용하며, PyTorch 텐서 형식으로 반환하도록 설정했습니다.</p>
<p>이러한 파라미터는 주로 자연어 처리(NLP) 모델을 훈련하거나 추론할 때 데이터 전처리 과정에서 많이 사용됩니다.</p>
</section>
</div>
</div>
<p><code>-</code> 기본사용1: 단어별로 다른숫자를 맵핑 + 처음과 끝은 항상 <code>101</code>, <code>102</code></p>
<div id="cell-12" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>토크나이저(<span class="st">"hi hello"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>{'input_ids': [101, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1]}</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>토크나이저(<span class="st">"hi hi hello"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>{'input_ids': [101, 7632, 7632, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1]}</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>토크나이저(<span class="st">"hi hi hello hello hello"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>{'input_ids': [101, 7632, 7632, 7592, 7592, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}</code></pre>
</div>
</div>
<div id="cell-15" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>토크나이저([<span class="st">"hi hello"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>{'input_ids': [[101, 7632, 7592, 102]], 'attention_mask': [[1, 1, 1, 1]]}</code></pre>
</div>
</div>
<p><code>-</code> 기본사용2: 텍스트 혹은 텍스트의 리스트, 리스트의 리스트를 전달가능</p>
<div id="cell-17" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>토크나이저([<span class="st">"hi hello"</span>, <span class="st">"hello hello"</span>, <span class="st">"hi hi"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>{'input_ids': [[101, 7632, 7592, 102], [101, 7592, 7592, 102], [101, 7632, 7632, 102]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}</code></pre>
</div>
</div>
<p><code>-</code> <code>truncation=True</code> 의 역할</p>
<div id="cell-19" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>dct <span class="op">=</span> 토크나이저(<span class="st">'hi hello'</span><span class="op">*</span><span class="dv">1000</span>,truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-2"><a href="#cb19-2"></a>dct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>{'input_ids': [101, 7632, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 4048, 7592, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</code></pre>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="bu">len</span>(dct[<span class="st">'input_ids'</span>]), <span class="bu">len</span>(dct[<span class="st">'attention_mask'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>(512, 512)</code></pre>
</div>
</div>
<p><code>-</code> maxlen, padding, attention_mask</p>
<div id="cell-22" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>dct <span class="op">=</span> 토크나이저(<span class="st">'hi hello'</span>,max_length<span class="op">=</span><span class="dv">10</span>,padding<span class="op">=</span><span class="st">"max_length"</span>)</span>
<span id="cb23-2"><a href="#cb23-2"></a>dct</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>{'input_ids': [101, 7632, 7592, 102, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}</code></pre>
</div>
</div>
<p><code>-</code> 고급내용: 토크나이저는 의미적으로 비슷한 단어를 비슷한 숫자들로 바꾸지 않는다.</p>
<div id="cell-24" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>positive_words <span class="op">=</span> [</span>
<span id="cb25-2"><a href="#cb25-2"></a>    <span class="st">"fun"</span>,</span>
<span id="cb25-3"><a href="#cb25-3"></a>    <span class="st">"heartwarming"</span>,</span>
<span id="cb25-4"><a href="#cb25-4"></a>    <span class="st">"exciting"</span>,</span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="st">"amazing"</span>,</span>
<span id="cb25-6"><a href="#cb25-6"></a>    <span class="st">"enjoyable"</span></span>
<span id="cb25-7"><a href="#cb25-7"></a>]</span>
<span id="cb25-8"><a href="#cb25-8"></a>negative_words <span class="op">=</span> [</span>
<span id="cb25-9"><a href="#cb25-9"></a>    <span class="st">"boring"</span>,</span>
<span id="cb25-10"><a href="#cb25-10"></a>    <span class="st">"dull"</span>,</span>
<span id="cb25-11"><a href="#cb25-11"></a>    <span class="st">"bad"</span>,</span>
<span id="cb25-12"><a href="#cb25-12"></a>    <span class="st">"slow"</span>,</span>
<span id="cb25-13"><a href="#cb25-13"></a>    <span class="st">"predictable"</span></span>
<span id="cb25-14"><a href="#cb25-14"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="miruetoto/yechan3" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->




</body></html>