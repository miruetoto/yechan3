{
 "cells": [
  {
   "cell_type": "raw",
   "id": "810cdbf2-2618-4386-a6a9-0e87ab8670ac",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"**[Essays]** 강화학습 v4\"\n",
    "author: \"신록예찬\"\n",
    "date: \"08/12/2023\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b9712-ddc7-4113-a953-ba9469d52545",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1236c41-f8e1-4f62-bb06-9d1a67c88159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch \n",
    "import collections\n",
    "import IPython\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8645b2-7a40-4e7e-b3e2-fa43eef0311f",
   "metadata": {},
   "source": [
    "# 기존알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07e06db1-b173-4c7d-ae9d-71a41b867b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 500*20   # replay buffer size\n",
    "BATCH_SIZE = 128     # minibatch size\n",
    "GAMMA = 0.99          # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76f462fe-d801-4ebc-8009-8f070b1e30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = 4\n",
    "        self.memory = collections.deque(maxlen=BUFFER_SIZE)  \n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.experience = collections.namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        \n",
    "        # Convert done from boolean to int\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c49a25ee-704b-4740-bebd-7094be91b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self,env):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = 8\n",
    "        self.action_size = 4\n",
    "        self.seed = 0 \n",
    "        self.eps = 1\n",
    "\n",
    "        # Q-Network\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8,128), # 8은 state_size\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,4)) # 4는 action_size \n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.0001)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer()\n",
    "    def step(self):\n",
    "        self.current_state = self.next_state \n",
    "        self.score += self.reward        \n",
    "    def act(self):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(self.current_state).float().unsqueeze(0)\n",
    "        action_values = self.net(state)            \n",
    "\n",
    "        if np.random.rand() > self.eps:\n",
    "            self.action = np.argmax(action_values.data.numpy())\n",
    "        else:\n",
    "            self.action = random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def __mul__(self,env): \n",
    "        self.next_state, self.reward, self.terminated, _, _ = env.step(self.action)\n",
    "        self.memory.add(self.current_state, self.action, self.reward, self.next_state, self.terminated)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ## Compute and minimize the loss\n",
    "        q_targets_next = self.net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        q_targets = rewards + GAMMA * q_targets_next * (1 - dones)\n",
    "\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.net(states).gather(1, actions)\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def reset(self,env): \n",
    "        self.current_state, _ = env.reset()\n",
    "        self.score = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c830a694-474a-4a1c-ae53-b7641a1d8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa5593c2-9b04-4395-80ba-01f7a2b06c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c0219-24e5-4248-b556-ec337c3e2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 98\tAverage Score: -137.34\tPlaytime: 106.65"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "max_t = 500\n",
    "eps_start = 1.0 \n",
    "eps_end = 0.05\n",
    "eps_decay = 0.995\n",
    "\n",
    "playtimes=[] \n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = collections.deque(maxlen=100)  # last 100 scores\n",
    "agent.eps = eps_start                    # initialize epsilon\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    agent.reset(env) \n",
    "    for t in range(max_t):\n",
    "        ## STEP1: \n",
    "        agent.act()\n",
    "        \n",
    "        ## STEP2: \n",
    "        agent * env \n",
    "        \n",
    "        ## STEP3: \n",
    "        if len(agent.memory) > BATCH_SIZE:\n",
    "            agent.learn(agent.memory.sample())\n",
    "        \n",
    "        agent.step()\n",
    "        \n",
    "        ## STEP4: \n",
    "        if agent.terminated:\n",
    "            playtimes.append(t)\n",
    "            break \n",
    "    scores_window.append(agent.score)       # save most recent score\n",
    "    scores.append(agent.score)              # save most recent score\n",
    "    agent.eps = max(eps_end, eps_decay*agent.eps) # decrease epsilon\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(playtimes)))\n",
    "        torch.save(agent.net.state_dict(), 'checkpoint.pth')\n",
    "    if np.mean(scores_window)>=200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        torch.save(agent.net.state_dict(), 'checkpoint.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947da12d-25bb-4344-93ba-74ae6128b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167a4fc-eacc-4d38-a02a-2eb4c16d1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')\n",
    "frames = []\n",
    "agent = Agent(env)\n",
    "agent.net.load_state_dict(torch.load('checkpoint.pth'))\n",
    "agent.reset(env)\n",
    "agent.terminated = False\n",
    "while not agent.terminated:\n",
    "    frames.append(env.render())\n",
    "    agent.act()\n",
    "    agent * env \n",
    "    agent.step()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da118e-a108-43ea-9b04-a2ecdb2f46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig,lambda i: ax.imshow(frames[::10][i]),frames=len(frames[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f680e-39b0-4a76-8d3e-4c98b861ce53",
   "metadata": {},
   "source": [
    "`-` 방법1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cea37-ee98-4f62-81d2-fd3269c9633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ani.save('LunarLander-v2.mp4', writer='ffmpeg', fps=15, extra_args=['-vcodec', 'mpeg4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bbe53-2eb9-40e0-884f-77c292541130",
   "metadata": {},
   "source": [
    "`-` 방법2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bb991-4268-43ae-bc96-724027a32fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefec185-46a2-4a32-9786-1058ea8d20fd",
   "metadata": {},
   "source": [
    "# 1 experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe92c7-3e2c-44a5-a2fb-d5ce9be4990e",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcc10ee-ab05-4c4b-bd9d-31e41589b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5d6c2f-62f6-4c18-9aa9-d74fe6cdec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8,128), # 8은 state_size\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,4)) # 4는 action_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e76d24-126d-488e-a707-2d6b362c343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizr = torch.optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbf8a81-d516-4467-8e30-9cae3335363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb956c7-4ab6-4b53-8b36-709ed5a56c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.1142959e-04,  1.4031194e+00, -2.1430280e-02, -3.4668925e-01,\n",
       "        2.5178038e-04,  4.8542689e-03,  0.0000000e+00,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dc91e3-4e52-4124-98c2-a4993fafc4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0187,  0.0370, -0.1520,  0.0079], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(current_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423b0a6a-e886-4a0d-b5ef-01750e1243f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_action1 = net(torch.tensor(current_state)).argmax().item()\n",
    "_action1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d70a6432-45b9-494c-96dc-e5234db34979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_action2 = random.choice([0,1,2,3])\n",
    "_action2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e7c24a-9c66-4b6b-b24b-d051873ba5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = lambda state, eps: net(torch.tensor(state)).argmax().item() if eps < random.random() else random.choice([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386818ad-6021-4508-84bc-b7818438f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 2, 2, 1, 2, 3, 1, 3, 2, 1, 0, 3, 0, 3, 3, 3, 0, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[act(current_state,0.95) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9bb022-5337-46ea-8c85-f01d2c307a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[act(current_state,0.1) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a054672-aa0f-4ca8-a6c6-fe897918eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = act(current_state,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb0d099b-42d3-4713-8818-8c5ca885a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ba2c3b-113e-48d4-81bc-b1b8aef78e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0186,  0.0368, -0.1524,  0.0075], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "830809de-1c5e-4cd7-a9dc-23738dc7c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0075750861699064"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998ceb02-ef70-4d41-a449-2022dde984ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036822687834501266"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvalue_next_state = net(torch.tensor(next_state)).data.max().item()\n",
    "qvalue_next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f14fe35-1a91-49a2-8349-cd613d499ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.97112062521375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward + 0.99 * qvalue_next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed4ff100-e0f4-40e9-8887-6f91effc7443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0370, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvalue_current_state = net(torch.tensor(current_state)).max()\n",
    "qvalue_current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea447fc-770c-42e9-8865-3124cfd6749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((qvalue_current_state - (reward + 0.99 * qvalue_next_state))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b848a267-d269-4928-9b1a-2a8eb16cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78423bd-f57e-487d-a68d-ee1a379d95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizr.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceba2626-3e67-4cde-af20-b5917ae17df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0182,  0.0349, -0.1537,  0.0066], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(current_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c5a42-4046-4fcb-8fa2-b0c02eeba0f7",
   "metadata": {},
   "source": [
    "## 코드정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f7397-79b4-4a6a-811f-aad72328999e",
   "metadata": {},
   "source": [
    "### env $\\to$ agent: current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a5db55-c445-43a9-be72-024ec6a3dbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.12180714e-03,  1.42049658e+00, -1.13641575e-01,  4.25618768e-01,\n",
       "        1.30666478e-03,  2.57414915e-02,  0.00000000e+00,  0.00000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_state, _ = env.reset()\n",
    "current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc687d1-12b2-431b-b128-23d7f94e45a6",
   "metadata": {},
   "source": [
    "### agent $\\to$ env: action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73b07571-97fb-4911-b4b5-205b78dc75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = lambda state, eps: net(torch.tensor(state)).argmax().item() if eps < random.random() else random.choice([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70030ed7-50d2-4dc7-b152-bd2977b25db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = act(current_state,0.05)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61d49f-b915-455e-8961-1a8c2bfe5c8b",
   "metadata": {},
   "source": [
    "### env $\\to$ agent: reward,next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c0248d-dce5-4f50-a1c9-73fe03e69b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb3989ad-9bd9-41c3-b38b-5653b946e77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0145,  0.0353, -0.1442,  0.0150], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4113ba48-7fa5-4f5a-848e-d78ba50f95bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0115680526115807"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b20818-f5d6-4a5b-8859-384bc6b83907",
   "metadata": {},
   "source": [
    "### agent: update ($q$-value generating) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8df6b467-6f4f-49c2-b5e3-fb2bcd96cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalue_current_state = net(torch.tensor(current_state))[action]\n",
    "qvalue_next_state = net(torch.tensor(next_state)).max().data\n",
    "reward2 = reward + 0.99 * qvalue_next_state \n",
    "loss = torch.mean((qvalue_current_state - reward2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ada1c12a-8e26-4b02-8324-58a61426b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8047c98-532f-41bf-8414-e9f27a712dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizr.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a395e-742b-41e7-9bc8-4ff0db9cc979",
   "metadata": {},
   "source": [
    "# 리플레이버퍼사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4691e27-3f5e-4fba-8b43-3fc7ca17ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state_history = collections.deque(maxlen=50000)\n",
    "action_history = collections.deque(maxlen=50000)\n",
    "next_state_history = collections.deque(maxlen=50000)\n",
    "terminated_history = collections.deque(maxlen=50000)\n",
    "reward_history = collections.deque(maxlen=50000)\n",
    "qvalue_current_state_history = collections.deque(maxlen=50000)\n",
    "qvalue_next_state_history = collections.deque(maxlen=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d41c64-0192-4bf5-9446-00d93231bec6",
   "metadata": {},
   "source": [
    "## episode=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e38f455-99d3-40de-aa55-301243f31c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### env -> agent: current_state\n",
    "current_state, _ = env.reset()\n",
    "current_state_history.append(current_state)\n",
    "current_state_history\n",
    "\n",
    "### agent -> env: action\n",
    "act = lambda state, eps: net(torch.tensor(state)).argmax().item() if eps < random.random() else random.choice([0,1,2,3])\n",
    "action = act(current_state,1)\n",
    "action_history.append(action)\n",
    "action_history\n",
    "\n",
    "### env -> agent: reward,next_state\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "next_state_history.append(next_state)\n",
    "reward_history.append(reward)\n",
    "terminated_history.append(terminated)\n",
    "\n",
    "### agent: update ($q$-value generating) network\n",
    "qvalue_current_state = net(torch.tensor(current_state))[action]\n",
    "qvalue_current_state_history.append(qvalue_current_state)\n",
    "qvalue_next_state = net(torch.tensor(next_state)).max().data\n",
    "qvalue_next_state_history.append(qvalue_next_state)\n",
    "#reward2 = reward + (0.99 * qvalue_next_state)*(~torch.tensor(terminated))\n",
    "#loss = torch.mean((qvalue_current_state - reward2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561d027-7948-4837-a829-92cf1c789ff5",
   "metadata": {},
   "source": [
    "## episode=2~100 정도까지 데이터를 쌓자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "512dbab6-2e25-4ff5-8980-b46537d668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "playtimes = [] \n",
    "scores = [] \n",
    "\n",
    "for e in range(100):\n",
    "    current_state, _ = env.reset()\n",
    "    eps = 1 \n",
    "    score = 0 \n",
    "    for t in range(500):        \n",
    "        current_state = next_state ## 수정\n",
    "        current_state_history.append(current_state)\n",
    "        \n",
    "        ### agent -> env: action\n",
    "        act = lambda state, eps: net(torch.tensor(state)).argmax().item() if eps < random.random() else random.choice([0,1,2,3])\n",
    "        action = act(current_state,eps)\n",
    "        action_history.append(action)\n",
    "        \n",
    "        ### env -> agent: reward,next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state_history.append(next_state)\n",
    "        reward_history.append(reward)\n",
    "        terminated_history.append(terminated)\n",
    "\n",
    "        ### agent: update ($q$-value generating) network\n",
    "        qvalue_current_state = net(torch.tensor(current_state))[action]\n",
    "        qvalue_current_state_history.append(qvalue_current_state)\n",
    "        qvalue_next_state = net(torch.tensor(next_state)).max().data\n",
    "        qvalue_next_state_history.append(qvalue_next_state)        \n",
    "\n",
    "        ### recore score \n",
    "        score = score + reward\n",
    "\n",
    "        ### check terminated         \n",
    "        if terminated:\n",
    "            playtimes.append(t)\n",
    "            scores.append(score) \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b035bc-6384-497f-ae20-ac2262fadefa",
   "metadata": {},
   "source": [
    "(loss계산을 위한 테스트코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f3ce81f-b748-4013-aa91-b79c2f4678f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,len(current_state_history),size=128) \n",
    "current_state_sampled = torch.tensor(np.array(current_state_history)[idx],dtype=torch.float32)\n",
    "actions_sampled = torch.tensor(np.array(action_history)[idx],dtype=torch.int64)\n",
    "reward_sampled = torch.tensor(np.array(reward_history)[idx],dtype=torch.float32)\n",
    "next_state_sampled = torch.tensor(np.array(next_state_history)[idx],dtype=torch.float32)\n",
    "terminated_sampled = torch.tensor(np.array(terminated_history)[idx],dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54df37db-daeb-48fb-8d5b-4e9793296e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 3, 0, 2, 0, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 3, 1, 3, 3, 2, 1, 1, 1, 1, 3, 0, 0, 3,\n",
       "        0, 1, 3, 3, 2, 2, 3, 0, 0, 1, 2, 2, 1, 2, 3, 2, 3, 3, 1, 3, 1, 3, 2, 3,\n",
       "        0, 0, 0, 0, 0, 1, 1, 3, 1, 3, 0, 0, 0, 3, 3, 3, 3, 2, 2, 0, 3, 1, 3, 0,\n",
       "        3, 3, 2, 3, 1, 1, 1, 2, 1, 0, 3, 1, 0, 2, 0, 2, 1, 0, 3, 3, 3, 0, 0, 3,\n",
       "        0, 0, 3, 0, 3, 3, 1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5103e93c-03c3-4c0c-9309-0fbdb5f4eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "qvalue_current_state = net(current_state_sampled).gather(1,actions_sampled.reshape(-1,1)).reshape(-1)\n",
    "qvalue_next_state = net(next_state_sampled).max(axis=1)[0].detach()\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "loss = loss_fn(qvalue_current_state, reward_sampled + (0.99 * qvalue_next_state)*(1-terminated_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "883ac29e-3ad2-479d-aac5-856733e7feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(283.9010, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46ed2175-c2e5-41ca-93eb-060052f5e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizr.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec6ed0-28fc-4fc3-b308-f92d6053a179",
   "metadata": {},
   "source": [
    "# 최종알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5640b8d2-ad15-4e28-9652-9686b61e4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state_history = collections.deque(maxlen=500*20)\n",
    "action_history = collections.deque(maxlen=500*20)\n",
    "next_state_history = collections.deque(maxlen=500*20)\n",
    "terminated_history = collections.deque(maxlen=500*20)\n",
    "reward_history = collections.deque(maxlen=500*20)    \n",
    "def save(): \n",
    "    current_state_history.append(current_state)\n",
    "    action_history.append(action)\n",
    "    next_state_history.append(next_state)\n",
    "    reward_history.append(reward)\n",
    "    terminated_history.append(terminated)    \n",
    "def sample():\n",
    "    idx = np.random.randint(0,len(current_state_history),size=1) \n",
    "    current_state_sampled = torch.tensor(np.array(current_state_history)[idx],dtype=torch.float32)\n",
    "    actions_sampled = torch.tensor(np.array(action_history)[idx],dtype=torch.int64)\n",
    "    reward_sampled = torch.tensor(np.array(reward_history)[idx],dtype=torch.float32)\n",
    "    next_state_sampled = torch.tensor(np.array(next_state_history)[idx],dtype=torch.float32)\n",
    "    terminated_sampled = torch.tensor(np.array(terminated_history)[idx],dtype=torch.int64)\n",
    "    return current_state_sampled, actions_sampled, reward_sampled, next_state_sampled, terminated_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4795fee7-35c4-431e-a453-8bc276cea4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43052)\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8,128), # 8은 state_size\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128,64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64,32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32,4)) # 4는 action_size \n",
    "\n",
    "optimizr = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() \n",
    "act = lambda state, eps: net(torch.tensor(state)).argmax().item() if eps < random.random() else random.choice([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db4cfc57-ee5a-40a0-95ff-cb192607cf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: -262.47\tPlaytime: 68.00\tExperience: 69\n",
      "Episode 78\tAverage Score: -98.32\tPlaytime: 62.32\tExperience: 50021"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgb2/anaconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -132.78\tPlaytime: 64.05\tExperience: 6574\n",
      "Episode 120\tAverage Score: -163.03\tPlaytime: 69.24\tExperience: 8386"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m### agnet: update network\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(current_state_history) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5000\u001b[39m:         \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#### step1: prepare data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     current_state_sampled, actions_sampled, reward_sampled, next_state_sampled, terminated_sampled \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#### step2: loss \u001b[39;00m\n\u001b[1;32m     23\u001b[0m     qvalue_current_state \u001b[38;5;241m=\u001b[39m net(current_state_sampled)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m,actions_sampled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[41], line 16\u001b[0m, in \u001b[0;36msample\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m current_state_sampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(current_state_history)[idx],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     15\u001b[0m actions_sampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(action_history)[idx],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m---> 16\u001b[0m reward_sampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(reward_history)[idx],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     17\u001b[0m next_state_sampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(next_state_history)[idx],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     18\u001b[0m terminated_sampled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(terminated_history)[idx],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "playtimes = [] \n",
    "scores = [] \n",
    "eps = 1 \n",
    "for e in range(2000):\n",
    "    eps = eps*0.995\n",
    "    current_state, _ = env.reset(seed=0)\n",
    "    score = 0\n",
    "    for t in range(500):\n",
    "            \n",
    "        ### agent -> env: action\n",
    "        action = act(current_state,eps)\n",
    "        \n",
    "        ### env -> agent: reward,next_state\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ### save\n",
    "        save()\n",
    "        ### agnet: update network\n",
    "        if len(current_state_history) > 5000:         \n",
    "            #### step1: prepare data\n",
    "            current_state_sampled, actions_sampled, reward_sampled, next_state_sampled, terminated_sampled = sample()\n",
    "            #### step2: loss \n",
    "            qvalue_current_state = net(current_state_sampled).gather(1,actions_sampled.reshape(-1,1))\n",
    "            qvalue_next_state = net(next_state_sampled).max(axis=1)[0].detach()           \n",
    "            loss = loss_fn(qvalue_current_state, reward_sampled + 0.99 * qvalue_next_state * (1-terminated_sampled))\n",
    "            #### step3: backward \n",
    "            loss.backward()        \n",
    "            #### step4: update \n",
    "            optimizr.step()        \n",
    "        \n",
    "        ### recore score \n",
    "        score = score + reward\n",
    "        \n",
    "        ### check terminated \n",
    "        if terminated:\n",
    "            playtimes.append(t)\n",
    "            scores.append(score) \n",
    "            break\n",
    "        ### \n",
    "        current_state = next_state\n",
    "        \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}\\tExperience: {}'.format(e, np.mean(scores[-100:]), np.mean(playtimes[-100:]),len(current_state_history)), end=\"\")\n",
    "    if e % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tPlaytime: {:.2f}'.format(e, np.mean(scores[-100:]), np.mean(playtimes[-100:])))\n",
    "        #torch.save(agent.network.state_dict(), 'checkpoint.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
