{
 "cells": [
  {
   "cell_type": "raw",
   "id": "09b845cf-5bce-4ea5-9c92-890ac41ed9b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"**[Essays]** 강화학습\"\n",
    "author: \"신록예찬\"\n",
    "date: \"08/02/2023\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f811c2f-fda8-472e-818b-16be2dc72e69",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df6688cb-b243-4e39-80c3-3886d1481186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f59b1b-5bef-461b-a47b-0d0a36627085",
   "metadata": {},
   "source": [
    "## Game1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a599e-56d5-4f52-a952-5d1839b01fe7",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두개의 버튼이 있다. 버튼1을 누르면 1의 보상을, 버튼2를 누르면 100의 보상을 준다고 가정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d771a51a-0733-42df-bf20-b06444164b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'button1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.random.choice(['button1','button2'])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ab8e9f-5d74-4600-bbc9-64ed47fec6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'button1': \n",
    "    reward = 1 \n",
    "else:\n",
    "    reward = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb693ba-a1f0-4886-88f2-093c8c632cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "button2 100\n",
      "button1 1\n",
      "button1 1\n",
      "button2 100\n",
      "button1 1\n",
      "button1 1\n",
      "button1 1\n",
      "button1 1\n",
      "button1 1\n",
      "button2 100\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(['button1','button2'])\n",
    "    if action == 'button1': \n",
    "        reward = 1 \n",
    "    else:\n",
    "        reward = 100\n",
    "    print(action,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3b95f-1ce7-4e51-af68-4b9e48e9cb61",
   "metadata": {},
   "source": [
    "`-` 게임을 푸는 방법? 버튼2를 누른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe09f5-0e0a-45a7-98b3-83b5862d7fea",
   "metadata": {},
   "source": [
    "`-` 용어 정리\n",
    "- Agent = 버튼을 누르는 사람\n",
    "- Action = 에이전트가 할 수 있는 행동 (현재는 2개의 action이 가능) \n",
    "- Env = Agent의 action을 보고 reward를 주는 존재 \n",
    "- 게임의 종료 = 버튼을 누르면 게임이 종료\n",
    "- 게임을 푸는 방법 = reward를 최대화하는 action을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43cc037-cb08-4627-839e-d4196e3644e5",
   "metadata": {},
   "source": [
    "## Game2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c83f66-8420-4f19-af24-35b64848d103",
   "metadata": {},
   "source": [
    "`-` 문제설명: 에이전트는 현재 2의 위치에 있다. 에이전트는 (1) 정지 (2) 왼쪽으로 이동 (3) 오른쪽으로 이동 하는 3개의 행동을 할 수 있다. 에이전트가 4의 위치에 도달하면 100의 보상을 얻고 게임이 종료된다. 에이전트가 0의 위치에 도달하면 보상없이 게임이 종료된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bbbb3-580b-4174-9ad2-492db8a07b5b",
   "metadata": {},
   "source": [
    "`-` 에이전트와 환경의 상호작용 구현1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d753194b-c29d-408d-bbb4-6e82d6455be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49be0f5a-2337-454b-ae5b-973628356a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.random.choice([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13527401-e8e7-4ded-be35-f6b25c83bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n"
     ]
    }
   ],
   "source": [
    "print(state,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6c4ab7-15e9-491e-8b44-392491213600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2의 위치에 도달, 보상 200점 획득\n"
     ]
    }
   ],
   "source": [
    "for _ in range(9999): \n",
    "    if state == 2:\n",
    "        print(state)\n",
    "        reward = 200         \n",
    "        print(\"2의 위치에 도달, 보상 {}점 획득\".format(reward))\n",
    "        break \n",
    "    elif state == -2:\n",
    "        print(state)\n",
    "        reward = 0 \n",
    "        print(\"-2의 위치에 도달, 보상 {}점 획득\".format(reward))\n",
    "        break\n",
    "    else:\n",
    "        print(state,action)        \n",
    "        state = state + action \n",
    "        action = np.random.choice(['<-','.','->'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994f222-6e08-40c1-bf6b-f2ba55efa168",
   "metadata": {},
   "source": [
    "`-` 에이전트와 환경의 상호작용 구현2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982dc58a-b969-4086-8f3a-994467a0899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.spaces.Discrete(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0577c8dd-546c-47e2-b6e8-86c4e3522e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | np.integer[Any]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | np.random.Generator | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | np.integer[Any]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A space consisting of finitely many elements.\n",
       "\n",
       "This class represents a finite subset of integers, more specifically a set of the form :math:`\\{ a, a+1, \\dots, a+n-1 \\}`.\n",
       "\n",
       "Example:\n",
       "    >>> from gymnasium.spaces import Discrete\n",
       "    >>> observation_space = Discrete(2, seed=42) # {0, 1}\n",
       "    >>> observation_space.sample()\n",
       "    0\n",
       "    >>> observation_space = Discrete(3, start=-1, seed=42)  # {-1, 0, 1}\n",
       "    >>> observation_space.sample()\n",
       "    -1\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Constructor of :class:`Discrete` space.\n",
       "\n",
       "This will construct the space :math:`\\{\\text{start}, ..., \\text{start} + n - 1\\}`.\n",
       "\n",
       "Args:\n",
       "    n (int): The number of elements of this space.\n",
       "    seed: Optionally, you can use this argument to seed the RNG that is used to sample from the ``Dict`` space.\n",
       "    start (int): The smallest element of this space.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/torch/lib/python3.11/site-packages/gymnasium/spaces/discrete.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gym.spaces.Discrete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943478ae-efb4-461e-9ec8-47b48c1050bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game2(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(3,start=-1) # Acition = {-1,0,1}  \n",
    "        self.observation_space = gym.spaces.Discrete(5,start=-2) # State = {-2,-1,0,1,2} \n",
    "        self.state = 0 \n",
    "        self.t = 0\n",
    "    def step(self,action):\n",
    "        self.state = self.state + action\n",
    "        self.t = self.t + 1 \n",
    "        if self.state == 2:\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = -1\n",
    "        info = {}\n",
    "        if self.state == -2 or self.state==2: \n",
    "            done = True\n",
    "        else: \n",
    "            done = False\n",
    "        return self.state, reward, done, info\n",
    "    def render(self):\n",
    "        print('state: {}'.format(self.state))\n",
    "    def reset(self):\n",
    "        self.state = 0 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6c4fa4-f2bc-416d-9b2d-e635cfcff6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Game2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495623a-50fe-4b20-935a-a883ae87a02e",
   "metadata": {},
   "source": [
    "## Game3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94554fe-b0f9-4bc0-b7a5-dba900312a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e438abfa-ad32-45f5-b5e2-2fa92f15d656",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Game4: LunarLander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733049b-960e-4710-a34e-94f524349295",
   "metadata": {},
   "source": [
    "### 환경만들기 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603fb2a-3df7-478c-8d56-414b35d0f30c",
   "metadata": {},
   "source": [
    "`-` 환경을 만드는 방법은 아래와 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e085d50-9d39-4e75-9151-843deb6ee4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee224f31-82c1-494d-b139-1c95a0bc2633",
   "metadata": {},
   "source": [
    "`-` 환경에 대한 기본 정보를 조사하여 보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e5d1b8-e68d-41da-9c08-26530b8fdacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       " 1.       ], (8,), float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfde2681-4846-4aa0-87ce-66333cd7485a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892a6bf-a463-4c6e-9824-214e6f47be77",
   "metadata": {},
   "source": [
    "### 환경관찰 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c163f7c-a2d5-4e8c-a39d-391deb80830d",
   "metadata": {},
   "source": [
    "`-` 환경관찰 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35312d3e-1276-4895-b94a-224a0053089b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00517321,  1.3996556 ,  0.5239665 , -0.5006593 , -0.00598759,\n",
       "        -0.11868618,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fa82115-7a62-4916-b62c-76548737af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01034651,  1.3878149 ,  0.52325904, -0.5262889 , -0.01185256,\n",
       "        -0.11730985,  0.        ,  0.        ], dtype=float32),\n",
       " -1.1491504152058667,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) # state, reward, done, _ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b60a437-f0ae-467e-8b5a-765de47d5d22",
   "metadata": {},
   "source": [
    "`-` action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37dafca5-cbbc-4b88-96a7-a180f77d9023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa418a18-2081-43fc-97e2-d908b59bb80b",
   "metadata": {},
   "source": [
    "- int형으로 전달 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb595a-7d9a-4f04-a0c8-47f4db7aae71",
   "metadata": {},
   "source": [
    "`-` action -> nextstate, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23f3049d-0c1d-438f-8fcc-3854ee723c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01545191,  1.375376  ,  0.51473296, -0.5528772 , -0.01599996,\n",
       "        -0.08295576,  0.        ,  0.        ], dtype=float32),\n",
       " -0.530722855526476,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cb75d3b-016b-42ce-8497-93120341a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "env.reset()\n",
    "for _ in range(300):\n",
    "    frames.append(env.render())\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ceab5f9-1e5a-457b-8464-20c9db84d769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig,lambda i: ax.imshow(frames[::100][i]),frames=len(frames[::100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fad79d6-2d16-49d1-b425-d2f7ad74896d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x7f32e4732050>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cba676-f377-4406-a8ff-9636a932ea9d",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ee868-9b9b-4c6d-9086-b69d615526da",
   "metadata": {},
   "source": [
    "`-` 랜덤액션을 연속적으로 생성하고 그 결과를 기록해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae71b487-9754-4f06-ba45-2acb2bdefda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "next_states = []\n",
    "dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b950bc8-0ae9-461e-ba22-0e9e6139c4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1500\u001b[39m):\n\u001b[1;32m      3\u001b[0m     _action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \n\u001b[0;32m----> 4\u001b[0m     _state2, _reward, _done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(_action) \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m## save code \u001b[39;00m\n\u001b[1;32m      6\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(_state1\u001b[38;5;241m.\u001b[39mtolist()) \n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "_state1 = env.reset()\n",
    "for t in range(1500):\n",
    "    _action = env.action_space.sample() \n",
    "    _state2, _reward, _done, _ = env.step(_action) \n",
    "    ## save code \n",
    "    states.append(_state1.tolist()) \n",
    "    actions.append(_action)\n",
    "    rewards.append(_reward)\n",
    "    next_states.append(_state2.tolist())\n",
    "    dones.append(_done)\n",
    "    ## save code end \n",
    "    _state1 = _state2 \n",
    "    if _done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68420ba4-13cf-4d28-b3ea-64fe03c66d81",
   "metadata": {},
   "source": [
    "`-` 모인 히스토리를 확인해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58524369-1817-4ee5-84ae-b05dfb323b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 116, 116, 116, 116)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states), len(actions), len(next_states), len(rewards), len(dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e3b4f-c5a0-45df-b547-020958dcc342",
   "metadata": {},
   "source": [
    "### Qnetwork 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab0e14-7b6b-4ca1-b6ea-caeba1cf9578",
   "metadata": {},
   "source": [
    "`-` 네트워크의 목적: 내가 여기서 뭘 해야하는지 알려줘! = 내가 이 상태에서, 어떠한 액션을 해야하는지 알려줘 $\\to$ 8개의 상태를 입력으로 받으면 4개의 액션에 대한 좋은 정도를 숫자로 표현하는 어떠한 함수를 만들자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e77ef3-03c3-4173-a8b0-c567ce44ddf3",
   "metadata": {},
   "source": [
    "`-` net 설계 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54860fb4-9cbd-4aab-9ce7-d716d07e0e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=8, out_features=128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=128, out_features=64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=64, out_features=32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=32, out_features=4)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab62e365-d629-42c6-91e0-f492ff3a99d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1159, -0.1567, -0.1098, -0.1431],\n",
       "        [ 0.1156, -0.1568, -0.1101, -0.1433],\n",
       "        [ 0.1161, -0.1562, -0.1095, -0.1434],\n",
       "        [ 0.1152, -0.1559, -0.1102, -0.1450],\n",
       "        [ 0.1151, -0.1553, -0.1102, -0.1457],\n",
       "        [ 0.1141, -0.1548, -0.1108, -0.1468],\n",
       "        [ 0.1134, -0.1542, -0.1112, -0.1476],\n",
       "        [ 0.1133, -0.1537, -0.1111, -0.1480],\n",
       "        [ 0.1141, -0.1530, -0.1105, -0.1479],\n",
       "        [ 0.1140, -0.1525, -0.1104, -0.1483],\n",
       "        [ 0.1137, -0.1526, -0.1107, -0.1482],\n",
       "        [ 0.1143, -0.1520, -0.1102, -0.1482],\n",
       "        [ 0.1134, -0.1515, -0.1108, -0.1492],\n",
       "        [ 0.1134, -0.1510, -0.1108, -0.1497],\n",
       "        [ 0.1125, -0.1506, -0.1113, -0.1507],\n",
       "        [ 0.1118, -0.1501, -0.1118, -0.1515],\n",
       "        [ 0.1117, -0.1502, -0.1115, -0.1516],\n",
       "        [ 0.1117, -0.1495, -0.1114, -0.1520],\n",
       "        [ 0.1114, -0.1487, -0.1117, -0.1528],\n",
       "        [ 0.1115, -0.1480, -0.1116, -0.1531],\n",
       "        [ 0.1121, -0.1476, -0.1110, -0.1530],\n",
       "        [ 0.1117, -0.1468, -0.1115, -0.1537],\n",
       "        [ 0.1114, -0.1477, -0.1112, -0.1532],\n",
       "        [ 0.1111, -0.1469, -0.1117, -0.1539],\n",
       "        [ 0.1117, -0.1464, -0.1110, -0.1538],\n",
       "        [ 0.1117, -0.1459, -0.1110, -0.1540],\n",
       "        [ 0.1116, -0.1465, -0.1105, -0.1538],\n",
       "        [ 0.1115, -0.1466, -0.1104, -0.1537],\n",
       "        [ 0.1112, -0.1457, -0.1108, -0.1543],\n",
       "        [ 0.1112, -0.1451, -0.1105, -0.1545],\n",
       "        [ 0.1111, -0.1440, -0.1104, -0.1550],\n",
       "        [ 0.1112, -0.1432, -0.1100, -0.1551],\n",
       "        [ 0.1113, -0.1423, -0.1095, -0.1551],\n",
       "        [ 0.1115, -0.1409, -0.1091, -0.1559],\n",
       "        [ 0.1117, -0.1394, -0.1087, -0.1567],\n",
       "        [ 0.1117, -0.1384, -0.1079, -0.1566],\n",
       "        [ 0.1114, -0.1390, -0.1078, -0.1561],\n",
       "        [ 0.1115, -0.1376, -0.1070, -0.1567],\n",
       "        [ 0.1115, -0.1365, -0.1061, -0.1566],\n",
       "        [ 0.1116, -0.1357, -0.1052, -0.1561],\n",
       "        [ 0.1115, -0.1359, -0.1050, -0.1558],\n",
       "        [ 0.1115, -0.1348, -0.1042, -0.1559],\n",
       "        [ 0.1122, -0.1331, -0.1035, -0.1566],\n",
       "        [ 0.1117, -0.1328, -0.1027, -0.1562],\n",
       "        [ 0.1107, -0.1323, -0.1018, -0.1553],\n",
       "        [ 0.1106, -0.1332, -0.1018, -0.1549],\n",
       "        [ 0.1100, -0.1325, -0.1007, -0.1544],\n",
       "        [ 0.1106, -0.1310, -0.1002, -0.1549],\n",
       "        [ 0.1102, -0.1301, -0.0991, -0.1546],\n",
       "        [ 0.1102, -0.1299, -0.0987, -0.1545],\n",
       "        [ 0.1103, -0.1292, -0.0975, -0.1542],\n",
       "        [ 0.1110, -0.1281, -0.0974, -0.1550],\n",
       "        [ 0.1105, -0.1286, -0.0960, -0.1549],\n",
       "        [ 0.1112, -0.1276, -0.0959, -0.1558],\n",
       "        [ 0.1114, -0.1269, -0.0947, -0.1556],\n",
       "        [ 0.1118, -0.1261, -0.0940, -0.1560],\n",
       "        [ 0.1122, -0.1252, -0.0933, -0.1563],\n",
       "        [ 0.1126, -0.1244, -0.0926, -0.1567],\n",
       "        [ 0.1125, -0.1243, -0.0910, -0.1571],\n",
       "        [ 0.1122, -0.1251, -0.0904, -0.1571],\n",
       "        [ 0.1120, -0.1254, -0.0892, -0.1573],\n",
       "        [ 0.1122, -0.1246, -0.0881, -0.1573],\n",
       "        [ 0.1125, -0.1245, -0.0870, -0.1577],\n",
       "        [ 0.1130, -0.1237, -0.0871, -0.1588],\n",
       "        [ 0.1131, -0.1239, -0.0869, -0.1590],\n",
       "        [ 0.1133, -0.1236, -0.0863, -0.1599],\n",
       "        [ 0.1135, -0.1232, -0.0857, -0.1608],\n",
       "        [ 0.1138, -0.1229, -0.0850, -0.1618],\n",
       "        [ 0.1140, -0.1226, -0.0841, -0.1626],\n",
       "        [ 0.1139, -0.1232, -0.0834, -0.1631],\n",
       "        [ 0.1138, -0.1234, -0.0817, -0.1634],\n",
       "        [ 0.1146, -0.1225, -0.0817, -0.1645],\n",
       "        [ 0.1156, -0.1217, -0.0819, -0.1655],\n",
       "        [ 0.1155, -0.1217, -0.0798, -0.1668],\n",
       "        [ 0.1159, -0.1213, -0.0792, -0.1675],\n",
       "        [ 0.1165, -0.1209, -0.0798, -0.1681],\n",
       "        [ 0.1163, -0.1210, -0.0783, -0.1690],\n",
       "        [ 0.1166, -0.1208, -0.0777, -0.1695],\n",
       "        [ 0.1163, -0.1207, -0.0760, -0.1699],\n",
       "        [ 0.1162, -0.1206, -0.0745, -0.1704],\n",
       "        [ 0.1164, -0.1207, -0.0741, -0.1708],\n",
       "        [ 0.1164, -0.1204, -0.0727, -0.1713],\n",
       "        [ 0.1160, -0.1211, -0.0719, -0.1717],\n",
       "        [ 0.1158, -0.1211, -0.0706, -0.1720],\n",
       "        [ 0.1160, -0.1213, -0.0700, -0.1723],\n",
       "        [ 0.1166, -0.1211, -0.0706, -0.1727],\n",
       "        [ 0.1172, -0.1210, -0.0711, -0.1730],\n",
       "        [ 0.1171, -0.1216, -0.0696, -0.1732],\n",
       "        [ 0.1173, -0.1218, -0.0691, -0.1734],\n",
       "        [ 0.1175, -0.1220, -0.0686, -0.1734],\n",
       "        [ 0.1177, -0.1222, -0.0682, -0.1734],\n",
       "        [ 0.1176, -0.1224, -0.0669, -0.1731],\n",
       "        [ 0.1182, -0.1225, -0.0676, -0.1733],\n",
       "        [ 0.1187, -0.1225, -0.0679, -0.1734],\n",
       "        [ 0.1189, -0.1227, -0.0675, -0.1733],\n",
       "        [ 0.1193, -0.1227, -0.0672, -0.1733],\n",
       "        [ 0.1195, -0.1228, -0.0669, -0.1733],\n",
       "        [ 0.1195, -0.1230, -0.0666, -0.1733],\n",
       "        [ 0.1200, -0.1229, -0.0674, -0.1734],\n",
       "        [ 0.1197, -0.1228, -0.0667, -0.1739],\n",
       "        [ 0.1194, -0.1232, -0.0659, -0.1737],\n",
       "        [ 0.1190, -0.1236, -0.0649, -0.1739],\n",
       "        [ 0.1199, -0.1232, -0.0661, -0.1741],\n",
       "        [ 0.1205, -0.1230, -0.0663, -0.1743],\n",
       "        [ 0.1223, -0.1225, -0.0675, -0.1742],\n",
       "        [ 0.1227, -0.1224, -0.0676, -0.1740],\n",
       "        [ 0.1234, -0.1222, -0.0670, -0.1749],\n",
       "        [ 0.1242, -0.1221, -0.0671, -0.1747],\n",
       "        [ 0.1250, -0.1220, -0.0672, -0.1744],\n",
       "        [ 0.1250, -0.1225, -0.0665, -0.1742],\n",
       "        [ 0.1266, -0.1217, -0.0677, -0.1740],\n",
       "        [ 0.1282, -0.1211, -0.0676, -0.1748],\n",
       "        [ 0.1297, -0.1203, -0.0687, -0.1744],\n",
       "        [ 0.1407, -0.1144, -0.0539, -0.1827],\n",
       "        [ 0.1454, -0.1127, -0.0571, -0.1802],\n",
       "        [ 0.2541, -0.0678, -0.1365, -0.2192]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab35b300-bc5a-43fb-8eeb-7e07dd08b37d",
   "metadata": {},
   "source": [
    "### Policy 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c015ad-930c-42bb-9f07-5d9a81e561c8",
   "metadata": {},
   "source": [
    "`-` 네트워크의 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78306592-ca58-4621-8e21-1acd4941278a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0028694153297692537,\n",
       "  1.4209814071655273,\n",
       "  -0.2906474471092224,\n",
       "  0.44716283679008484,\n",
       "  0.0033316602930426598,\n",
       "  0.06583605706691742,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [-0.005824565887451172,\n",
       "  1.4311281442642212,\n",
       "  -0.29839688539505005,\n",
       "  0.4509572386741638,\n",
       "  0.006180517841130495,\n",
       "  0.05698297545313835,\n",
       "  0.0,\n",
       "  0.0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[0],states[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba397f6-33b6-429c-b718-321f69c1f765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1159, -0.1567, -0.1098, -0.1431],\n",
       "        [ 0.1156, -0.1568, -0.1101, -0.1433]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(states[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd633a7-9707-4175-965a-e65650d94958",
   "metadata": {},
   "source": [
    "- 상태0에서는 액션0이, 상태1에서도 액션0이 가장 좋다는 의미 (왜? q-value가 젤 높으니까..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d50d4-9c70-4abd-9ace-403ca0c5ce5c",
   "metadata": {},
   "source": [
    "`-` 따라서 Agent는 아래와 같이 행동해야 한다. (네트워크가 잘 학습되었다는 전제가 필요함) \n",
    "- state[0] -> action = 0 \n",
    "- state[1] -> action = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b630b0e-bf3b-48a3-bce5-7fd135237ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.1159, 0.1156], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(states[0:2])).max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cbfbbec-8253-46de-8a60-de5ae1bb8e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(states[0:2])).max(axis=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de119acb-a57d-427c-bce9-43f85b80b9b7",
   "metadata": {},
   "source": [
    "`-` 네트워크가 있으므로 이제 어떠한 state에 대해서도 뭘 해야할지 (=어떤 액션을 해야할지) 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f604754-d57a-41b4-a1cb-387cd21a4311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04434567, -0.00875275,  0.42108542,  0.08506536, -1.7435462 ,\n",
       "       -0.67104954,  1.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state1 # 어떤 state에 대해서도.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "078f79b8-a357-4bea-be91-e7213beede86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1422, -0.1106, -0.0881, -0.1557], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.tensor(_state1)) # q-value를 계산할 수 있고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7beb3de0-1ead-4e15-b15d-4ca6be971d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(torch.argmax(net(torch.tensor(_state1)))) # 그래서 다음에 우리가 어떤행동을 해야할 지 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b955b-e3e9-477a-9134-1ef987bf4e2f",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a10912-d2ce-4fca-a177-76c23a1ab623",
   "metadata": {},
   "source": [
    "`-` 네트워크를 학습시키자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c71eee5-de1f-41c9-ab1b-23a8ba923a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e0e9cba-2f10-4f55-bc0a-d20a6177725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "playtimes=[] \n",
    "eps = 1\n",
    "opt = torch.optim.Adam(net.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac5087-13fb-443b-8fe5-68e7a7fc774c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94\tScore: -117.16\tPlaytime: 999.00"
     ]
    }
   ],
   "source": [
    "for epsd in range(1,2001): # 게임 2000판 시켜줌.. \n",
    "    state1 = env.reset() # 환경리셋 + 초기화된 환경을 state라는 변수에 저장 \n",
    "    score = 0 \n",
    "    for t in range(1000): # 게임1판당 max 1000프레임만 할 수 있음\n",
    "        # (step1) Agent: action \n",
    "        if np.random.rand() < eps: \n",
    "            action = env.action_space.sample() # 랜덤액션을 뽑음 \n",
    "        else:\n",
    "            action = int(torch.argmax(net(torch.tensor(state1).to(\"cuda:0\")))) # 네트워크가 알려주는 action을 뽑음 \n",
    "        \n",
    "        # (step2) Agent -> Env // Env -> Agent \n",
    "        state2, reward, done, _ = env.step(action) # 액션을 환경에 전달 -> (next_state, reward, done) 을 받음 \n",
    "        \n",
    "        # (step3) Agnet: save data and learn \n",
    "        ## save data \n",
    "        states.append(state1.tolist())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        next_states.append(state2.tolist())\n",
    "        dones.append(done)\n",
    "    \n",
    "        ## 최근 5000개의 자료만 준비함. \n",
    "        if len(states)>5000:\n",
    "            _states = torch.tensor(states[-5000:])\n",
    "            _actions = torch.tensor(actions[-5000:]).reshape(-1,1)\n",
    "            _next_states = torch.tensor(next_states[-5000:])\n",
    "            _rewards = torch.tensor(rewards[-5000:]).reshape(-1,1)\n",
    "            _dones = torch.tensor(dones[-5000:]).to(torch.float).reshape(-1,1) \n",
    "        else:\n",
    "            _states = torch.tensor(states)\n",
    "            _actions = torch.tensor(actions).reshape(-1,1)\n",
    "            _next_states = torch.tensor(next_states)\n",
    "            _rewards = torch.tensor(rewards).reshape(-1,1)\n",
    "            _dones = torch.tensor(dones).to(torch.float).reshape(-1,1)\n",
    "\n",
    "        ## 최근 5000개의 자료에서 128개를 임의로 추출함. \n",
    "        _n = len(_states)\n",
    "        _index = np.random.choice(_n,128) # 128 is batch_size \n",
    "        _states = _states[_index]\n",
    "        _actions = _actions[_index]\n",
    "        _next_states = _next_states[_index]\n",
    "        _rewards = _rewards[_index]\n",
    "        _dones = _dones[_index]\n",
    "        \n",
    "        ## GPU로 이동 \n",
    "        _states = _states.to(\"cuda:0\")\n",
    "        _actions = _actions.to(\"cuda:0\")\n",
    "        _next_states = _next_states.to(\"cuda:0\")\n",
    "        _rewards = _rewards.to(\"cuda:0\")\n",
    "        _dones = _dones.to(\"cuda:0\")\n",
    "        \n",
    "        ## leanrn with pytorch \n",
    "        yhat = net(_states).gather(1,_actions) ## (s,a) -> q(s,a) // 내가 현재상태 state에서, 현재 action을 하여 얻을 것이라 예상하는 보상 (net가 알려주는) \n",
    "        y = _rewards + 0.99 * net(_next_states).detach().max(1)[0].reshape(-1,1)*(1-_dones) ## 그런데 실제로는 이게 맞다고 봐야지~ \n",
    "        loss = torch.mean((y-yhat)**2)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # (step4) Agent: prepare next steps \n",
    "        state1 = state2  \n",
    "        eps = max(0.05, 0.99*eps) \n",
    "        score += reward\n",
    "        \n",
    "        # terminate \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            playtimes.append(t)\n",
    "            break\n",
    "            \n",
    "    print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, scores[-1],playtimes[-1]), end=\"\")\n",
    "    if epsd % 100 == 0:\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}\\tPlaytime: {:.2f}'.format(epsd, np.mean(scores[-100:]),np.mean(playtimes[-100])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
