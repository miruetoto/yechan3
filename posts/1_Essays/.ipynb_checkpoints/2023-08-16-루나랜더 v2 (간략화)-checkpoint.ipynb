{
 "cells": [
  {
   "cell_type": "raw",
   "id": "810cdbf2-2618-4386-a6a9-0e87ab8670ac",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"**[Essays]** 루나랜더 v2 (간략화)\"\n",
    "author: \"신록예찬\"\n",
    "date: \"08/16/2023\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b9712-ddc7-4113-a953-ba9469d52545",
   "metadata": {},
   "source": [
    "# imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1236c41-f8e1-4f62-bb06-9d1a67c88159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import torch \n",
    "import collections\n",
    "import IPython\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c065e0-1deb-4af8-95dc-273f83ca36bc",
   "metadata": {},
   "source": [
    "# 클래스선언 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49a25ee-704b-4740-bebd-7094be91b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,env):\n",
    "        self.buffer_size = 10000 \n",
    "        self.batch_size = 128\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.eps = 0\n",
    "        self.n_experiences = 0\n",
    "        self.n_epoch = 0\n",
    "        self.scores = [] \n",
    "        self.playtimes = []\n",
    "\n",
    "        # Q-Network\n",
    "        self.q_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.state_size,128), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128,64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,self.action_size)) \n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n",
    "\n",
    "        # ReplayBuffer\n",
    "        self.memory = collections.deque(maxlen=self.buffer_size)\n",
    "        self.current_states = collections.deque(maxlen=self.buffer_size)\n",
    "        self.actions = collections.deque(maxlen=self.buffer_size)\n",
    "        self.rewards = collections.deque(maxlen=self.buffer_size)\n",
    "        self.next_states = collections.deque(maxlen=self.buffer_size)\n",
    "        self.terminations = collections.deque(maxlen=self.buffer_size)\n",
    "\n",
    "    # agent >> env \n",
    "    def __rshift__(self,env):\n",
    "        if np.random.rand() > self.eps:\n",
    "            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n",
    "        else:\n",
    "            self.action = random.choice(np.arange(self.action_size))\n",
    "        env.received_action = self.action \n",
    "\n",
    "    # agent << env \n",
    "    def __lshift__(self,env): \n",
    "        self.next_state, self.reward, self.terminated, _, _ = env.step(env.received_action)\n",
    "        \n",
    "    def save_experience(self):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.current_states.append(self.current_state)\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(self.next_state)\n",
    "        self.terminations.append(self.terminated) \n",
    "        self.n_experiences = len(self.current_states)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        idx = np.random.randint(0,self.n_experiences,size=self.batch_size) \n",
    "        self.current_states_batch = torch.tensor(np.array(self.current_states)[idx],dtype=torch.float32)\n",
    "        self.actions_batch = torch.tensor(np.array(self.actions)[idx],dtype=torch.int64).reshape(self.batch_size,1) \n",
    "        self.rewards_batch = torch.tensor(np.array(self.rewards)[idx],dtype=torch.float32).reshape(self.batch_size,-1) \n",
    "        self.next_states_batch = torch.tensor(np.array(self.next_states)[idx],dtype=torch.float32)\n",
    "        self.terminations_batch = torch.tensor(np.array(self.terminations)[idx],dtype=torch.int64).reshape(self.batch_size,-1) \n",
    "\n",
    "    def learn(self):\n",
    "        if self.n_experiences < self.batch_size:\n",
    "            pass\n",
    "        else: \n",
    "            self.get_batch()\n",
    "            \n",
    "            q_targets_next = self.q_net(self.next_states_batch).detach().max(1)[0].unsqueeze(1)\n",
    "            q_targets = self.rewards_batch + 0.99 * q_targets_next * (1 - self.terminations_batch)\n",
    "            q_expected = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n",
    "            \n",
    "            loss = torch.nn.functional.mse_loss(q_expected, q_targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    # agent**env: reset \n",
    "    def __pow__(self,env): \n",
    "        self.current_state, _ = env.reset()\n",
    "        self.terminated = False\n",
    "        self.score = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3260e4-5661-4092-863a-c38c90207887",
   "metadata": {},
   "source": [
    "# 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c830a694-474a-4a1c-ae53-b7641a1d8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49689c0-a310-40a9-8332-32b4c3e6e710",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b38d10-b00e-4388-8961-88b703c1d1d3",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c0219-24e5-4248-b556-ec337c3e2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: -111.70\t\n",
      "Episode 26\tAverage Score: -182.42\t"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "max_t = 500\n",
    "agent.eps = 1.0 \n",
    "for e in range(2000):\n",
    "    ### 1. 본질적인 코드\n",
    "    agent ** env \n",
    "    for t in range(500):\n",
    "        # step1: \n",
    "        agent >> env \n",
    "        # step2: \n",
    "        agent << env \n",
    "        # step3: 데이터저장 및 학습\n",
    "        agent.save_experience()\n",
    "        agent.learn()\n",
    "        # step4: 다음 iteration 준비 + 종료조건체크\n",
    "        agent.current_state = agent.next_state \n",
    "        agent.score += agent.reward\n",
    "        if agent.terminated: break \n",
    "    agent.scores.append(agent.score) \n",
    "    agent.eps = agent.eps * 0.995 \n",
    "    ### 2. 비본질적 코드\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\t'.format(e, np.mean(agent.scores[-100:])), end=\"\")\n",
    "    if e % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(agent.scores[-100:])))\n",
    "        torch.save(agent.q_net.state_dict(), 'checkpoint.pth')\n",
    "    if np.mean(agent.scores[-100:])>=200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(agent.scores[-100:])))\n",
    "        torch.save(agent.q_net.state_dict(), 'checkpoint.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947da12d-25bb-4344-93ba-74ae6128b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent.scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167a4fc-eacc-4d38-a02a-2eb4c16d1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2',render_mode='rgb_array')\n",
    "agent = Agent(env)\n",
    "agent ** env\n",
    "agent.net.load_state_dict(torch.load('checkpoint.pth'))\n",
    "agent.terminated = False\n",
    "figs = [] \n",
    "\n",
    "while not agent.terminated:\n",
    "    figs.append(env.render())\n",
    "    agent >> env \n",
    "    agent << env \n",
    "    agent.current_state = agent.next_state \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da118e-a108-43ea-9b04-a2ecdb2f46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig,lambda i: ax.imshow(figs[::10][i]),frames=len(figs[::10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f680e-39b0-4a76-8d3e-4c98b861ce53",
   "metadata": {},
   "source": [
    "`-` 방법1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6cea37-ee98-4f62-81d2-fd3269c9633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ani.save('LunarLander-v2.mp4', writer='ffmpeg', fps=15, extra_args=['-vcodec', 'mpeg4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bbe53-2eb9-40e0-884f-77c292541130",
   "metadata": {},
   "source": [
    "`-` 방법2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bb991-4268-43ae-bc96-724027a32fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
